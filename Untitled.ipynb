{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: COVID-19 in Senegal Live application \n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "Due to the lack of regularly updated database of COVID-19 cases in Senegal, I decided to build an open database and a web application to display this information. \n",
      "\n",
      "I am updating daily (and manually) this database using the official communications of the \"Ministère de la Santé et de l'Action Sociale du Sénégal\". The web application is built using Streamlit, Altair, Bokeh and Plotly.\n",
      "\n",
      "The tool can be accessed [here](https://covid-sn.onrender.com/): [https://covid-sn.onrender.com/](https://covid-sn.onrender.com/)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/covid.png)\n",
      "\n",
      "The Github repository with the daily updated database can be found here:\n",
      "\n",
      "<div class=\"github-card\" data-github=\"maelfabien/COVID-19-Senegal\" data-width=\"100%\" data-height=\"\" data-theme=\"default\"></div>\n",
      "<script src=\"//cdn.jsdelivr.net/github-cards/latest/widget.js\"></script>\n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "published: false\n",
      "title: Introduction to Graph Database\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Neo4J\"\n",
      "---\n",
      "\n",
      "Neo4J is a NoSQL is a graph-oriented database management system. It is originally developed in Java and accessible across all platforms. In this article, we'll study the basic building blocks of Neo4J. \n",
      "\n",
      "# Split the data sets\n",
      "\n",
      "Suppose we're building a pretty easy request in which we would like to know the number of articles per day, language and country in which the event took place.\n",
      "\n",
      "Mentions contains for a given EventID the language in which the article was written.\n",
      "```scala\n",
      "val mentions_1 = mentionsDF.withColumn(\"_tmp\", $\"value\").select(\n",
      "    $\"_tmp\".getItem(0).as(\"globaleventid\"),\n",
      "    $\"_tmp\".getItem(14).as(\"language\")\n",
      "    )\n",
      "```\n",
      "\n",
      "The country of the event, as well as the day of the event, are in the Export table. Instead of selecting all columns (more than 50), we'll focus on some specific ones :\n",
      "\n",
      "```scala\n",
      "val events_1 = exportDF.withColumn(\"_tmp\", $\"value\").select(\n",
      "    $\"_tmp\".getItem(0).as(\"globaleventid\"),\n",
      "    $\"_tmp\".getItem(1).as(\"day\"),\n",
      "    $\"_tmp\".getItem(33).as(\"numarticles\"),\n",
      "    $\"_tmp\".getItem(53).as(\"actioncountry\")\n",
      "    )\n",
      "```\n",
      "\n",
      "We can also replicate those steps for the translated data :\n",
      "\n",
      "```scala\n",
      "val mentions_trans_1 = mentionsDF_trans.withColumn(\"_tmp\", $\"value\").select(\n",
      "    $\"_tmp\".getItem(0).as(\"globaleventid\"),\n",
      "    $\"_tmp\".getItem(14).as(\"language\")\n",
      "    )\n",
      "val events_trans_1 = exportDF_trans.withColumn(\"_tmp\", $\"value\").select(\n",
      "    $\"_tmp\".getItem(0).as(\"globaleventid\"),\n",
      "    $\"_tmp\".getItem(1).as(\"day\"),\n",
      "    $\"_tmp\".getItem(33).as(\"numarticles\"),\n",
      "    $\"_tmp\".getItem(53).as(\"actioncountry\")\n",
      "    )\n",
      "```\n",
      "\n",
      "# Join the tables\n",
      "\n",
      "Once we selected the essential columns of both tables, we can join the tables :\n",
      "```scala\n",
      "val df_events_1 = events_1.union(events_trans_1)\n",
      "val df_mentions_1 = mentions_1.union(mentions_trans_1)\n",
      "\n",
      "// Join events and mentions\n",
      "val df_1 = df_mentions_1.join(df_events_1,\"GlobalEventID\")\n",
      "```\n",
      "\n",
      "# Build the Cassandra Table\n",
      "\n",
      "Start `cqlsh` from the terminal of your instance and create a table to welcome the data :\n",
      "```SQL\n",
      "CREATE TABLE q1_1(\n",
      "day int,\n",
      "language text,\n",
      "actioncountry text,\n",
      "numarticles int,\n",
      "PRIMARY KEY (day, language, actioncountry));\n",
      "```\n",
      "\n",
      "Make sure to have the name for the fields, and no capital letters. It happened to cause some troubles in our project.\n",
      "\n",
      "# Write the data in Cassandra\n",
      "\n",
      "Once the data set has been created, since Scala Spark is a lazy evaluation framework, we have to compute the data set and load the data into Cassandra at the same time :\n",
      "\n",
      "```scala\n",
      "df_1.write.cassandraFormat(\"q1_1\", \"gdelt_datas\").save()\n",
      "val df_1_1 = spark.read.cassandraFormat(\"q1_1\", \"gdelt_datas\").load()\n",
      "df_1_1.createOrReplaceTempView(\"q1_1\")\n",
      "```\n",
      "\n",
      "It might take some time (several minutes). Once done, all your data for this specific query is in Cassandra!\n",
      "\n",
      "# Query Cassandra Tables\n",
      "\n",
      "Since we prepared the data to fit the queries, our queries are really simple to make in Zeppelin :\n",
      "```z.show(spark.sql(\"\"\" SELECT * FROM q1_1 ORDER BY NumArticles DESC LIMIT 10 \"\"\"))```\n",
      "\n",
      "The results will be displayed directly in Zeppelin :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/q1_2.png)\n",
      "\n",
      "> **Conclusion **: This project is now over! We have loaded several GB of zipped files in S3, built a resilient architecture using AWS, Cassandra and ZooKeeper, and finally manipulated and transferred the data to make fast, simple queries on large data sets.\n",
      "---\n",
      "title: Introduction to Continuous Signal Processing\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Signal Processing\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "I am relying on the excellent series on Youtube by Iman: Signal Processing 101 for this series of articles.\n",
      "\n",
      "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tPVduVtOJac\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
      "\n",
      "<br> \n",
      "\n",
      "Note: This series focuses on continuous signal processing. I am also working on a digital/discrete signal processing series.\n",
      "\n",
      "# 1. What is signal processing?\n",
      "\n",
      "> A signal is a time-varying physical process. \n",
      "\n",
      "Signals can be :\n",
      "- voice\n",
      "- videos or images\n",
      "- temperature records\n",
      "- stock prices\n",
      "- health records\n",
      "- ...\n",
      "\n",
      "The act of processing a signal using a system is called signal processing. A signal contains information. A system processes this information.\n",
      "\n",
      "Signal processing is implied when you:\n",
      "- make a phone call\n",
      "- use a voice assistance\n",
      "- listen to the radio\n",
      "- edit a picture\n",
      "- ...\n",
      "\n",
      "# 2. Signal representation\n",
      "\n",
      "Signal can be :\n",
      "- 1-dimensional : On a voice record for example, each point can be represented on a value vs. time plot. If you know the time, you can retrieve the value.\n",
      "- 2-dimensional: An image is 2 dimensional, since you need both x and y to characterize the value of a pixel on an image\n",
      "- 3-dimensional: A video is made of a sequence of images. You need x, y and the index of the image `t` to know the value of a pixel.\n",
      "\n",
      "This is called signal representation. We will however focus on 1-dimensional signals.\n",
      "\n",
      "# 3. Discrete vs. Continuous\n",
      "\n",
      "Signal processing is divided in 2 categories:\n",
      "- **continuous/analog** signal processing : a signal continuous in time taking continuous range of amplitude values, defined for all times.\n",
      "- **discrete/digital** signal processing : a discrete signal for which we only know values of the signal at discrete points in time.\n",
      "\n",
      "A 1-dimensional **continous** signal could for example be:\n",
      "\n",
      "$$ x(t) = sin(2 \\pi f_o t) $$\n",
      "\n",
      "Where:\n",
      "- `t` represents the time\n",
      "- $$ f_o $$ represents the frequency, in Hz (number of cycles per second)\n",
      "- $$ 2 \\pi f_o t $$ is an angle measured in radians\n",
      "\n",
      "A **discrete-time** signal quantizes the time and the signal amplitude. We might have a discrete signal taking the following form:\n",
      "\n",
      "```\n",
      "(0, 0)\n",
      "(1, 0.31)\n",
      "(2, 0.59)\n",
      "(3, 0.81)\n",
      "...\n",
      "```\n",
      "\n",
      "Where the first value is the index, and the second is the value. The discrete signal can be represented as :\n",
      "\n",
      "$$ x(n) = sin(2 \\pi f_o n t_s) $$\n",
      "\n",
      "Where:\n",
      "- n is the index (0, 1, 2, 3...)\n",
      "- $$ t_s $$ is the time sample, i.e. the time between 2 indexs\n",
      "\n",
      "Continuous and discrete signals can be represented as such:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/audio_5.png)\n",
      "\n",
      "# 4. Signal transformation\n",
      "\n",
      "## 4.a. Continuous\n",
      "\n",
      "We can apply transformations to a continuous signal $$ x(t) $$. For example:\n",
      "- $$ x(2t) $$ : doubles the speed of a signal. We talk about signal compressing in time direction.\n",
      "- $$ x(t/2) $$ : reduces by 2 the speed of a signal. We talk about signal expansion in time direction.\n",
      "- $$ x(-t) $$ : time reversal transformation.\n",
      "- $$ x(t+2) $$ : time shifting transformation to the left. We play the signal two units sooner.\n",
      "- $$ x(t-2) $$ : time shifting transformation to the right. We play the signal two units later.\n",
      "\n",
      "Transformations applied in the parenthesis usually affect the time direction. However, transformations applied outside the parenthesis affect the value axis:\n",
      "- $$ 2x(t) $$ : magnify the amplitude. We multiply the amplitude by two. If the value of 2 is smaller than 1, we say that we compress the signal in value direction.\n",
      "- $$ x(t) + 2 $$ : shift the whole signal up by 1 unit.\n",
      "\n",
      "To summarize, we can apply the following transformations:\n",
      "- time shifting\n",
      "- time scaling\n",
      "- amplitude shifting\n",
      "- amplitude scaling\n",
      "\n",
      "## 4.b. Discrete\n",
      "\n",
      "A Discrete System is any software operating on a discrete-time signal sequence, and producing a discrete output sequence using a transformation. A simple Discrete System can be defined by a *difference equation*:\n",
      "\n",
      "$$ y(n) = 2 x(n) - 1 $$\n",
      "\n",
      "The same concepts as for continuous signals regarding the transformations apply.\n",
      "\n",
      "> For what comes next, we will focus exclusively on continuous signals.\n",
      "\n",
      "# 5. Signal properties\n",
      "\n",
      "## Even signals\n",
      "\n",
      "> If $$ x(t) = x(-t) $$ this means that the signal is symmetric around the x-axis. \n",
      "\n",
      "We talk about even signals.\n",
      "\n",
      "## Odd signals\n",
      "\n",
      "> If $$ x(t) = -x(-t) $$, we reflect the signal with respect to the origin and the signal is odd.\n",
      "\n",
      "To test if a function is even or odd or neither, just replace t by -t in the expression and check for the equality.\n",
      "\n",
      "## Periodic\n",
      "\n",
      "> A signal is periodic if the same pattern repeats for ever.\n",
      "\n",
      "Some common periodic functions are:\n",
      "- $$ sin(a t) $$\n",
      "- $$ cos(a t) $$\n",
      "- $$ e^{j a t} $$\n",
      "\n",
      "The fundamental period for these signals is $$ \\tau = \\frac{2 \\pi}{a} $$.\n",
      "\n",
      "\n",
      "---\n",
      "title: Interactive Map with D3.js\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "I developped an interactive D3.js plot of the population density of France. The tool highlights dense regions of France, and has a zoom feature.\n",
      "\n",
      "A demonstration hosted on my site can be found [here](https://maelfabien.github.io/viz).\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/map_d3.jpg)\n",
      "\n",
      "---\n",
      "title: Predicting the song of the year 2019 (2/3)\n",
      "layout: post\n",
      "subtitle : \"Hackathon\"\n",
      "---\n",
      "\n",
      "So far, we identified the countries you should focus on at the release of your next hit, the features you should consider, the nationality that could help your song hit the top charts, and the kind of music you should play. \n",
      "\n",
      "In this article, we are going to focus mainly on the lyrics you should write.\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "## The data\n",
      "\n",
      "Our first step was to fetch the lyrics of all top 200 songs of the Spotify charts. To do so, we use the Genius.com API. We found a great library that does not require a Redirect URI called: `lyricsgenius`.\n",
      "\n",
      "The tracks we want to dowload the datas from are the following :\n",
      "\n",
      "```python\n",
      "tracks = pd.read_csv('Tracks.csv')\n",
      "tracks = tracks.sort_values('Streams', ascending = False)\n",
      "tracks.head()\n",
      "```\n",
      "![image](https://maelfabien.github.io/assets/images/tracks.jpg){:height=\"100%\" width=\"100%\"}\n",
      "\n",
      "Then, using `lyricsgenius` :\n",
      "```python\n",
      "api = genius.Genius('VGxZYl4kHnoBcj_hMiUA0DtweOQvySa8c7hi_fvyqbKd__3or_Lkn75yCG6_immb')\n",
      "\n",
      "i = 0\n",
      "for track in zip(tracks['Track Name'], tracks['Artist']) :\n",
      "    try :\n",
      "        song = api.search_song(str(track[0]), str(track[1]))\n",
      "        song.save_lyrics('***/Hackathon/New_songs/' + str(track[1] + str(i)))\n",
      "    except : \n",
      "        pass\n",
      "    i = i + 1\n",
      "```\n",
      "This will write the songs lyrics in separate .txt files in a new folder. To treat them at a single file :\n",
      "\n",
      "```python\n",
      "files = sorted(glob(op.join('***/Hackathon/New_songs/', '*.txt')))\n",
      "songs = [open(f).read() for f in files]\n",
      "```\n",
      "We proceed to a quick text cleaning to remove line jumps, backslashes and expressions in brackets :\n",
      "```python\n",
      "for i in range(0, len(songs)) :\n",
      "    songs[i] = songs[i].replace(\"\\n\", \" \").replace(\"\\'\", \" \")\n",
      "    songs[i] = re.sub(r\"\\[(.*?)\\]\", \" \", songs[i])\n",
      "```\n",
      "\n",
      "Then we remove stop words :\n",
      "```python \n",
      "cachedStopWords = stopwords.words(\"english\")\n",
      "\n",
      "words = []\n",
      "filtered = []\n",
      "\n",
      "for i in range(0, len(songs)) :\n",
      "    words.append(re.split(\"(?:(?:[^a-zA-Z]+')|(?:'[^a-zA-Z]+))|(?:[^a-zA-Z']+)\", songs[i]))\n",
      "    filtered.append(' '.join([word for word in songs[i].split() if word not in cachedStopWords]))\n",
      "```\n",
      "The `words` variable is a list of list that contains all individual words of each song. We made here a pretty big assumption that all the lyrics are in english. \n",
      "\n",
      "Then, the variable we are interested in are typically:\n",
      "- how many words are used on average?\n",
      "- how many times is each word repeated on average?\n",
      "- and how many words are left when we remove stop words and repeated words?\n",
      "\n",
      "```python\n",
      "voc = []\n",
      "voc_unique = []\n",
      "\n",
      "for i in range(0, len(songs)) :\n",
      "    voc.append(len(words[i]))\n",
      "    voc_unique.append(len(filtered[i].split()))\n",
      "```\n",
      "\n",
      "## How many words on average?\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,5))\n",
      "plt.hist(np.array(voc), bins=40)\n",
      "plt.title('Number of words in a top 2018 song')\n",
      "plt.show()\n",
      "```\n",
      "![image](https://maelfabien.github.io/assets/images/words1.jpg){:height=\"100%\" width=\"100%\"}\n",
      "\n",
      "On average, a hit song is 490 words long.\n",
      "\n",
      "## How many time should you repeat each word?\n",
      "\n",
      "```python\n",
      "length = [len(set(word)) for word in words]\n",
      "round(np.array(length).mean(),2)\n",
      "```\n",
      "On average, only 161 distinct words are used, which means an average of each words being repeated 3 times.\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,5))\n",
      "plt.hist(np.array(length), bins=40)\n",
      "plt.title('Number of unique words in a top 2018 song')\n",
      "plt.show()\n",
      "```\n",
      "![image](https://maelfabien.github.io/assets/images/words2.jpg){:height=\"100%\" width=\"100%\"}\n",
      "\n",
      "## And if we remove stop words?\n",
      "\n",
      "Removing stop words dramatically reduces the number of words left. \n",
      "\n",
      "```python\n",
      "length_fil = [len(set(word)) for word in filtered]\n",
      "round(np.array(length_fil).mean(),2)\n",
      "```\n",
      "Only 45.5 words left on average. \n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,5))\n",
      "plt.hist(np.array(length_fil), bins=40)\n",
      "plt.title('Number of unique words in a top 2018 song, without stopwords')\n",
      "plt.show()\n",
      "```\n",
      "![image](https://maelfabien.github.io/assets/images/words3.jpg){:height=\"100%\" width=\"100%\"}\n",
      "\n",
      "## What words should you use?\n",
      "\n",
      "For a graphical view on this question, we decided to use a WordCloud representation.\n",
      "\n",
      "```python\n",
      "word_cloud = list(itertools.chain.from_iterable(words))\n",
      "\n",
      "str1 = ' '.join(word_cloud)\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(str(str1))\n",
      "\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.imshow(wordcloud, interpolation='bilinear')\n",
      "plt.axis(\"off\")\n",
      "plt.show()\n",
      "```\n",
      "![image](https://maelfabien.github.io/assets/images/wordcould.jpg){:height=\"100%\" width=\"100%\"}\n",
      "\n",
      "## Should the lyrics be positive?\n",
      "\n",
      "For this question, we use an NLTK pre-trained model on the IMDb dataset. This pre-trained model is trained to identify positive and negative comments for movies reviews. \n",
      "\n",
      "```python\n",
      "filename = 'model_sentiment_analysis.sav'\n",
      "loaded_model = pickle.load(open(filename, 'rb'))\n",
      "\n",
      "result = loaded_model.predict(songs)\n",
      "print(result.mean())\n",
      "```\n",
      "The average positivity of the lyrics is 0.325. On average, among our data set, only 32% of the songs express a positive feeling. \n",
      "\n",
      "## Going deeper in the feelings...\n",
      "\n",
      "We also used another pre-trained model on this one to detect a few sentiments: Angry, Sad, Happy and Relax.\n",
      "\n",
      "```python\n",
      "filename = 'sentiment_model.sav'\n",
      "loaded_model = pickle.load(open(filename, 'rb'))\n",
      "result = loaded_model.predict(songs)\n",
      "unique, counts = np.unique(result, return_counts=True)\n",
      "```\n",
      "Overall, Relax is the feeling that came out as the most frequent, followed by Angry. \n",
      "\n",
      "> **Conclusion **: In the next (and last !) article, we will focus on the musical features on the 2019 hit song!\n",
      "---\n",
      "title: Interpretability and explainability (2/2)\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Better ML\"\n",
      "search: false\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "In previous blog posts [\"Complexity vs. explainability\"](https://www.explorium.ai/complexity-vs-explainability/) and \"Interpretability and explainability (1/2)\", we highlighted the tradeoff between increasing the model's complexity and loosing explainability, and the importance of interpretable models. In this article, we will finish the discussion and cover the notion of explainability in machine learning.\n",
      "\n",
      "As previously, we will use the [UCI Machine learning repository Breast Cancer](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29) data set. It is also available on [Kaggle](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data/downloads/breast-cancer-wisconsin-data.zip/2). Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. There are 30 features, including the radius of the tumor, the texture, the perimeter... Our task will be to perform a binary classification of the tumor, that is either malignant (M) or benign (B). \n",
      "\n",
      "Start off by importing the packages :\n",
      "\n",
      "```python\n",
      "# Handle data and plot\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Interpretable models\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "import statsmodels.api as sm\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.tree import export_graphviz\n",
      "import graphviz\n",
      "\n",
      "# Explainable models\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "```\n",
      "\n",
      "Then, read the data and apply a simply numeric transformation of the label (\"M\" or \"B\").\n",
      "\n",
      "```python\n",
      "df = pd.read_csv('data.csv').drop(['id', 'Unnamed: 32'], axis=1)\n",
      "\n",
      "def to_category(diag):\n",
      "if diag == \"M\" :\n",
      "return 1\n",
      "else :\n",
      "return 0\n",
      "\n",
      "df['diagnosis'] = df['diagnosis'].apply(lambda x : to_category(x))\n",
      "df.head()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/df_head.jpg)\n",
      "\n",
      "```python\n",
      "X = df.drop(['diagnosis'], axis=1)\n",
      "y = df['diagnosis']\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
      "```\n",
      "\n",
      "# Model explainability\n",
      "\n",
      "If we are not constrained to interpretable models and need higher performance, we tend to use black box models such as XGBoost for example. For various reasons we might want to provide an explanation of the outcome and the internal mechanics of the model. In such case, using model explainability techniques is the right choice. \n",
      "\n",
      "Explainability is useful for :\n",
      "- establishing trust in an outcome\n",
      "- overcoming legal restrictions\n",
      "- debugging\n",
      "- ... \n",
      "\n",
      "The main questions model explainability answers are :\n",
      "- what are the most important features ?\n",
      "- how can you explain a single prediction ?\n",
      "- how can you explain the whole model ?\n",
      "\n",
      "We will explore several techniques of model explainability :\n",
      "- Feature Importance\n",
      "- Individual Conditional Expectation (ICE)\n",
      "- Partial Dependence Plots (PDP)\n",
      "- Shapley Values (SHAP Values)\n",
      "- Appriximation (Surrogate) Models\n",
      "- Local Interpretable Model-agnostic Explanations (LIME)\n",
      "\n",
      "## 1. Feature Importance\n",
      "\n",
      "What features have the biggest impact on predictions? There are many ways to compute feature importance. We will focus on **permutation importance**, which is fast to compute and widely used.\n",
      "\n",
      "Permutation importance is computed after a model has been fitted. It shows how randomly shuffling the rows of a single column of the validation data, leaving the target and all other columns in place affects the accuracy.\n",
      "\n",
      "For example, say that as before, we try to predict if a breast tumor is malignant or benign. We will randomly shuffle, column by column, the rows of the texture, the perimeter, the area, the smoothness...\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/pred_17.jpg)\n",
      "\n",
      "Randomly re-ordering a single column should decrease the accuracy. Depending on how relevant the feature is, it will more or less impact the accuracy. Let's illustrate this concept with a Random Forest Classifier.\n",
      "\n",
      "```python\n",
      "rf = RandomForestClassifier()\n",
      "rf.fit(X_train, y_train)\n",
      "```\n",
      "\n",
      "We can compute the Permutation Importance with [Eli5 library](https://eli5.readthedocs.io/en/latest/). Eli5 is a Python library which allows to visualize and debug various Machine Learning models using unified API. It has built-in support for several ML frameworks and provides a way to explain black-box models.\n",
      "\n",
      "To install `eli5` :\n",
      "\n",
      "`pip install eli5`\n",
      "\n",
      "We can then compute the permutation importance :\n",
      "\n",
      "```python\n",
      "import eli5\n",
      "from eli5.sklearn import PermutationImportance\n",
      "\n",
      "perm = PermutationImportance(rf, random_state=1).fit(X_test, y_test)\n",
      "eli5.show_weights(perm, feature_names = X_test.columns.tolist())\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/pred_6.jpg)\n",
      "\n",
      "In our example, the most important feature is `concave points_worst`. The first number in each row shows how much model performance decreased with a random shuffling (in this case, using \"accuracy\" as the performance metric). We measure the randomness by repeating the process with multiple shuffles.\n",
      "\n",
      "## 2. Individual Conditional Expectation (ICE)\n",
      "\n",
      "How does the prediction change when 1 feature changes ? Individual Conditional Expectation, as its name suggests, is a plot that shows how a change in an individual feature changes the outcome of each individual prediction (one line per prediction). It can be used for regression tasks only. Since we face a classification task, we will re-use the linear regression model fitted above, and make our classification task look like a regression one.\n",
      "\n",
      "To build ICE plots, simply use `pycebox`. Start off by installing the package : \n",
      "\n",
      "`pip install pycebox`\n",
      "\n",
      "```python\n",
      "from pycebox.ice import ice, ice_plot\n",
      "\n",
      "ice_radius = ice(data=X_train, column='radius_mean', predict=model.predict)\n",
      "ice_concave = ice(data=X_train, column='concave points_worst', predict=model.predict)\n",
      "ice_smooth = ice(data=X_train, column='smoothness_se', predict=model.predict)\n",
      "```\n",
      "\n",
      "And build the plots :\n",
      "\n",
      "```python\n",
      "ice_plot(ice_concave, c='dimgray', linewidth=0.3)\n",
      "plt.ylabel('Prob. Malignant')\n",
      "plt.xlabel('Worst concave points');\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/pred_7.jpg)\n",
      "\n",
      "```python\n",
      "ice_plot(ice_radius, c='dimgray', linewidth=0.3)\n",
      "plt.ylabel('Prob. Malignant')\n",
      "plt.xlabel('Radius mean');\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/pred_8.jpg)\n",
      "\n",
      "Logically, since our linear model involves a linear relation between the inputs and the output, the ICE plots are linear. However, if we use a Gradient Boosting Regressor to perform the same task, the linear relation does not hold anymore.\n",
      "\n",
      "```python\n",
      "gb = GradientBoostingRegressor()\n",
      "gb.fit(X_train, y_train)\n",
      "ice_concave = ice(data=X_train, column='concave points_worst', predict=gb.predict)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/pred_9.jpg)\n",
      "\n",
      "Thanks to ICEs, we understand the impact of a feature on the value of the outcome for each individual instance, and we easily understand trends. However, the ICE curves only display one feature at a time, and we cannot plot the joint importance of 2 features for example. Partial dependence plots overcome this issue.\n",
      "\n",
      "## 3. Partial dependence plots\n",
      "\n",
      "### 1D Partial Dependence Plot\n",
      "\n",
      "Just like ICEs, Partial Dependence Plots (PDP) show how a feature affects predictions. They are however more powerful since they can plot joint effects of 2 features on the output. \n",
      "\n",
      "Partial dependence plots are calculated after a model has been fitted. It tries to split the effect of every feature in the overall model's predictions.\n",
      "\n",
      "We start by selecting a single row. We will use the fitted model to predict the prediction of that row. But we repeatedly **alter the value** for **one variable** to make a series of predictions.\n",
      "\n",
      "For example, in the breast cancer example used above, we could predict the outcome for different values of the radius : 10, 12, 14, 16...\n",
      "\n",
      "We build the plot by:\n",
      "- representing on the x-axis the value change in the radius\n",
      "- and on the y-axis the change of the outcome\n",
      "\n",
      "We don't use only a single row, but many rows to do build this plot. The blue area corresponds to an empirical confidence interval. PDPs can be compared with ICEs for these kind of plots, but they show the average trend and confidence levels instead of individual lines. It makes trends easier to understand, although we loose the low-level vision for each prediction.\n",
      "\n",
      "We can plot the Partial Dependence Plot using [PDPbox](https://pdpbox.readthedocs.io/en/latest/). The goal of this library is to visualize the impact of certain features towards model prediction for any supervised learning algorithm using partial dependence plots. \n",
      "\n",
      "To install PDPbox : `pip install pdpbox`\n",
      "\n",
      "```python\n",
      "from pdpbox import pdp, get_dataset, info_plots\n",
      "\n",
      "pdp_rad = pdp.pdp_isolate(model=rf, dataset=X_test, model_features=X_test.columns, feature='radius_mean')\n",
      "\n",
      "pdp.pdp_plot(pdp_rad, 'Radius Mean')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/pred_10.jpg)\n",
      "\n",
      "### 2D Partial Dependence Plots\n",
      "\n",
      "We can also plot interactions between features on a 2D graph.\n",
      "\n",
      "```python\n",
      "features_to_plot = ['radius_mean', 'smoothness_se']\n",
      "\n",
      "inter1  =  pdp.pdp_interact(model=gb, dataset=X_test, model_features=X.columns, features=features_to_plot)\n",
      "\n",
      "pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour', x_quantile=True, plot_pdp=True)\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/pred_11.jpg)\n",
      "\n",
      "This plot helps to identify regions in which the tumor is more likely to be benign (darker regions) rather than malignant (lighter regions) based on the interaction between the mean of the radios and the standard error of the smoothness. We can then create similar plots for all pairs of variables.\n",
      "\n",
      "### Actual Prediction Plot\n",
      "\n",
      "Actual prediction plots show the medium value of actual predictions through different feature values for 2 predictions :\n",
      "\n",
      "```python\n",
      "fig, axes, summary_df = info_plots.actual_plot_interact(\n",
      "model=rf, X=X_train, features=features_to_plot, feature_names=features_to_plot\n",
      ")\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/pred_12.jpg)\n",
      "\n",
      "## 4. Shapley Values\n",
      "\n",
      "### Force plots\n",
      "\n",
      "We have seen so far techniques to extract general insights from a machine learning model. Shapley values are used to break down a single prediction.\n",
      "\n",
      "SHAP (SHapley Additive exPlanations) values show the impact of having a certain value for a given feature in comparison to the prediction we'd make if that feature took some baseline value.\n",
      "\n",
      "In the breast cancer example, we could wonder how much was a prediction driven by the fact that the radius was 17.1mm, instead of some baseline number? That could help a doctor explain the predictions to a patient and understand how the internal mechanics of a model lead to the given outcome.\n",
      "\n",
      "We can decompose a prediction with the following equation:\n",
      "\n",
      "`sum(SHAP values for all features) = pred_for_patient - pred_for_baseline_values`\n",
      "\n",
      "We will use the [SHAP library](https://github.com/slundberg/shap). We will look at SHAP values for a single row of the dataset (we arbitrarily chose row 5). To install the `shap` package : \n",
      "\n",
      "```python\n",
      "pip install shap\n",
      "```\n",
      "\n",
      "Then, compute the Shapley values for this row, using our random forest classifier fitted previously.\n",
      "\n",
      "```python\n",
      "import shap\n",
      "\n",
      "row = 5\n",
      "data_for_prediction = X_test.iloc[row]  # use 1 arbitrary row of data\n",
      "data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n",
      "\n",
      "explainer = shap.TreeExplainer(rf)\n",
      "shap_values = explainer.shap_values(data_for_prediction)\n",
      "```\n",
      "\n",
      "The `shap_values` is a list with two arrays. It's cumbersome to review raw arrays, but the shap package has a nice way to visualize the results.\n",
      "\n",
      "```python\n",
      "shap.initjs()\n",
      "shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/pred_13.jpg)\n",
      "\n",
      "The output prediction is 0, which means the model classifies this observation as benign.\n",
      "\n",
      "The base value is 0.3633. Feature values that push towards a malignant tumor causing are in pink, and the length of the region shows how much the feature contributes to this effect. Feature values decreasing the prediction and making our tumor benign are in blue. The biggest impact comes from `radius_worst`.\n",
      "\n",
      "If you subtract the length of the blue bars from the length of the pink bars, it equals the distance from the base value to the output.\n",
      "\n",
      "We explored so far Tree based models. `shap.DeepExplainer` works with Deep Learning models, and `shap.KernelExplainer` works with all models.\n",
      "\n",
      "### Summary plots\n",
      "\n",
      "We can also just take the mean absolute value of the SHAP values for each feature to get a standard bar plot. It produces stacked bars for multi-class outputs:\n",
      "\n",
      "```python\n",
      "shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/pred_15.jpg)\n",
      "\n",
      "## 5. Approximation (Surrogate) models\n",
      "\n",
      "Approximation models (or global surrogate) is a simple and quite efficient trick. The idea is really simple. We train an interpretable model to approach the predictions of a black-box algorithm. \n",
      "\n",
      "We keep the original data, and use as targets the predictions made by the black-box algorithm. We can use any interpretable model, and benefit from all the advantages of the model chosen.\n",
      "\n",
      "We must however pay attention to the performance of the interpretable model, since it might perform poorly in some regions.\n",
      "\n",
      "## 6. Local Interpretable Model-agnostic Explanations (LIME)\n",
      "\n",
      "Instead of training an interpretable model to approximate a black box model, LIME focuses on training local explainable models to explain individual predictions. We want the explanation to reflect the behavior of the model \"around\" the instance that we predict. This is called \"local fidelity\".\n",
      "\n",
      "LIME uses an exponential smoothing kernel to define the notion of neighborhood of an instance of interest.\n",
      "\n",
      "We first select the instance we want to explain. By making small variations in the input data to the black-box model, we generate a new training set with these samples and their predicted labels. We then train an interpretable classifier on those new samples, and weight each sample according to how \"close\" it is to the instance we want to explain.\n",
      "\n",
      "We benefit from the advantages of the interpretable model to explain each prediction.\n",
      "\n",
      "We can implement LIME algorithm in Python with LIME package :\n",
      "\n",
      "`pip install lime`\n",
      "\n",
      "Then, lime takes only numpy arrays as inputs :\n",
      "\n",
      "```python\n",
      "import lime\n",
      "import lime.lime_tabular\n",
      "\n",
      "explainer = lime.lime_tabular.LimeTabularExplainer(np.array(X_train), feature_names=np.array(X_train.columns), class_names=np.array([0, 1]), discretize_continuous=True)\n",
      "```\n",
      "\n",
      "We have defined the explainer. We can now explain an instance :\n",
      "\n",
      "```python\n",
      "i = np.random.randint(0, np.array(X_test).shape[0])\n",
      "exp = explainer.explain_instance(np.array(X_test)[i], rf.predict_proba, num_features=5, top_labels=1)\n",
      "```\n",
      "\n",
      "We make the choice to use 5 features here, but we could use more. To display the explanation :\n",
      "\n",
      "```python\n",
      "exp.show_in_notebook(show_table = True, show_all= False)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/pred_16.jpg)\n",
      "\n",
      "Since we had the `show_all` parameter set to false, only the features used in the explanation are displayed. The Feature - Value table is a summary of the instance we'd like to explain. The value column displays the original value for each feature.\n",
      "\n",
      "The prediction probabilities of the black box model are displayed on the left. \n",
      "\n",
      "The prediction of the local surrogate model stands under the 0 or the 1. Here, the local surrogate and the black box model both lead to the same output. It might happen, but it's quite rare, that the local surrogate model and the black box one do not give the same output. In the middle graph, we observe the contribution of each feature in the local interpretable surrogate model, normalized to 1. This way, we know the extent to which a given variable contributed to the prediction of the black-box model.\n",
      "\n",
      "> Machine learning explainability techniques are an opportunity to use more complex and less transparent models, that usually perform well, and maintain trust in the output of the model. \n",
      "\n",
      "If you'd like to read more on these topics, make sure to check these references :\n",
      "- [Interpretable ML Book](https://christophm.github.io/interpretable-ml-book)\n",
      "- [Kaggle Learn](https://www.kaggle.com/learn/machine-learning-explainability)\n",
      "- [Savvas Tjortjoglou's blog](http://savvastjortjoglou.com/intrepretable-machine-learning-nfl-combine.html)\n",
      "- [Zhiya Zuo's blog](https://zhiyzuo.github.io/Python-Plot-Regression-Coefficient/).\n",
      "- [Lime's documentation](https://github.com/marcotcr/lime)\n",
      "\n",
      "---\n",
      "title: Linear Discriminant Analysis (LDA), QDA\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Supervised Learning Algorithms\"\n",
      "---\n",
      "\n",
      "Linear Discriminant Analysis is a generative model for classification. It is a generalization of Fisher's linear discriminant. LDA works on continuous variables. If the classification task includes categorical variables, the equivalent technique is called the discriminant correspondence analysis.\n",
      "\n",
      "The goal of Linear Discriminant Analysis is to project the features in higher dimension space onto a lower-dimensional space to both reduce the dimension of the problem and achieve classification.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "## Key ideas\n",
      "\n",
      "- Generative Model that tries to estimate $$ P (X = x \\mid Y = 1) $$ and $$ P (X = x \\mid Y = -1) $$\n",
      "- Used for classification through dimension reduction\n",
      "- Requires continuous variables\n",
      "- Relies on normality assumption for $$ P(X \\mid Y = 1) $$ and $$ P(X \\mid Y = 0) $$\n",
      "- Requires homoscedasticity and full rank covariances\n",
      "\n",
      "## Concept\n",
      "\n",
      "Consider a simple problem in which we want to decide whether a drug should be given to patients or not. Our features will be the gene expressions, and we will have 2 labels, for patients for which the drug worked, and for which it did not.\n",
      "\n",
      "In PCA, we are interested in genes with the largest variations. In LDA, we are interested in maximizing the separability between the 2 known groups to make better decisions.\n",
      "\n",
      "We'll illustrate how we can improve the separability by this simple example. We suppose that we have 2 features, one for each gene. This means that we can plot the graph on a 2D plane.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lda_2.jpg)\n",
      "\n",
      "*How can we reduce the dimension of this problem to 1D ?*\n",
      "\n",
      "A bad way to approach this problem would be to project on the X-axis.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lda_3.jpg)\n",
      "\n",
      "This would imply losing a lot of information from the Y-axis. what LDA does is that it projects the data onto a new axis in a way to maximize the separation between the 2 categories.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lda_4.jpg)\n",
      "\n",
      "The projection, therefore, looks like this now, which is a good improvement :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lda_5.jpg)\n",
      "\n",
      "*How is this new axis created ?*\n",
      "\n",
      "- The new axis should maximize the distance between the two means :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lda_6.jpg)\n",
      "\n",
      "- The new axis should minimize the variation, i.e scatter within each category\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lda_7.jpg)\n",
      "\n",
      "Both of the criteria can be optimized using the following ratio :\n",
      "\n",
      "$$ \\frac { (\\mu_+ - \\mu_-)^2 } {( \\sigma_+ + \\sigma_-)} = \\frac { d^2 } {( \\sigma_+ + \\sigma_-)} $$\n",
      "\n",
      "The numerator should ideally be large, and the numerator should be small. If we have 3 dimensions or more, the process remains the same!\n",
      "\n",
      "If we have 3 categories, the process pretty much remains the same. We compute the distance to a central point and maximize the distance with respect to it.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lda_8.jpg)\n",
      "\n",
      "The new criteria is now :\n",
      "\n",
      "$$  \\frac { {d_{1-2}}^2 + {d_{1-3}}^2 + {d_{2-3}}^2  } {( \\sigma_1 + \\sigma_2 + \\sigma_3)} $$\n",
      "\n",
      "Another thing that changes when adding this class is that we do now project on a new place, not a single axis. This is because the 3 central points for each category define a plane. We can now build new X and Y axis, optimized for classification.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lda_9.jpg)\n",
      "\n",
      "Being able to draw the data on 2 axes is interesting if we have an initial high dimensional problem. Once we redefined our X and Y axis, it is really easy to apply fit a linear regression on top! And voilà, this is the LDA!\n",
      "\n",
      "## Theory\n",
      "\n",
      "Back to our example with 2 classes. We would like to find the two underlying marginal distributions. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lda_10.jpg)\n",
      "\n",
      "We'll define the distributions of the 2 classes :\n",
      "- $$ G = L(X \\mid Y=1) $$ distribution of the class 1\n",
      "- $$ H = L(X \\mid Y=-1) $$ distribution of the class -1\n",
      "\n",
      "We can plot the probability densities :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lda_11.jpg)\n",
      "\n",
      "The LDA relies on some strong hypothesis which we'll explicit now.\n",
      "\n",
      "### Gaussian marginal distributions \n",
      "\n",
      "- $$ G = N(\\mu_+, \\sigma_+) $$\n",
      "- $$ H = N(\\mu_-, \\sigma_-) $$\n",
      "\n",
      "where :\n",
      "\n",
      "$$ N(\\mu, \\sigma^2) = \\frac {1} {\\sqrt {2  \\pi \\sigma^2}}  e^{ \\frac {-1} {2 \\sigma^2} {(x-\\mu)^2}} $$\n",
      "\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lda_1.jpg)\n",
      "\n",
      "### Homoscedasticity\n",
      "\n",
      "LDA should be used when the covariance matrices are equal among the 2 classes :\n",
      "\n",
      "$$ \\sigma_+ = \\sigma_- = \\sigma $$\n",
      "\n",
      "## Learning process\n",
      "\n",
      "To understand the intuition behind how LDA works, we can define a likelihood ratio :\n",
      "\n",
      "$$ \\phi(X) = \\frac { \\delta G} { \\delta H} (x) = \\frac {P(X = x \\mid Y = 1)} {P(X = x \\mid Y = -1)} $$\n",
      "\n",
      "Using Bayes' theorem :\n",
      "\n",
      "$$ \\phi(X) = \\frac {P(Y = 1 \\mid X = x) \\frac {P(X=x)} {P(Y=1)}} {P(Y = -1 \\mid X = x) \\frac {P(X=x)} {P(Y=-1)}} $$\n",
      "\n",
      "$$ \\phi(X) = \\frac { \\frac { P(Y = 1 \\mid X = x) } { P(Y=1) } } { \\frac { P(Y = -1 \\mid X = x) } { P(Y=-1) } } $$\n",
      "\n",
      "We can re-define $$ P(Y=1) $$ as $$ p $$ and $$ P(Y=1 \\mid X = x) $$ as the prior probability $$ \\eta(x) $$.\n",
      "\n",
      "$$ \\phi(X) = \\frac {1-p} {p} \\frac {\\eta(x)} {1-\\eta(x)} $$\n",
      "\n",
      "We can easily isolate the prior probability $$ \\eta(x) $$ :\n",
      "\n",
      "$$ \\eta(x) = \\frac { p \\phi(x) } {(1-p) + p \\phi(x)} $$ \n",
      "\n",
      "\n",
      "## Computation\n",
      "\n",
      "How do we find the parameters of the model? How does the learning process work?\n",
      "\n",
      "$$ \\eta(x) = \\frac { e^{ ( \\frac {-1} {2} ( x - \\mu_+ )^T \\sigma^{-1} (x-\\mu_+) ) } } {e^{ ( \\frac {-1} {2} (x - \\mu_-)^T \\sigma^{-1} (x- \\mu_- ) ) } } $$\n",
      "\n",
      "$$ = e^{ ( \\frac {-1} {2} (x-\\mu_+)^T \\sigma^{-1} (x-\\mu_+) + \\frac {-1} {2} (x-\\mu_-)^T \\sigma^{-1} (x-\\mu_-) ) } $$\n",
      "\n",
      "$$ = e^{ (x^T \\sigma^{-1} {\\mu_+}^T - \\frac {1} {2} \\mu_+ \\sigma_{-1} \\mu_- - x^T \\sigma^{-1} \\mu_- + \\frac {1} {2} {\\mu_-}^T \\sigma_{-1} \\mu_- ) } $$\n",
      "\n",
      "If $$ \\eta(x) > \\frac {1} {2} $$, then $$ \\phi(x) ≥ \\frac {1-p} {p} $$ . This means that :\n",
      "\n",
      "$$ x^T \\sigma^{-1} (\\mu_+ - \\mu_-) + \\frac {1} {2} ( {\\mu_+}^T \\sigma_{-1} \\mu_- {\\mu_+}^T \\sigma^{-1} \\mu_+) ≥ log \\frac {p} {1-p} $$\n",
      "\n",
      "Which can be re-written as :\n",
      "\n",
      "$$ \\alpha + \\beta^T x ≥ 0 $$\n",
      "\n",
      "Where :\n",
      "\n",
      "$$ \\beta = \\sigma^{-1}(\\mu_+ - \\mu_-) $$\n",
      "\n",
      "$$ \\alpha = \\frac {1} {2} ({\\mu_-}^t \\sigma^{-1} \\mu_- - {\\mu_+}^t \\sigma^{-1} \\mu_+) - \\log \\frac {p} {1-p} $$\n",
      "\n",
      "This should remind you of the ratio to optimize we defined in the first part of the article! The parameter $$ \\alpha $$ and $$ \\beta $$ are the parameters of the linear regression fitted on the modified axis plane.\n",
      "\n",
      "## Parameter estimation\n",
      "\n",
      "The question now becomes : How can we estimate $$ \\alpha $$ and $$ \\beta $$ ? By Maximum Likelihood, we obtain the following :\n",
      "\n",
      "$$ \\hat{p} = \\frac { \\sum_i I_{y_i = 1} } { n} = \\frac {n_+} {n} $$\n",
      "\n",
      "$$ \\hat{\\mu_+} = \\frac { 1} {n_+} \\sum_{Y_i = 1} X_i $$\n",
      "\n",
      "$$ \\hat{\\mu_-} = \\frac {1} {n_-} \\sum_{Y_i = -1} X_i $$\n",
      "\n",
      "$$ \\hat{\\sigma} = \\frac {n_+} {n} \\hat{\\sigma_+} + \\frac {n_-} {n} \\hat{\\sigma_-} $$\n",
      "\n",
      "We then simply replace those values to find $$ \\hat{\\alpha} $$ and $$ \\hat{\\beta} $$.\n",
      "\n",
      "## Quadratic Discriminant Analysis (QDA)\n",
      "\n",
      "So far, we supposed that the class variance had to be the same : $$ \\sigma_+ = \\sigma_- = \\sigma $$. If we relax this hypothesis, we obtain the QDA, where $$ \\sigma_+ ≠ \\sigma_- $$.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lda_12.jpg)\n",
      "\n",
      "\n",
      "> **Conclusion** : I hope this introduction to LDA was helpful. Let me know in the comments if you have any question.\n",
      "---\n",
      "title: Structural Analysis of Criminal Network and Predicting Hidden Links using Machine Learning\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Criminal Networks\"\n",
      "---\n",
      "\n",
      "In this article, I will discuss and summarize the paper: [\"Structural Analysis of Criminal Network and Predicting Hidden Links using Machine Learning\"](https://arxiv.org/pdf/1507.05739.pdf) by Emrah Budur, Seungmin Lee and Vein S Kong.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# Background\n",
      "\n",
      "As in most criminal network projects, data is key. However, data is not always available, and if so, only in limited amount. Therefore, link prediction can make sense from that perspective. In this paper, authors gathered large amount of data and turned link prediction in graphs (which is a hard task) into binary classification problems (which is much easier).\n",
      "\n",
      "# Data\n",
      "\n",
      "The authors gathered a dataset of 1.5 millions nodes worldwide with 4 millions undirected edges from the Office of Foreign Asset Control. I think that they refer to a dataset available [here](https://www.treasury.gov/resource-center/sanctions/SDN-List/Pages/sdn_data.aspx).\n",
      "\n",
      "With such amount of data, the authors propose to turn a link prediction (predict links between times T and T+1), to finding hidden links by a binary classifier (at time T). There are several steps in the project:\n",
      "- train a Gradient Boosting Model (supervised learning approach) on the current data for finding hidden links\n",
      "- remove 5 to 50% of the edges to create a test set and compute the accuracy of the model\n",
      "- distroy criminal networks using Weighted Pagerank index\n",
      "\n",
      "To treat edge prediction as a classification problem, one should have features on each node and learn from them. The authors use as features:\n",
      "- the number of common neighbors\n",
      "- the jaccard index\n",
      "- the preferential attachment index\n",
      "- the hub index\n",
      "- the Adamic/Adar index\n",
      "- the Leicht Holme index\n",
      "- the Salton index\n",
      "- the Sorensen index\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph2feat.png)\n",
      "\n",
      "They build a training dataset (75%), and a test one in which they remove edges. To build the feature matrix, they compute combinations between all possible nodes. Therefore, they end up with more negative edges ($$ E_{-} $$) than positive edges ($$ E_{+} $$). To balance the training set, they apply a random undersampling on the negative edges.\n",
      "\n",
      "# Model and performance\n",
      "\n",
      "The metric chosen is the prediction imbalance:\n",
      "\n",
      "$$ Pred_{Imbalance} = \\mid err_{+} − err_{-} \\mid $$\n",
      "\n",
      "The authors also compute the Area under the curve (AUC) criteria. They gradually remove from 5% up to 50% of the edges, and the findings are quite interesting:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/res_gbm.png)\n",
      "\n",
      "The light gray line shows the performance of the model when we build a train and a test set from a corrupted network. The more edges were removed before splitting in $$ D_{train} $$ and $$ D_{test} $$, the lower the model performance. \n",
      "\n",
      "The dark line is the ability of the built model to find hidden links. And surprisingly, AUC increases with the proportion of links removed. Authors argue that it comes from the fact that the model might overfit on the structure of the graph when given too many links in training, and that removing some edges during training (typically anywhere close to 25-30%) could improve the model.\n",
      "\n",
      "# Destroying networks\n",
      "\n",
      "In the paper \"Disrupting Resilient criminal networks through data analysis\", authors identify the nodes with the highest betweenness centrality and remove them gradually in order to disrupt the network. \n",
      "\n",
      "In this paper, authors argue that removing nodes with the highest pagerank score first improved the disruption of the network.\n",
      "\n",
      "The data does not provide weights on edges. These weights are added by making the model predict the existing edges. A score closer to 1 will give a weight closer to 1, and a score closer to 0 will give a weight closer to 0.\n",
      "\n",
      "Weighted pagerank scores are then computed in the network, for each node. The index computed by the pagerank algorithm builds a \"suspiciousness index\", and more suspicious nodes should be removed first. The authors compared several node removal strategies (Unweighted pagerank, weighted pagerank, node degree, jaccard index...).\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/LCC_3.png)\n",
      "\n",
      "The removal of nodes having maximum Weighted PageRank score reduces the largest connected component (LCC) of the network much faster than removal of nodes based on any other metric, except for unweighted pagerank (no clear difference).\n",
      "\n",
      "All pagerank methods reach a bottleneck after some time, since the pagerank is computed only once, and not after each iteration. Therefore, 2 options:\n",
      "- re-compute pagerank at each step, which can get quite expensive in terms of computation\n",
      "- use the hybrid method proposed by the authors to maximize the WCC score: parameters $$ W_{hybrid} ≈ min(W_{Weighted Pagerank}, W_{Attack}) $$\n",
      "\n",
      "The first approach was not implemented by authors, but the second one seems to improve the percentage of the network removed where the pagerank reached a bottleneck.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/LCC_4.png)\n",
      "\n",
      "# Discussion\n",
      "\n",
      "The work discussed in this paper is quite interesting by the volume of the data considered. A supervised machine learning model could be trained, and undersampling seemed to help. One should try other models and other features in future works. \n",
      "\n",
      "The dataset, although massive, lacks the notion of weights and does not have a temporal notion, it's only a snapshot.\n",
      "---\n",
      "title: Kaldi for Speaker Verification\n",
      "layout: post\n",
      "tags: [signal]\n",
      "subtitle : \"Speech Processing\"\n",
      "---\n",
      "\n",
      "After the quick introduction to Kaldi, we'll move on to an example. I'm mostly reading about and working on speaker verficiation, rather than ASR so far, and I'll run a x-vector speaker verifciation example.\n",
      "\n",
      "# Speaker Verification Pipeline\n",
      "\n",
      "Go to the `voxceleb` folder, read the README, and go to v2. v1 uses GMM-UBM, i-vector and PLDA method. v2 uses DNN speaker embeddings (x-vector), which is currently SOTA, reason why we'll run it. \n",
      "\n",
      "# What's in it\n",
      "\n",
      "The v2 folder contains several folders and files:\n",
      "\n",
      "```bash\n",
      "README.txt  cmd.sh  conf  local  path.sh  run.sh  sid  steps  utils\n",
      "```\n",
      "\n",
      "Here is the organisation of a typical Kaldi `egs` directory, as well illustrated in [this](https://www.eleanorchodroff.com/tutorial/kaldi/familiarization.html) Kaldi tutorial.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/directorystructure.png)\n",
      "\n",
      "These folders contain:\n",
      "- scripts ready to launch, such as `run.sh` that launches the whole example and `path.sh` which makes sure that there is a proper configuration file\n",
      "- `cmd.sh`, a script to specify the type of computation you're choosing\n",
      "- `conf` which is a folder that contains configuration settings for MFCC feature extraction and energy-based voice activity detection (VAD), e.g setting the threshold under which you don't detect a voice\n",
      "- `local` which contains code to setup the dataset in the correct format and shape the features for the x-vector pipeline\n",
      "- `sid` is really important and contains code to compute the VAD, extract the i-vector, the x-vector, training the UBM...\n",
      "- `steps` contains lower level scripts such as feature extraction, functions to re-format the training data and other utilities\n",
      "\n",
      "The easy way out is to launch `run.sh`. This is the high-level script that runs everything mentioned. Rather than running it, we'll break it down into several steps.\n",
      "\n",
      "# The data\n",
      "\n",
      "The example does not come with data, you need to place it in the repository. TODO\n",
      "\n",
      "We are now going to break down the `run.sh` script. This script was written by Daniel Garcia-Romero, Daniel Povey, David Snyder and Ewald Enzinger (Johns Hopkins University). I am either adding comments above the code, or in-between the lines.\n",
      "\n",
      "# Set the paths\n",
      "\n",
      "The first step is to set the correct path to the training files. The Musan data are used for data augmentation in the X-vector generation.\n",
      "\n",
      "```bash\n",
      ". ./cmd.sh\n",
      ". ./path.sh\n",
      "set -e\n",
      "mfccdir=`pwd`/mfcc\n",
      "vaddir=`pwd`/mfcc\n",
      "\n",
      "# The trials file is downloaded by local/make_voxceleb1_v2.pl.\n",
      "voxceleb1_trials=data/voxceleb1_test/trials\n",
      "voxceleb1_root=/export/corpora/VoxCeleb1\n",
      "voxceleb2_root=/export/corpora/VoxCeleb2\n",
      "nnet_dir=exp/xvector_nnet_1a\n",
      "musan_root=/export/corpora/JHU/musan\n",
      "```\n",
      "\n",
      "# Build the datasets\n",
      "\n",
      "We must now format the training and testing data:\n",
      "\n",
      "```bash\n",
      "stage=0\n",
      "\n",
      "# Control if it is the first stage\n",
      "if [ $stage -le 0 ]; then\n",
      "  # Apply the formating to the train and test data\n",
      "  local/make_voxceleb2.pl $voxceleb2_root dev data/voxceleb2_train\n",
      "  local/make_voxceleb2.pl $voxceleb2_root test data/voxceleb2_test\n",
      "  # The evaluation set becomes Voxceleb1 test data\n",
      "  local/make_voxceleb1_v2.pl $voxceleb1_root dev data/voxceleb1_train\n",
      "  local/make_voxceleb1_v2.pl $voxceleb1_root test data/voxceleb1_test\n",
      "  # We'll train on all of VoxCeleb2, plus the training portion of VoxCeleb1.\n",
      "  # This should give 7,323 speakers and 1,276,888 utterances.\n",
      "  # We combine the datasets\n",
      "  utils/combine_data.sh data/train data/voxceleb2_train data/voxceleb2_test data/voxceleb1_train\n",
      "fi\n",
      "```\n",
      "\n",
      "# Build the features\n",
      "\n",
      "We now have a training set ready with 7323 speakers and 1.276 million utterances. What we should do is extract the features for the whole training set. The process is in 2 steps. We extract features for the train and test set, and compute the voice activity detection decision.\n",
      "\n",
      "```bash\n",
      "if [ $stage -le 1 ]; then\n",
      "  # Make MFCCs and compute the energy-based VAD for each dataset\n",
      "  for name in train voxceleb1_test; do\n",
      "  \t# Compute the MFCC\n",
      "    steps/make_mfcc.sh --write-utt2num-frames true --mfcc-config conf/mfcc.conf --nj 40 --cmd \"$train_cmd\" \\\n",
      "      data/${name} exp/make_mfcc $mfccdir\n",
      "    utils/fix_data_dir.sh data/${name}\n",
      "    # Compute the VAD\n",
      "    sid/compute_vad_decision.sh --nj 40 --cmd \"$train_cmd\" \\\n",
      "      data/${name} exp/make_vad $vaddir\n",
      "    utils/fix_data_dir.sh data/${name}\n",
      "  done\n",
      "fi\n",
      "``` \n",
      "\n",
      "# Data augmentation\n",
      "\n",
      "X-vector is based on a robust embedding, and the major guarantee for the robustness is the data augmentation process. We can augment the initial dataset using several techniques:\n",
      "- add reverberation to the speech\n",
      "- add background music\n",
      "- add background noise\n",
      "- add babble noise\n",
      "\n",
      "Note, there are plenty of ways to augment data. One could also speed up the sammple, combine all augmentations on a single utternace...\n",
      "\n",
      "```bash\n",
      "if [ $stage -le 2 ]; then\n",
      "  frame_shift=0.01\n",
      "  awk -v frame_shift=$frame_shift '{print $1, $2*frame_shift;}' data/train/utt2num_frames > data/train/reco2dur\n",
      "\n",
      "  # Download the package that includes the real RIRs, simulated RIRs, isotropic noises and point-source noises\n",
      "  if [ ! -d \"RIRS_NOISES\" ]; then\n",
      "    wget --no-check-certificate http://www.openslr.org/resources/28/rirs_noises.zip\n",
      "    unzip rirs_noises.zip\n",
      "  fi\n",
      "\n",
      "  # Make a version with reverberated speech\n",
      "  rvb_opts=()\n",
      "  rvb_opts+=(--rir-set-parameters \"0.5, RIRS_NOISES/simulated_rirs/smallroom/rir_list\")\n",
      "  rvb_opts+=(--rir-set-parameters \"0.5, RIRS_NOISES/simulated_rirs/mediumroom/rir_list\")\n",
      "\n",
      "  # Make a reverberated version of the VoxCeleb2 list. No additive noise.\n",
      "  steps/data/reverberate_data_dir.py \\\n",
      "    \"${rvb_opts[@]}\" \\\n",
      "    --speech-rvb-probability 1 \\\n",
      "    --pointsource-noise-addition-probability 0 \\\n",
      "    --isotropic-noise-addition-probability 0 \\\n",
      "    --num-replications 1 \\\n",
      "    --source-sampling-rate 16000 \\\n",
      "    data/train data/train_reverb\n",
      "  cp data/train/vad.scp data/train_reverb/\n",
      "  utils/copy_data_dir.sh --utt-suffix \"-reverb\" data/train_reverb data/train_reverb.new\n",
      "  rm -rf data/train_reverb\n",
      "  mv data/train_reverb.new data/train_reverb\n",
      "\n",
      "  # Prepare the MUSAN corpus, which consists of music, speech, and noise\n",
      "  # suitable for augmentation.\n",
      "  steps/data/make_musan.sh --sampling-rate 16000 $musan_root data\n",
      "\n",
      "  # Get the duration of the MUSAN recordings.  This will be used by the\n",
      "  # script augment_data_dir.py.\n",
      "  for name in speech noise music; do\n",
      "    utils/data/get_utt2dur.sh data/musan_${name}\n",
      "    mv data/musan_${name}/utt2dur data/musan_${name}/reco2dur\n",
      "  done\n",
      "\n",
      "  # Augment with musan_noise\n",
      "  steps/data/augment_data_dir.py --utt-suffix \"noise\" --fg-interval 1 --fg-snrs \"15:10:5:0\" --fg-noise-dir \"data/musan_noise\" data/train data/train_noise\n",
      "  # Augment with musan_music\n",
      "  steps/data/augment_data_dir.py --utt-suffix \"music\" --bg-snrs \"15:10:8:5\" --num-bg-noises \"1\" --bg-noise-dir \"data/musan_music\" data/train data/train_music\n",
      "  # Augment with musan_speech\n",
      "  steps/data/augment_data_dir.py --utt-suffix \"babble\" --bg-snrs \"20:17:15:13\" --num-bg-noises \"3:4:5:6:7\" --bg-noise-dir \"data/musan_speech\" data/train data/train_babble\n",
      "\n",
      "  # Combine reverb, noise, music, and babble into one directory.\n",
      "  utils/combine_data.sh data/train_aug data/train_reverb data/train_noise data/train_music data/train_babble\n",
      "fi\n",
      "```\n",
      "\n",
      "Alright, this long script was useful to augment the dataset. We now must compute the MFCC features on the augmented dataset. This will approximately double the training set size. Note that this process is really different from what data augmentation in Computer Vision would look like. Indeed, in CV, we usually way more than double the number of training data, and apply the augmentation on-the-fly.\n",
      "\n",
      "```bash\n",
      "if [ $stage -le 3 ]; then\n",
      "  # Take a random subset of the augmentations\n",
      "  utils/subset_data_dir.sh data/train_aug 1000000 data/train_aug_1m\n",
      "  utils/fix_data_dir.sh data/train_aug_1m\n",
      "\n",
      "  # Make MFCCs for the augmented data.  Note that we do not compute a new\n",
      "  # vad.scp file here.  Instead, we use the vad.scp from the clean version of\n",
      "  # the list.\n",
      "  steps/make_mfcc.sh --mfcc-config conf/mfcc.conf --nj 40 --cmd \"$train_cmd\" \\\n",
      "    data/train_aug_1m exp/make_mfcc $mfccdir\n",
      "\n",
      "  # Combine the clean and augmented VoxCeleb2 list.  This is now roughly\n",
      "  # double the size of the original clean list.\n",
      "  utils/combine_data.sh data/train_combined data/train_aug_1m data/train\n",
      "fi\n",
      "```\n",
      "\n",
      "# Normalization\n",
      "\n",
      "We then apply a cepstral mean and variance normalization (CMVN) on the features, as it is required as an input for the x-vector.\n",
      "\n",
      "```bash\n",
      "# Now we prepare the features to generate examples for xvector training.\n",
      "if [ $stage -le 4 ]; then\n",
      "  # This script applies CMVN and removes nonspeech frames.  Note that this is somewhat\n",
      "  # wasteful, as it roughly doubles the amount of training data on disk.  After\n",
      "  # creating training examples, this can be removed.\n",
      "  local/nnet3/xvector/prepare_feats_for_egs.sh --nj 40 --cmd \"$train_cmd\" \\\n",
      "    data/train_combined data/train_combined_no_sil exp/train_combined_no_sil\n",
      "  utils/fix_data_dir.sh data/train_combined_no_sil\n",
      "fi\n",
      "```\n",
      "\n",
      "# Filter training data\n",
      "\n",
      "Among training data, you'll encounter some that are:\n",
      "- too short once silence has been removed (under 5s.)\n",
      "- don't have enough utterance per speaker (under 8)\n",
      "\n",
      "We need to remove those training data.\n",
      "\n",
      "```bash\n",
      "if [ $stage -le 5 ]; then\n",
      "  # Now, we need to remove features that are too short after removing silence\n",
      "  # frames.  We want atleast 5s (500 frames) per utterance.\n",
      "  min_len=400\n",
      "  mv data/train_combined_no_sil/utt2num_frames data/train_combined_no_sil/utt2num_frames.bak\n",
      "  awk -v min_len=${min_len} '$2 > min_len {print $1, $2}' data/train_combined_no_sil/utt2num_frames.bak > data/train_combined_no_sil/utt2num_frames\n",
      "  utils/filter_scp.pl data/train_combined_no_sil/utt2num_frames data/train_combined_no_sil/utt2spk > data/train_combined_no_sil/utt2spk.new\n",
      "  mv data/train_combined_no_sil/utt2spk.new data/train_combined_no_sil/utt2spk\n",
      "  utils/fix_data_dir.sh data/train_combined_no_sil\n",
      "\n",
      "  # We also want several utterances per speaker. Now we'll throw out speakers\n",
      "  # with fewer than 8 utterances.\n",
      "  min_num_utts=8\n",
      "  awk '{print $1, NF-1}' data/train_combined_no_sil/spk2utt > data/train_combined_no_sil/spk2num\n",
      "  awk -v min_num_utts=${min_num_utts} '$2 >= min_num_utts {print $1, $2}' data/train_combined_no_sil/spk2num | utils/filter_scp.pl - data/train_combined_no_sil/spk2utt > data/train_combined_no_sil/spk2utt.new\n",
      "  mv data/train_combined_no_sil/spk2utt.new data/train_combined_no_sil/spk2utt\n",
      "  utils/spk2utt_to_utt2spk.pl data/train_combined_no_sil/spk2utt > data/train_combined_no_sil/utt2spk\n",
      "\n",
      "  utils/filter_scp.pl data/train_combined_no_sil/utt2spk data/train_combined_no_sil/utt2num_frames > data/train_combined_no_sil/utt2num_frames.new\n",
      "  mv data/train_combined_no_sil/utt2num_frames.new data/train_combined_no_sil/utt2num_frames\n",
      "\n",
      "  # Now we're ready to create training examples.\n",
      "  utils/fix_data_dir.sh data/train_combined_no_sil\n",
      "fi\n",
      "```\n",
      "\n",
      "# Training the DNN\n",
      "\n",
      "Steps 6 to 8 are grouped in the training of the DNN. The aim of the training is to learn an embedding.\n",
      "\n",
      "```bash\n",
      "# Stages 6 through 8 are handled in run_xvector.sh\n",
      "local/nnet3/xvector/run_xvector.sh --stage $stage --train-stage -1 \\\n",
      "  --data data/train_combined_no_sil --nnet-dir $nnet_dir \\\n",
      "  --egs-dir $nnet_dir/egs\n",
      "```\n",
      "\n",
      "# Extract the x-vector\n",
      "\n",
      "By extracting the last hidden layer, before the prediction, we have an embedding called an x-vector, representing the information extacted from the voice of the speaker. We run the script `extract_xvectors.sh` to extract it in the training and test set.\n",
      "\n",
      "```bash\n",
      "if [ $stage -le 9 ]; then\n",
      "  # Extract x-vectors for centering, LDA, and PLDA training.\n",
      "  sid/nnet3/xvector/extract_xvectors.sh --cmd \"$train_cmd --mem 4G\" --nj 80 \\\n",
      "    $nnet_dir data/train \\\n",
      "    $nnet_dir/xvectors_train\n",
      "\n",
      "  # Extract x-vectors used in the evaluation.\n",
      "  sid/nnet3/xvector/extract_xvectors.sh --cmd \"$train_cmd --mem 4G\" --nj 40 \\\n",
      "    $nnet_dir data/voxceleb1_test \\\n",
      "    $nnet_dir/xvectors_voxceleb1_test\n",
      "fi\n",
      "```\n",
      "\n",
      "# Mean vector, LDA and PLDA\n",
      "\n",
      "We then compute the mean vector to center the evaluation x-vectors. We then reduce the dimension of the x-vectors to 200 using LDA. Finally, the PLDA acts as a classifier. We train it here.\n",
      "\n",
      "```bash\n",
      "if [ $stage -le 10 ]; then\n",
      "  # Compute the mean vector for centering the evaluation xvectors.\n",
      "  $train_cmd $nnet_dir/xvectors_train/log/compute_mean.log \\\n",
      "    ivector-mean scp:$nnet_dir/xvectors_train/xvector.scp \\\n",
      "    $nnet_dir/xvectors_train/mean.vec || exit 1;\n",
      "\n",
      "  # This script uses LDA to decrease the dimensionality prior to PLDA.\n",
      "  lda_dim=200\n",
      "  $train_cmd $nnet_dir/xvectors_train/log/lda.log \\\n",
      "    ivector-compute-lda --total-covariance-factor=0.0 --dim=$lda_dim \\\n",
      "    \"ark:ivector-subtract-global-mean scp:$nnet_dir/xvectors_train/xvector.scp ark:- |\" \\\n",
      "    ark:data/train/utt2spk $nnet_dir/xvectors_train/transform.mat || exit 1;\n",
      "\n",
      "  # Train the PLDA model.\n",
      "  $train_cmd $nnet_dir/xvectors_train/log/plda.log \\\n",
      "    ivector-compute-plda ark:data/train/spk2utt \\\n",
      "    \"ark:ivector-subtract-global-mean scp:$nnet_dir/xvectors_train/xvector.scp ark:- | transform-vec $nnet_dir/xvectors_train/transform.mat ark:- ark:- | ivector-normalize-length ark:-  ark:- |\" \\\n",
      "    $nnet_dir/xvectors_train/plda || exit 1;\n",
      "fi\n",
      "```\n",
      "\n",
      "# Make predictions and assess performance\n",
      "\n",
      "All models are trained, we can now make predictions on the test set of Voxceleb1. This script will also generate and display performance metrics (Equal Error Rate and min DCF).\n",
      "\n",
      "```bash\n",
      "if [ $stage -le 11 ]; then\n",
      "  $train_cmd exp/scores/log/voxceleb1_test_scoring.log \\\n",
      "    ivector-plda-scoring --normalize-length=true \\\n",
      "    \"ivector-copy-plda --smoothing=0.0 $nnet_dir/xvectors_train/plda - |\" \\\n",
      "    \"ark:ivector-subtract-global-mean $nnet_dir/xvectors_train/mean.vec scp:$nnet_dir/xvectors_voxceleb1_test/xvector.scp ark:- | transform-vec $nnet_dir/xvectors_train/transform.mat ark:- ark:- | ivector-normalize-length ark:- ark:- |\" \\\n",
      "    \"ark:ivector-subtract-global-mean $nnet_dir/xvectors_train/mean.vec scp:$nnet_dir/xvectors_voxceleb1_test/xvector.scp ark:- | transform-vec $nnet_dir/xvectors_train/transform.mat ark:- ark:- | ivector-normalize-length ark:- ark:- |\" \\\n",
      "    \"cat '$voxceleb1_trials' | cut -d\\  --fields=1,2 |\" exp/scores_voxceleb1_test || exit 1;\n",
      "fi\n",
      "\n",
      "if [ $stage -le 12 ]; then\n",
      "  eer=`compute-eer <(local/prepare_for_eer.py $voxceleb1_trials exp/scores_voxceleb1_test) 2> /dev/null`\n",
      "  mindcf1=`sid/compute_min_dcf.py --p-target 0.01 exp/scores_voxceleb1_test $voxceleb1_trials 2> /dev/null`\n",
      "  mindcf2=`sid/compute_min_dcf.py --p-target 0.001 exp/scores_voxceleb1_test $voxceleb1_trials 2> /dev/null`\n",
      "  echo \"EER: $eer%\"\n",
      "  echo \"minDCF(p-target=0.01): $mindcf1\"\n",
      "  echo \"minDCF(p-target=0.001): $mindcf2\"\n",
      "  # EER: 3.128%\n",
      "  # minDCF(p-target=0.01): 0.3258\n",
      "  # minDCF(p-target=0.001): 0.5003\n",
      "fi\n",
      "```\n",
      "\n",
      "---\n",
      "title: I trained a Network to Speak Like Me (and it's funny)\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Natural Language Processing\"\n",
      "---\n",
      "\n",
      "Over the course of the past months, I wrote over 100 articles on my blog. That's quite a large amount of content. An idea then came to my mind : train a language generation model to **speak like me**. Or more specifically, to write like me. This is the perfect way to illustrate the main concepts of language generation, its implementation using Keras, and the limits of my model.\n",
      "\n",
      "I have found [this Kaggle Kernel](https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms) to be a useful resource.\n",
      "\n",
      "# Language generation\n",
      "\n",
      "Language Generation is a subfield of Natural Language Processing that aims to generate meaningful textual content. Most often, the content is generated as a sequence of individual words. \n",
      "\n",
      "For the big idea, here is how it works :\n",
      "- you train a model to predict the next word of a sequence\n",
      "- you give the trained model an input\n",
      "- and iterate N times so that it generates the next N words\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lgen_6.png)\n",
      "\n",
      "## Dataset Creation\n",
      "\n",
      "The first step is to build a dataset that can be understood by the network we are later on going to build. Start by importing the following packages :\n",
      "\n",
      "```python\n",
      "from keras.preprocessing.sequence import pad_sequences\n",
      "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras.callbacks import EarlyStopping\n",
      "from keras.models import Sequential\n",
      "import keras.utils as ku \n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import string, os \n",
      "```\n",
      "\n",
      "### Load the data\n",
      "\n",
      "The header of each and every article I have written follows this template :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lgen_1.png)\n",
      "\n",
      "This is the type of content we would typically not like to have in our final dataset. We will instead focus on the text itself. First, we need to point to the folder that contains the articles :\n",
      "\n",
      "```python\n",
      "import glob, os\n",
      "\n",
      "os.chdir(\"/MYFOLDER/maelfabien.github.io/_posts/\")\n",
      "```\n",
      "\n",
      "### Sentence Tokenizing\n",
      "\n",
      "Then, open each article, and append the content of each article to a list. However, since our aim is to generate sentences, and not whole articles (so far...), we will split each article into a list of sentences, and append each sentences to the list `all_sentences` :\n",
      "\n",
      "```python\n",
      "all_sentences= []\n",
      "\n",
      "for file in glob.glob(\"*.md\"):\n",
      "    f = open(file,'r')\n",
      "    txt = f.read().replace(\"\\n\", \" \")\n",
      "    try: \n",
      "        sent_text = nltk.sent_tokenize(''.join(txt.split(\"---\")[2]).strip())\n",
      "        for k in sent_text :\n",
      "            all_sentences.append(k)\n",
      "    except : \n",
      "        pass\n",
      "```\n",
      "\n",
      "Overall, we have a little more than 6'800 training sentences :\n",
      "\n",
      "```python\n",
      "len(all_sentences)\n",
      "```\n",
      "\n",
      "`6858`\n",
      "\n",
      "The process so far is the following :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lgen_2.png)\n",
      "\n",
      "### N-gram creation\n",
      "\n",
      "Then, the idea is to create N-grams of words that occur together. To do so, we need to :\n",
      "- fit a tokenizer on the corpus to associate an index to each token\n",
      "- break down each sentence in the corpus as a sequence of tokens\n",
      "- store sequences of tokens that happens together\n",
      "\n",
      "It can be illustrated in the following way :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lgen_3.png)\n",
      "\n",
      "Let's implement this. We first need to fit the tokenizer :\n",
      "\n",
      "```python\n",
      "tokenizer = Tokenizer()\n",
      "tokenizer.fit_on_texts(all_sentences)\n",
      "total_words = len(tokenizer.word_index) + 1\n",
      "```\n",
      "\n",
      "The variable `total_words` contains the total number of different words that have been used. Here, 8976. Then, for each sentence, get the corresponding tokens and generate the N-grams :\n",
      "\n",
      "```python\n",
      "input_sequences = []\n",
      "for sent in all_sentences:\n",
      "    token_list = tokenizer.texts_to_sequences([sent])[0]\n",
      "    for i in range(1, len(token_list)):\n",
      "        n_gram_sequence = token_list[:i+1]\n",
      "        input_sequences.append(n_gram_sequence)\n",
      "```\n",
      "\n",
      "\n",
      "The `token_list` variable contains the sentence as a sequence of tokens :\n",
      "\n",
      "```python\n",
      "[656, 6, 3, 2284, 6, 3, 86, 1283, 640, 1193, 319]\n",
      "[33, 6, 3345, 1007, 7, 388, 5, 2128, 1194, 62, 2731]\n",
      "[7, 17, 152, 97, 1165, 1, 762, 1095, 1343, 4, 656]\n",
      "```\n",
      "\n",
      "Then, the `n_gram_sequences` creates the n-grams. It starts with the first two words, and then gradually adds words :\n",
      "\n",
      "```python\n",
      "[656, 6]\n",
      "[656, 6, 3]\n",
      "[656, 6, 3, 2284]\n",
      "[656, 6, 3, 2284, 6]\n",
      "[656, 6, 3, 2284, 6, 3]\n",
      "...\n",
      "```\n",
      "\n",
      "### Padding\n",
      "\n",
      "We are now facing the following problem : not all sequences have the same length ! How can we solve this ?\n",
      "\n",
      "We will use paddding. Paddings adds sequences of 0's before each line of the variable `input_sequences` so that each line has the same length as the longest line.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lgen_4.png)\n",
      "\n",
      "In order to pad all sentences to the maximum length of the sentences, we must first find the longest sentence :\n",
      "\n",
      "```python\n",
      "max_sequence_len = max([len(x) for x in input_sequences])\n",
      "```\n",
      "\n",
      "It is equal to `792` in my case. Well, that looks quite large for a single sentence ! Since my blog contains some code and tutorials, I expect this single sentence to actually by Python code. Let's plot the histogram of the length of the sequences :\n",
      "\n",
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist([len(x) for x in input_sequences], bins=50)\n",
      "plt.axvline(max_sequence_len, c=\"r\")\n",
      "plt.title(\"Sequence Length\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lgen_8.png)\n",
      "\n",
      "There are indeed very few examples with 200 + words in a single sequence. How about setting the maximal sequence length to 200 ?\n",
      "\n",
      "\n",
      "```python\n",
      "max_sequence_len = 200\n",
      "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
      "```\n",
      "\n",
      "It returns something like :\n",
      "\n",
      "```python \n",
      "array([[   0,    0,    0, ...,    0,  656,    6],\n",
      "       [   0,    0,    0, ...,  656,    6,    3],\n",
      "       [   0,    0,    0, ...,    6,    3, 2284],\n",
      "       ...,\n",
      "```\n",
      "\n",
      "### Split X and y\n",
      "\n",
      "We now have fixed length arrays, most of them are filled with 0's before the actual sequence. Right, how do we turn that into a training set? We need to split X and y! Remember that our aim is to predict the next word of a sequence. We must therefore takes all tokens except for the last one as our `X`, and take the last one as our `y`.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lgen_5.png)\n",
      "\n",
      "In Python, it's as simple as that :\n",
      "\n",
      "```python\n",
      "X, y = input_sequences[:,:-1],input_sequences[:,-1]\n",
      "```\n",
      "\n",
      "We will now see this problem as a multi-class classification task. As usual, we must first one-hot encode the `y` to get a sparse matrix that contains a 1 in the column that corresponds to the token, and 0 eslewhere :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lgen_7.png)\n",
      "\n",
      "In Python, using keras utils `to_categorical` :\n",
      "\n",
      "```python\n",
      "y = ku.to_categorical(y, num_classes=total_words)\n",
      "```\n",
      "\n",
      "Lets us now check the sizes of `X` and `y` :\n",
      "\n",
      "```python\n",
      "X.shape\n",
      "```\n",
      "\n",
      "`(164496, 199)`\n",
      "\n",
      "```python\n",
      "y.shape\n",
      "```\n",
      "\n",
      "`(164496, 8976)`\n",
      "\n",
      "We have 165'000 training samples. X is 199 columns wide since it corresponds to the longest sequence we allow (200) minus one, the label to predict. Y has 8976 columns, which corresponds to a sparse matrix of all the vocabulary words. The dataset is now ready !\n",
      "\n",
      "## Build the model\n",
      "\n",
      "We will be using Long Short-Term Memory networks (LSTM). LSTM have the important advantage of being able to understand depenence over a whole sequence, and therefore, the beginning of a sentence might have an impact on the 15th word to predict. On the other hand, Recurrent Neural Networks (RNN) only imply a dependence on the previous state of the network, and only the previous word would help predict the next one. We would quickly miss context if we chose RNNs, and therefore, LSTMs seem to be the right choice. \n",
      "\n",
      "### Model architecture\n",
      "\n",
      "Since the training can be very (very) (very) (very) (very) (no joke) long, we will build a simple 1 Embedding + 1 LSTM layer + 1 Dense network :\n",
      "\n",
      "```python\n",
      "def create_model(max_sequence_len, total_words):\n",
      "\n",
      "    input_len = max_sequence_len - 1\n",
      "\n",
      "    model = Sequential()\n",
      "    \n",
      "    # Add Input Embedding Layer\n",
      "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
      "    \n",
      "    # Add Hidden Layer 1 - LSTM Layer\n",
      "    model.add(LSTM(100))\n",
      "    model.add(Dropout(0.1))\n",
      "    \n",
      "    # Add Output Layer\n",
      "    model.add(Dense(total_words, activation='softmax'))\n",
      "\n",
      "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
      "    \n",
      "    return model\n",
      "\n",
      "model = create_model(max_sequence_len, total_words)\n",
      "model.summary()\n",
      "```\n",
      "\n",
      "First, we add an embedding layer. We pass that into an LSTM with 100 neurons, add a dropout to control neuron co-adaptation, and end with a dense layer. Notice that we apply a softmax activation function on the last layer to get the probability that the output belongs to each class. The loss used is the categorical cross-entropy, since it is a multi-class classification problem.\n",
      "\n",
      "The summary of the model is :\n",
      "\n",
      "```\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 199, 10)           89760     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8976)              906576    \n",
      "=================================================================\n",
      "Total params: 1,040,736\n",
      "Trainable params: 1,040,736\n",
      "Non-trainable params: 0\n",
      "____________________________\n",
      "\n",
      "```\n",
      "\n",
      "### Train the model\n",
      "\n",
      "We are now (finally) ready to train the model ! \n",
      "\n",
      "```python\n",
      "model.fit(X, y, batch_size=256, epochs=100, verbose=True)\n",
      "```\n",
      "\n",
      "The training of the model will then start :\n",
      "\n",
      "```python\n",
      "Epoch 1/10\n",
      "164496/164496 [==============================] - 471s 3ms/step - loss: 7.0687\n",
      "Epoch 2/10\n",
      "73216/164496 [============>.................] - ETA: 5:12 - loss: 7.0513\n",
      "```\n",
      "\n",
      "On a CPU, a single epoch takes around 8 minutes. On a GPU, you should modify the Keras LSTM network used since it cannot be used on GPU. You would instead need this :\n",
      "\n",
      "```python\n",
      "# Modify Import\n",
      "from keras.layers import Embedding, LSTM, Dense, Dropout, CuDNNLSTM\n",
      "\n",
      "# In the Moddel\n",
      "...\n",
      "\tmodel.add(CuDNNLSTM(100))\n",
      "...\n",
      "```\n",
      "\n",
      "This reduces training time to 2 minutes per epoch, which makes it acceptable. I have personnaly trained this model on Google Colab. I tend to stop the training at several steps to make so sample predictions and control the quality of the model given several values of the cross entropy.\n",
      "\n",
      "Here are my observations :\n",
      "\n",
      "| Loss Value | Sentence Generated |\n",
      "| --- | --- |\n",
      "| ± 7| Generates only the word \"The\" since most frequent |\n",
      "| ± 4| Easily falls into cyclical patterns if the same word occurs twice |\n",
      "| ± 2.8| Becomes interesting e.g \"Machine\" inputs leads to \"Learning algorithms ...\" |\n",
      "\n",
      "## Generating sequences \n",
      "\n",
      "If you have read the article up to here, you basically came here for that : generate sentences ! To generate sentences, we need to apply the same transformations to the input text. We will build a loop that generates for a given number of iterations the next word :\n",
      "\n",
      "```python\n",
      "input_txt = \"Machine\"\n",
      "\n",
      "for _ in range(10):\n",
      "    \n",
      "    # Get tokens\n",
      "    token_list = tokenizer.texts_to_sequences([input_txt])[0]\n",
      "    # Pad the sequence\n",
      "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
      "    # Predict the class\n",
      "    predicted = model.predict_classes(token_list, verbose=0)\n",
      "    \n",
      "    output_word = \"\"\n",
      "    \n",
      "    # Get the corresponding work\n",
      "    for word,index in tokenizer.word_index.items():\n",
      "        if index == predicted:\n",
      "            output_word = word\n",
      "            break\n",
      "            \n",
      "    input_txt += \" \"+output_word\n",
      "```\n",
      "\n",
      "When the loss is around 3.1, here is the sentence it generates with \"Google\" as an input :\n",
      "\n",
      "`Google is a large amount of data produced worldwide`\n",
      "\n",
      "It does not really mean anything, but it sucessfully associates Google to the notion of large amount of data. It's quite impressive since it simply relies on the co-occurence of words, and does not integrate any grammatical notion. If we wait a bit longer in the training and let the loss decrease to 2.6, and give it the input \"In this article\" :\n",
      "\n",
      "`In this article we'll cover the main concepts of the data and the dwell time is proposed mentioning the number of nodes`\n",
      "\n",
      "> I hope this article was useful. I have tried to illustrate the main concepts, challenges and limits of language generation. Larger networks and transfer learning are definitely sources of improvement compared to the approach we discussed in this article. Please leave a comment if you have any question :)\n",
      "\n",
      "Sources :\n",
      "- [Kaggle Kernel](https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms)\n",
      "\n",
      "\n",
      "---\n",
      "published: false\n",
      "title: Predicting the next hit song\n",
      "layout: post\n",
      "tags: [project]\n",
      "search: false\n",
      "---\n",
      "\n",
      "The music industry is a tough one. When you decide to produce an artist or invest in a marketing campaign for a song, there are many factors to take into account. But what if data science could help with this task? What if it could help predict whether a song is going to be a hit or not? \n",
      "\n",
      "# The Context\n",
      "\n",
      "Several articles and papers try to explain why a song became a hit, and the features these songs share. We will try to go a bit further, and build a hit song classifier. To build such a classifier, we typically will need a lot of data enrichment, since there is no single source of data that can help with such a vast task. We will use the following sources to help us build the dataset :\n",
      "- Google Trends\n",
      "- Spotify \n",
      "- Billboard\n",
      "- Genius.com\n",
      "\n",
      "We will consider the following :\n",
      "- A song is a hit if it reaches the top 10 of the most popular songs of the year\n",
      "- Otherwise, it's not a hit\n",
      "\n",
      "In this two-parts article, we are going to implement the following pipeline and build our hit song classifier !\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_22.png)\n",
      "\n",
      "# Data\n",
      "\n",
      "## Trends\n",
      "\n",
      "Let's start by checking the major trends in the music industry using [Google Trends](https://trends.google.com/).\n",
      "\n",
      "We will compare the major musical genres :\n",
      "- Pop\n",
      "- Rock\n",
      "- Rap\n",
      "- Country\n",
      "- Jazz\n",
      "\n",
      "<script type=\"text/javascript\" src=\"https://ssl.gstatic.com/trends_nrtr/1845_RC03/embed_loader.js\"></script> <script type=\"text/javascript\"> trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"/m/064t9\",\"geo\":\"\",\"time\":\"2004-01-01 2019-08-22\"},{\"keyword\":\"/m/06by7\",\"geo\":\"\",\"time\":\"2004-01-01 2019-08-22\"},{\"keyword\":\"/m/01lyv\",\"geo\":\"\",\"time\":\"2004-01-01 2019-08-22\"},{\"keyword\":\"/m/03_d0\",\"geo\":\"\",\"time\":\"2004-01-01 2019-08-22\"},{\"keyword\":\"/m/06bxc\",\"geo\":\"\",\"time\":\"2004-01-01 2019-08-22\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"date=all&q=%2Fm%2F064t9,%2Fm%2F06by7,%2Fm%2F01lyv,%2Fm%2F03_d0,%2Fm%2F06bxc\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"}); </script>\n",
      "\n",
      "Rap is the leading music in the world currently, and has taken over other genres such as Rock or Pop. A geographical analysis could also help us understand which countries listen to what music_\n",
      "\n",
      "<script type=\"text/javascript\" src=\"https://ssl.gstatic.com/trends_nrtr/1845_RC03/embed_loader.js\"></script> <script type=\"text/javascript\"> trends.embed.renderExploreWidget(\"GEO_MAP\", {\"comparisonItem\":[{\"keyword\":\"/m/064t9\",\"geo\":\"\",\"time\":\"2004-01-01 2019-08-22\"},{\"keyword\":\"/m/06by7\",\"geo\":\"\",\"time\":\"2004-01-01 2019-08-22\"},{\"keyword\":\"/m/01lyv\",\"geo\":\"\",\"time\":\"2004-01-01 2019-08-22\"},{\"keyword\":\"/m/03_d0\",\"geo\":\"\",\"time\":\"2004-01-01 2019-08-22\"},{\"keyword\":\"/m/06bxc\",\"geo\":\"\",\"time\":\"2004-01-01 2019-08-22\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"date=all&q=%2Fm%2F064t9,%2Fm%2F06by7,%2Fm%2F01lyv,%2Fm%2F03_d0,%2Fm%2F06bxc\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"}); </script>\n",
      "\n",
      "It seems like the US, South Africa and India are strong Rap markets, China and Indonesia are strong Pop markets, and South America is overall a great Rock market. \n",
      "\n",
      "## Top 100\n",
      "\n",
      "We will first scrap data from the Billboard Year-End 100 singles of the year. This will be our main data source. This approach has some limits since we consider that for a given song, it will at least hit the top 100 of the world charts. However, if you are trying to sell an ML-based solution to a music label, knowing whether a song will reach the top 10 of the year or remain in the bottom of the ranking has a huge financial impact. \n",
      "\n",
      "The year-end chart is calculated using an inverse point system based on the weekly Billboard charts (100 points for a week at number one, 1 point for a week at number 100, etc), for every year since 1946. \n",
      "\n",
      "The 2018 Billboard Year-End of 2018 can be found on Wikipedia : [https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2018](https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2018)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_0.png)\n",
      "\n",
      "Training a classifier using data from 1946 would however make no sense since we need the data to be relevant for the prediction task. We will focus on data between 2010 and 2018, and make the assumption that the year is not a relevant feature for predicting future hits. \n",
      "\n",
      "## Build the dataset\n",
      "\n",
      "We first need to retrieve the table from Wikipedia for all the years between 2010 and 2018. Open a notebook, and import the following packages :\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import re\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "```\n",
      "\n",
      "We then build a function to handle scraping requests :\n",
      "\n",
      "```python\n",
      "def _handle_request(request_result):\n",
      "    if request_result.status_code == 200:\n",
      "        html_doc =  request_result.text\n",
      "        soup = BeautifulSoup(html_doc,\"html.parser\")\n",
      "        return soup\n",
      "```\n",
      "\n",
      "The table to scrap is of type `table` and has the class: `wikitable sortable jquery-tablesorter`. It can be observed directly from the developer's console :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_1.png)\n",
      "\n",
      "We need to iterate on all the years between 2010 and 2018 :\n",
      "\n",
      "```python\n",
      "artist_array = []\n",
      "\n",
      "for i in range(2010, 2019):\n",
      "\n",
      "    # Iterate over this link and change year\n",
      "    website = 'https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_'+str(i)\n",
      "    \n",
      "    # Get the table\n",
      "    res = requests.get(website)\n",
      "    specific_class = \"wikitable sortable\"\n",
      "    soup = _handle_request(res)\n",
      "    table = soup.find(\"table\", class_= specific_class)\n",
      "    \n",
      "    # Get the body\n",
      "    table_body = table.find('tbody')\n",
      "    \n",
      "    # Get the rows\n",
      "    rows = table_body.find_all('tr')\n",
      "\n",
      "    # For each row\n",
      "    for row in rows:\n",
      "\n",
      "        try :\n",
      "            # Find the ranking\n",
      "            num = row.find_all('th')\n",
      "            num = [ele.text.strip() for ele in num]\n",
      "            \n",
      "            # Assess if the ranking is greater than 1 or not\n",
      "            if int(num[0]) > 10 :\n",
      "                num = 0\n",
      "            else :\n",
      "                num = 1\n",
      "\n",
      "            # Find the title and name of artist\n",
      "            cols = row.find_all('td')\n",
      "            cols = [ele.text.strip() for ele in cols]\n",
      "            \n",
      "            artist_array.append([num, cols[0].replace('\"', ''), cols[1]])\n",
      "\n",
      "        except : \n",
      "            pass\n",
      "```\n",
      "\n",
      "Then, transform this array into a dataframe :\n",
      "\n",
      "```python\n",
      "df = pd.DataFrame(artist_array)\n",
      "df.columns=[\"Hit\", \"Title\", \"Artist\"]\n",
      "df.head(n=10)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_2.png)\n",
      "\n",
      "We now have many points. Some songs might be in the charts in two different years. We want to keep only the first occurrence of a song to avoid having duplicates in the table :\n",
      "\n",
      "```python\n",
      "df = df.drop_duplicates(subset=[\"Title\", \"Artist\"], keep=\"first\")\n",
      "df.shape\n",
      "```\n",
      "\n",
      "The shape of the data frame is now: `(816, 3)`. Notice that in some cases, the \"Artist\" column contains the featuring. We will first create a simple feature where we split the name of the artist column if the word \"featuring\" is present, and add a feature \"featuring\" that is equal to 1 if there is a featuring, and 0 elsewhere. \n",
      "\n",
      "```python\n",
      "def featuring(artist):\n",
      "    if \"featuring\" in artist :\n",
      "        return 1\n",
      "    else :\n",
      "        return 0\n",
      "\n",
      "def featuring_substring(artist):\n",
      "    if \"featuring\" in artist :\n",
      "        return artist.split(\"featuring\")[0]\n",
      "    else :\n",
      "        return artist\n",
      "\n",
      "df[\"Featuring\"] = df.apply(lambda row: featuring(row['Artist']), axis=1)\n",
      "df[\"Artist_Feat\"] = df.apply(lambda row: featuring_substring(row['Artist']), axis=1)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_3.png)\n",
      "\n",
      "## Explore the data\n",
      "\n",
      "We can quickly explore the data to observe the most popular artists, the number of hits or the number of featurings. There is by construction an imbalance in the number of hits vs. non-hits :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(10,6))\n",
      "plt.hist(df[\"Hit\"], bins=3)\n",
      "plt.title(\"Non-Hits vs. Hits\")\n",
      "plt.show()\n",
      "````\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_4.png)\n",
      "\n",
      "To assess the performance of a model, we will use the F1-Score, which handles imbalanced datasets by providing a harmonic average between the precision and the recall.\n",
      "\n",
      "Are featurings common?\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(10,6))\n",
      "plt.hist(df[\"Featuring\"], bins=3)\n",
      "plt.title(\"No Featuring vs. Featuring\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_5.png)\n",
      "\n",
      "Who are the most popular artists over the years?\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "df['Artist_Feat'].value_counts()[:20].plot(kind=\"bar\")\n",
      "plt.title(\"Most popular artists in the charts\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_6.png)\n",
      "\n",
      "Drake seems to be performing well!\n",
      "\n",
      "## A first model\n",
      "\n",
      "Let's now build a first \"benchmark\" model that uses as features :\n",
      "- the fact that there is a featuring or not\n",
      "- the name of the main artist\n",
      "\n",
      "This model is a naive benchmark and will rely on a simple decision tree.\n",
      "\n",
      "```python\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "```\n",
      "\n",
      "We need to encode the name of the artists into categories to make it understandable for the models we will train :\n",
      "\n",
      "```python\n",
      "le = LabelEncoder()\n",
      "df[\"Artist_Feat_Num\"] = le.fit_transform(df[\"Artist_Feat\"])\n",
      "```\n",
      "\n",
      "Then, we split the `X` and the `y`, and create train and test sets :\n",
      "\n",
      "```python\n",
      "X = df[[\"Artist_Feat_Num\", \"Featuring\"]]\n",
      "y = df[\"Hit\"]\n",
      "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0) \n",
      "```\n",
      "\n",
      "We will use a simple decision tree classifier with the default parameters as a benchmark :\n",
      "\n",
      "```python\n",
      "dt = DecisionTreeClassifier()\n",
      "dt.fit(X_train, y_train)\n",
      "y_pred = dt.predict(X_test)\n",
      "f1_score(y_pred, y_test)\n",
      "```\n",
      "\n",
      "The resulting f1 score is: `0.066`, which is low. The accuracy is close to 86% since our model tends to predict too often that the song is systematically not a hit. There is room for better data and better models. \n",
      "\n",
      "# Data Enrichment through Spotify\n",
      "\n",
      "Where could we get data from? Well, popular music services like Spotify provide cool APIs that gather a lot of information on artists, albums and tracks. Using external APIs can sometimes be cumbersome. Hopefully, there is a great package called [Spotipy](https://spotipy.readthedocs.io/en/latest/#) that does most of the work for us!\n",
      "\n",
      "Spotipy is available on [Github](https://github.com/plamere/spotipy). If you follow the instructions, you will simply have to :\n",
      "- Install the package via PyPI\n",
      "- Create a project from the developer's console of Spotify\n",
      "- Write down your redirect URI and TokenID\n",
      "- Configure the URI and token in the util file of the package\n",
      "- and that's it!\n",
      "\n",
      "It's pretty well explained on the setup page of Spotipy, so let's move on to the data enrichment. When using Spotipy for the first time, you are required to validate the redirect URI (I have used `http://localhost/` ). \n",
      "\n",
      "```python\n",
      "import sys\n",
      "import spotipy\n",
      "import spotipy.util as util\n",
      "\n",
      "scope = 'user-library-read'\n",
      "\n",
      "if len(sys.argv) > 1:\n",
      "    username = sys.argv[1]\n",
      "else:\n",
      "    print(\"Usage: %s username\" % (sys.argv[0],))\n",
      "sys.exit()\n",
      "\n",
      "token = util.prompt_for_user_token(username, scope)\n",
      "```\n",
      "\n",
      "It will open an external web page. Simply follow it an copy-paste the URL of the page once logged-in.\n",
      "\n",
      "Start the Spotipy session : \n",
      "\n",
      "```python\n",
      "sp = spotipy.Spotify(auth=token)\n",
      "\n",
      "sp.trace = True # turn on tracing\n",
      "sp.trace_out = True # turn on trace out\n",
      "```\n",
      "\n",
      "The Spotify's API has a \"search\" feature. Type in the name of an artist or a track (or both combined), and it returns a JSON that contains much of the relevant information needed. We will use information from several levels :\n",
      "- the artist: popularity index and the total number of followers. Notice that these values in the API are the values of today, and therefore take into account some information from the future when you compare it to a song published in 2015 for example.\n",
      "- the album: how many songs were there on the album overall, the date of the release, the number of markets it is available in\n",
      "- the song: Spotify has a number of feature pre-computed such as the speechiness, the loudness, the danceability, the duration...\n",
      "\n",
      "This will allow us to collect 17 features overall from the Spotify's API ! \n",
      "\n",
      "```python\n",
      "def artist_info(lookup) :\n",
      "\n",
      "    try :\n",
      "        artist = sp.search(lookup)\n",
      "        artist_uri = artist['tracks']['items'][0]['album']['artists'][0]['uri']\n",
      "        track_uri = artist['tracks']['items'][0]['uri']\n",
      "\n",
      "        available_markets = len(artist['tracks']['items'][0]['available_markets'])\n",
      "        release_date = artist['tracks']['items'][0]['album']['release_date']\n",
      "\n",
      "        artist = sp.artist(artist_uri)\n",
      "        total_followers = artist['followers']['total']\n",
      "        genres = artist['genres']\n",
      "        popularity = artist['popularity']\n",
      "\n",
      "        audio_features = sp.audio_features(track_uri)[0]\n",
      "\n",
      "        acousticness = audio_features['acousticness']\n",
      "        danceability = audio_features['danceability']\n",
      "        duration_ms = audio_features['duration_ms']\n",
      "        energy = audio_features['energy']\n",
      "        instrumentalness = audio_features['instrumentalness']\n",
      "        key = audio_features['key']\n",
      "        liveness = audio_features['liveness']\n",
      "        loudness = audio_features['loudness']\n",
      "        speechiness = audio_features['speechiness']\n",
      "        tempo = audio_features['tempo']\n",
      "        time_signature = audio_features['time_signature']\n",
      "        valence = audio_features['valence']\n",
      "\n",
      "        return available_markets, release_date, total_followers, genres, popularity, acousticness, danceability, duration_ms, energy, instrumentalness, key, liveness, loudness, speechiness, tempo, time_signature, valence\n",
      "\n",
      "    except :\n",
      "        return [None]*17\n",
      "```\n",
      "\n",
      "To enhance our chances to identify the song from the search menu, we will create a feature called \"Lookup\" that combines the title of the song and the name of the main artist.\n",
      "\n",
      "```python\n",
      "df['lookup'] = df['Title'] + \" \" + df[\"Artist_Feat\"]\n",
      "```\n",
      "\n",
      "Then, apply the function above to create all columns :\n",
      "\n",
      "```python\n",
      "df['available_markets'], df['release_date'], df['total_followers'], df['genres'], df['popularity'], df['acousticness'], df['danceability'], df['duration_ms'], df['energy'], df['instrumentalness'], df['key'], df['liveness'], df['loudness'], df['speechiness'], df['tempo'], df['time_signature'], df['valence'] = zip(*df['lookup'].map(artist_info))\n",
      "```\n",
      "\n",
      "The new data frame looks like this :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_7.png)\n",
      "\n",
      "We need to make sure that the API sent back relevant information :\n",
      "\n",
      "```python\n",
      "df.shape\n",
      "```\n",
      "\n",
      "`(816,25)`\n",
      "\n",
      "```python\n",
      "df.dropna(how='any').shape\n",
      "```\n",
      "\n",
      "`(814,25)`\n",
      "\n",
      "For 2 of the input songs, we were not able to retrieve the information from the API. We will simply drop those observations :\n",
      "\n",
      "```python\n",
      "df = df.dropna()\n",
      "```\n",
      "\n",
      "Not all of the features are exploitable as such. Indeed, the release date is under a date format. We initially specified that we wanted our model not to depend on the year. However, the month of release, the day of the month or even the day of the week might be relevant features.\n",
      "\n",
      "```python\n",
      "df['release_date'] = pd.to_datetime(df['release_date'])\n",
      "df['month_release'] = df['release_date'].apply(lambda x: x.month)\n",
      "df['day_release'] = df['release_date'].apply(lambda x: x.day)\n",
      "df['weekday_release'] = df['release_date'].apply(lambda x: x.weekday())\n",
      "```\n",
      "\n",
      "## Data Exploration\n",
      "\n",
      "We now have many features and can proceed to a further data exploration. Let's start by analyzing the features we just created related to the release date :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist(df['weekday_release'], bins=14)\n",
      "plt.title(\"Weekday release\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_8.png)\n",
      "\n",
      "More songs seem to be released on Fridays! That's an interesting insight.\n",
      "\n",
      "Regarding the release month :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist(df['month_release'], bins=24)\n",
      "plt.title(\"Month release\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_9.png)\n",
      "\n",
      "January seems to be a popular choice, although we should probably be careful. Some missing data might be filled by default to January 1st. During the months of July and August, there are however few songs being released. Most songs are available to most markets :\n",
      "\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist(df['available_markets'], bins=50)\n",
      "plt.title(\"Number of markets\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_10.png)\n",
      "\n",
      "A strong feature will probably be the popularity of the artist :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist(df[df['Hit']==1]['popularity'], bins=50, density=True, alpha=0.5, label=\"Hit\")\n",
      "plt.hist(df[df['Hit']==0]['popularity'], bins=50, density=True, alpha=0.5, label=\"Not Hit\")\n",
      "plt.title(\"Artist Popularity\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_11.png)\n",
      "\n",
      "In both cases, the popularity of the artist as defined by Spotify's API is really high. Finally, let's explore the effect of the duration on the hit songs :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist(df[df['Hit']==1]['duration_ms'], bins=50, density=True, alpha=0.5, label=\"Hit\")\n",
      "plt.hist(df[df['Hit']==0]['duration_ms'], bins=50, density=True, alpha=0.5, label=\"Not Hit\")\n",
      "plt.title(\"Duration in ms.\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_12.png)\n",
      "\n",
      "The distribution looks quite similar in both cases. \n",
      "\n",
      "## Same model, better data\n",
      "\n",
      "We can now build a second classifier. However, we still have a quite limited number of data points and an imbalanced dataset. Can oversampling help?\n",
      "\n",
      "We will use the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE is implemented in the package `imblearn` for Python.\n",
      "\n",
      "```python\n",
      "from imblearn.over_sampling import SMOTE\n",
      "\n",
      "X = df.drop([\"Artist_Feat\", \"Artist\", \"Artist_Feat_Num\", \"Title\", \"Hit\", \"lookup\", \"release_date\", \"genres\"], axis=1)\n",
      "y = df[\"Hit\"]\n",
      "\n",
      "sm = SMOTE(random_state=42)\n",
      "X_res, y_res = sm.fit_resample(X, y)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_res,y_res, test_size=0.2, random_state=42) \n",
      "```\n",
      "\n",
      "Lets us apply the same decision tree we used before :\n",
      "\n",
      "```python\n",
      "dt = DecisionTreeClassifier(max_depth=100)\n",
      "dt.fit(X_train, y_train)\n",
      "y_pred = dt.predict(X_test)\n",
      "f1_score(y_pred, y_test)\n",
      "```\n",
      "\n",
      "The F1-score reaches 83.4 %. Since the data is not imbalanced anymore, we can compute the accuracy :\n",
      "\n",
      "```python\n",
      "accuracy_score(y_pred, y_test)\n",
      "```\n",
      "\n",
      "It reaches 84%.\n",
      "\n",
      "## Better model, better data\n",
      "\n",
      "The decision tree is a good choice for a first model to explore. However, more complex models might improve overall performance. Let's try this with a Random Forest Classifier :\n",
      "\n",
      "```python\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "rf=RandomForestClassifier(n_estimators=100)\n",
      "rf.fit(X_train, y_train)\n",
      "y_pred = rf.predict(X_test)\n",
      "\n",
      "f1_score(y_pred, y_test)\n",
      "```\n",
      "\n",
      "The F1-Score reaches 93.3 %! The accuracy is 94.5 %. Plotting the confusion matrix helps understand the errors our classifier made :\n",
      "\n",
      "```python\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import seaborn as sns\n",
      "\n",
      "cm = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "sns.heatmap(cm, annot=True)\n",
      "plt.title(\"Confusion Matrix\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_13.png)\n",
      "\n",
      "We can now try to understand the output of the classifier by looking at the feature importance :\n",
      "\n",
      "```python\n",
      "importances = rf.feature_importances_\n",
      "indices = np.argsort(importances)\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.title('Feature Importances')\n",
      "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
      "plt.yticks(range(len(indices)), [X.columns[i] for i in indices])\n",
      "plt.xlabel('Relative Importance')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_14.png)\n",
      "\n",
      "The most important feature is whether there is a featuring or not. Then, the most important features are related to the release date (the famous summer hit), and the popularity of the artist. After that, we find all features related to the song itself.\n",
      "\n",
      "This analysis highlights a major fact. A song is a hit if it essentially relies on a good featuring, it is released at the right moment, and the artists who release it are popular. All of this seems logical, but it's also verified empirically by our model!\n",
      "\n",
      "So far, we have not used the lyrics. Could we further improve the model by adding the lyrics? \n",
      "\n",
      "# Data Enrichment through Genius.com\n",
      "\n",
      "Genius.com is a great resource if you are looking for song lyrics. It offers a great API, all of which is packaged in a great library called `lyricsgenius`. Start by installing the package (instructions can be found on [GitHub](https://github.com/johnwmillr/LyricsGenius)).\n",
      "\n",
      "You will have to get a token from [Genius.com developer's website](https://docs.genius.com/).\n",
      "\n",
      "Start by importing the package :\n",
      "\n",
      "```python\n",
      "import lyricsgenius as genius\n",
      "api = genius.Genius('YOUR_TOKEN_GOES_HERE')\n",
      "```\n",
      "\n",
      "As before, the API has a powerful search functionality :\n",
      "\n",
      "```python\n",
      "def lookup_lyrics(song):\n",
      "    try :\n",
      "        return api.search_song(song).lyrics\n",
      "    except :\n",
      "        return None\n",
      "```\n",
      "\n",
      "And create a column \"lyrics\" that contains the lyrics of each song. This one might take some time.\n",
      "\n",
      "```python\n",
      "df['lyrics'] = df['lookup'].apply(lambda x: lookup_lyrics(x))\n",
      "```\n",
      "\n",
      "Notice how some of the text is not clean and contains `\\n` to denote a new line, or text between brackets to split sections :\n",
      "\n",
      "```python\n",
      "def clean_txt(song):\n",
      "    song = ' '.join(song.split(\"\\n\"))\n",
      "    song = re.sub(\"[\\[].*?[\\]]\", \"\", song)\n",
      "    return song\n",
      "\n",
      "df['lyrics'] = df['lyrics'].apply(lambda x: clean_txt(x))\n",
      "df = df.dropna() #Drop song if we don't have lyrics\n",
      "```\n",
      "\n",
      "Some features we could add are :\n",
      "- the length of the lyrics\n",
      "- the number of unique words used\n",
      "- the length of the lyrics without stopwords\n",
      "- the number of unique words used without stopwords\n",
      "\n",
      "We will use NLTK stop words list in english, but we should also consider that some of the songs of the Billboard Top 100 Year-End are not english songs.\n",
      "\n",
      "```python\n",
      "from nltk.corpus import stopwords \n",
      "from nltk.tokenize import word_tokenize \n",
      "stop_words = set(stopwords.words('english'))\n",
      "\n",
      "def len_lyrics(song):\n",
      "    return len(song.split())\n",
      "\n",
      "def len_unique_lyrics(song):\n",
      "    return len(list(set(song.split())))\n",
      "\n",
      "def rmv_stop_words(song):\n",
      "    song = [w for w in song.split() if not w in stop_words] \n",
      "    return len(song)\n",
      "\n",
      "def rmv_set_stop_words(song):\n",
      "    song = [w for w in song.split() if not w in stop_words] \n",
      "    return len(list(set(song)))\n",
      "```\n",
      "\n",
      "Then, apply this to the dataset :\n",
      "\n",
      "```python\n",
      "df['len_lyrics'] = df['lyrics'].apply(lambda x: len_lyrics(x))\n",
      "df['len_unique_lyrics'] = df['lyrics'].apply(lambda x: len_unique_lyrics(x))\n",
      "df['without_stop_words'] = df['lyrics'].apply(lambda x: rmv_stop_words(x))\n",
      "df['unique_without_stop_words'] = df['lyrics'].apply(lambda x: rmv_set_stop_words(x))\n",
      "```\n",
      "\n",
      "## Data exploration\n",
      "\n",
      "Just like in the first article, some data exploration might bring us additional insights.\n",
      "\n",
      "How many words are used in the lyrics?\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist(df[df['len_lyrics']<2000]['len_lyrics'], bins=70) #Not plot outliers\n",
      "plt.title(\"Number of words\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_15.png)\n",
      "\n",
      "The histogram above does not represent outliers, but a few songs count over 2000 words. On average, there are 467 words in a song and 166 unique words. This can be verified by :\n",
      "\n",
      "```python\n",
      "np.mean(df['len_lyrics'])\n",
      "np.mean(df['len_unique_lyrics'])\n",
      "```\n",
      "\n",
      "The ratio of unique words over total words is 35%. We can also plot the distribution of this ratio :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist(df['len_unique_lyrics']/df['len_lyrics'], bins=50)\n",
      "plt.title(\"Ratio Unique Words over total words\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_20.png)\n",
      "\n",
      "To illustrate the diversity of the vocabulary used in the songs, we can compute the ratio of words that are not stop words over all words :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_21.png)\n",
      "\n",
      "This is it for the count of words. Now, what are the most common words that singers use in their texts?\n",
      "\n",
      "```python\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "word_cloud = df['lyrics'].values\n",
      "\n",
      "str1 = ' '.join(word_cloud)\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(str(str1))\n",
      "\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.imshow(wordcloud, interpolation='bilinear')\n",
      "plt.axis(\"off\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_16.png)\n",
      "\n",
      "We won't spend too much time commenting that, but Yeah, Oh and Baby should definitely be on your hit-song to-do list.\n",
      "\n",
      "## Lyrics Sentiment\n",
      "\n",
      "Should a song be positive? Negative? Neutral? To assess the positiveness of a song and its intensity, we will use Valence Aware Dictionary and sEntiment Reasoner (VADER), a lexicon and rule-based sentiment analysis tool, available on [Github](https://github.com/cjhutto/vaderSentiment). This method relies on lexicons, and has over 7500 words annotated by linguists. This kind of algorithm was used before the rise of Natural Language Processing, but can still be useful in cases like this one where we do not have labeled data or trained models for song sentiment classification.\n",
      "\n",
      "```python\n",
      "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
      "analyzer = SentimentIntensityAnalyzer()\n",
      "\n",
      "df['sentimentVaderPos'] = df['lyrics'].apply(lambda x: analyzer.polarity_scores(x)['pos'])\n",
      "df['sentimentVaderNeg'] = df['lyrics'].apply(lambda x: analyzer.polarity_scores(x)['neg'])\n",
      "df['sentimentVaderComp'] = df['lyrics'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
      "df['sentimentVaderNeu'] = df['lyrics'].apply(lambda x: analyzer.polarity_scores(x)['neu'])\n",
      "```\n",
      "\n",
      "We can also create a feature that is the difference between the positive and the negative score :\n",
      "\n",
      "```python\n",
      "df['Vader'] = df['sentimentVaderPos'] - df['sentimentVaderNeg']\n",
      "```\n",
      "\n",
      "What are the sentiments expressed in the songs?\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist(df['Vader'], bins=50)\n",
      "plt.axvline(0, c='r')\n",
      "plt.title(\"Average sentiment\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_17.png)\n",
      "\n",
      "On average, the sentiment is slightly positive.\n",
      "\n",
      "## New model\n",
      "\n",
      "Let us now train a new model and see whether the performance was improved. First, we create the train and test sets and apply oversampling :\n",
      "\n",
      "```python\n",
      "X = df.drop([\"Artist_Feat\", \"Artist\", \"Artist_Feat_Num\", \"Title\", \"Hit\", \"lookup\", \"release_date\", \"genres\", \"lyrics\"], axis=1)\n",
      "y = df[\"Hit\"]\n",
      "\n",
      "sm = SMOTE(random_state=42)\n",
      "X_res, y_res = sm.fit_resample(X, y)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_res,y_res, test_size=0.2, random_state=42) \n",
      "```\n",
      "\n",
      "Then, we ddefine the random forest classifier and train the model :\n",
      "\n",
      "```python\n",
      "rf=RandomForestClassifier(n_estimators=100)\n",
      "rf.fit(X_train, y_train)\n",
      "\n",
      "y_pred = rf.predict(X_test)\n",
      "accuracy_score(y_pred, y_test)\n",
      "```\n",
      "\n",
      "The accuracy score improved by close to 5% and reaches 98.3%.\n",
      "\n",
      "What are the most important features in this new model?\n",
      "\n",
      "```python\n",
      "importances = rf.feature_importances_\n",
      "indices = np.argsort(importances)\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.title('Feature Importances')\n",
      "plt.barh(range(len(indices)), importances[indices], align='center')\n",
      "plt.yticks(range(len(indices)), [X.columns[i] for i in indices])\n",
      "plt.xlabel('Relative Importance')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_18.png)\n",
      "\n",
      "The order of the important features remains the same, but the compound sentiment feature is now one of the most important features.\n",
      "\n",
      "# Making predictions \n",
      "\n",
      "## Prediction function\n",
      "\n",
      "We can build a predictor that takes as an input the name of the song and the singer, creates the features, and output the probability of being a hit. Since the algorithm has never been trained on 2019 songs, we can feed it with recent songs and observe the outcome. \n",
      "\n",
      "We can recall the whole pipeline first :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_22.png)\n",
      "\n",
      "Let's build this pipeline and try it with \"Lover\" by Taylor Swift, a song that was recently published when we wrote the article:\n",
      "\n",
      "\n",
      "```python\n",
      "def model_prediction(artist, title):\n",
      "\n",
      "    df_pred = pd.DataFrame.from_dict({\n",
      "    \"Artist\":[artist], \n",
      "    \"Title\":[title]})\n",
      "\n",
      "    df_pred[\"Featuring\"] = df_pred.apply(lambda row: featuring(row['Artist']), axis=1)\n",
      "    df_pred[\"Artist_Feat\"] = df_pred.apply(lambda row: featuring_substring(row['Artist']), axis=1)\n",
      "    df_pred['Title_Length'] = df_pred['Title'].apply(lambda x: num_words(x))\n",
      "    df_pred['lookup'] = df_pred['Title'] + \" \" + df_pred[\"Artist_Feat\"]\n",
      "    df_pred['available_markets'], df_pred['release_date'], df_pred['total_followers'],\n",
      "    df_pred['genres'], df_pred['popularity'], df_pred['acousticness'], df_pred['danceability'],\n",
      "    df_pred['duration_ms'], df_pred['energy'], df_pred['instrumentalness'], df_pred['key'],\n",
      "    df_pred['liveness'], df_pred['loudness'], df_pred['speechiness'], df_pred['tempo'],\n",
      "    df_pred['time_signature'], df_pred['valence'] = zip(*df_pred['lookup'].map(artist_info))\n",
      "    df_pred['release_date'] = pd.to_datetime(df_pred['release_date'])\n",
      "    df_pred['month_release'] = df_pred['release_date'].apply(lambda x: x.month)\n",
      "    df_pred['day_release'] = df_pred['release_date'].apply(lambda x: x.day)\n",
      "    df_pred['weekday_release'] = df_pred['release_date'].apply(lambda x: x.weekday())\n",
      "    df_pred['lookup'] = df_pred['Title'] + \" \" + df_pred[\"Artist\"]\n",
      "    df_pred['lyrics'] = df_pred['lookup'].apply(lambda x: lookup_lyrics(x))\n",
      "    df_pred['lyrics'] = df_pred['lyrics'].apply(lambda x: clean_txt(x))\n",
      "    df_pred['len_lyrics'] = df_pred['lyrics'].apply(lambda x: len_lyrics(x))\n",
      "    df_pred['len_unique_lyrics'] = df_pred['lyrics'].apply(lambda x: len_unique_lyrics(x))\n",
      "    df_pred['without_stop_words'] = df_pred['lyrics'].apply(lambda x: rmv_stop_words(x))\n",
      "    df_pred['unique_without_stop_words'] = df_pred['lyrics'].apply(lambda x: rmv_set_stop_words(x))\n",
      "    df_pred['sentimentVaderPos'] = df_pred['lyrics'].apply(lambda x: analyzer.polarity_scores(x)['pos'])\n",
      "    df_pred['sentimentVaderNeg'] = df_pred['lyrics'].apply(lambda x: analyzer.polarity_scores(x)['neg'])\n",
      "    df_pred['sentimentVaderComp'] = df_pred['lyrics'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
      "    df_pred['sentimentVaderNeu'] = df_pred['lyrics'].apply(lambda x: analyzer.polarity_scores(x)['neu'])\n",
      "    df_pred['Vader'] = df_pred['sentimentVaderPos'] - df_pred['sentimentVaderNeg']\n",
      "\n",
      "    X = df_pred.drop([\"Artist_Feat\", \"Artist\", \"Title\", \"lookup\", \"release_date\", \"genres\", \"lyrics\"], axis=1).astype(float)\n",
      "    y_pred = rf.predict_proba(X)\n",
      "\n",
      "    print(\"It's a NOT hit with probability : \" + str(y_pred[0][0]))\n",
      "    print(\"It's a hit with probability : \" + str(y_pred[0][1]))\n",
      "\n",
      "    return y_pred\n",
      "```\n",
      "\n",
      "We can create an interactive form in the Notebook to ask the user for the name of the artist and title of the song, and output the prediction.\n",
      "\n",
      "```python\n",
      "from ipywidgets import widgets, interact\n",
      "\n",
      "artist = widgets.Text()\n",
      "title = widgets.Text()\n",
      "\n",
      "ui = widgets.HBox([artist, title])\n",
      "\n",
      "def f(artist, title):\n",
      "return model_prediction(artist, title)\n",
      "```\n",
      "\n",
      "And in the next cell, type :\n",
      "\n",
      "```python\n",
      "interact(f, artist='Taylor Swift', title='Lover')\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_19.png)\n",
      "\n",
      "According to our algorithm, there are only 22% chances that the song \"Lover\" by TaylorSwift will make it to the top 10 of the most popular songs of 2019. \n",
      "\n",
      "# Conclusion\n",
      "\n",
      "Through this article, we illustrated the importance of external data sources for most data science problems. A good enrichment can boost the performance of your model, and relevant feature engineering can help gain additional performance. \n",
      "\n",
      "Here is a performance summary of the different steps of our model :\n",
      "\n",
      "| Description | Model | Performance |\n",
      "| -- | -- | -- |\n",
      "| Data from billboard | Decision Tree | F1-Score : 6.6% |\n",
      "| Enrich with Spotify and oversample | Random Forest | Accuracy : 93% |\n",
      "| Enrich with Genius | Random Forest | Accuracy : 98% |\n",
      "\n",
      "Sources and resources:\n",
      "- [SpotiPy](https://github.com/plamere/spotipy)\n",
      "- [Billboard Ranking, Wikipedia](https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2018)\n",
      "- [VADER](https://github.com/cjhutto/vaderSentiment)\n",
      "- [Lyricsgenius](https://github.com/johnwmillr/LyricsGenius)\n",
      "\n",
      "---\n",
      "title: Markov Decision Process\n",
      "layout: post\n",
      "tags: [RL]\n",
      "subtitle : \"Advanced AI\"\n",
      "---\n",
      "\n",
      "*David Silver's YouTube series on Reinforcement Learning, Episode 2*. \n",
      "\n",
      "The full lesson is the following:\n",
      "\n",
      "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/lfHX2hHRMVQ\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "A **Markov Decision Process** descrbes an environment for reinforcement learning. The environment is fully observable. In MDPs, the current state completely characterises the process.\n",
      "\n",
      "# Markov Process (MP)\n",
      "\n",
      "The **Markov Property** states the following:\n",
      "\n",
      "> A state $$ S_t $$ is **Markov** if and only if $$ P(S_{t+1} \\mid S_t) = P(S_{t+1} \\mid S_1, ..., S_t) $$\n",
      "\n",
      "The transition between a state $$ s $$ and the next state $$ s' $$ is characterized by a **transition probability**. It is defined by :\n",
      "\n",
      "$$ P_{ss'} = P(S_{t+1} = s' \\mid S_t = s) $$\n",
      "\n",
      "We can characterize a **state transition matrix** $$ P $$, describing all transition probabilities from all states $$ s $$ to all successor states $$ s' $$, where each row of the matrix sums to 1.\n",
      "\n",
      "$$\n",
      "P = \n",
      "\\begin{bmatrix} \n",
      "P_{11} &  \\cdots & P_{1n} \\\\\n",
      "\\cdots &  \\cdots & \\cdots \\\\\n",
      "P_{n1} &  \\cdots & P_{nn} \\\\\n",
      "\\end{bmatrix}\n",
      "$$\n",
      "\n",
      "A **Markov Process** is a memoryless random process. It is a sequence of randdom states $$ S_1, S_2, \\cdots $$ with the Markov Property.\n",
      "\n",
      "A Markov Process, also known as Markov Chain, is a tuple $$ (S,P) $$, where :\n",
      "- $$ S $$ is a finite set of states\n",
      "- $$ P $$ is a state transition probability matrix such that $$ P_{ss'} = P(S_{t+1} = s' \\mid S_t = s) $$\n",
      "\n",
      "We can represent a Markov Decision Process schematically the following way :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_4.jpg)\n",
      "\n",
      "**Samples** describe chains that take different states. For example, it could be :\n",
      "- 1, 1, 2, 1, 2, 3, 4, 3, Exit\n",
      "- 1, 2, 3, Exit\n",
      "- ...\n",
      "\n",
      "The transition matrix corresponding to this problem is :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_5.jpg)\n",
      "\n",
      "# Markov Reward Process (MRP)\n",
      "\n",
      "## Markov Reward\n",
      "\n",
      "A Markov Reward is a Markov Chain a value function. A **Markov Reward Process** is a tuple $$ (S, P, R, \\gamma) $$ where :\n",
      "- $$ R $$ is a reward function $$ R_s = E(R_{t+1} \\mid S_t = s) $$\n",
      "- $$ \\gamma $$ is a discount factor between 0 and 1\n",
      "- all other components are the same as before\n",
      "\n",
      "We can therefore attach a reward to each state in the following graph :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_7.png)\n",
      "\n",
      "Then, the **Return** is the total discounte reward from time-step $$ t $$ :\n",
      "\n",
      "$$ G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots = \\sum_{k=0} \\gamma^k R_{t+k+1} $$\n",
      "\n",
      "Just like in Finance, we compute the present value of future rewards. This represents the fact that we prefer to get reward now instead of getting it in the future.\n",
      "- if $$ \\gamma $$ is close to 0, we have a \"myopic\" evaluation where almost only the present matters\n",
      "- if $$ \\gamma $$ is close to 1, we have a \"far-sighted\" evaluation\n",
      "\n",
      "A simple return for the sequence 1-1-2-3-Exit and with $$ \\gamma = 0.8 $$ would be :\n",
      "\n",
      "$$ G_1 = (-10) + (-10) * 0.8 + (-5) * 0.8^2 + (-1) * 0.8^3 $$\n",
      "\n",
      "But why do we use a discount factor ?\n",
      "- there is uncertainty in the future, and our model is not perfect\n",
      "- it avoids infinite returns in cyclical Markov Processes\n",
      "- animals and humans have a preference for immediate reward\n",
      "\n",
      "## Bellman Equation\n",
      "\n",
      "The **value function** $$ v(s) $$ gives the long-term value of a state $$ s $$. It reflects the expected return when we are in a given state :\n",
      "\n",
      "$$ v(s) = E(G_t \\mid S_t = s) $$\n",
      "\n",
      "The value function can be decomposed in two parts :\n",
      "- the immediate reward $$ R_{t+1} $$\n",
      "- the discounted value of the successor rate $$ \\gamma v(S_{t+1}) $$\n",
      "\n",
      "This is the **Bellman Equation** for MRP :\n",
      "\n",
      "$$ v(s) = E(G_t \\mid S_t = s) = E(R_{t+1} + \\gamma R_{t+2} + \\cdots \\mid S_t = s) $$\n",
      "\n",
      "$$ = E(R_{t+1} + \\gamma(R_{t+2} + \\gamma R_{t+3} + \\cdots) \\mid S_t = s $$\n",
      "\n",
      "$$ = E(R_{t+1} + \\gamma G_{t+1} \\mid S_t = s)  = E(R_{t+1} + \\gamma v(S_{t+1}) \\mid S_t = s)  $$\n",
      "\n",
      "$$ = R_s + \\gamma \\sum_{s' \\in S} P_{ss'} v(s') $$\n",
      "\n",
      "If we consider that $$ \\gamma $$ is equal to 1, we can compute the value function at state 2 in our previous example :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_8.png)\n",
      "\n",
      "We can summarize the Bellman equation is a matrix form :\n",
      "\n",
      "$$ v = R + \\gamma P v $$\n",
      "\n",
      "$$\n",
      "\\begin{bmatrix} \n",
      "v(1) \\\\\n",
      "\\cdots \\\\\n",
      "v(n) \\\\\n",
      "\\end{bmatrix}\n",
      "\n",
      "= \n",
      "\n",
      "\\begin{bmatrix} \n",
      "R_1 \\\\\n",
      "\\cdots \\\\\n",
      "R_n \\\\\n",
      "\\end{bmatrix}\n",
      "\n",
      "+ \\gamma\n",
      "\n",
      "\\begin{bmatrix} \n",
      "P_{11} &  \\cdots & P_{1n} \\\\\n",
      "\\cdots &  \\cdots & \\cdots \\\\\n",
      "P_{n1} &  \\cdots & P_{nn} \\\\\n",
      "\\end{bmatrix}\n",
      "\n",
      "\\begin{bmatrix} \n",
      "v(1) \\\\\n",
      "\\cdots \\\\\n",
      "v(n) \\\\\n",
      "\\end{bmatrix}\n",
      "\n",
      "$$\n",
      "\n",
      "We can solve this equation as a simple linear equation :\n",
      "\n",
      "$$ v = R + \\gamma P v $$\n",
      "\n",
      "$$ (1 - \\gamma P) v = R $$\n",
      "\n",
      "$$ v = (1 - \\gamma P)^{-1} R $$\n",
      "\n",
      "However, solving this equation this way has a computational complexity of $$ O(n^3) $$ for $$ n $$ states since it contains a matrix inversion step. There are several ways to compute it faster, and we'll develop those solutions later on.\n",
      "\n",
      "# Markov Decision Process (MDP)\n",
      "\n",
      "So far, we have not seen the action component. Markov Decision Process (MDP) is a Markov Reward  Process with decisions. As defined at the beginning of the article, it is an environment in which all states are Markov.\n",
      "\n",
      "A **Markov Decision Process** is a tuple of the form : $$ (S, A, P, R, \\gamma) $$ where :\n",
      "- $$ A $$ is a finite set of actions\n",
      "- $$ P $$ the state probability matrix is now modified : $$ P_{ss'}^a = P(S_{t+1} = s' \\mid S_t = s, A_t = a) $$\n",
      "- $$ R $$ the reward function is now modified : $$ R_s^a = E(R_{t+1} \\mid S_t = s, A_t = a) $$\n",
      "- all other components are the same as before\n",
      "\n",
      "We now have more control on the actions we can take :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_9.png)\n",
      "\n",
      "There might stil be some states in which we cannot take action and are subject to the transition probabilities, but in other states, we have an action choice to make. This is the reality of an agent, and we needd to maximize the reward and find the best path to reach the final state.\n",
      "\n",
      "But what does it mean to actually **make a decision** ?\n",
      "\n",
      "## Policies\n",
      "\n",
      "The agent chooses a policy. A policy $$ \\pi $$ is a distribution over actions given states. \n",
      "\n",
      "$$ \\pi(a \\mid s) = P(A_t = a \\mid S_t = s) $$\n",
      "\n",
      "A policy describes the behavior of an agent. Policies are *time stationary*, they donnot depend on time. \n",
      "\n",
      "$$ A_t \\sim \\pi(. \\mid S_t) $$\n",
      "\n",
      "Given an MDP $$ M = (S, A, P,  R, \\gamma) $$ and a policy $$ \\pi $$ :\n",
      "- the state sequence $$ S_1, S_2, \\cdots $$ is a Markov Process $$ (S, P^{\\pi}) $$.\n",
      "- the state and reward sequence $$ S_1, R_2, S_2, \\cdots $$ is a Markov Reward Process $$ (S, P^{\\pi}, R^{\\pi}, \\gamma) $$.\n",
      "\n",
      "We compte the Markov Reward Process values by averaging over the dynamics that result of each choice. In other words :\n",
      "\n",
      "$$ P_{s,s'}^{\\pi} = \\sum_{a \\in A} \\pi(a \\mid s) P_{s,s'}^a $$\n",
      "\n",
      "$$ R_{s}^{\\pi} = \\sum_{a \\in A} \\pi(a \\mid s) R_{s}^a $$\n",
      "\n",
      "## Value Function\n",
      "\n",
      "The **state-value function** $$ v_{\\pi}(s) $$ of an MDP is now conditional to the chosen policy $$ \\pi $$. It is the expected return starting from state $$ s $$ and following policy $$ \\pi $$ :\n",
      "\n",
      "$$ v_{\\pi}(s) = E_{\\pi}(G_t \\mid S_t = s) $$\n",
      "\n",
      "The **action-value function** $$ q_{\\pi}(s, a) $$ is the expected return starting from a state $$ s $$, taking action $$ a $$ and following policy $$ \\pi $$ :\n",
      "\n",
      "$$ q_{\\pi}(s,a) = E_{\\pi}(G_t \\mid S_t = s, A_t = a) $$\n",
      "\n",
      "The state-value function can again be decomposed into immediate reward plus discounted value of successor rate. This is the **Bellman Expectation Equation** :\n",
      "\n",
      "$$ v_{\\pi}(s) = E_{\\pi} (R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\mid S_t = s) $$\n",
      "\n",
      "The action-value function can be decomposed similarly :\n",
      "\n",
      "$$ q_{\\pi}(s, a) = E_{\\pi} (R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}) \\mid S_t = s, A_t = a) $$\n",
      "\n",
      "Let's illustrate those concepts ! Suppose we start in the state $$ s $$. We can take actions, either the one on the left or on the right. To each action, we attach a q-value, which gives the value of taking this action. The value of being in state $$ s $$ is therefore an average of both actions :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_10.png)\n",
      "\n",
      "This is the Bellman Expectation Equation for $$ v_{\\pi} $$ :\n",
      "\n",
      "$$ v_{\\pi}(s) = \\sum_{a \\in A} \\pi (a \\mid s) q_{\\pi}(s,a) $$\n",
      "\n",
      "What if we now consider the inverse ? We start from an action, and have two resulting states. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_11.png)\n",
      "\n",
      "The state shows how good it is to be in a state. The action tells us how good it is to take an action. This is the Bellman Expectation Equation for $$ q_{\\pi} $$ :\n",
      "\n",
      "$$ q_{\\pi}(s, a) = R_s^a + \\gamma \\sum_{s \\in S} P_{ss'}^a v_{\\pi}(s') $$\n",
      "\n",
      "We can now group both interpretations into a single graph :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_12.png)\n",
      "\n",
      "This shows us a recursion that expresses $$ v $$ in terms of itself. This is how we solve the Markov Decision Process. At the root of the tree, we know how gooddit is to be in a state. We then consider all the actions we might do given the policy. For each action, there are possible outcome states.\n",
      "\n",
      "We can now express the Bellman Equation a for the state-value as :\n",
      "\n",
      "$$ v_{\\pi}(s) = \\sum_{a \\in A} \\pi (a \\mid s) (R_s^a + \\gamma \\sum_{s' \\in S} P_{ss'}^a v_{\\pi}(s')) $$ \n",
      "\n",
      "And similarly for the action-value :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_13.png)\n",
      "\n",
      "$$ q_{\\pi}(s, a) = R_s^a + \\gamma \\sum_{s' \\in S} P_{ss'}^a \\sum_{a' \\in A} \\pi(a'\\mid s') q_{\\pi}(s', a') $$\n",
      "\n",
      "We can simply illustrate how this Bellman Expectation works. We suppose here that there is no discount, and that our policy is to pick each action with a probability of 50%.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_14.png)\n",
      "\n",
      "We can represent the Bellman Expectation Equation in a matrix form :\n",
      "\n",
      "$$ v_{\\pi} = R^{\\pi} + \\gamma P^{\\pi}v_{\\pi} $$\n",
      "\n",
      "The solution is given by :\n",
      "\n",
      "$$ v_{\\pi} = (1 - \\gamma P^{\\pi})^{-1} R^{\\pi} $$\n",
      "\n",
      "Again, the inversion of the matrix is of complexity $$ O(n^3) $$, and we there need more efficient ways to solve this.\n",
      "\n",
      "## Optimal Value Function\n",
      "\n",
      "> The **optimal state-value** function $$ v_{*}(s) $$ is the maximum value function over all policies : $$ v_{*}(s) = max_{\\pi} v_{\\pi}(s) $$\n",
      "\n",
      "It reflects the maximum reward we can get by following the best policy.\n",
      "\n",
      "> The **optimal action-value** function $$ q_{*}(s, a) $$ is the maximum action-value function over all policies. $$ q_{*}(s, a) = max_{\\pi} q_{\\pi}(s, a) $$\n",
      "\n",
      "It shows given that we commit to a particular action in state $$ s $$, what is the maximum reward we can get.\n",
      "\n",
      "An optimal value function specifies the best possible performance in the MDP. An MDP is said to be **solved** if we know the optimal value function.\n",
      "\n",
      "## Optimal Policy\n",
      "  \n",
      "The **optimal policy** defines the best possible way to behave in an MDP. We first define a partial ordering over policies :\n",
      "  \n",
      "  $$ \\pi ≥ \\pi^' $$ if $$ v_{\\pi}(s) ≥ v_{\\pi^'}(s) $$\n",
      "  \n",
      "  > For any MDP, there exists an optimal policy $$ \\pi $$ that is better than or equal to all other policies. \n",
      "  \n",
      "- All optimal policies achieve the optimal value function : $$ v_{\\pi^*}(s) = v_{*}(s) $$\n",
      "- All optimal policies achieve the optimal action-value function : $$ q_{\\pi^*}(s, a) = q_{*}(s, a) $$\n",
      "  \n",
      "So, how do we find this policy ? We must maximise over $$ q_{*}(s, a) $$ :\n",
      "  \n",
      "$$ \\pi_{*}(a \\mid s) = 1 $$ if $$ a = argmax_{a \\in A} q_{*}(s, a) $$, and $$ 0 $$ otherwise.\n",
      "  \n",
      "This tells us that once we have found $$ q_{*}(s, a) $$, we are done. This now brings the problem to : How do we find $$ q_{*}(s, a) $$ ?\n",
      "  \n",
      "Once we are in the final state, it's quite easy. We know which action will lead to the maximal reward. If we move back to one state before, we know that the state we were in leads to the maximum reward. We therefore pick this action since it maximizes the reward. If we move another step before, we ...\n",
      "  \n",
      "You get the idea. This simply means that we can move backward, and take at each state the action that maximizes the reward :\n",
      "  \n",
      "![image](https://maelfabien.github.io/assets/images/rl_15.png)\n",
      "  \n",
      "$$ v_{*}(s) = max_a q_{*}(s,a) $$  \n",
      "\n",
      "However, when picking an action, we must average over what the environment might do to us once we have picke this action.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_16.png)\n",
      "\n",
      "$$ q_{*}(s,a) = R_s^a + \\gamma \\sum_{s \\in S} P_{ss'}^a v_{*}(s') $$\n",
      "\n",
      "The Bellman Optimality Equation for $$ V^* $$ can be obtained by combining both : \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_17.png)\n",
      "\n",
      "$$ v_{*}(s) = max_a R_s^a + \\gamma \\sum_{s' \\in S} P_{ss'}^a v_{*}(s') $$\n",
      "\n",
      "And finally, we can switch the order andd start with the action to derive the Bellman Equation for $$ Q^* $$. We start by taking the action $$ a $$, and there is an uncertainty on the state the environment is going to lead us in. Then, wherever we are, we get to make a decision to maximise the reward.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_18.png)\n",
      "\n",
      "$$ q_{*}(s,a) = R_s^a + \\gamma \\sum_{s' \\in S} P_{ss'}^a max_{a'} q_{*}(s', a') $$\n",
      "\n",
      "## Solving the Bellman Equation\n",
      "\n",
      "The Bellman Equation is a non-linear problem. There is no closed form solution in general. We need to use iterative solutions, among which :\n",
      "- value iteration\n",
      "- policy iteration\n",
      "- Q-learning\n",
      "- Sarsa \n",
      "\n",
      "Value and policy iteration are Dynamic Programming algorithms, and we'll cover them in the next article.\n",
      "\n",
      "---\n",
      "title: Introduction to Graph Database\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Neo4J\"\n",
      "---\n",
      "\n",
      "Neo4J is a NoSQL is one of the most popular graph-oriented databases. Why is it so popular? What are the building blocks of Neo4J?\n",
      "\n",
      "# What is Neo4J?\n",
      "\n",
      "Neo4J is a NoSQL Graph Database Management System (DBMS). It is originally developed in Java and accessible across all platforms. It respects the ACID properties. The graph schema is very flexible. \n",
      "\n",
      "Neo4J allows requests through Cypher Query Language (CQL).\n",
      "\n",
      "## Key components\n",
      "\n",
      "In a graph, the building blocks are :\n",
      "- a node\n",
      "- a relation\n",
      "- a label\n",
      "\n",
      "A ***node*** contains the properties stored under a key-value format. The values stored might be :\n",
      "- numeric (integer, float)\n",
      "- string\n",
      "- boolean\n",
      "- list\n",
      "\n",
      "A node cannot contain another node. There is no limit storage capacity. \n",
      "\n",
      "A ***relation***, also known as an edge, is used to connect two nodes. It can also store properties under the key-value principle, without any storage limit. A relation is always directed, from a node to the other. \n",
      "\n",
      "A ***label*** is a name given to a set of nodes or relations. There is a limit at 64'000 different labels in the community edition of Neo4J, and 16'000'000 in the Enterprise edition. This is, however, an important point. Imagine a social network graph in which someone reached over 16 million followers. The ID of the follower could therefore not be used as the label... We'll see later on how to manage this.\n",
      "\n",
      "## Relational Database vs. Graph Database\n",
      "\n",
      "The first question you should ask is: \"Why should I use a graph database ?\"\n",
      "\n",
      "Graph databases are quite specific and should be used only when needed. Among popular applications of Graph databases, you'll find :\n",
      "- social networks \n",
      "- customer journey (rental services for example)\n",
      "- customer sessions \n",
      "- log anomaly detection \n",
      "- ...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> **Conclusion** : I hope this quick introduction to Neo4J was useful. Don't hesitate to drop a comment if you have a question.\n",
      "---\n",
      "title: Implementing YoloV3 for object detection\n",
      "layout: post\n",
      "tags: [computervision]\n",
      "subtitle : \"Computer Vision\"\n",
      "---\n",
      "\n",
      "Yolo is one of the greatest algorithm for real-time object detection. In its large version, it can detect thousands of object types in a quick and efficient manner. I this article, I won't cover the technical details of YoloV3, but I'll jump straight to the implementation. We will learn to build a simple web application with Streamlit that detects the objects present in an image. This implementation is a simplified version of Streamlit's yolo demo.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# Download the weights and config\n",
      "\n",
      "Create a separate folder for your web application, and download the weights via your command line:\n",
      "\n",
      "```bash\n",
      "wget https://pjreddie.com/media/files/yolov3.weight\n",
      "```\n",
      "\n",
      "Then, download the configuration file:\n",
      "\n",
      "```bash\n",
      "wget https://pjreddie.com/media/files/yolov3.cfg\n",
      "```\n",
      "\n",
      "Finally, download the classes predicted by the YoloV3 algorithm in a text file:\n",
      "\n",
      "```bash\n",
      "wget https://pjreddie.com/media/files/classes.txt\n",
      "```\n",
      "\n",
      "# App layout\n",
      "\n",
      "```python\n",
      "# Add a title and sidebar\n",
      "st.title(\"Object Detection\")\n",
      "st.sidebar.markdown(\"# Model\")\n",
      "confidence_threshold = st.sidebar.slider(\"Confidence threshold\", 0.0, 1.0, 0.5, 0.01)\n",
      "```\n",
      "\n",
      "There are now 3 main steps:\n",
      "- a function to download the image from the selected file\n",
      "- a function to apply the object detection on the image and plot the boxes\n",
      "- a selector on the sidemenu to pick the input image\n",
      "\n",
      "The first function is quick to implement :\n",
      "\n",
      "```python\n",
      "@st.cache(show_spinner=False)\n",
      "def read_img(img):\n",
      "    image = cv2.imread(img, cv2.IMREAD_COLOR)\n",
      "    image = image[:, :, [2, 1, 0]] # BGR -> RGB\n",
      "    return image\n",
      "```\n",
      "\n",
      "Then, build the function to identify the bounding boxes. The code is commented in order to understandd the key steps.\n",
      "\n",
      "```python\n",
      "def yolo_v3(image, confidence_threshold=0.5, overlap_threshold=0.3):\n",
      "\n",
      "\t# Load model architecture\n",
      "    net = cv2.dnn.readNetFromDarknet(\"yolov3.cfg\", \"yolov3.weights\")\n",
      "    output_layer_names = net.getLayerNames()\n",
      "    output_layer_names = [output_layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
      "\n",
      "    # Set input and get output\n",
      "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
      "    net.setInput(blob)\n",
      "    layer_outputs = net.forward(output_layer_names)\n",
      "\n",
      "    boxes, confidences, class_IDs = [], [], []\n",
      "    H, W = image.shape[:2]\n",
      "\n",
      "    # For each detected object, compute the box, find the score, ignore if below\n",
      "    for output in layer_outputs:\n",
      "        for detection in output:\n",
      "            scores = detection[5:]\n",
      "            classID = np.argmax(scores)\n",
      "            confidence = scores[classID]\n",
      "            if confidence > confidence_threshold:\n",
      "                box = detection[0:4] * np.array([W, H, W, H])\n",
      "                centerX, centerY, width, height = box.astype(\"int\")\n",
      "                x, y = int(centerX - (width / 2)), int(centerY - (height / 2))\n",
      "                boxes.append([x, y, int(width), int(height)])\n",
      "                confidences.append(float(confidence))\n",
      "                class_IDs.append(classID)\n",
      "\n",
      "    # Write the name of detected objects above image\n",
      "    f = open(\"classes.txt\", \"r\")\n",
      "    f = f.readlines()\n",
      "    f = [line.rstrip('\\n') for line in list(f)]\n",
      "    try:\n",
      "    \tst.subheader(\"Detected objects: \" + ', '.join(list(set([f[obj] for obj in class_IDs]))))\n",
      "    except IndexError:\n",
      "    \tst.write(\"Nothing detected\")\n",
      "\n",
      "    # Apply non-max suppression to identify best bounding box\n",
      "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, overlap_threshold)\n",
      "    xmin, xmax, ymin, ymax, labels = [], [], [], [], []\n",
      "    \n",
      "    if len(indices) > 0:\n",
      "        for i in indices.flatten():\n",
      "            x, y, w, h = boxes[i][0], boxes[i][1], boxes[i][2], boxes[i][3]\n",
      "            xmin.append(x)\n",
      "            ymin.append(y)\n",
      "            xmax.append(x+w)\n",
      "            ymax.append(y+h)\n",
      "    boxes = pd.DataFrame({\"xmin\": xmin, \"ymin\": ymin, \"xmax\": xmax, \"ymax\": ymax})\n",
      "\n",
      "    # Add a layer on top on a detected object \n",
      "\tLABEL_COLORS = [0, 255, 0]\n",
      "\timage_with_boxes = image.astype(np.float64)\n",
      "\tfor _, (xmin, ymin, xmax, ymax) in boxes.iterrows():\n",
      "\t\timage_with_boxes[int(ymin):int(ymax),int(xmin):int(xmax),:] += LABEL_COLORS\n",
      "\t\timage_with_boxes[int(ymin):int(ymax),int(xmin):int(xmax),:] /= 2\n",
      "\n",
      "\t# Display the final image\n",
      "\tst.image(image_with_boxes.astype(np.uint8), use_column_width=True)\n",
      "```\n",
      "\n",
      "Finally, let the user choose from several inout images such as car images, people, animals or a meeting, and run your pipeline on top of it.\n",
      "\n",
      "```python\n",
      "img_type = st.sidebar.selectbox(\"Select image type?\", ['Cars', 'People', 'Animals', \"Meeting\"])\n",
      "\n",
      "if img_type == 'People':\n",
      "    image_url = \"images/people.jpg\"\n",
      "elif img_type == 'Cars':\n",
      "    image_url = \"images/cars.jpg\"\n",
      "elif img_type == 'Animals':\n",
      "    image_url = \"images/animal.jpg\"\n",
      "elif img_type == 'Meeting':\n",
      "    image_url = \"images/meeting.jpg\"\n",
      "\n",
      "image = read_img(image_url)\n",
      "\n",
      "# Get the boxes for the objects detected by YOLO by running the YOLO model.\n",
      "yolo_v3(image, confidence_threshold)\n",
      "```\n",
      "\n",
      "Finally, to run your Streamlit app, supposing that you called your app \"app.py\", simply run :\n",
      "\n",
      "```bash\n",
      "streamlit run app.py\n",
      "```\n",
      "\n",
      "And you should be able to see this:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/screen_home.png)\n",
      "\n",
      "> **Conclusion** : This project is adapted from Streamlit's demo of Yolo. I tried to make the overall steps easier to understand, and executable on your own images.\n",
      "---\n",
      "published: false\n",
      "title: Practical introduction to Continual Learning (CL)\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Advanced Machine Learning\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "This article summarizes the talk given by `cnvrg.io` during their Webinar on Continual Learning.\n",
      "\n",
      "> **Continual learning** (CL) is the ability of a model to learn continually from a stream of data, updating the model in production to maintain performance and relevancy. It's in some way an autopilot mode for ML algorithms.\n",
      "\n",
      "The typical CL pipeline is the following :\n",
      "- Input Data\n",
      "- Data Validation\n",
      "- **AutoML & Hyperparameter optimization (HPO)**\n",
      "- Model Validation\n",
      "- **Model deployment**\n",
      "- Predictions \n",
      "- **Monitoring**\n",
      "- Cleaning & Labelling\n",
      "- Back to input data \n",
      "- ...\n",
      "\n",
      "We will explore a CL algorithm for MNIST data set, and focus on the elements of the pipeline above in bold.\n",
      "\n",
      "# AutoML & HPO\n",
      "\n",
      "The first step is to choose the right algorithms to use, and the range of hyper-parameters to explore. There are many options. If it's a computer vision, it is advised to use transfer learning on pre-trained ResNet VGG or Inception models. \n",
      "\n",
      "The idea here is to define a set of algorithms and hyperparameters among which we can automatically select the best model. We then train all the algorithms and keep track of each algorithm.\n",
      "\n",
      "# Deployment\n",
      "\n",
      "This AutoML part also requires that we have clusters available at all time and that our infrastructure is also automated (GCP, AWS + Kubernetes).\n",
      "\n",
      "The deployment should be progressive, and we should run tests before, during and after deployment, and define our benchmarks. Usually, for ML models, the deployment is done using the Canary Release technique, a technique to reduce the risk of introducing a new software version in production by slowly rolling out the change to a small subset of users, before making it gradually available to everybody.\n",
      "\n",
      "[Martin Fowler](https://martinfowler.com/bliki/CanaryRelease.html) provides a good illustration of this concept :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/canary.png)\n",
      "\n",
      "Regarding the deployment, it is recommended to use Kubernetes ([this](https://cnvrg.io/deploy-models-with-kubernetes/) tutorial explains it well).\n",
      "\n",
      "# Monitoring\n",
      "\n",
      "We should monitor :\n",
      "- our input data\n",
      "- our predictions\n",
      "\n",
      "For the input data, we should look for :\n",
      "- unexpected values\n",
      "- correlation between production and training data\n",
      "- new tests to add in production\n",
      "\n",
      "For the prediction monitoring, we need to watch the model confidence, the bias...\n",
      "\n",
      "# Retrain the model\n",
      "\n",
      "The ML pipeline should be triggered based on :\n",
      "- periodically (once a day, once a week...)\n",
      "- new training data coming in\n",
      "- model decay/alerts in production\n",
      "\n",
      "Since the retraining typically occurs automatically, it is essential to track and validate the triggers.\n",
      "\n",
      "\n",
      "> **Conclusion** : That's it ! I hope this introduction to Online Learning was clear. Don't hesitate to drop a comment if you have any question.\n",
      "---\n",
      "title: SoundMap, assistive device for blind and visually impaired\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "My team recently won the [International Create Challenge](https://www.createchallenge.org/) (ICC) in Martigny. We won both the AI 1st prize and the AI healthcare award for a total of 7'000 CHF, with a project called SoundMap. We made a small website that explains the solution: http://soundmap.ml/\n",
      "\n",
      "# What is SoundMap\n",
      "\n",
      "SoundMap is a smart wearable belt, equipped with a camera, able to provide real-time information on the surrounding environment of a person through Audio Augmented Reality (Audio AR).\n",
      "\n",
      "Similarly to the way we easily identify the position of whistling birds, we aim to scan in real-time the surrounding environment and produce directional sounds (through audio AR) in the earphones connected to the device. Blind and visually impaired people are, therefore, able to map and understand their environment.\n",
      "\n",
      "We aim to improve this prototype over the course of the next months, and we are looking for interested partners (universities, associations, blind or visually impaired).\n",
      "\n",
      "![](https://maelfabien.github.io/assets/images/soundmap1.jpg)\n",
      "\n",
      "![](https://maelfabien.github.io/assets/images/soundmap2.jpg)\n",
      "\n",
      "# Demonstration\n",
      "\n",
      "Here's a small demonstration of the device when trying to locate people in a room. The person had no prior information on the number of persons to find and their position. \n",
      "\n",
      "<iframe width=\"700\" height=\"500\" src=\"https://www.youtube.com/embed/854VI5L5lfE\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
      "\n",
      "<br>\n",
      "\n",
      "# How does it work?\n",
      "\n",
      "## 1. Object Recognition\n",
      "\n",
      "Our object recognition technologies relies on deep-learning to identify more than 20 classes of objects like chairs, tables, benches, bikes, cars... We process the images in real-time, up to 25 frames per second, using optimized hardware, a Raspberry Pi, and a simple camera. We estimate distance of objects, and their direction.\n",
      "\n",
      "![](https://maelfabien.github.io/assets/images/soundmap3.jpg)\n",
      "\n",
      "## 2. Sound Map creation\n",
      "\n",
      "Based on the detected objects, their angle and their distance, we generate a specific type of sound for each object. Each object is then given a sound, and the sound is played in the corresponding direction, with a volume that corresponds to the distance of the object.\n",
      "\n",
      "![](https://maelfabien.github.io/assets/images/soundmap4.jpg)\n",
      "\n",
      "## 3. Change of scene detection\n",
      "\n",
      "As soon as the scene observed by the camera has changed, a new scanning of the environment is triggered and the user can locate the surrounding objects again.\n",
      "\n",
      "# Why we built this?\n",
      "\n",
      "285 million of people in the world are blind or visually impaired, among which 39 million people are blind. In Switzerland, 20% of the elder suffer from visual impairment.\n",
      "\n",
      "The world is full of environment understanding softwares, but this softwares are mostly made for autonomous vehicles and robotics. We want to bring these technologies to blind and visually impaired people.\n",
      "\n",
      "# Working with us\n",
      "\n",
      "We are interested in working with associations for blind and visually impaired people, but also with hospitals and research centers, to collaborate on research projects, and improve the product. Also, if you or someone from you family is visually impaired, and would like to test the product, just send us an [email](mailto:mael.fabien@gmail.com?subject=[SoundMap]%20Question%20From%20Blog)\n",
      "---\n",
      "title: AutoHome, a tool to find your dream house\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "My girlfriend and I were recently looking for a house to buy. Rather than spending time on each of the real-estate websites individually, I decided to build a web application that scraps 5 of the most common real-estate agencies in the specific region of France we were looking at:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/autohome.png)\n",
      "\n",
      "It basically:\n",
      "- scraps 5 real-estate agencies in the North-West part of France\n",
      "- gathers the results in a single dataframe\n",
      "- displays and sorts the results on a Streamlit web application\n",
      "\n",
      "It mainly relies on:\n",
      "- BeautifulSoup\n",
      "- Streamlit\n",
      "\n",
      "## Features\n",
      "\n",
      "The application:\n",
      "- shows you details and pictures on houses from OuestFrance Immo and other real-estate agencies\n",
      "- has a filter on sea view\n",
      "- allows you to select a minimum and maximum budget\n",
      "- allows you to sort by date, price, self-determined score...\n",
      "- allows you to specify the amount of money you need to borrow, the interest rate, and computes your monthly payments\n",
      "- re-directs you to the source link with a simple click\n",
      "\n",
      "Cool things:\n",
      "- you can click on the \"Actualiser\" button, and it will re-scrap the whole set of websites (± 1mn)\n",
      "- otherwise, results are stored in a dataframe, which makes the navigation way faster\n",
      "\n",
      "## GitHub\n",
      "\n",
      "The Github repository can also be found here:\n",
      "\n",
      "<div class=\"github-card\" data-github=\"maelfabien/AutoHome\" data-width=\"100%\" data-height=\"\" data-theme=\"default\"></div>\n",
      "<script src=\"//cdn.jsdelivr.net/github-cards/latest/widget.js\"></script>\n",
      "\n",
      "To run it, simply use:\n",
      "\n",
      "```\n",
      "pip install -r requirements.txt\n",
      "```\n",
      "\n",
      "And launch the app via:\n",
      "\n",
      "```\n",
      "streamlit run app.py\n",
      "```\n",
      "\n",
      "\n",
      "---\n",
      "title: Speaker Verification using Gaussian Mixture Model (GMM-UBM)\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Speech Processing\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "The method introduced below is called GMM-UBM, which stands for Gaussian Mixture Model - Universal Background Model. This method has, for a long time, been a state-of-the-art approach.\n",
      "\n",
      "I will use as a reference the paper: \"A Tutorial on Text-Independent Speaker Verification\" by Frédétic Bimbot et al. Although from 2002, this tutorial describes this classical approach pretty well.\n",
      "\n",
      "This article requires that you have understood the [basics of Speaker Verification](https://maelfabien.github.io/machinelearning/basics_speech/).\n",
      "\n",
      "In this article, I will present the main steps of a Speaker Verification system.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/bs_1.png)\n",
      "\n",
      "The main steps of speaker verification are:\n",
      "- Development: learn speaker-idenpendent models using large amount of data. This is a pre-training part, called a Universal Background Model (UBM). It can be gender-specific, in the sense that we have 1 for Males, and 1 for Females.\n",
      "- Enrollment: learn distinct characteristics of a speaker’s voice. This step typically creates one model per unique speaker considered. This is the training part.\n",
      "- Verification: distinct characteristics of a claimant’s voice are compared with previously enrolled claimed speaker models. This is the prediction part.\n",
      "\n",
      "The first step is to extract features from the development set, enrollment set and verification set.\n",
      "\n",
      "# I. Speech acquisition and Feature extraction\n",
      "\n",
      "We should extract features from the signal to convert the raw signal into a sequence of acoustic feature vectures which we will use to identify the speaker. We make the assumption that each audio sample that we have contains only one speaker.\n",
      "\n",
      "Most speech features used in speaker verification rely on a cepstral representation of speech.\n",
      "\n",
      "## 1. Filterbank-based cepstral parameters (MFCC)\n",
      "\n",
      "### Pre-emphasis\n",
      "\n",
      "The first step is usually to apply a pre-emphasis of the signal to enhance the high frequencies of the spectrum, reduced by the speech production process:\n",
      "\n",
      "$$ x_p(t) = x(t) - a x(t-1) $$ \n",
      "\n",
      "Where $$ a $$ takes values between 0.95 and 0.98. \n",
      "\n",
      "### Framing\n",
      "\n",
      "The signal is then split into successive frames. Most of the time, a length of frame of 20 milliseconds is used, with a shift of 10 milliseconds. \n",
      "\n",
      "### Windowing\n",
      "\n",
      "Then, a windowing is applied. Indeed, when you cut your signal into frames, it is most likely that the end of a frame will not match the start of the next frame. Therefore, a windowing function is needed. The Hamming window is one of the most common approaches. Windowing also gives a more accurate idea of the original signal's frequency spectrum, as is \"cuts off\" signals at their end.\n",
      "\n",
      "The Hamming window is given by:\n",
      "\n",
      "$$ w[n]=a_{0}-\\underbrace {(1-a_{0})} _{a_{1}}\\cdot \\cos \\left({\\tfrac {2\\pi n}{N}}\\right),\\quad 0\\leq n\\leq N $$\n",
      "\n",
      "Where $$ a_0 = 0.53836 $$ is the optimal value.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hamming.png)\n",
      "\n",
      "All windowing functions can be found on [Wikipedia](https://en.wikipedia.org/wiki/Window_function).\n",
      "\n",
      "### Fast Fourier Transform (FFT)\n",
      "\n",
      "Then, a FFT algorithm is picked (most often Cooley–Tukey) to compute efficiently the Discrete Fourrier Transform (DFT):\n",
      "\n",
      "$$ X_{k}=\\sum _{n=0}^{N-1}x_{n}e^{-i2\\pi kn/N}\\qquad k=0,\\ldots ,N-1 $$ \n",
      "\n",
      "We typically make the computation on 512 points.\n",
      "\n",
      "### Modulus\n",
      "\n",
      "The absolute value of the FFT is then computed, which gives the magnitude. At that point, we have a *power spectrum* sampled over 512 points. However, since the spectrum is symmetric, only half of those points are useful. \n",
      "\n",
      "### Mel Filters\n",
      "\n",
      "The spectrum at that point has lots of fluctuations, and we don't need them. We need to apply a smoothing, which will reduce the size of the spectral vectors. We therefore multiply the spectrum by a filterbank, a series of bandpass frequency filters. \n",
      "\n",
      "Filters can be central, right or left, and defined by their shape (triangular most often). A common choice is the Bark/Mel scale for the frequency localization, a scale similar to the frequency scale of the human ear. A Mel is a unit of measure based on the human ears perceived frequency. It does not correspond linearly to the physical frequency of the tone, as the human auditory system apparently does not perceive pitch linearly. The Mel scale is approximately a linear frequency spacing below 1 kHz and a logarithmic spacing above 1 kHz. See more [here](https://link.springer.com/content/pdf/bbm%3A978-3-319-49220-9%2F1.pdf).\n",
      "\n",
      "$$ f_{MEL} = 2595 * \\log_{10} ( {1 + \\frac{f}{700}} ) $$\n",
      "\n",
      "Where $$ f $$ is the physical frequency in Hz, and $$ f_{MEL} $$ should be close to the percieved frequency.\n",
      "\n",
      "We can now compute the Mel spectrum of the magnitude spectrum:\n",
      "\n",
      "$$ s(m) = \\sum_{k=0}^{N-1} [ {\\mid X(k) \\mid}^2 H_m(k) ] $$\n",
      "\n",
      "Where $$ H_m(k) $$ is the weight given to the $$ k^{th} $$ energy spectrum bin contributing to the $$ m^{th} $$ output band.\n",
      "\n",
      "### Discrete Cosine Transform (DCT)\n",
      "\n",
      "Finally, we take the log of the spectrum and a Discrete Cosine Transform is applied. We obtain the Mel-Frequency Cepstral Coefficients (MFCC), and since most of the information is gathered in the first few coefficients, we can only select the first few ones (usually 12 or 20).\n",
      "\n",
      "$$ c_n = \\sum_{m=0}^{M-1} log_{10}(s(m)) cos(\\frac{\\pi}{M} n(m-\\frac{1}{2})) $$\n",
      "\n",
      "There we are, we obtained the MFCC coefficients describing the input signal window.\n",
      "\n",
      "### Cepstral Mean Substraction (CMS)\n",
      "\n",
      "Finally, and espacially in Speaker Verification tasks, the cepstral mean vector is substracted from each vector. This step is called Cepstral Mean Substraction (CMS) and removes slowly varying convolutive noises.\n",
      "\n",
      "### Cepstral mean variance normalization (CMVN)\n",
      "\n",
      "Cepstral mean variance normalization (CMVN) minimizes distortion by noise contamination for robust feature extraction by linearly transforming the cepstral coefficients to have the same segmental statistics (mean 0, variance 1).\n",
      "\n",
      "It is however known to degrade the performance of speaker verification tasks on short utterances.\n",
      "\n",
      "## 2. LPC-based cepstral parameters\n",
      "\n",
      "In Linear Predictive Coding (LPC) analysis, we represent the audio using the information of linear predictive models. We first split the input signal into the fundamental elements of the speech production apparatus:\n",
      "- the glottal source, (the space between the vocal folds) produces the buzz, characterizes the intensity (loudness) and frequency (pitch)\n",
      "- the vocal tract (the throat and mouth) forms the tube, which is characterized by its resonances\n",
      "- the nasal tract\n",
      "- the lips, generates hisses and pops\n",
      "\n",
      "And we model each of the with an Auto Regressive filter on each window. More specifically:\n",
      "- a lowpass filter for the glottal source\n",
      "- an AR filter for the vocal tract\n",
      "- an ARMA filter for the nasal tract\n",
      "- an MA filter for the lips\n",
      "\n",
      "Overall, the speech production process becomes an ARMA process, simplified in an AR process. We take each window and estimate the coefficients of an AR filter on the speech signal.\n",
      "\n",
      "$$ c_0 = ln(\\sigma^2) $$\n",
      "\n",
      "$$ c_m = a_m + \\sum_{k=1}^{m-1} (\\frac{k}{m}) c_k a_{m-k}, 1 ≤ m ≤ p $$\n",
      "\n",
      "$$ c_m = \\sum_{k=1}^{m-1} (\\frac{k}{m}) c_k a_{m-k}, p < m $$\n",
      "\n",
      "Where $$ \\sigma^2 $$ is the gain term in LPC model, $$ a_m $$ are the LPC coefficients and $$ p $$ the number of LPC coefficients calculated.\n",
      "\n",
      "There are many other features to extract, but MFCCs are the most frequent ones, LPC is sometimes used, so I won't dive deeper in this.\n",
      "\n",
      "## 3. Voice Activity Detection\n",
      "\n",
      "We might now want to discard useless information in the frames we extracted features for. We do so by removing frames that do not contain speech using Voice Activity Detection (VAD).\n",
      "\n",
      "A common approach is the Gaussian-based VAD, but one can also use the energy-based VAD. The aim of a VAD is to aquire speech only when it occurs. I described a bit further the concept and implementation of Voice Activity Detection in [this project](https://maelfabien.github.io/project/Speech_proj/#).\n",
      "\n",
      "The main steps behind building a VAD are:\n",
      "- Break audio signal into frames\n",
      "- Extract features from each frame\n",
      "- Train a classifier on a known set of speech and silence frames (could ba a gaussian model or a rule-based decision)\n",
      "- Classify unseen frames as speech or silence\n",
      "\n",
      "VAD performs well on audio with relatively low signal-to-noice ratio (SNR), a ratio which compares the level of a desired signal to the level of background noise.\n",
      "\n",
      "# II. Statistical Modeling\n",
      "\n",
      "The core of the speaker verification decision is the likelihood ratio. Say that we want to determine if speech sample Y (from the verification set) was spoken by S. \n",
      "\n",
      "Then, the verification task is a basic hypothesis testing:\n",
      "\n",
      "$$ H_0 $$ : Y is from speaker S\n",
      "$$ H_1 $$ : Y is not from speaker S\n",
      "\n",
      "The test to decide whether to accept $$ H_0 $$ or not is the Likelihood Ratio (LR):\n",
      "\n",
      "$$ LR = \\frac{p(Y \\mid H_0)}{p(Y \\mid H_1)} $$\n",
      "\n",
      "If the Likelihood ratio is greater than the threshold $$ \\theta $$, we accept $$ H_0 $$, otherwise we accept $$ H_1 $$. \n",
      "\n",
      "If we talk in terms of logs, then the log-likelihood ration is simply the difference between the logs of the 2 probability density functions:\n",
      "\n",
      "$$ \\log(LR) = \\log(p(Y \\mid H_0)) - \\log(p(Y \\mid H_1)) $$\n",
      "\n",
      "We have a speaker to test for $$ H_0 $$, and we can build a model, say $$ \\lambda_{hyp} $$, being for example a Gaussian Distribution of the features extracted.\n",
      "\n",
      "However, we do not have an alternative model for $$ H_1 $$. We must compute what is called a \"Background Model\", which would be a Gaussian Model $$ \\lambda_{\\overline{hyp}} $$.\n",
      "\n",
      "There are 2 options for the background model:\n",
      "- either consider the closed set of other speakers and compute: $$ p(X \\mid \\lambda_{\\overline{hyp}}) = f ( p(X \\mid \\lambda_1), ..., p(X \\mid \\lambda_N)) $$, where $$ f $$ is an aggregative function like the mean or the max. It however requires a model per alternative hypothesis, i.e. per speaker\n",
      "- or consider a pool of several different speakers to train a single model, called the Universal Background Model (UBM)\n",
      "\n",
      "The main advantage of the UBM is that it is *universal* in the sense that it can be used by any of the speakers, without having to re-train a model. \n",
      "\n",
      "The pipeline can be represented as such:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/bs_2.png)\n",
      "\n",
      "## 1. Universal Background Model : Development\n",
      "\n",
      "A UBM is a high-order Gaussian Mixture Model (usually 512 to 2048 mixtures with 24 dimensionsa) trained on a large quantity of speech, from a wide population. This step is used to learn speaker-independent distribution of features, used in the alternative hypothesis in the likelihood ratio.\n",
      "\n",
      "For a D-dimensional feature vector $$ x $$, the mixture density is:\n",
      "\n",
      "$$ P(x \\mid \\lambda) = \\sum_{k=1}^M w_k \\times g(x \\mid \\mu_k, \\Sigma_k) $$\n",
      "\n",
      "Where:\n",
      "- $$ x $$ is a D-dimensional feature vector\n",
      "- $$ w_k, k = 1, 2, ..., M $$ is the mixture weights s.t. they sum to 1\n",
      "- $$ \\mu_k, k = 1, 2, ..., M $$ is mean of each Gaussian\n",
      "- $$ \\Sigma_k, k = 1, 2, ..., M $$ is the covariance of each Gaussian\n",
      "- $$ g(x \\mid \\mu_k, \\Sigma_k) $$ are the Gaussian densities such that:\n",
      "\n",
      "$$ g(x \\mid \\mu_k, \\Sigma_k) = \\frac{1}{(2 \\pi)^{\\frac{D}{2}} {\\mid \\Sigma_k \\mid}^{\\frac{1}{2}}} exp^{ - \\frac{1}{2}(x - \\mu_k)^T \\Sigma_k^{-1} (x-\\mu_k)} $$\n",
      "\n",
      "The parameters of the GMM are therefore : $$ \\lambda = (w_k, \\mu_k, \\Sigma_k), k = 1, 2, 3, ..., M $$.\n",
      "\n",
      "We typically use a diagonal covariance-matrix rather than a full-covariance one since it is more computationally efficient and empirically works better.\n",
      "\n",
      "The GMM is trained on a collection of training vectors. The parameters of the GMM are computed iteratively using Expectation-Maximization (EM) algorithm, and therefore there are no guarantees that it will converce twice to the same solution depending on the initialization.\n",
      "\n",
      "Under assumption of idependent feature vectors, the log-likelihood of a model $$ \\lambda $$ for a sequence $$ X = (x_1, x_2, ..., x_T) $$ is simply the average over all feature vectors:\n",
      "\n",
      "$$ \\log p(X \\mid \\lambda) = \\frac{1}{T} \\sum_t \\log p(x_t \\mid \\lambda) $$\n",
      "\n",
      "## 2. Speaker Enrollment\n",
      "\n",
      "The last step before the verification is to perform the speaker enrollment. The aim is still to also train one Gaussian Mixture Model on the extracted features for each speaker, thus resulting in 20 models if we have 20 speakers.\n",
      "\n",
      "There are 2 approaches to model the speakers:\n",
      "- train a lower dimensional GMM (64 to 256 mixtures) depending on the amount of enrollment data that we have\n",
      "- adapt the UBM GMM to the speaker model using Maximum a Posteriori Adaptation (MAP), usually the approach selected\n",
      "\n",
      "In MAP, we simply start the EM algorithm with the parameters learned by the UBM. Through this step, we only adapt the mean, and not the covariance, since updating the covariance does not improve the performance.\n",
      "\n",
      "For the mean to update, we perform a *maximum a posteriori adaptation* :\n",
      "\n",
      "$$ \\mu_k^{MAP} = \\alpha_k \\mu_k + (1 - \\alpha_k) \\mu_k^{UBM} $$\n",
      "\n",
      "Where :\n",
      "- $$ \\alpha_k = \\frac{n_k}{n_k + \\tau_k} $$ is the mean adaptation coefficient\n",
      "- $$ n_k $$ is the count for the adaptation data\n",
      "- $$ \\tau_k $$ is the relevance factor, between 8 and 32\n",
      "\n",
      "## 3. Speaker Verification\n",
      "\n",
      "For a sample in the test folder, we compute the score of the claimed identity GMM in the enrollment set. We substract the score of the GMM of the UBM for each, and obtain the likelihood ratio. We then compare the score to our threshold (usually 0), and accept or decline the identity of the speaker.\n",
      "\n",
      "However, the scores might not always be independent from the speaker, and there might also be differences between the enrollment and the test data. For this reason, in litterature, there has been lots of research on score normalization. Among popular techniques:\n",
      "- cohort-based normalizations\n",
      "- centered impostor distribution\n",
      "- Znorm\n",
      "- Hnorm\n",
      "- Tnorm\n",
      "- HTnorm\n",
      "- Cnorm\n",
      "- Dnorm\n",
      "- WMAP\n",
      "\n",
      "# Limits of GMM-UBM\n",
      "\n",
      "Nowadays, GMM-UBM are not state-of-the-art approaches anymore. Indeed, it requires too much training data in general. Better performing approaches have been developed such as :\n",
      "- SVM-based methods\n",
      "- I-vector methods\n",
      "- Deep-learning based methods\n",
      "---\n",
      "title: Image formation and Filtering\n",
      "layout: post\n",
      "tags: [computervision]\n",
      "subtitle : \"Computer Vision\"\n",
      "---\n",
      "\n",
      "How is an image created? And what kind of filters can we apply to it? We'll try to answer those questions in this article.\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "For what comes next, we'll work a bit in Python. Import the following packages :\n",
      "\n",
      "```python\n",
      "import cv2\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "```\n",
      "\n",
      "# I. How is an image created?\n",
      "\n",
      "An image will always depend on :\n",
      "- lighting conditions\n",
      "- scene geometry\n",
      "- surface properties\n",
      "- camera optics\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_2.jpg)\n",
      "\n",
      "Using the imaging system, the photons that arrive at each cell are integrated and the digitized. An image appears as a grid of intensity values, corresponding to the value of each pixel.\n",
      "\n",
      "`0 = black, 255=white`\n",
      "\n",
      "An image can be compared to a function $$ f : R^2 → R $$ giving an intensity at each point $$ (x,y) $$.\n",
      "\n",
      "# II. Image filtering\n",
      "\n",
      "A filter can be seen as any kind of operator that can be applied to an image.\n",
      "\n",
      "Filtering is often used for :\n",
      "- image enhancement (denoise, resize...)\n",
      "- extract information (texture, edges)\n",
      "- detect patterns (template matching)\n",
      "\n",
      "I'll use a picture I have taken recently, but feel free to use any picture you'd like as long as there is some kind of color change or contrast in the image. \n",
      "\n",
      "```python\n",
      "img = cv2.imread('vision_6.jpg')\n",
      "\n",
      "b,g,r = cv2.split(img)\n",
      "img = cv2.merge([r,g,b])\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.imshow(img)\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_8.jpg)\n",
      "\n",
      "## 1. Noise reduction\n",
      "\n",
      "A first way to reduce noise is to average out several noised images, but it typically requires a lot of images, which we generally do not have. We need to use smoothing filters to overcome this issue.\n",
      "\n",
      "Smoothing filters have several properties :\n",
      "- all values are positive\n",
      "- they all sum to 1\n",
      "- the amount of smoothing is proportional to the mask size\n",
      "- it removes high-frequency components\n",
      "\n",
      "### a. Averaging\n",
      "\n",
      "*Intuition *: Replace each pixel by the average of its neighbors. This assumes that neighboring pixels are similar and the noise is independent across pixels.\n",
      "\n",
      "We define a filter size and apply a convolutional filter (moving average) on the input image.\n",
      "\n",
      "Here's a small animation of the filter evolution :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Convolution_schematic.gif)\n",
      "\n",
      "On an image, the process leads to the following averaged image :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_3.jpg)\n",
      "\n",
      "The filter, when in size $$ 2 \\times 2 $$ can be expressed as :\n",
      "\n",
      "$$ \\frac {1} {4} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} $$\n",
      "\n",
      "This is already implemented in OpenCV. We'll use a kernel size of $$ 15 \\times 15 $$ :\n",
      "\n",
      "```python\n",
      "blur = cv2.blur(img,(5,5))\n",
      "\n",
      "plt.figure(figsize=(15,12))\n",
      "\n",
      "plt.subplot(121)\n",
      "plt.imshow(img)\n",
      "plt.title('Original')\n",
      "\n",
      "plt.subplot(122)\n",
      "plt.imshow(blur)\n",
      "plt.title('Blurred')\n",
      "\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_9.jpg)\n",
      "\n",
      "Another way to compute smoothing is to apply a Median Filtering, in which we take the median of all pixels within the kernel window. This is also really easy in OpenCV :\n",
      "\n",
      "```python\n",
      "median = cv2.medianBlur(img,5)\n",
      "```\n",
      "\n",
      "### b. Linear Filtering \n",
      "\n",
      "*Intuition *: This involved a weighted combination of pixels in the small neighborhood of the pixel we want to estimate.\n",
      "\n",
      "$$ g(i,j) = \\sum_{k,l} f(i+k, j+l) h (k,l) $$\n",
      "\n",
      "We call $$ h(k,I) $$ a kernel, or mask, and the problem can be rewritten using the correlation/convolution operator :\n",
      "$$ G = H ⊗ F $$ .\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_5.jpg)\n",
      "\n",
      "In Python :\n",
      "\n",
      "```python\n",
      "ddepth = -1 \n",
      "\n",
      "# Define the kernel\n",
      "kernel_size = 15\n",
      "kernel = np.ones((kernel_size, kernel_size), dtype=np.float32)\n",
      "kernel /= (kernel_size * kernel_size)\n",
      "\n",
      "dst = cv2.filter2D(img, ddepth, kernel)\n",
      "\n",
      "plt.figure(figsize=(15,12))\n",
      "\n",
      "plt.subplot(121)\n",
      "plt.imshow(img)\n",
      "plt.title('Original')\n",
      "\n",
      "plt.subplot(122)\n",
      "plt.imshow(dst)\n",
      "plt.title('Blurred')\n",
      "\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_10.jpg)\n",
      "\n",
      "### c. Gaussian Filtering\n",
      "\n",
      "*Intuition *: We might want the pixels closer to the center to have more influence on the output. For this reason, we apply Gaussian filtering. It also removes high-frequency components from the image. \n",
      "\n",
      "The Gaussian kernel is defined as :\n",
      "\n",
      "$$ h(u,v) = \\frac {1} {2 \\pi \\sigma^2 } e^{ - \\frac {u^2 + v^2} {\\sigma^2}} $$\n",
      "\n",
      "The Gaussian Filtering is highly efficient at removing Gaussian noise in an image.\n",
      "\n",
      "We can choose the size of the kernel or mask, and the variance, which determines the extent of smoothing.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_11.jpg)\n",
      "\n",
      "In Python, Gaussian Filtering can be implemented using OpenCV :\n",
      "\n",
      "```python\n",
      "blur = cv2.GaussianBlur(img,(15,15),10)\n",
      "\n",
      "plt.figure(figsize=(15,12))\n",
      "\n",
      "plt.subplot(121)\n",
      "plt.imshow(img)\n",
      "plt.title('Original')\n",
      "\n",
      "plt.subplot(122)\n",
      "plt.imshow(blur)\n",
      "plt.title('Blurred')\n",
      "\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_12.jpg)\n",
      "\n",
      "## 2. Convolution filters\n",
      "\n",
      "### a. Convolution vs. Correlation\n",
      "\n",
      "Convolution and Correlation are slightly different operations :\n",
      "\n",
      "1. Correlation :\n",
      "\n",
      "$$ g(i,j) = \\sum_{k,l} f(i + k, j + l) h(k,l) $$\n",
      "\n",
      "$$ G = H ⊗ F $$ \n",
      "\n",
      "2. Convolution :\n",
      "\n",
      "$$ g(i,j) = \\sum_{k,l} f(i - k, j - l) h(k,l) $$\n",
      "\n",
      "$$ G = H * F $$ \n",
      "\n",
      "### b. Examples of convolutions\n",
      "\n",
      "a. Do nothing \n",
      "![image](https://maelfabien.github.io/assets/images/vision_13.jpg)\n",
      "\n",
      "b. Shift the image\n",
      "![image](https://maelfabien.github.io/assets/images/vision_14.jpg)\n",
      "\n",
      "c. Sharpen an image\n",
      "![image](https://maelfabien.github.io/assets/images/vision_15.jpg)\n",
      "\n",
      "In Python, simply use :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "\n",
      "plt.imshow(2*img - blur)\n",
      "plt.title('Sharpened image')\n",
      "\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_16.jpg)\n",
      "\n",
      "d. Find only the details of an image\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_17.jpg)\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "\n",
      "plt.imshow(img - blur)\n",
      "plt.title('Edges only')\n",
      "\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "And apply this to our image :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_18.jpg)\n",
      "\n",
      "The convolution is both commutative and associative. The Fourier transform of two convolved images is the product of their Fourier transforms. Both correlation and convolution are linear shift-invariant operators. \n",
      "\n",
      "### c. Separable convolution\n",
      "\n",
      "A convolution requires $$ K^2 $$ operations per pixel, where $$ K $$ is the size of the convolution kernel.\n",
      "\n",
      "In many cases, this operation can be accelerated by first performing a 1D horizontal convolution followed by a 1D vertical convolution, requiring 2K operations. If this is possible, then the convolution kernel is called **separable** and is the outer product of 2 kernels: $$ K = vh^T $$.\n",
      "\n",
      "A Kernel is separable if, in its Singular Value Decomposition (SVD), only *one* singular value is non-zero.\n",
      "\n",
      "## 3. Edge Detection\n",
      "\n",
      "### a. What is an edge?\n",
      "\n",
      "In edge detection, we map image from a 2d array to a set of curves or line segments. We look for strong gradients. An edge is a place of rapid change in the image intensity function.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_20.jpg)\n",
      "\n",
      "What causes an edge?\n",
      "- reflectance change: appearance, information, texture\n",
      "- change in surface orientation: shape\n",
      "- shadows\n",
      "- depth discontinuity: object boundary\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_19.jpg)\n",
      "\n",
      "### b. Image gradient\n",
      "\n",
      "How do we compute the derivative of a digital image $$ F(x,y) $$ with convolution ?\n",
      "- Using the continuous image : $$ \\frac {\\delta (x,y)} {\\delta x} = {lim}_{ε→0} \\frac {f(x+ε,y) - f(x)} {ε} $$\n",
      "- Take discrete derivative  $$ \\frac {\\delta (x,y)} {\\delta x} ≈ \\frac {f[x+1,y] - f[x]} {1} $$\n",
      "\n",
      "The gradient of an image is :\n",
      "\n",
      "$$ \\Delta f = [ \\frac {\\delta f} {\\delta x} \\frac {\\delta f} {\\delta y} ] $$\n",
      "\n",
      "The gradient points in the direction of most rapid change in intensity. The partial derivative with respect to $$ x $$ is the horizontal direction, and the one with respect to $$ y $$ is the vertical direction.\n",
      "\n",
      "The gradient direction is given by :\n",
      "\n",
      "$$ \\theta = {tan}^{-1} ( \\frac {\\delta f} {\\delta y} / \\frac {\\delta f} {\\delta x}) $$\n",
      "\n",
      "The edge strength is given by the magnitude :\n",
      "\n",
      "$$ \\mid \\mid \\Delta f \\mid \\mid = \\sqrt {  (\\frac {\\delta f} {\\delta x})^2  (\\frac {\\delta f} {\\delta y})^2 } $$\n",
      "\n",
      "Let's consider a single row. We can plot the intensity as a function of position. This gives a signal.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_21.jpg)\n",
      "\n",
      "To identify the effect of noise, we smooth first and we look for picks in the differentiation of the smoothed original signal :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_22.jpg)\n",
      "\n",
      "The filter we can apply in Python can be of various types :\n",
      "- Sobel derivatives (X or Y), a joint Gaussian smoothing plus differentiation operation in which we can specify the direction (vertical, horizontal) of the derivatives. \n",
      "- Band-pass filters such as Laplacian derivatives, obtained by convolving with a Gaussian filter and which filter low and high frequencies\n",
      "\n",
      "```python\n",
      "laplacian = cv2.Laplacian(img,-3,ksize=9)\n",
      "sobelx = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=5)\n",
      "sobely = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=5)\n",
      "\n",
      "plt.figure(figsize=(15,12))\n",
      "\n",
      "plt.subplot(2,2,1),plt.imshow(img,cmap = 'gray')\n",
      "plt.title('Original')\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.imshow(laplacian,cmap = 'gray')\n",
      "plt.title('Laplacian')\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.imshow(sobelx,cmap = 'gray')\n",
      "plt.title('Sobel X')\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.imshow(sobely,cmap = 'gray')\n",
      "plt.title('Sobel Y')\n",
      "\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_23.jpg)\n",
      "\n",
      "### c. Find edges with Canny Edge Detector\n",
      "\n",
      "A common way to find edges is to use the Canny Edge Detector :\n",
      "- filter an image with a derivative of Gaussian\n",
      "- find magnitude and orientation of the gradient\n",
      "- non-maximum suppression: check if a pixel is a local maximum along gradient direction (requires interpolation)\n",
      "- define 2 thresholds, low and high, and use the high to start edge curves and the low one to continue them\n",
      "\n",
      "```python\n",
      "img = cv2.imread('vision_6.jpg', 0)\n",
      "\n",
      "edges = cv2.Canny(img,300,300)\n",
      "plt.figure(figsize=(15,12))\n",
      "\n",
      "plt.subplot(121)\n",
      "plt.imshow(img,cmap = 'gray')\n",
      "plt.title('Original Image')\n",
      "\n",
      "plt.subplot(122)\n",
      "plt.imshow(edges,cmap = 'gray')\n",
      "plt.title('Edge Image')\n",
      "\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_24.jpg)\n",
      "\n",
      "Canny edge detector does however not work that well on images such as this one, in which we have several trees and a lot of different textures.\n",
      "\n",
      "> **Conclusion** : I hope this article on image filtering was helpful. Don't hesitate to drop a comment if you have any question.\n",
      "---\n",
      "title: The Rosenblatt's Perceptron\n",
      "layout: post\n",
      "tags: [deeplearning]\n",
      "subtitle : \"Deep Neural Networks\"\n",
      "---\n",
      "In this series of articles, I am going to focus on the basis of Deep Learning, and progressively move toward recent research papers and more advanced techniques. As I am particularly interested in computer vision, I will explore some examples applied to object detection or emotion recognition for example.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# History of Deep Learning\n",
      "\n",
      "[Favio Vázquez](https://medium.com/@faviovazquez) has created a great summary of the deep learning timeline :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ros_1.jpg)\n",
      "\n",
      "Among the most important events on this timeline, I would highlight :\n",
      "- 1958: the Rosenblatt's Perceptron\n",
      "- 1974: Backpropagation\n",
      "- 1985: Boltzmann Machines\n",
      "- 1986: MLP, RNN\n",
      "- 2012: Dropout\n",
      "- 2014: GANs\n",
      "\n",
      "# Why neurons?\n",
      "\n",
      "Neuronal networks have been at the core of the development of Deep Learning these past years. But what is the link between a neuron biologically speaking and a deep learning algorithm?\n",
      "\n",
      "Neural networks are a set of algorithms that have been developed imitate the human brain in the way we identify patterns. In neurology, researchers study the way we process information. We have outstanding abilities to process information quickly and extract patterns. \n",
      "\n",
      "Take a quick example: we can process information pre-attentively. Indeed, in less time than an eye blink (200ms), we can identify elements that pop out from an image. On the other hand, if the element does not pop out enough, we need to make a sequential search, which is much longer.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/preattentive.jpg)\n",
      "\n",
      "The information that we process in this example allows us to make a binary classification (major class vs the outlier we're trying to identify). To understand what's going on, I'll make a brief introduction (to the extent of my limited knowledge in this field) to the architecture of a neuron biologically speaking.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/neuron.jpg)\n",
      "\n",
      "Here's what the different components are made for :\n",
      "- Dendrite: Receives signals from other neurons\n",
      "- Soma: Processes the information\n",
      "- Axon: Transmits the output of a neuron\n",
      "- Synapse: Point of connection to other neurons\n",
      "\n",
      "A neuron takes an input signal (dendrite), processes the information (soma) and passes the output to other connected neurons (axon to synapse to other neuron’s dendrite). \n",
      "\n",
      "Now, this might be biologically inaccurate as there is a lot more going on out there but on a higher level, this is what is going on with a neuron in our brain — takes an input, processes it, throws out an output.\n",
      "\n",
      "Suppose that you are walking on a crosswalk and want to determine whether there is a dangerous situation or not. The information to process might be : \n",
      "- visual, e.g. a car is close\n",
      "- audio, e.g. the sound of the car, a klaxon...\n",
      "\n",
      "A series of neurons will process the information. Intrinsically, using both channels, you will :\n",
      "- determine how close the car is\n",
      "- and how fast the car is going\n",
      "\n",
      "The neurons are activated depending on the given criteria. This will eventually lead to some sort of binary classification: Is there a danger or not? During the information processing, a large number of neurons will activate sequentially, and eventually lead to a single output.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/neuron_2.jpg)\n",
      "\n",
      "This is an overly simplified representation, and I don't have sufficient knowledge to expand this section.\n",
      "\n",
      "# The McCulloch-Pitts Neuron (1943)\n",
      "\n",
      "The first computational model of a neuron was proposed by Warren McCulloch and Walter Pitts in 1943. We'll cover this first simple model as an introduction to the Rosenblatt's Perceptron. \n",
      "\n",
      "How does the McCulloch-Pitts neuron work?\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/neuron_3.jpg)\n",
      "\n",
      "The first part is to process a series of **boolean** inputs (just like dendrites). If an input takes the value 1, we say that neuron **fires**.\n",
      "\n",
      "We then process the information into an aggregative function `g` (can be compared to Soma) that performs a simple aggregation of the values of each input. Then, the function `f` compares the output of `g` to a threshold or a condition.\n",
      "\n",
      "We can make several algorithms with this :\n",
      "- OR: the `f` function checks if the sum `g` is equal to 1\n",
      "- AND: the `f` function checks if the sum `g` is equal to the number of inputs\n",
      "- GREATER THAN: the `f` function checks if the sum `g` is equal to a threshold $$ \\theta $$\n",
      "\n",
      "The simplest binary classification can be achieved the following way :\n",
      "\n",
      "$$ y = 1 $$ if $$ \\sum_i x_i ≥ 0 $$, else $$ y = 0 $$\n",
      "\n",
      "There are however several limitations to McCulloch-Pitts Neurons : \n",
      "- it cannot process non-boolean inputs\n",
      "- it gives equal weights to each input\n",
      "- the threshold $$ \\theta $$ much be chosen by hand\n",
      "- it implies a linearly separable underlying distribution of the data\n",
      "\n",
      "For all these reasons, a necessary upgrade was required.\n",
      "\n",
      "# The Rosenblatt's Perceptron (1957)\n",
      "\n",
      "## The classic model\n",
      "\n",
      "The Rosenblatt's Perceptron was designed to overcome most issues of the McCulloch-Pitts neuron :\n",
      "- it can process non-boolean inputs\n",
      "- and it can assign different weights to each input automatically\n",
      "- the threshold  $$ \\theta $$ is computed automatically\n",
      "\n",
      "A perceptron is a single layer Neural Network. A perceptron can simply be seen as a set of inputs, that are weighted and to which we apply an activation function. This produces sort of a weighted sum of inputs, resulting in an output. This is typically used for classification problems, but can also be used for regression problems.\n",
      "\n",
      "The perceptron was first introduced in 1957 by Franck Rosenblatt. Since then, it has been the core of Deep Learning. We can represent schematically a perceptron as :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/neuron_4.jpg)\n",
      "\n",
      "We attach to each input a weight ( $$w_i$$) and notice how we add an input of value 1 with a weight of $$ - \\theta $$. This is called bias. What we are doing is instead of having only the inputs and the weight and compare them to a threshold, we also learn the threshold as a weight for a standard input of value 1.\n",
      "\n",
      "The inputs can be seen as neurons and will be called the **input layer**. Altogether, these neurons and the function (which we'll cover in a minute) form a **perceptron**.\n",
      "\n",
      "How do we make classification using a perceptron then?\n",
      "\n",
      "$$ y = 1 $$ if $$ \\sum_i w_i x_i ≥ 0 $$, else $$ y = 0 $$\n",
      "\n",
      "One limitation remains: the inputs need to be linearly separable since we split the input space into two halves. \n",
      "\n",
      "## Minsky and Papert (1969)\n",
      "\n",
      "The version of Perceptron we use nowadays was introduced by Minsky and Papert in 1969. They bring a major improvement to the classic model: they introduced an activation function. The **activation function** might take several forms and should \"send\" the weighted sum into a smaller set of possible values that allows us to classify the output. It's a smoother version than the thresholding applied before.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/neuron_5.jpg)\n",
      "\n",
      "In the classical Rosenblatt's perceptron, we split the space into two halves using a HeavySide function (sign function) where the vertical split occurs at the threshold $$ \\theta $$ :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Signum_function.svg.jpg)\n",
      "\n",
      "This is harsh (since an outcome of 0.49 and 0.51 lead to different values), and we cannot apply gradient descent on this function. For this reason, for binary classification, for example, we'll tend to use a sigmoid activation function. Using a sigmoid activation will assign the value of a neuron to either 0 if the output is smaller than 0.5, or 1 if the neuron is larger than 0.5. The sigmoid function is defined by : $$ f(x) = \\frac {1} {1 + e^{-u}} $$\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/sigmoid.jpg)\n",
      "\n",
      "This activation function is smooth, differentiable (allows back-propagation) and continuous. We don't have to output a 0 or a 1, but we can output probabilities to belong to a class instead. If you're familiar with it, this version of the perceptron is a logistic regression with 0 hidden layers.\n",
      "\n",
      "## Some details\n",
      "\n",
      "A given observation can be either well classified, or in the wrong class. As in most optimization problems, we want to minimize the cost, i.e the sum of the individual losses on each training observation. A pseudo-code corresponding to our problem is :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/pseudo.jpg)\n",
      "\n",
      "In the most basic framework of Minsky and Papert perceptron, we consider essentially a classification rule than can be represented as :\n",
      "\n",
      "$$ g(x) = sig({\\alpha + \\beta^tx}) $$\n",
      "\n",
      "where :\n",
      "- the bias term is $$ {\\alpha} $$\n",
      "- the weights on each neuron is $$ {\\beta} $$\n",
      "- the activation function is sigmoid, denoted as $$ sig $$.\n",
      "\n",
      "We need to apply a stochastic gradient descent. The perceptron \"learns\" how to adapt the weights using backpropagation. The weights and bias are firstly set randomly, and we compute an error rate. Then, we proceed to backpropagation to adjust the parameters that we did not correctly identify, and we start all over again for a given number of epochs.\n",
      "\n",
      "We will further detail the concepts of stochastic gradient descent and backpropagation in the context of Multilayer Perceptron.\n",
      "\n",
      "Even the Minsky and Papert perceptron has a major drawback. If the categories are linearly separable for example, it identifies a single separating hyper-plane without taking into account the notion of margin we would like to maximize. This problem is solved by the Support Vector Machine (SVM) algorithm.\n",
      "\n",
      "## Logical operators\n",
      "\n",
      "Perceptron can be used to represent logical operators. For example, one can represent the perceptron as an \"AND\" operator.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_1.jpg)\n",
      "\n",
      "A simple \"AND\" perceptron can be built in the following way :\n",
      "\n",
      "```\n",
      "weight1 = 1.0\n",
      "weight2 = 1\n",
      "bias = -1.2\n",
      "\n",
      "linear_combination = weight1 * input_0 + weight2 * input_1 + bias\n",
      "output = int(linear_combination >= 0)\n",
      "```\n",
      "\n",
      "Where `input_0` and `input_1` represent the two feature inputs. We are shifting the bias by 1.2 to isolate the positive case where both inputs are 1.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_2.jpg)\n",
      "\n",
      "However, solving the XOR problem is impossible :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_3.jpg)\n",
      "\n",
      "This is why Multi-layer perceptrons were introduced.\n",
      "\n",
      "# Implementation in Keras\n",
      "\n",
      "In Keras, it is extremely easy to build a Perceptron :\n",
      "\n",
      "```python\n",
      "from keras.models import Sequential\n",
      "model = Sequential()\n",
      "model.add(Dense(1, input_dim=6, activation='sigmoid'))\n",
      "\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "model.fit(X_train, y_train, epochs=20,batch_size=128)\n",
      "```\n",
      "\n",
      "# Implementation in Tensorflow\n",
      "\n",
      "Using the famous MNIST database as an example, a perceptron can be built the following way in Tensorflow. This simple application heads an accuracy of around 80 percents. This example is taken from the book: \"Deep Learning for Computer Vision\" by Dr. Stephen Moore, which I recommend. The following code is in Tensorflow 1 :\n",
      "\n",
      "```python\n",
      "# Imports\n",
      "from tensorflow.examples.tutorials.mnist import input_data\n",
      "import tensorflow as tf\n",
      "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
      "\n",
      "input_size = 784 # (28*28 flattened images)\n",
      "no_classes = 10\n",
      "batch_size = 100\n",
      "total_batches = 200\n",
      "\n",
      "x_input = tf.placeholder(tf.float32, shape=[None, input_size])\n",
      "y_input = tf.placeholder(tf.float32, shape=[None, no_classes])\n",
      "weights = tf.Variable(tf.random_normal([input_size, no_classes]))\n",
      "bias = tf.Variable(tf.random_normal([no_classes]))\n",
      "\n",
      "logits = tf.matmul(x_input, weights) + bias\n",
      "\n",
      "softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_input, logits=logits)\n",
      "loss_operation = tf.reduce_mean(softmax_cross_entropy)\n",
      "optimiser = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss_operation)\n",
      "```\n",
      "\n",
      "Then create and run the training session :\n",
      "```python\n",
      "session = tf.Session()\n",
      "session.run(tf.global_variables_initializer())\n",
      "\n",
      "for batch_no in range(total_batches):\n",
      "    mnist_batch = mnist_data.train.next_batch(batch_size)\n",
      "    _, loss_value = session.run([optimiser, loss_operation], feed_dict={\n",
      "        x_input: mnist_batch[0],\n",
      "        y_input: mnist_batch[1]\n",
      "    })\n",
      "    print(loss_value)\n",
      "```\n",
      "\n",
      "And compute the accuracy on the test images :\n",
      "\n",
      "```python\n",
      "predictions = tf.argmax(logits, 1)\n",
      "correct_predictions = tf.equal(predictions, tf.argmax(y_input, 1))\n",
      "accuracy_operation = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
      "\n",
      "test_images, test_labels = mnist_data.test.images, mnist_data.test.labels\n",
      "accuracy_value = session.run(accuracy_operation, feed_dict={\n",
      "    x_input: test_images,\n",
      "    y_input: test_labels\n",
      "})\n",
      "\n",
      "print('Accuracy : ', accuracy_value)\n",
      "session.close()\n",
      "```\n",
      "\n",
      "This heads an accuracy of around `80%` which can be largely improved by the next techniques we are going to cover.\n",
      "\n",
      "> **Conclusion** : Next step, we are going to explore the Multilayer Perceptron!\n",
      "\n",
      "Sources :\n",
      "- Télécom Paris, IP Paris Lecture on Perceptron\n",
      "- https://towardsdatascience.com/mcculloch-pitts-model-5fdf65ac5dd1\n",
      "- https://towardsdatascience.com/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a\n",
      "- https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d\n",
      "\n",
      "---\n",
      "title: Python Interpreter\n",
      "layout: post\n",
      "author_profile: false\n",
      "classes: wide\n",
      "---\n",
      "\n",
      "<iframe src=\"https://trinket.io/embed/python/a537edfe04\" width=\"100%\" height=\"600\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" allowfullscreen></iframe>\n",
      "\n",
      "---\n",
      "title: Easy Question Answering with AllenNLP\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "AllenNLP is an Apache 2.0 NLP research library, built on PyTorch, for developing state-of-the-art deep learning models on a wide variety of linguistic tasks. AllenNLP is built and maintained by the Allen Institute for Artificial Intelligence, in close collaboration with researchers at the University of Washington and elsewhere.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# What is Question Answering?\n",
      "\n",
      "Question Answering (QA), or Machine Comprehension (MC) aims to answer a query about a given context by modeling the interactions between both context and queries. Typical approaches in QA rely on attention mechanismes, in order to focus on a small part of the text and summarize it with a fixed-size vector.\n",
      "\n",
      "AllenNLP implements a pre-trained Bi-Directional Attention Flow (BIDAF). This network is a multi-stage hierarchical process that represents the context at different levels of granularity and uses bidirectional attention flow mechanism to obtain a query-aware context representation without early summarization. This approach was published in 2017 by the Allen Institute for Artificial Intelligence in [this paper](https://arxiv.org/pdf/1611.01603.pdf).\n",
      "\n",
      "According to the original paper, the steps of the BIDAF are the following:\n",
      "- Character Embedding Layer maps each word to a vector space using character-level CNNs.\n",
      "- Word Embedding Layer maps each word to a vector space using a pre-trained word embedding model.\n",
      "- Contextual Embedding Layer utilizes contextual cues from surrounding words to refine\n",
      "the embedding of the words. These first three layers are applied to both the query and context.\n",
      "- Attention Flow Layer couples the query and context vectors and produces a set of queryaware feature vectors for each word in the context.\n",
      "- Modeling Layer employs a Recurrent Neural Network to scan the context.\n",
      "- Output Layer provides an answer to the query.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/bidaf.png)\n",
      "\n",
      "# Using AllenNLP's library\n",
      "\n",
      "To install AllenNLP, simply run : \n",
      "\n",
      " ```bash\n",
      " pip install allennlp\n",
      " ```\n",
      "\n",
      "If you have SpaCy installed and encounter some issues with your current SpaCy version, I encourage you to switch to version 2.1.8 of SpaCy and downgrade the `en_core_news` package to version 2.1.0. This worked out for me. We'll deploy this pre-trained QA algorithm on a Streamlit web application. To do so, import the following packages:\n",
      "\n",
      "```python\n",
      "from allennlp import pretrained\n",
      "import streamlit as st\n",
      "import seaborn as sns\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "```\n",
      "\n",
      "Then, start to build the application:\n",
      "\n",
      "```python\n",
      "st.title(\"Question Answering\")\n",
      "\n",
      "# Avoids loading the model each time in streamlit\n",
      "# Loads the model\n",
      "model = st.cache(\n",
      "       pretrained.bidirectional_attention_flow_seo_2017,\n",
      "       allow_output_mutation=True\n",
      ")()\n",
      "```\n",
      "\n",
      "Once the pre-trained model has been loaded, we can run it on the Wikipedia article of Netflix:\n",
      "\n",
      "```python\n",
      "passage = st.text_area(\"Article\", \"\"\"Netflix, Inc. is an American media-services provider and production company headquartered in Los Gatos, California, founded in 1997 by Reed Hastings and Marc Randolph in Scotts Valley, California. The company's primary business is its subscription-based streaming service which offers online streaming of a library of films and television programs, including those produced in-house. As of April 2019, Netflix had over 148 million paid subscriptions worldwide, including 60 million in the United States, and over 154 million subscriptions total including free trials. It is available worldwide except in mainland China (due to local restrictions), Syria, North Korea, and Crimea (due to US sanctions). The company also has offices in the Netherlands, Brazil, India, Japan, and South Korea. Netflix is a member of the Motion Picture Association (MPA). Netflix's initial business model included DVD sales and rental by mail, but Hastings abandoned the sales about a year after the company's founding to focus on the initial DVD rental business. Netflix expanded its business in 2010 with the introduction of streaming media while retaining the DVD and Blu-ray rental business. The company expanded internationally in 2010 with streaming available in Canada, followed by Latin America and the Caribbean. Netflix entered the content-production industry in 2012, debuting its first series Lilyhammer. Since 2012, Netflix has taken more of an active role as producer and distributor for both film and television series, and to that end, it offers a variety of \"Netflix Original\" content through its online library. By January 2016, Netflix services operated in more than 190 countries. Netflix released an estimated 126 original series and films in 2016, more than any other network or cable channel. Their efforts to produce new content, secure the rights for additional content, and diversify through 190 countries have resulted in the company racking up billions in debt: $21.9 billion as of September 2017, up from $16.8 billion from the previous year. $6.5 billion of this is long-term debt, while the remaining is in long-term obligations. In October 2018, Netflix announced it would raise another $2 billion in debt to help fund new content.\"\"\")\n",
      "```\n",
      "\n",
      "We must then define a question to ask:\n",
      "\n",
      "```python\n",
      "question = st.text_input(\"Question\", \"Where are the headquarters of Netflix?\")\n",
      "```\n",
      "\n",
      "We compute the result easily through this function:\n",
      "\n",
      "```python\n",
      "result = model.predict(question, passage)\n",
      "```\n",
      "\n",
      "From the result, we want the \"best_span\", \"question_tokens\", and \"passage_tokens\" which contain respectively the position of the answer, a tokenized version of the question and a tokenized version of the article/passage.\n",
      "\n",
      "```python\n",
      "start, end = result[\"best_span\"]\n",
      "question_tokens = result[\"question_tokens\"]\n",
      "passage_tokens = result[\"passage_tokens\"]\n",
      "```\n",
      "\n",
      "In order to display the result, we will only pick the 10 words before and the 10 words after the answer. We also display in bold the exact words which contain the answer:\n",
      "\n",
      "```python\n",
      "mds = [f\"**{token}**\" if start <= i <= end else token if start - 10 <= i <= end + 10 else \"\" for i, token in enumerate(passage_tokens)]\n",
      "st.markdown(\" \".join(mds))\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/qa_1.png)\n",
      "\n",
      "At that moment, the web application works well but the explainability remains limited. In order to imporve that, we can plot the attention layers. The X-axis represents the question, and the Y-axis represents the input text. The darker the column, the most important the attention is in this area. We notice that words such as \"When\" or \"Where\" play a big word in the attention layers, since they expect a date or a place in return.\n",
      "\n",
      "```python\n",
      "attention = result[\"passage_question_attention\"]\n",
      "\n",
      "plt.figure(figsize=(12,12))\n",
      "sns.heatmap(attention, cmap=\"YlGnBu\")\n",
      "plt.autoscale(enable=True, axis='x')\n",
      "plt.xticks(np.arange(len(question_tokens)), labels=question_tokens)\n",
      "st.pyplot()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/qa_2.png)\n",
      "\n",
      "> **Conclusion** : I hope that this introduction was useful. I simply wanted to demonstrate how easy it can be to create a small QA web service.\n",
      "---\n",
      "title: Emotion Recognition WebApp\n",
      "layout: post\n",
      "tags: [project]\n",
      "show_subtitles : false\n",
      "---\n",
      "\n",
      "We developped a multimodal emotion recognition platform to analyze the emotions of job candidates, in partnership with the French Employment Agency.\n",
      "\n",
      "We analyze facial, vocal and textual emotions, using mostly deep learning based approaches.\n",
      "\n",
      "# Real-Time Multimodal Emotion Recognition\n",
      "\n",
      "## In a nutshell\n",
      "\n",
      "We deployed a web app using Flask :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/PE_Images/webapp2.png)\n",
      "\n",
      "We have also written a [paper on our work](https://www.overleaf.com/read/xvtrrfpvzwhf)\n",
      "\n",
      "In this project, we are exploring state of the art models in multimodal sentiment analysis. We have chosen to explore text, sound and video inputs and develop an ensemble model that gathers the information from all these sources and displays it in a clear and interpretable way.\n",
      "\n",
      "*Contributors : Raphael Lederman, Anatoli De Bradke, Stéphane Reynal, Maël Fabien*\n",
      "\n",
      "The Github of the project can be found here :\n",
      "\n",
      "<div class=\"github-card\" data-github=\"maelfabien/Multimodal-Emotion-Recognition\" data-width=\"100%\" data-height=\"\" data-theme=\"default\"></div>\n",
      "<script src=\"//cdn.jsdelivr.net/github-cards/latest/widget.js\"></script>\n",
      "\n",
      "<br>\n",
      "\n",
      "## Technologies\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/PE_Images/techno.png)\n",
      "\n",
      "## I. Context\n",
      "\n",
      "Affective computing is a field of Machine Learning and Computer Science that studies the recognition and the processing of human affects.\n",
      "Multimodal Emotion Recognition is a relatively new discipline that aims to include text inputs, as well as sound and video. This field has been rising with the development of social network that gave researchers access to a vast amount of data.\n",
      "\n",
      "\n",
      "## II. Data Sources\n",
      "We have chosen to diversify the data sources we used depending on the type of data considered. All data sets used are free of charge and can be directly downloaded.\n",
      "- For the text input, we are using the **Stream-of-consciousness** dataset that was gathered in a study by Pennebaker and King [1999]. It consists of a total of 2,468 daily writing submissions from 34 psychology students (29 women and 5 men whose ages ranged from 18 to 67 with a mean of 26.4). The writing submissions were in the form of a course unrated assignment. For each assignment, students were expected to write a minimum of 20 minutes per day about a specific topic. The data was collected during a 2-week summer course between 1993 to 1996. Each student completed their daily writing for 10 consecutive days. Students’ personality scores were assessed by answering the Big Five Inventory (BFI) [John et al., 1991]. The BFI is a 44-item self-report questionnaire that provides a score for each of the five personality traits. Each item consists of short phrases and is rated using a 5-point scale that ranges from 1 (disagree strongly) to 5 (agree strongly). An instance in the data source consists of an ID, the actual essay, and five classification labels of the Big Five personality traits. Labels were originally in the form of either yes (‘y’) or no (‘n’) to indicate scoring high or low for a given trait.\n",
      "- For audio data sets, we are using the **Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)**. This database contains 7356 files (total size: 24.8 GB). The database contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity(normal, strong), with an additional neutral expression. All conditions are avail-able in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video(720p H.264, AAC 48kHz, .mp4), and Video-only (no sound).” [here](https://zenodo.org/record/1188976#.XCx-tc9KhQI)\n",
      "- For the video data sets, we are using the popular **FER2013** Kaggle Challenge data set. The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. The data set remains quite challenging to use, since there are empty pictures, or wrongly classified images. [here](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data)\n",
      "\n",
      "## III. Download\n",
      "\n",
      "| Modality | Data | Processed Data (for training) | Pre-trained Model | Colab Notebook | Other |\n",
      "|:--------:|:----:|:-----------------------------:|:-----------------:|:--------------:|:-----:|\n",
      "| Text | [here](https://drive.google.com/file/d/1bbbn8kSBmcVObafdzAQEipRBc4SVqwtb/view?usp=sharing) | [X-train](https://drive.google.com/file/d/1sgxv40PkrzxqFCfoaVGyTxpSMvgzRyyx/view?usp=sharing) [y-train](https://drive.google.com/file/d/1iL4N_k2501fGb6WiDLECvDQaJrCVzOjV/view?usp=sharing) [X-test](https://drive.google.com/file/d/1ez38Be4__hA0quIrcLkiSl_zAl_HXktL/view?usp=sharing) [y-test](https://drive.google.com/file/d/1G0Tm9Vq5UoJcdQw0q11xgV4dnrP8JiPI/view?usp=sharing) | [Weights](https://drive.google.com/file/d/1XpFAMykCdmphzMw9umS21f8ITf5QwVRg/view?usp=sharing) [Model](https://drive.google.com/file/d/1mXn3poSmg0chYGXKNB7gjFl50E10kuU2/view?usp=sharing) | ---  | --- |\n",
      "| Audio | [here](https://drive.google.com/file/d/1OL2Kx9dPdeZWoue6ofHcUNs5jwpfh4Fc/view?usp=sharing) | [X-train](https://drive.google.com/file/d/1qv-y0FhaRy5Np8DF3a8Xty8xLvvv4QH4/view?usp=sharing) [y-train](https://drive.google.com/file/d/1y5j43I09Xe6RHK8BsHP8_ZNkUuTehhgY/view?usp=sharing) [X-test](https://drive.google.com/file/d/1MN1Fxc_sDR1ZDQmPdFMwlnhP4qn9d8bT/view?usp=sharing) [y-test](https://drive.google.com/file/d/1ovvCXumkEP1oLxErgMgyIg1Z1Eih430W/view?usp=sharing)| [Weights](https://drive.google.com/file/d/1pQ5QahXJ3dPDXhyPkQ7rS1fOHWKHcIdX/view?usp=sharing) [Model](https://drive.google.com/file/d/1TuKN2PbFvoClaobL3aOW1KmA0e2eEc-O/view?usp=sharing) | [Colab Notebook](https://colab.research.google.com/drive/1EY8m7uj3BzU-OsjAPGBqoapw1OSUHhum)  | --- |\n",
      "| Video | [here](https://drive.google.com/file/d/1hWqVdOYNvCuioiDk-CBgMtKOgl05aA--/view?usp=sharing) | [X-train](https://drive.google.com/file/d/14xs-0nZNQuuMdtTOwqcJQm_GZ_rTO8mB/view?usp=sharing) [y-train](https://drive.google.com/file/d/1EX5KkPquwpHD9ZKpTxGhk3_RFmVDD8bf/view?usp=sharing) [X-test](https://drive.google.com/file/d/1TFH3kvGDS0iWjqKYo3lZuIu65I9h0LYr/view?usp=sharing) [y-test](https://drive.google.com/file/d/1HTzGc_J4kTQRFvLIvcMQA3mt6PnyNT53/view?usp=sharing) |  [Weights](https://drive.google.com/file/d/1-L3LnxVXv4vByg_hqxXMZPvjKSQ12Ycs/view?usp=sharing) [Model](https://drive.google.com/file/d/1_dpHN9L6hsQYzTX2zk9K5JF2CZ1FOcZh/view?usp=sharing) | [Colab Notebook](https://colab.research.google.com/drive/1dV1IvYLV24vXGvyzMFNAA18csu8btV2-) | [Face Detect Model](https://drive.google.com/file/d/18YMrAStwXbN-aPZ45ylNrdAXQQPJx0Hd/view?usp=sharing) |\n",
      "\n",
      "## IV. Methodology\n",
      "Our aim is to develop a model able to provide a live sentiment analysis with a visual user interface.Therefore, we have decided to separate two types of inputs :\n",
      "- Textual input, such as answers to questions that would be asked to a person from the platform\n",
      "- Video input from a live webcam or stored from an MP4 or WAV file, from which we split the audio and the images\n",
      "\n",
      "### a. Text Analysis\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/PE_Images/text_app.png)\n",
      "\n",
      "#### Pipeline\n",
      "\n",
      "The text-based personality recognition pipeline has the following structure :\n",
      "- Text data retrieving\n",
      "- Custom natural language preprocessing :\n",
      "- Tokenization of the document\n",
      "- Cleaning and standardization of formulations using regular expressions\n",
      "- Deletion of the punctuation\n",
      "- Lowercasing the tokens\n",
      "- Removal of predefined *stopwords*\n",
      "- Application of part-of-speech tags on the remaining tokens\n",
      "- Lemmatization of tokens using part-of-speech tags for more accuracy.\n",
      "- Padding the sequences of tokens of each document to constrain the shape of the input vectors.\n",
      "- 300-dimension **Word2Vec** trainable embedding\n",
      "- Prediction using our pre-trained model\n",
      "\n",
      "#### Model\n",
      "\n",
      "We have chosen a neural network architecture based on both one-dimensional convolutional neural networks and recurrent neural networks.\n",
      "The one-dimensional convolution layer plays a role comparable to feature extraction : it allows finding patterns in text data. The Long-Short Term Memory cell is then used in order to leverage on the sequential nature of natural language : unlike regular neural network where inputs are assumed to be independent of each other, these architectures progressively accumulate and capture information through the sequences. LSTMs have the property of selectively remembering patterns for long durations of time.\n",
      "Our final model first includes 3 consecutive blocks consisting of the following four layers : one-dimensional convolution layer - max pooling - spatial dropout - batch normalization. The numbers of convolution filters are respectively 128, 256 and 512 for each block, kernel size is 8, max pooling size is 2 and dropout rate is 0.3.\n",
      "Following the three blocks, we chose to stack 3 LSTM cells with 180 outputs each. Finally, a fully connected layer of 128 nodes is added before the last classification layer.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/PE_Images/text_pipeline.png)\n",
      "\n",
      "### b. Audio Analysis\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/PE_Images/audio_app.png)\n",
      "\n",
      "#### Pipeline\n",
      "\n",
      "The speech emotion recognition pipeline was built the following way :\n",
      "- Voice recording\n",
      "- Audio signal discretization\n",
      "- Log-mel-spectrogram extraction\n",
      "- Split spectrogram using a rolling window\n",
      "- Make a prediction using our pre-trained model\n",
      "\n",
      "#### Model\n",
      "\n",
      "The model we have chosen is a **Time Distributed Convolutional Neural Network**.\n",
      "\n",
      "The main idea of a **Time Distributed Convolutional Neural Network** is to apply a rolling window (fixed size and time-step) all along the log-mel-spectrogram.\n",
      "Each of these windows will be the entry of a convolutional neural network, composed by four Local Feature Learning Blocks (LFLBs) and the output of each of these convolutional networks will be fed into a recurrent neural network composed by 2 cells LSTM (Long Short Term Memory) to learn the long-term contextual dependencies. Finally, a fully connected layer with *softmax* activation is used to predict the emotion detected in the voice.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/PE_Images/sound_pipeline.png)\n",
      "\n",
      "To limit overfitting, we tuned the model with :\n",
      "- Audio data augmentation\n",
      "- Early stopping\n",
      "- And kept the best model\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/PE_Images/Accuracy_Speech.png)\n",
      "\n",
      "### c. Video Analysis\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/PE_Images/video_app.png)\n",
      "\n",
      "#### Pipeline\n",
      "\n",
      "The video processing pipeline was built the following way :\n",
      "- Launch the webcam\n",
      "- Identify the face by Histogram of Oriented Gradients\n",
      "- Zoom on the face\n",
      "- Dimension the face to `48 * 48` pixels\n",
      "- Make a prediction on the face using our pre-trained model\n",
      "- Also identify the number of blinks on the facial landmarks on each picture\n",
      "\n",
      "#### Model\n",
      "\n",
      "The model we have chosen is an **XCeption** model, since it outperformed the other approaches we developed so far. We tuned the model with :\n",
      "- Data augmentation\n",
      "- Early stopping\n",
      "- Decreasing learning rate on plateau\n",
      "- L2-Regularization\n",
      "- Class weight balancing\n",
      "- And kept the best model\n",
      "\n",
      "As you might have understood, the aim was to limit overfitting as much as possible in order to obtain a robust model.\n",
      "\n",
      "- To know more on how we prevented overfitting, check [this article](https://maelfabien.github.io/deeplearning/regu/)\n",
      "- To know more on the **XCeption** model, check [this article](https://maelfabien.github.io/deeplearning/xception/)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/PE_Images/model_fit.png)\n",
      "\n",
      "The XCeption architecture is based on DepthWise Separable convolutions that allow to train much fewer parameters, and therefore reduce training time on Colab's GPUs to less than 90 minutes.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/PE_Images/video_pipeline2.png)\n",
      "\n",
      "When it comes to applying CNNs in real life application, being able to explain the results is a great challenge. We can indeed  plot class activation maps, which display the pixels that have been activated by the last convolution layer. We notice how the pixels are being activated differently depending on the emotion being labeled. The happiness seems to depend on the pixels linked to the eyes and mouth, whereas the sadness or the anger seem for example to be more related to the eyebrows.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/PE_Images/light.png)\n",
      "\n",
      "### d. Ensemble Model\n",
      "\n",
      "The ensemble model has not been implemented on this version.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/PE_Images/ensemble_pipeline.png)\n",
      "\n",
      "## V. How to use it ?\n",
      "\n",
      "There are several resources available :\n",
      "- the working notebooks can be found in the Text/Video/Audio sections\n",
      "- the final notebooks can be accessed through the Google Colab link in the table at the beginning\n",
      "\n",
      "To use the web app :\n",
      "- Clone the project locally\n",
      "- Go in the WebApp folder\n",
      "- Run `$ pip install -r requirements.txt`\n",
      "- Launch `python app.py`\n",
      "\n",
      "## VI. Research Paper\n",
      "\n",
      "If you are interested in the research paper, here it is :\n",
      "\n",
      "<embed src=\"https://maelfabien.github.io/assets/images/PE.pdf\" type=\"application/pdf\" width=\"600px\" height=\"500px\" />\n",
      "\n",
      "## VII. Contributors\n",
      "\n",
      "<table><tr><td align=\"center\">\n",
      "<a href=\"https://github.com/Anatoli-deBRADKE\">\n",
      "<img src=\"https://avatars1.githubusercontent.com/u/43547776?v=4\" width=\"100px;\" alt=\"Anatoli-deBRADKE\"/>\n",
      "<br />\n",
      "<sub><b>Anatoli-deBRADKE</b></sub>\n",
      "</a><br /><a href=\"https://github.com/maelfabien/Multimodal-Emotion-Recognition/commits?author=Anatoli-deBRADKE\" title=\"Code\">💻</a></td>\n",
      "<td align=\"center\"><a href=\"https://github.com/maelfabien\"><img src=\"https://avatars0.githubusercontent.com/u/24256555?v=4\" width=\"100px;\" alt=\"mfix22\"/>\n",
      "<br /><sub><b>maelfabien</b></sub>\n",
      "</a><br /><a href=\"https://github.com/maelfabien/Multimodal-Emotion-Recognition/commits?author=maelfabien\" title=\"Code\">💻</a></td>\n",
      "<td align=\"center\"><a href=\"https://github.com/RaphaelLederman\"><img src=\"https://avatars2.githubusercontent.com/u/38351531?v=4\" width=\"100px;\" alt=\"mfix22\"/>\n",
      "<br /><sub><b>RaphaelLederman</b></sub>\n",
      "</a><br /><a href=\"https://github.com/maelfabien/Multimodal-Emotion-Recognition/commits?author=RaphaelLederman\" title=\"Code\">💻</a></td>\n",
      "<td align=\"center\"><a href=\"https://github.com/STF-R\"><img src=\"https://avatars0.githubusercontent.com/u/43505879?v=4\" width=\"100px;\" alt=\"mfix22\"/>\n",
      "<br /><sub><b>STF-R</b></sub>\n",
      "</a><br /><a href=\"https://github.com/maelfabien/Multimodal-Emotion-Recognition/commits?author=STF-R\" title=\"Code\">💻</a></td>\n",
      "</tr></table>\n",
      "\n",
      "---\n",
      "title: Hidden Markov Model (HMM)\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Markov Processes and HMM\"\n",
      "---\n",
      "\n",
      "So far, we covered Markov Chains. Now, we'll dive into more complex models: Hidden Markov Models.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "{% highlight matlab %}\n",
      "{% endhighlight %}\n",
      "\n",
      "Hidden Markov Models (HMM) are widely used for :\n",
      "- speech recognition\n",
      "- writing recognition\n",
      "- object or face detection\n",
      "- part-of-speech tagging and other NLP tasks...\n",
      "\n",
      "I recommend checking the introduction made by Luis Serrano on HMM on [YouTube](https://www.youtube.com/watch?v=kqSzLo9fenk)\n",
      "\n",
      "We will be focusing on Part-of-Speech (PoS) tagging. Part-of-speech tagging is the process by which we can tag a given word as being a noun, pronoun, verb, adverb... \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_12.jpg)\n",
      "\n",
      "PoS can, for example, be used for Text to Speech conversion or Word sense disambiguation.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_13.jpg)\n",
      "\n",
      "In this specific case, the same word `bear` has completely different meanings, and the corresponding PoS is therefore different. \n",
      "\n",
      "Let's consider the following scenario. In your office, 2 colleagues talk a lot. You know they either talk about **Work** or **Holidays**. Since they look cool, you'd like to join them. But you're too far to understand the whole conversation, and you only get some words of the sentence\n",
      "\n",
      "Before joining the conversation, in order not to sound too weird, you'd like to guess whether he talks about **Work** or **Holidays**. For example, here is the kind of sentence your friends might be pronouncing :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_14.jpg)\n",
      "\n",
      "## Emission probabilities\n",
      "\n",
      "You only hear distinctively the words **python** or **bear**, and try to guess the context of the sentence. Since your friends are Python developers, when they talk about work, they talk about Python 80% of the time.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_15.jpg)\n",
      "\n",
      "These probabilities are called the Emission probabilities.\n",
      "\n",
      "## Transition probabilities\n",
      "\n",
      "You listen to their conversations and keep trying to understand the subject every minute. There is some sort of coherence in the conversation of your friends. Indeed, if one hour they talk about work, there is a lower probability that the next minute they talk about holidays.\n",
      "\n",
      "We can define what we call the Hidden Markov Model for this situation :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_16.jpg)\n",
      "\n",
      "The probabilities to change the topic of the conversation or not are called the transition probabilities.\n",
      "\n",
      "- The words you understand are called the *observations* since you observe them. Logic.\n",
      "- The subject they talk about is called the hidden state since you can't observe it\n",
      "\n",
      "## Discrete Hidden Markov Models\n",
      "\n",
      "An HMM $$ \\lambda $$ is a sequence made of a combination of 2 stochastic processes :\n",
      "- an observed one : $$ O = o_1, o_2, ..., o_T $$, here the words\n",
      "- a hidden one : $$ q = q_1, q_2, ... q_T $$, here the topic of the conversation. This is called the state of the process.\n",
      "\n",
      "A HMM model is defined by :\n",
      "- the vector of initial probabilities $$ \\pi = [ \\pi_1, ... \\pi_q ] $$, where $$ \\pi_i = P(q_1 = i) $$\n",
      "- a transition matrix for unobserved sequence $$ A $$ : $$ A = [a_{ij}] = P(q_t  = j \\mid q_{t-1} = j) $$\n",
      "- a matrix of the probabilities of the observations $$ B = [b_{ki}] = P(o_t = s_k \\mid q_t = i) $$ \n",
      "\n",
      "What are the main hypothesis behind HMMs ?\n",
      "- independence of the observations conditionally to the hidden states : $$ P(o_1, ..., o_t, ..., o_T \\mid q_1, ..., q_t, ..., q_T, \\lambda) = \\prod_i P(o_t \\mid q_t, \\lambda) $$\n",
      "- the stationary Markov Chain : $$ P(q_1, q_2, ..., q_T) = P(q_1) P(q_2 \\mid q_1) P(q_3 \\mid q_2) ... P(q_T \\mid q_{T-1}) $$\n",
      "- Joint probability for a sequence of observations and states : $$ P(o_1, o_2, ... o_T, q_1, ..., q_T \\mid \\lambda) = P(o_1, ..., o_T  \\mid q_1, ..., q_T, \\lambda) P(q_1, ..., q_T) $$\n",
      "\n",
      "An HMM is a subcase of Bayesian Networks.\n",
      "\n",
      "## How can we find the transition probabilities?\n",
      "\n",
      "They are based on the observations we have made. We can suppose that after carefully listening, every minute, we manage to understand the topic they were talking about. This does not give us the full information on the topic they are currently talking about though.\n",
      "\n",
      "You have 15 observations, taken over the last 15 minutes, **W** denotes Work and **H**  Holidays.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_17.jpg)\n",
      "\n",
      "We notice that in 2 cases out of 5, the topic Work lead to the topic Holidays, which explains the transition probability in the graph above.\n",
      "\n",
      "## How can we find the emission probabilities?\n",
      "\n",
      "Well, since we have observations on the topic they were discussing, and we observe the words that were used during the discussion, we can define estimates of the emission probabilities :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_18.jpg)\n",
      "\n",
      "## What is the probability for each topic at a random minute?\n",
      "\n",
      "Suppose that you have to grab a coffee, and when you come back, they are still talking. You have no clue what they are talking about! What is at that random moment the probability that they are talking about Work or Holidays?\n",
      "\n",
      "We can count from the previous observations: 10 times they were talking about Holidays, 5 times about Work. Therefore, it states that we have $$ \\frac {1} {3} $$ chance that they talk about Work, and $$ \\frac {2} {3} $$ chance that they talk about Holidays.\n",
      "\n",
      "## If you hear the word \"Python\", what is the probability of each topic?\n",
      " \n",
      " If you hear the word \"Python\", the probability that the topic is Work or Holidays is defined by Bayes Theorem!\n",
      " \n",
      " $$ P(Work \\mid Python) = \\frac { P(Python \\mid Work) P(Work) } {P(Python)} $$\n",
      " \n",
      " We can replace the probabilities :\n",
      " \n",
      " $$ P(Work \\mid Python) = \\frac { 0.8 \\times \\frac {1} {3} } { 0.3 \\times \\frac {2} {3} + 0.8 \\times \\frac {1} {3}} $$\n",
      " \n",
      " Which heads to 57%.\n",
      " \n",
      "## If you hear a sequence of words, what is the probability of each topic?\n",
      "\n",
      "Let's start with 2 observations in a row. Let's suppose that we hear the words \"Python\" and \"Bear\" in a row. What are the possible combinations?\n",
      "- Python was linked to Work, Bear was linked to work\n",
      "- Python was linked to Holidays, Bear was linked to work\n",
      "- Python was linked to Holidays, Bear was linked to Holidays\n",
      "- Python was linked to Work, Bear was linked to Holidays\n",
      "\n",
      "These scenarios can be summarized this way :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_19.jpg)\n",
      " \n",
      "Therefore, the most likely hidden states are Holidays and Holidays. What if you hear more than 2 words? Let's say 50? It becomes challenging to compute all the possible paths! This is why the Viterbi Algorithm was introduced, to overcome this issue.\n",
      " \n",
      "## Decoding with Viterbi Algorithm\n",
      " \n",
      "The main idea behind the Viterbi Algorithm is that when we compute the optimal *decoding* sequence, we don't keep all the potential paths, but only the path corresponding to the maximum likelihood. \n",
      " \n",
      " Here's how it works. We start with a sequence of observed events, say `Python, Python, Python, Bear, Bear, Python`. This sequence corresponds simply to a sequence of observations : $$ P(o_1, o_2, ..., o_T \\mid \\lambda_m) $$.\n",
      " \n",
      " ![image](https://maelfabien.github.io/assets/images/hmm_20.jpg)\n",
      " \n",
      "For the first observation, the probability that the subject is Work given that we observe Python is the probability that it is Work times the probability that it is Python given that it is Work.\n",
      "\n",
      "The most likely sequence of states simply corresponds to : $$ \\hat{m} = argmax_m P(o_1, o_2, ..., o_T \\mid \\lambda_m) $$\n",
      " \n",
      "We can then move on to the next day. Here's what will happen :\n",
      " \n",
      "![image](https://maelfabien.github.io/assets/images/hmm_21.jpg)\n",
      " \n",
      "For each position, we compute the probability using the fact that the previous topic was either Work or Holidays, and for each case, we only keep the maximum since we aim to find the maximum likelihood. Therefore, the next step is to estimate the same thing for the Holidays topic and keep the maximum between the 2 paths.\n",
      " \n",
      "![image](https://maelfabien.github.io/assets/images/hmm_22.jpg)\n",
      "   \n",
      "If you decode the whole sequence, you should get something similar to this (I've rounded the values, so you might get slightly different results) :\n",
      "   \n",
      "![image](https://maelfabien.github.io/assets/images/hmm_23.jpg)\n",
      " \n",
      " The most likely sequence when we observe `Python, Python, Python, Bear, Bear, Python` is, therefore `Work, Work, Work, Holidays, Holidays, Holidays`. \n",
      " \n",
      "If you finally go talk to your colleagues after such a long stalking time, you should expect them to be talking about holidays :)\n",
      "\n",
      "Let's go a little deeper in the Viterbi Algorithm and formulate it properly.\n",
      "\n",
      "The joint probability of the best sequence of potential states ending in-state $$ i $$ at time $$ t $$ and corresponding to observations $$ o_1, ..., o_T $$ is denoted by $$ \\delta_T(i) $$. This is one of the potential paths described above.\n",
      "\n",
      "$$ \\delta_T(i) = max_{q_1, ..., q_{t+1}} P(q_1, ... q_t = i, o_1, ..., o_t, o_{t+1} \\mid \\lambda) $$\n",
      "\n",
      "By recursion, it can be shown that :\n",
      "\n",
      "$$ \\delta_{t+1}(j) = b_j(o_{t+1}) max_i a_{ij} \\delta_t{i} $$\n",
      "\n",
      "Where $$ b_j $$ denotes a probability of the matrix of observations $$ B $$ and $$ a_{ij} $$ denotes a value of the transition matrix for unobserved sequence. Those parameters are estimated from the sequence of observations and states available. The $$ \\delta $$ is simply the maximum we take at each step when moving forward.\n",
      "\n",
      "I won't go into further details here. You should simply remember that there are 2 ways to solve Viterbi, forward (as we have seen) and backward.\n",
      "\n",
      "When we only observe partially the sequence and face incomplete data, the EM algorithm is used.\n",
      "\n",
      "## Generating a sequence\n",
      "\n",
      "As we have seen with Markov Chains, we can generate sequences with HMMs. In order to do so, we need to :\n",
      "- generate first the hidden state $$ q_1 $$ then $$ o_1 $$, e.g Work then Python\n",
      "- then generate the transition $$ q_1 $$ to $$ q_2 $$\n",
      "- then from $$ q_2 $$, generate $$ o_2 $$\n",
      "\n",
      "How does the process work? As stated above, this is now a 2 step process, where we first generate the state, then the observation.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_24.jpg)\n",
      " \n",
      "> **Conclusion** : I hope this was clear enough! HMMs are interesting topics, so don't hesitate to drop a comment!\n",
      "---\n",
      "title: Text Preprocessing\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Natural Language Processing\"\n",
      "---\n",
      "\n",
      "One of the main challenges, when dealing with text, is to build an efficient preprocessing pipeline.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "{% highlight matlab %}\n",
      "{% endhighlight %}\n",
      "\n",
      "# I. What is preprocessing?\n",
      "\n",
      "Preprocessing in Natural Language Processing (NLP) is the process by which we try to \"standardize\" the text we want to analyze.\n",
      "\n",
      "A challenge that arises pretty quickly when you try to build an efficient preprocessing NLP pipeline is the diversity of the texts you might deal with :\n",
      "- tweets that would be highly informal\n",
      "- cover letters from candidates in an HR company\n",
      "- Slack messages within a team\n",
      "- Even code sometimes if you try to analyze Github comments for example\n",
      "\n",
      "The diversity makes the whole thing tricky. Usually, a given pipeline is developed for a certain kind of text. The pipeline should give us a \"clean\" text version.\n",
      "\n",
      "Another challenge that arises when dealing with text preprocessing is the language. The English language remains quite simple to preprocess. German or french use for example much more special characters like \"é, à, ö, ï\". \n",
      "\n",
      "You might now wonder what are the main steps of preprocessing?\n",
      "- A first step is to remove words that are made of special characters (if needed in your case): `@,#, /,!.\\'+-= `\n",
      "- In English, some words are short versions of actuals words, e.g \"I'm\" for \"I am\". To treat them as separate words, you'll need to split them.\n",
      "- We then would like to remove specific syntax linked to our text extraction, e.g \"\\n\" every time there is a new line\n",
      "- Remove the stop words, which are mainstream words like \"the, I, would\"...\n",
      "- Once this step is done, we are ready to tokenize the text, i.e split by word\n",
      "- To make sure that the words \"Shoe\" and \"shoe\" are later understood as the same, lower case the tokens\n",
      "- Lemmatize the tokens to extract the \"root\" of each word.\n",
      "\n",
      "The process can be illustrated in the following way :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_1.jpg)\n",
      "\n",
      "## Tokenization\n",
      "\n",
      "Given a sequence of characters, tokenization aims to cut the sentence into pieces, called tokens. Tokenization consists of splitting large chunks of text into sentences, and sentences into a list of single words also called tokens. This step also referred to as segmentation or lexical analysis, is necessary to perform further processing.\n",
      "\n",
      "Consider a really simple example :\n",
      "\n",
      "```\n",
      "My name is Paul.\n",
      "```\n",
      "\n",
      "There are different ways to tokenize this :\n",
      "- either we consider all tokens indifferently: ```(My, name, is, Paul, .)``` (Notice that we keep the dot)\n",
      "- either consider that each token must be a word: ```(My, name, is, Paul)```\n",
      "- or by removing stop words before tokenizing: ```(name, Paul)```\n",
      "\n",
      "There are many tricky cases when it comes to tokenizing :\n",
      "- Compound words, i.e. at the end of a sentence in a book, separated by a dash. You might want to keep these words together.\n",
      "- Dates, which are not easy to process: `01.03.2019`. Should you extract the date textually before? As often, it depends on your needs.\n",
      "-  Contractions such as \"I'm\" or \"aren't\" should also be considered, since they typically contain 2 pieces of information\n",
      "- Named entities such as \"Los Angeles\" which should be considered as the same word in tokenization\n",
      "\n",
      "There is no unique good tokenization technique. The methods mentioned above all make mistakes. \n",
      "\n",
      "Some extensions include :\n",
      "- Heuristic-based methods, containing a large vocabulary, but hardly handle unknown words\n",
      "- Using Machine Learning models (Hidden Markov Models, Conditional Random Fields, Recurrent Neural Networks...)\n",
      "\n",
      "## Part of Speech (PoS) tagging\n",
      "\n",
      "PoS tagging is the task that attributes grammatical categories to a given token. The aim is to detect Nouns, Verbs, Adjectives, Adverbs...\n",
      "\n",
      "This might be useful to detect :\n",
      "- noun phrases\n",
      "- phrases\n",
      "- end of sentences\n",
      "- ...\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/pos.jpg)\n",
      "\n",
      "The 2 main types of methods for this task are :\n",
      "- based on linguistic expertise, but it's hardly scaling to new vocabulary and noisy inputs and it's time-consuming\n",
      "- using Machine Learning on large labeled corpus :\n",
      "    - using HMMs to learn transition probabilities from a grammatical category to another\n",
      "    - using a more general approach with CRFs\n",
      "    \n",
      "This is used in the context of disambiguation for example, and it increases the accuracy for the next step.\n",
      "\n",
      "## Filtering words, Stemming and Lemmatizing\n",
      "\n",
      "As described above, when filtering words, we might be interested in removing :\n",
      "- punctuation\n",
      "- dates\n",
      "- stop words\n",
      "- hapax (specific words in the corpus)\n",
      "\n",
      "We also want to reduce the size of the vocabulary by gathering inflectional forms and derivationally related forms :\n",
      "- The inflectional form is a change in a word that shows a change in the way it is used in the sentence\n",
      "- The morphological derivation is the process of forming a new word from an existing one by adding prefix or suffix for example.\n",
      "\n",
      "We gather the forms of words around :\n",
      "- their stems, a process that chops off the ends of words (e.g Porter's algorithm)\n",
      "- their lemmas, considered as the base, or dictionary form of a word\n",
      "\n",
      "The goal of both stemming and lemmatization is to reduce derivationally related forms of a word to a common base form. Stemming works usually well in German, but the choice between stemming and lemmatization depends on the language considered.\n",
      "\n",
      "# II. In Python\n",
      "\n",
      "We are now ready to implement this in Python! First, import some packages :\n",
      "\n",
      "```python\n",
      "from nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\n",
      "from nltk.corpus import stopwords as sw, wordnet as wn\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras.preprocessing.sequence import pad_sequences\n",
      "import string \n",
      "```\n",
      "\n",
      "We'll be using NLTK as our reference package for these tasks.\n",
      "\n",
      "## 1. Preprocessing per document\n",
      "\n",
      "We can define the preprocessing pipeline that will process each document as a single entity and apply the preprocessing on it :\n",
      "\n",
      "\n",
      "```python\n",
      "def preprocess(document, max_features = 150, max_sentence_len = 300):\n",
      "    \"\"\"\n",
      "    Returns a normalized, lemmatized list of tokens \n",
      "    from a list of document, applying word/punctuation\n",
      "    tokenization, and finally part of speech tagging. It uses the part of\n",
      "    speech tags to look up the lemma in WordNet, and returns the lowercase  \n",
      "    version of all the words, removing stopwords and punctuation.\n",
      "    \"\"\"\n",
      "\n",
      "    def lemmatize(token, tag):\n",
      "        \"\"\"\n",
      "        Converts the tag to a WordNet POS tag, then uses that   \n",
      "        tag to perform an accurate WordNet lemmatization.\n",
      "        \"\"\"\n",
      "        tag = {\n",
      "            'N': wn.NOUN,\n",
      "            'V': wn.VERB,\n",
      "            'R': wn.ADV,\n",
      "            'J': wn.ADJ\n",
      "        }.get(tag[0], wn.NOUN)\n",
      "\n",
      "    return WordNetLemmatizer().lemmatize(token, tag)\n",
      "\n",
      "    def vectorize(doc, max_features, max_sentence_len):\n",
      "        \"\"\"\n",
      "        Converts a document into a sequence of indices of length max_sentence_len retaining only max_features unique words\n",
      "        \"\"\"\n",
      "        tokenizer = Tokenizer(num_words=max_features)   \n",
      "        tokenizer.fit_on_texts(doc)\n",
      "        doc = tokenizer.texts_to_sequences(doc)\n",
      "        doc_pad = pad_sequences(doc, padding = 'pre', truncating = 'pre', maxlen = max_sentence_len)\n",
      "        return np.squeeze(doc_pad), tokenizer.word_index\n",
      "\n",
      "    cleaned_document = []\n",
      "    vocab = []\n",
      "\n",
      "    # For each document inside the corpus\n",
      "    for sent in document:\n",
      "\n",
      "        sent = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", sent)\n",
      "        sent = re.sub(r\"what's\", \"what is \", sent)\n",
      "        sent = re.sub(r\"\\'\", \" \", sent)\n",
      "        sent = re.sub(r\"@\", \" \", sent)\n",
      "        sent = re.sub(r\"\\'ve\", \" have \", sent)\n",
      "        sent = re.sub(r\"can't\", \"cannot \", sent)\n",
      "        sent = re.sub(r\"n't\", \" not \", sent)\n",
      "        sent = re.sub(r\"i'm\", \"i am \", sent)\n",
      "        sent = re.sub(r\"\\'re\", \" are \", sent)\n",
      "        sent = re.sub(r\"\\'d\", \" would \", sent)\n",
      "        sent = re.sub(r\"\\'ll\", \" will \", sent)\n",
      "        sent = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", sent)\n",
      "        sent = sent.replace(\"\\n\", \" \")\n",
      "\n",
      "        lemmatized_tokens = []\n",
      "\n",
      "        # Break the sentence into part of speech tagged tokens\n",
      "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
      "\n",
      "            # Apply preprocessing to the tokens\n",
      "            token = token.lower()\n",
      "            token = token.strip()\n",
      "            token = token.strip('_')\n",
      "            token = token.strip('*')\n",
      "\n",
      "            # If punctuation ignore token and continue\n",
      "            if all(char in set(string.punctuation) for char in token): #token in set(sw.words('english')) or \n",
      "                continue\n",
      "\n",
      "            # Lemmatize the token\n",
      "            lemma = lemmatize(token, tag)\n",
      "            lemmatized_tokens.append(lemma)\n",
      "            vocab.append(lemma)\n",
      "\n",
      "        cleaned_document.append(lemmatized_tokens)\n",
      "\n",
      "    vocab = sorted(list(set(vocab)))\n",
      "\n",
      "    return cleaned_document, vocab\n",
      "```\n",
      "\n",
      "```python\n",
      "df, vocab = preprocess(list(df))\n",
      "```\n",
      "\n",
      "## 2. Preprocessing per sentence\n",
      "\n",
      "If you aim to do an embedding per sentence, without taking into account the structure of the documents within the corpus, then, this pipeline might be more appropriate :\n",
      "\n",
      "\n",
      "```python\n",
      "def preprocess(document, max_features = 150, max_sentence_len = 300):\n",
      "    \"\"\"\n",
      "    Returns a normalized, lemmatized list of tokens from a document by\n",
      "    applying segmentation (breaking into sentences), then word/punctuation\n",
      "    tokenization, and finally part of speech tagging. It uses the part of\n",
      "    speech tags to look up the lemma in WordNet, and returns the lowercase\n",
      "    version of all the words, removing stopwords and punctuation.\n",
      "    \"\"\"\n",
      "\n",
      "    def lemmatize(token, tag):\n",
      "        \"\"\"\n",
      "        Converts the tag to a WordNet POS tag, then uses that\n",
      "        tag to perform an accurate WordNet lemmatization.\n",
      "        \"\"\"\n",
      "        tag = {\n",
      "            'N': wn.NOUN,\n",
      "            'V': wn.VERB,\n",
      "            'R': wn.ADV,\n",
      "            'J': wn.ADJ\n",
      "        }.get(tag[0], wn.NOUN)\n",
      "\n",
      "        return WordNetLemmatizer().lemmatize(token, tag)\n",
      "\n",
      "    def vectorize(doc, max_features, max_sentence_len):\n",
      "        \"\"\"\n",
      "        Converts a document into a sequence of indices of length max_sentence_len retaining only max_features unique words\n",
      "        \"\"\"\n",
      "        tokenizer = Tokenizer(num_words=max_features)\n",
      "        tokenizer.fit_on_texts(doc)\n",
      "        doc = tokenizer.texts_to_sequences(doc)\n",
      "        doc_pad = pad_sequences(doc, padding = 'pre', truncating = 'pre', maxlen = max_sentence_len)\n",
      "        return np.squeeze(doc_pad), tokenizer.word_index\n",
      "\n",
      "    cleaned_document = []\n",
      "    vocab = []\n",
      "\n",
      "    # Clean the text using a few regular expressions\n",
      "    document = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", document)\n",
      "    document = re.sub(r\"what's\", \"what is \", document)\n",
      "    document = re.sub(r\"\\'\", \" \", document)\n",
      "    document = re.sub(r\"@\", \" \", document)\n",
      "    document = re.sub(r\"\\'ve\", \" have \", document)\n",
      "    document = re.sub(r\"can't\", \"cannot \", document)\n",
      "    document = re.sub(r\"n't\", \" not \", document)\n",
      "    document = re.sub(r\"i'm\", \"i am \", document)\n",
      "    document = re.sub(r\"\\'re\", \" are \", document)\n",
      "    document = re.sub(r\"\\'d\", \" would \", document)\n",
      "    document = re.sub(r\"\\'ll\", \" will \", document)\n",
      "    document = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", document)\n",
      "    document = document.replace(\"\\n\", \" \")\n",
      "\n",
      "\n",
      "    # Break the document into sentences\n",
      "    for sent in sent_tokenize(document):\n",
      "        lemmatized_tokens = []\n",
      "\n",
      "        # Break the sentence into part of speech tagged tokens\n",
      "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
      "\n",
      "            # Apply preprocessing to the tokens\n",
      "            token = token.lower()\n",
      "            token = token.strip()\n",
      "            token = token.strip('_')\n",
      "            token = token.strip('*')\n",
      "\n",
      "            # If punctuation ignore token and continue\n",
      "            if all(char in set(string.punctuation) for char in token): #token in set(sw.words('english')) or \n",
      "                continue\n",
      "\n",
      "            # Lemmatize the token\n",
      "            lemma = lemmatize(token, tag)\n",
      "            lemmatized_tokens.append(lemma)\n",
      "            vocab.append(lemma)\n",
      "\n",
      "        cleaned_document.append(lemmatized_tokens)\n",
      "\n",
      "    vocab = sorted(list(set(vocab)))    \n",
      "    return cleaned_document, vocab\n",
      "```\n",
      "\n",
      "And apply this function on a string version of the whole corpus :\n",
      "\n",
      "```python\n",
      "df, vocab = preprocess(str(list(df)))\n",
      "```\n",
      "\n",
      "> **Conclusion** : I hope this quick introduction to preprocessing in NLP was helpful. Don't hesitate to drop a comment if you have a comment.\n",
      "---\n",
      "title: Business Analyst vs. Data Analyst vs. Data Scientist\n",
      "layout: post\n",
      "tags: [da_tuto]\n",
      "subtitle : \"Data Analysis Basics\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "So, before starting this journey, it might be worth wondering what you will learn and what typical job this leads to.\n",
      "\n",
      "This track I have created should lead you in the direction of becoming a Data Analyst, but it will also depend on your background and the time you invest obviously. This track is also the introduction track for the data science one.\n",
      "\n",
      "So, the find question that we need to answer probably is: what is data analysis? And how is it different from all the data-x jobs you see around?\n",
      "\n",
      "Let's start with this table, it's a mix of different sources, and it also reflects my personal view on the topic:\n",
      "\n",
      "| Skills | Business Analyst | Data Analyst | Data Scientist | ML Engineer |\n",
      "| --- | --- | --- | --- | --- |\n",
      "| Business Understanding | High | High | Medium | Medium |\n",
      "| Programming & Software | Low | High | High | High |\n",
      "| Data Viz | High | High | High | High |\n",
      "| Databases | Low | Medium | High | High |\n",
      "| Maths & Stats & ML| Low | Medium | High | High |\n",
      "| Data Wrangling | High | High | High | High |\n",
      "| Cloud Computing | Low | Low | Medium | High |\n",
      "\n",
      "So, you see how Business and Data Analysts tend to be closer to business, to data viz and to deriving insights from data, less than building complex models and building predictive algorithms.\n",
      "\n",
      "Now, what are the key skills to become a Data Analyst? Accoring to [TargetJobs](https://targetjobs.co.uk/careers-advice/job-descriptions/454089-data-analyst-job-description) :\n",
      "- A high level of mathematical ability\n",
      "- Programming languages, such as SQL, Oracle and Python\n",
      "- The ability to analyse, model and interpret data\n",
      "- Problem-solving skills\n",
      "- A methodical and logical approach\n",
      "- The ability to plan work and meet deadlines\n",
      "- Accuracy and attention to detail\n",
      "- Interpersonal skills\n",
      "- Teamworking skills\n",
      "- Written and verbal communication skills\n",
      "\n",
      "And what are the typical jobs of a Data Analyst in a company?\n",
      "- build dashboards\n",
      "- clean and manipulate data\n",
      "- derive insights from data\n",
      "- assess the quality of the data, control bias...\n",
      "\n",
      "> If you found the article useful or see ways in which it could be improved, please leave a comment :)\n",
      "---\n",
      "title: Execute MapReduce Job in Python locally\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Parallel and Distributed Computing\"\n",
      "---\n",
      "\n",
      "Alternatively, instead of using HortonWorks Sandbox, we can execute MapReduce jobs locally. \n",
      "\n",
      "1) First of all, download the [Hadoop compressed file from Apache's website](http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz)\n",
      "\n",
      "2) Unzip this file, and put it at your root: Users/yourname\n",
      "\n",
      "3) Create a folder on your working directory (for example on your desktop). \n",
      "\n",
      "4) Create a short text file called `file.txt`\n",
      "\n",
      "5) Test the Hadoop MapReduce utility by running the following command in your terminal :\n",
      "\n",
      "```bash\n",
      "mapred streaming -input Path-To-Input-File/file.txt -output Path-To-Input-File/Output -mapper /bin/cat -reducer /usr/bin/wc\n",
      "```\n",
      "\n",
      "If the MapReduce utility works correctly, you should have an output folder created, and 2 new files inside of it. \n",
      "If you have a problem, make sure you have the environment variable JAVA_HOME set (refer to Java SKD download).\n",
      "\n",
      "6) To submit MapReduce Jobs, as previously, create 2 files: `mapper.py` and `reducer.py`.\n",
      "\n",
      "`mapper.py` :\n",
      "\n",
      "```python\n",
      "#!/usr/bin/python3\n",
      "import sys\n",
      "def main(argv):\n",
      "    for line in sys.stdin:\n",
      "        wordlist = line.strip().split()\n",
      "        for word in wordlist:\n",
      "            print(word+\"\\t\"+\"1\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main(sys.argv)\n",
      "```\n",
      "\n",
      "`reducer.py` :\n",
      "\n",
      "```python\n",
      "#!/usr/bin/python3\n",
      "import sys\n",
      "def main(argv):\n",
      "    current_word = None\n",
      "    wordcount = 0\n",
      "    for line in sys.stdin:\n",
      "        word, n = line.strip().split(\"\\t\",1)\n",
      "        n = int(n)\n",
      "        if current_word == word:\n",
      "            wordcount += n\n",
      "        else:\n",
      "            if current_word:\n",
      "                print(current_word+\"\\t\"+str(wordcount))\n",
      "                wordcount = n\n",
      "            current_word = word\n",
      "            print(current_word+\"\\t\"+str(wordcount))\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main(sys.argv)\n",
      "```\n",
      "\n",
      "You should now be able to execute this MapReduce job with the following command :\n",
      "\n",
      "```python\n",
      "mapred streaming -input Path-To-Input-File/file.txt -output Path-To-Input-File/Output -mapper \"python Path-To-Mapper/mapper.py\" -reducer \"python Path-To-Reducer/reducer.py\"\n",
      "```\n",
      "\n",
      "I'm explicitly specifying the Python instruction to execute the mapper and reducer files to avoid access denial. \n",
      "\n",
      "You should now observe an output with 2 files: a Sucess file, and `part-0000` which contains the output of the WordCount.\n",
      "\n",
      "> Conclusion: I hope this short tutorial was helpful. I'd be happy to answer any question you might have in the comments section.\n",
      "---\n",
      "title: Lab - Running a Apache Spark job on Cloud DataProc\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Road to Google Cloud Platform Certification\"\n",
      "---\n",
      "\n",
      "In this lab, we will launch Apache Spark jobs on Could DataProc, to estimate the digits of Pi in a distributed fashion.\n",
      "\n",
      "From the console on GCP, on the side menu, click on DataProc and Clusters.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_54.jpg)\n",
      "\n",
      "You might be needed to enable API, so just click on the button. Then, you are redirected to a page where you can create clusters. Click on Create Clusters.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_55.jpg)\n",
      "\n",
      "Change the name of the cluster if you want, and leave all the other parameters to default. Then, click on \"Create\" to create the cluster.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_56.jpg)\n",
      "\n",
      "The cluster will now start. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_57.jpg)\n",
      "\n",
      "Then, move to the \"Jobs\" tab. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_58.jpg)\n",
      "\n",
      "Click on the \"Submit a job\" button. You can give your job a specific name, and make sure to change the job's type to Spark rather than Hadoop.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_59.jpg)\n",
      "\n",
      "We will be using one of the pre-defined jobs in Spark examples. To do so, in the field \"Main class or jar\", simply type :\n",
      "\n",
      "```\n",
      "org.apache.spark.examples.SparkPi\n",
      "```\n",
      "\n",
      "The code for this job can be found on Github. What this essentially does is to run a Monte Carlo simulation of pairs of X and Y coordinates in a unit circle and use the definition of the area to retrieve the Pi estimate.\n",
      "\n",
      "```java\n",
      "package org.apache.spark.examples;\n",
      "\n",
      "import org.apache.spark.SparkConf;\n",
      "import org.apache.spark.api.java.JavaRDD;\n",
      "import org.apache.spark.api.java.JavaSparkContext;\n",
      "\n",
      "import java.util.ArrayList;\n",
      "import java.util.List;\n",
      "\n",
      "public final class SparkPi {\n",
      "\n",
      "    public static void main(String[] args) throws Exception {\n",
      "        final SparkConf sparkConf = new SparkConf().setAppName(\"SparkPi\");\n",
      "        final JavaSparkContext jsc = new JavaSparkContext(sparkConf);\n",
      "\n",
      "        final int slices = (args.length == 1) ? Integer.parseInt(args[0]) : 2;\n",
      "        final int n = 100000 * slices;\n",
      "        final List<Integer> l = new ArrayList<>(n);\n",
      "        for (int i = 0; i < n; i++) {\n",
      "            l.add(i);\n",
      "        }\n",
      "\n",
      "        final JavaRDD<Integer> dataSet = jsc.parallelize(l, slices);\n",
      "\n",
      "        final int count = dataSet.map(integer -> {\n",
      "            double x = Math.random() * 2 - 1;\n",
      "            double y = Math.random() * 2 - 1;\n",
      "            return (x * x + y * y < 1) ? 1 : 0;\n",
      "        }).reduce((a, b) -> a + b);\n",
      "\n",
      "        System.out.println(\"Pi is roughly \" + 4.0 * count / n);\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "You can set the Jar files field to :\n",
      "\n",
      "```\n",
      "file:///usr/lib/spark/examples/jars/spark-examples.jar\n",
      "```\n",
      "\n",
      "The job takes arguments, which can be set to 1000 here :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_60.jpg)\n",
      "\n",
      "You can then click on \"Submit\" to submit your job. From the cluster tab, you can click on the name of the cluster and access a cluster monitoring dashboard :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_61.jpg)\n",
      "\n",
      "If you click on \"Jobs\" in the Cluster tabs, you'll notice the progress of the job we launched. It took 46 seconds in my case.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_62.jpg)\n",
      "\n",
      "You can click on the name of the job.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_63.jpg)\n",
      "\n",
      "You will directly see the output that states :\n",
      "\n",
      "```\n",
      "19/07/31 21:55:46 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1564609038185_0001\n",
      "\n",
      "Pi is roughly 3.1418519514185195\n",
      "\n",
      "19/07/31 21:56:12 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@5288ab42{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "\n",
      "Job output is complete\n",
      "```\n",
      "\n",
      "If you don't need it anymore, make sure to **DELETE** the cluster.\n",
      "\n",
      "---\n",
      "title: The phylosophy game in Wikipedia\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "According to ReadWriteWeb, all articles in the English version of Wikipedia lead to the article \"Philosophy\". If you click on the first link of each article, you will come across the Philosophy article after a while.\n",
      "\n",
      "In order to verify this assertion, I developped a small WebApp with 2 simple options :\n",
      "- A manual Wikipedia Link explorer that requests Wikipedia's API\n",
      "- An automatic exploration that systematically chooses the first link among results\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/img.png)\n",
      "\n",
      "Here are the rules of this game:\n",
      "- Select start word and validate\n",
      "- From each word, choose the next word\n",
      "- If the next word is \"Philosophy\", then it's won\n",
      "\n",
      "![alt text](https://maelfabien.github.io/assets/images/AS_Images/model.png)\n",
      "\n",
      "The Github of the project can be found here :\n",
      "\n",
      "<div class=\"github-card\" data-github=\"maelfabien/WebApp\" data-width=\"100%\" data-height=\"\" data-theme=\"default\"></div>\n",
      "<script src=\"//cdn.jsdelivr.net/github-cards/latest/widget.js\"></script>\n",
      "\n",
      "<br>\n",
      "\n",
      "---\n",
      "title: Interview by DataCast\n",
      "layout: post\n",
      "tags: [thoughts]\n",
      "subtitle : \"Thoughts\"\n",
      "---\n",
      "\n",
      "I am extremely humbled to have been interviewed by [James Le](https://jameskle.com/) from the excellent podcast: [DataCast](https://datacast.simplecast.fm/). This podcast offers high-quality raw conversations with data science professionals. \n",
      "\n",
      "In my conversation with James, I talked about moving from Actuarial Science to Data Science, gaining experience as a young graduate, blogging, the tech community in Paris and much more.\n",
      "\n",
      "If you go through the conversation or some of it, I’d be really happy to get your feedback. It’s a bit intimidating to have your name on this podcast standing next to people you follow, but I hope that you’ll enjoy the conversation!\n",
      "\n",
      "If you do not yet follow James’ work, there are 23 other great conversations on [DataCast](https://datacast.simplecast.fm/)! \n",
      "\n",
      "<iframe height=\"200px\" width=\"100%\" frameborder=\"no\" scrolling=\"no\" seamless src=\"https://player.simplecast.com/283201b5-12cc-4488-a80c-2dffc1e71e4a?dark=false\"></iframe>\n",
      "---\n",
      "published: false\n",
      "title: Landing an internship in Data Science\n",
      "layout: post\n",
      "classes: wide\n",
      "image: \"https://maelfabien.github.io/assets/images/wolf.jpg\"\n",
      "youtubeId: ZreEaLSgQcc\n",
      "---\n",
      "\n",
      "End of 2018, I started to look for an end of study internship in data science. After a first Master in Actuarial Science in Switzerland and an additional year of specialized Master in Big Data in France, I chose to apply to positions outside Europe mostly in order to get a real international experience.\n",
      "\n",
      "I'll try to share my tips along the way, give you some statistics on my applications (answer rate, number of interviews, salary...)\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "## Applications\n",
      "\n",
      "**Volumetry** : I've applied to 56 companies overall, some of which included Facebook, Apple, Amazon, but also much smaller companies and also a large amount of startups. \n",
      "\n",
      "**Geography** : Most of my applications were for positions outside Europe, especially in :\n",
      "- USA \n",
      "- Australia\n",
      "- Canada\n",
      "\n",
      "The internships I've applied to were rather technical, often in some specific domains (e.g computer vision).\n",
      "\n",
      "Among my 56 applications, I've had 2 offers, and kept 1 of them.\n",
      "\n",
      "## Graphical Analysis\n",
      "\n",
      "Let's first of all take a look at how I performed through the rounds of application :\n",
      "![image](https://maelfabien.github.io/assets/images/apply_1.jpg)\n",
      "\n",
      "I've received quite a lot \n",
      "\n",
      "\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/img1.jpg)\n",
      "\n",
      "\n",
      "> **Conclusion** : I hope you enjoyed this quick tutorial on OpenPose for macOS. I am looking forward to making a more developped article on the field of pose recognition !\n",
      "\n",
      "---\n",
      "title: Image subsampling and downsampling\n",
      "layout: post\n",
      "tags: [computervision]\n",
      "subtitle : \"Computer Vision\"\n",
      "---\n",
      "\n",
      "How can you scale down an image? What other transformations can be applied?\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "For what comes next, we'll work a bit in Python. Import the following packages :\n",
      "\n",
      "```python\n",
      "import cv2\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "```\n",
      "\n",
      "# I. Image sub-sampling\n",
      "\n",
      "The key idea in image sub-sampling is to throw away every other row and column to create a half-size image. When the sampling rate gets too low, we are not able to capture the details in the image anymore.\n",
      "\n",
      "Instead, we should have a minimum signal/image rate, called the Nyquist rate.\n",
      "\n",
      "Using Shannons Sampling Theorem, the minimum sampling should be such that :\n",
      "\n",
      "$$ f_s ≥ 2 f_{max} $$\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_25.jpg)\n",
      "\n",
      "Image subsampling by dropping rows and columns will typically look like this :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_26.jpg)\n",
      "\n",
      "The original image has frequencies that are too high. How can we solve this? Filter the image first, and then subsample.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_27.jpg)\n",
      "\n",
      "To reduce the dimension, we apply a \"decimation\" :\n",
      "\n",
      "$$ g(i,j) = \\sum_{i,j} f(k,l) h(i-k/r, j-l/r) $$\n",
      "\n",
      "# II. Image up-sampling\n",
      "\n",
      "A classical method would be to repeat each row and column several times. This is called the Nearest Neighbor Interpolation. However, as you might expect, it's not an efficient method. \n",
      "\n",
      "Recall that a digital image can be formed the following way :\n",
      "\n",
      "$$ F[x,y] = quantize (f(xd,yd)) $$\n",
      "\n",
      "It's a discrete point sampling of a continuous function. If we don't know $$ f $$, we need to interpolate and guess an approximation. We convert $$ F $$ to a continuous function :\n",
      "\n",
      "$$ f_F(x) = F( \\frac {x} {d}) $$  if $$ \\frac {x} {d} $$ is an integer, 0 otherwise.\n",
      "\n",
      "We then reconstruct the image by convolution with a reconstruction filter $$ h $$ : $$ \\hat{f} = h f_F $$\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_28.jpg)\n",
      "\n",
      "We can also use cubic filters which are quite common. To interpolate / upsample, we must select an interpolation kernel to convolve the image :\n",
      "\n",
      "$ g(i,j) = \\sum_{i,j} f(k,l) h(i-rk, j-rl) $$\n",
      "\n",
      "The up and down sampling can be achieved using the `resize` function in OpenCV :\n",
      "\n",
      "```\n",
      "res = cv2.resize(img, None, fx=0.2, fy=0.2, interpolation = cv2.INTER_CUBIC)\n",
      "\n",
      "plt.figure(figsize=(15,12))\n",
      "\n",
      "plt.subplot(121)\n",
      "plt.imshow(img,cmap = 'gray')\n",
      "plt.title('Original Image')\n",
      "\n",
      "plt.subplot(122)\n",
      "plt.imshow(res,cmap = 'gray')\n",
      "plt.title('Downsampled Image')\n",
      "\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_29.jpg)\n",
      "\n",
      "The Laplacian Pyramid offers a multi-resolution representation. \n",
      "- blur the image\n",
      "- subsample the image\n",
      "- subtract the low pass version of the original to get a band-pass Laplacian image\n",
      "- the Laplacian pyramid has a perfect reconstitution. \n",
      "\n",
      "This is pretty close to autoencoders in some sense.\n",
      "\n",
      "# III. Advanced filters\n",
      "\n",
      "Other types of filters exist, and include :\n",
      "- oriented filters for texture analysis, edge detection, compression... Apply many versions of the same filter to find the response. \n",
      "- Another method: apply a few filters corresponding to angles, and interpolate. Steerable filters are a class of filters in which a filter of arbitrary orientation is synthesized as a linear combination of a set of basic filters.\n",
      "\n",
      "## a. Steerable Filters\n",
      "\n",
      "In Steerable filters, we'll select a Gaussian filter and take the first derivative with respect to x and y. We combine the two derivatives (basis filters) into a linear combination (interpolation function).\n",
      "\n",
      "This is an example of steerable filters :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_30.jpg)\n",
      "\n",
      "## b. Integral Images\n",
      "\n",
      "The integral image is the running sum of all the pixels from the origin :\n",
      "\n",
      "$$ s(i,j) = \\sum_k sum_l f(k,l) = s(i-1,j) + s(i,j-1) - s(i-1,j-1) + f(i,j) $$\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_31.jpg)\n",
      "\n",
      "The information within an integral image can be represented in a so-called summed-area table.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_32.jpg)\n",
      "\n",
      "## c. Non-linear filters\n",
      "\n",
      "We can, first of all, apply Median filtering to introduce non-linearity.\n",
      "\n",
      "### Bilateral Filtering\n",
      "\n",
      "Bilateral filtering is a weighted filter kernel with a better outlier rejection. Instead of rejecting a fixed percentage, we reject (in a soft way) pixels whose values differ too much from the central pixel value. \n",
      "\n",
      "```python\n",
      "blur = cv2.bilateralFilter(img,9,75,75)\n",
      "\n",
      "plt.figure(figsize=(15,12))\n",
      "\n",
      "plt.subplot(121)\n",
      "plt.imshow(img,cmap = 'gray')\n",
      "plt.title('Original Image')\n",
      "\n",
      "plt.subplot(122)\n",
      "plt.imshow(blur,cmap = 'gray')\n",
      "plt.title('Bilateral Filtered Image')\n",
      "\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_33.jpg)\n",
      "\n",
      "### Distance Transform\n",
      "\n",
      "In the distance transform, we compute the Manhattan Distance using :\n",
      "- a Forward pass: each non-zero pixel is replaced by the minimum of 1 + the distance of its north or west neighbor\n",
      "- a Backward pass: each non-zero pixel is replaced by the minimum of 1 + the distance of its south or east neighbor\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_34.jpg)\n",
      "\n",
      "Fourier analysis can be used to analyze the frequency characteristics of various filters. I won't cover this part into much more details, but it's an interesting topic and it links all filters (Gabor, Laplacian, Gaussian, box...).\n",
      "\n",
      "\n",
      "> **Conclusion** : I hope this article on image filtering was helpful. Don't hesitate to drop a comment if you have any question.\n",
      "---\n",
      "title: Fundamental Speech Processing Papers\n",
      "layout: post\n",
      "tags: [signal]\n",
      "subtitle : \"Speech Processing\"\n",
      "---\n",
      "\n",
      "I am gathering in this article fundamental Speech Processing papers, and specifically Speaker Verification, and will provide summary of most of them over time.\n",
      "\n",
      "# Databases\n",
      "\n",
      "[LibriSpeech](https://maelfabien.github.io/assets/litterature/databases/librispeech.pdf): A fundamental english database based on audio-book recordings for text-independent speaker recognition.\n",
      "\n",
      "[Speaker In The Wild](https://maelfabien.github.io/assets/litterature/databases/SITW.pdf): A large hand-annotated real-condition database for text-independent speaker recognition.\n",
      "\n",
      "[VoxCeleb 1](https://maelfabien.github.io/assets/litterature/databases/voxceleb_1.pdf): Large amount of open-source data extracted from Youtube using Computer Vision techniques for speaker recongition and speaker diarization.\n",
      "\n",
      "[VoxCeleb 2](https://maelfabien.github.io/assets/litterature/databases/voxceleb_2.pdf): An even larger corpus extracted with an improved pipeline.\n",
      "\n",
      "[RSR](https://maelfabien.github.io/assets/litterature/databases/RSR.pdf): A text-dependent speaker recognition database using multiple pass phrase, in English, from Singapore.\n",
      "\n",
      "Other famous databases include NIST Speaker Recognition Evaluation Challenge or the \"Ok Google\" proprietary speaker verification database.\n",
      "\n",
      "# Speaker Verification Fundamentals\n",
      "\n",
      "[SVM GMM-Supervector Speaker Verification](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.604&rep=rep1&type=pdf): A method relying on GMM-supervectors and SVM to classify speakers in Speaker Verification tasks.\n",
      "\n",
      "[Probabilistic Linear Discriminant Analysis](https://maelfabien.github.io/assets/litterature/maths/PLDA.pdf): A key element used for dimension reduction and speaker classification.\n",
      "\n",
      "[Front-End Factor Analysis For Speaker Verification](https://maelfabien.github.io/assets/litterature/representation/i-vector.pdf): The paper describing the i-vector feature extraction.\n",
      "\n",
      "[X-vectors: Robust DNN Embeddings for speaker recognition](https://maelfabien.github.io/assets/litterature/representation/x_vector.pdf): Applying time-delay NN and data augmentation to create robust embeddings called x-vectors.\n",
      "\n",
      "# Text-dependent Speaker Recognition\n",
      "\n",
      "[DNN for small footprint text-dependent speaker verification](https://maelfabien.github.io/assets/litterature/representation/d-vector.pdf): A NN approach to feature extraction called the d-vector.\n",
      "\n",
      "# Text-independent Speaker Recognition\n",
      "\n",
      "[Deep Neural Network Embeddings for Text-Independent Speaker Verification](https://maelfabien.github.io/assets/litterature/representation/xvector.pdf): Learning speaker embeddings with DNN with a PDLA background. Building block of the x-vector.\n",
      "\n",
      "# Evaluation Metrics\n",
      "\n",
      "[Application-Independent Evaluation of Speaker Recognition Systems](https://maelfabien.github.io/assets/litterature/databases/metrics.pdf): A summary of the different speaker recognition systems used including false alarms, misses, DET-plot, EER and Detection Cost Function.\n",
      "\n",
      "\n",
      "---\n",
      "title: Introduction to Automatic Speech Recognition (ASR)\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Speech Processing\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "This article provides a summary of the course [\"Automatic speech recognition\" by Gwénolé Lecorvé from the Research in Computer Science (SIF) master](http://people.irisa.fr/Gwenole.Lecorve/lectures/ASR.pdf), to which I added notes of the Statistical Sequence Processing course of EPFL, and from some tutorials/personal notes. All references are presented at the end.\n",
      "\n",
      "# Introduction to ASR\n",
      "\n",
      "## What is ASR?\n",
      "\n",
      "> Automatic Speech Recognition (ASR), or Speech-to-text (STT) is a field of study that aims to transform raw audio into a sequence of corresponding words.\n",
      "\n",
      "Some of the speech-related tasks involve:\n",
      "- speaker diarization: which speaker spoke when?\n",
      "- speaker recognition: who spoke?\n",
      "- spoken language understanding: what's the meaning?\n",
      "- sentiment analysis: how does the speaker feel?\n",
      "\n",
      "The classical pipeline in an ASR-powered application involves the Speech-to-text, Natural Language Processing and Text-to-speech.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_0.png)\n",
      "\n",
      "ASR is not easy since there are lots of variabilities:\n",
      "- acoustics:\n",
      "\t- variability between speakers (inter-speaker)\n",
      "\t- variability for the same speaker (intra-speaker)\n",
      "\t- noise, reverberation in the room, environment...\n",
      "- phonetics:\n",
      "\t- articulation\n",
      "\t- elisions (grouping some words, not pronouncing them)\n",
      "\t- words with similar pronounciation\n",
      "- linguistics:\n",
      "\t- size of vocabulary\n",
      "\t- word variations\n",
      "\t- ...\n",
      "\n",
      "From a Machine Learning perspective, ASR is also really hard:\n",
      "- very high dimensional output space, and a complex sequence to sequence problem\n",
      "- few annotated training data\n",
      "- data is noisy\n",
      "\n",
      "## How is speech produced?\n",
      "\n",
      "Let us first focus on how speech is produced. An excitation $$ e $$ is produced through lungs. It takes the form of an initial waveform, describes as an airflow over time.\n",
      "\n",
      "Then, vibrations are produced by vocal cords, filters $$ f $$ are applied through pharynx, tongue...\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_1.png)\n",
      "\n",
      "The output signal produced can be written as $$ s = f * e $$, a convolution between the excitation and the filters. Hence, assuming $$ f $$ is linear and time-independent:\n",
      "\n",
      "$$ s(t) = \\int_{-\\infty}^{+\\infty} e(t) f(t-\\tau)d \\tau $$\n",
      "\n",
      "From the initial waveform, we generate the glotal spectrum, right out of the vocal cords. A bit higher the vocal tract, at the level of the pharynx, pitches are formed and produce the formants of the vocal tract. Finally, the **output spectrum** gives us the intensity over the range of frequencies produced.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_2.png)\n",
      "\n",
      "## Breaking down words\n",
      "\n",
      "In automatic speech recognition, you do not train an Artificial Neural Network to make predictions on a set of 50'000 classes, each of them representing a word. \n",
      "\n",
      "In fact, you take an input sequence, and produce an output sequence. And each word is represented as a **phoneme**, a set of elementary sounds in a language based on the International Phonetic Alphabet (IPA). To learn more about linguistics and phonetic, feel free to check [this course](https://scholar.harvard.edu/files/adam/files/phonetics.ppt.pdf) from Harvard. There are around 40 to 50 different phonemes in English.\n",
      "\n",
      "**Phones** are speech sounds defined by the acoustics, potentially unlimited in number, \n",
      "\n",
      "For example, the word \"French\" is written under IPA as : / f ɹ ɛ n t ʃ /. The phoneme describes the voiceness / unvoiceness as well as the position of articulators.\n",
      "\n",
      "Phonemes are language-dependent, since the sounds produced in languages are not the same. We define a **minimal pair** as two words that differ by only one phoneme. For example, \"kill\" and \"kiss\".\n",
      "\n",
      "For the sake of completeness, here are the consonant and vowel phonemes in standard french:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_3.png)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_4.png)\n",
      "\n",
      "There are several ways to see a word:\n",
      "- as a sequence of phonemes\n",
      "- as a sequence of graphemes (mostly a written symbol representing phonemes)\n",
      "- as a sequence of morphemes (meaningful morphological unit of a language that cannot be further divided) (e.g \"re\" + \"cogni\" + \"tion\")\n",
      "- as a part-of-speech (POS) in morpho-syntax: grammatical class, e.g noun, verb, ... and flexional information, e.g singular, plural, gender...\n",
      "- as a syntax describing the function of the word (subject, object...)\n",
      "- as a meaning\n",
      "\n",
      "Therefore, labeling speech can be done at several levels:\n",
      "- word\n",
      "- phones\n",
      "- ...\n",
      "\n",
      "And the labels may be **time-algined** if we know when they occur in speech.\n",
      "\n",
      "The **vocabulary** is defined as the set of words in a specific task, a language or several languages based on the ASR system we want to build. If we have a large vocabulary, we talk about **Large vocabulary continuous speech recognition (LVCSR)**. If some words we encounter in production have never been seen in training, we talk about **Out Of Vocabulary** words (OOV). \n",
      "\n",
      "We distinguish 2 types of speech recognition tasks:\n",
      "- isolated word recognition\n",
      "- continuous speech recognition, which we will focus on\n",
      "\n",
      "## Evaluation metrics\n",
      "\n",
      "We usually evaluate the performance of an ASR system using Word Error Rate (WER). We take as a reference a manual transcript. We then compute the number of mistakes made by the ASR system. Mistakes might include:\n",
      "- Substitutions, $$ N_{SUB} $$,  a word gets replaced\n",
      "- Insertions, $$ N_{INS} $$, a word which was not pronounced in added \n",
      "- Deletions, $$ N_{DEL} $$, a word is omitted from the transcript\n",
      "\n",
      "The WER is computed as:\n",
      "\n",
      "$$ WER = \\frac{N_{SUB} + N_{INS} + N_{DEL}}{\\mid N_{words-transcript} \\mid} $$\n",
      "\n",
      "The perfect WER should be as close to 0 as possible. The number of substitutions, insertions and deletions is computed using the Wagner-Fischer dynamic programming algorithm for word alignment.\n",
      "\n",
      "# Statistical historical approach to ASR\n",
      "\n",
      "Let us denote the optimal word sequence $$ W^{\\star} $$ from the vocabulary. Let the input sequence of acoustic features be $$ X $$. Stastically, our aim is to identify the optimal sequence such that:\n",
      "\n",
      "$$ W^{\\star} = argmax_W P(W \\mid X) $$\n",
      "\n",
      "This is known as the \"Fundamental Equation of Statistical Speech Processing\". Using Bayes Rule, we can rewrite is as :\n",
      "\n",
      "$$ W^{\\star} = argmax_W \\frac{P(X \\mid W) P(W)}{P(X)} $$\n",
      "\n",
      "Finally, we suppose independence and remove the term $$ P(X) $$. Hence, we can re-formulate our problem as:\n",
      "\n",
      "$$ W^{\\star} = argmax_W P(X \\mid W) P(W) $$\n",
      "\n",
      "Where:\n",
      "- $$ argmax_W $$ is the search space, a function of the vocabulary\n",
      "- $$ P(X \\mid W) $$ is called the acoustic model\n",
      "- $$ P(W) $$ is called the language model\n",
      "\n",
      "The steps are presented in the following diagram:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_5.png)\n",
      "\n",
      "## Feature extraction $$ X $$\n",
      "\n",
      "From the speech analysis, we should extract features $$ X $$ which are:\n",
      "- robust across speakers\n",
      "- robust against noise and channel effects\n",
      "- low dimension, at equal accuracy\n",
      "- non-redondant among features\n",
      "\n",
      "Features we typically extract include:\n",
      "- Mel-Frequency Cepstral Coefficients (MFCC), as desbribed [here](https://maelfabien.github.io/machinelearning/Speech9/#6-mel-frequency-cepstral-coefficients-mfcc)\n",
      "- Perceptual Linear Prediction (PLP)\n",
      "- ...\n",
      "\n",
      "We should then normalize the features extracted to avoid mismatches across samples with mean and variance normalization.\n",
      "\n",
      "## Acoustic model\n",
      "\n",
      "### **1. HMM-GMM acoustic Model**\n",
      "\n",
      "The acoustic model is a complex model, usually based on Hidden Markov Models and Artificial Neural Networks, modeling the relationship between the audio signal and the phonetic units in the language.\n",
      "\n",
      "In isolated word/pattern recognition, the acoustic features (here $$ Y $$) are used as an input to a classifier whose rose is to output the correct word. However, we take input sequence and should output sequences too when it comes to *continuous speech recognition*.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_6.png)\n",
      "\n",
      "The acoustic model goes further than a simple classifier. It outputs a sequence of phonemes. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_7.png)\n",
      "\n",
      "Hidden Markov Models are natural candidates for Acoustic Models since they are great at modeling sequences. If you want to read more on HMMs and HMM-GMM training, you can read [this article](https://maelfabien.github.io/machinelearning/GMM/). The HMM has underlying states $$ s_i $$, and at each state, observations $$ o_i $$ are generated. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_8.png)\n",
      "\n",
      "In HMMs, 1 phoneme is typically represented by a 3 or 5 state linear HMM (generally the beginning, middle and end of the phoneme).\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_9.png)\n",
      "\n",
      "The topology of HMMs is flexible by nature, and we can choose to have each phoneme being represented by a single state, or 3 states for example:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_10.png)\n",
      "\n",
      "The HMM supposes observation independence, in the sense that:\n",
      "\n",
      "$$ P(o_t = x \\mid s_t = q_i, s_{t-1} = q_j, ...) = P(o_t = x \\mid s_t = q_i) $$\n",
      "\n",
      "The HMM can also output context-dependent phonemes, called triphones. Triphones are simply a group of 3 phonemes, the left one being the left context, and the right one, the right context.\n",
      "\n",
      "The HMM is trained using Baum-Welsch algorithm. The HMMs learns to give the probability of each end of phoneme at time t. We usually suppose the observations are generated by a mixture of Gaussians (Gaussian Mixture Models, GMMs) at each state, i.e:\n",
      "\n",
      "$$ P(o_t = y \\mid s_t = q_i) = P(X \\mid W) = \\sum_{m=1} \\mathcal{N}(y, \\mu_{jm}, \\Sigma_{jm}) $$\n",
      "\n",
      "The training of the HMM-GMM is solved by Expectation Maximization (EM). In the EM training, the outputs of the GMM $$ P(X \\mid W) $$ are used as inputs for the GMM training iteratively, and the Viterbi or Baum Welsch algorithm trains the HMM (i.e. identifies the transition matrices) to produce the best state sequence.\n",
      "\n",
      "The full pipeline is presented below:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_11.png)\n",
      "\n",
      "### **2. HMM-DNN acoustic model**\n",
      "\n",
      "Latest models focus on hybrid HMM-DNN architectures and approach the acoustic model in another way. In such approach, we do not care about the acoustic model $$ P(X \\mid W) $$, but we directly tackle $$ P(W \\mid X) $$ as the probability of observing state sequences given $$ X $$.\n",
      "\n",
      "Hence, back to the first acoustic modeling equation, we target:\n",
      "\n",
      "$$ W^{\\star} = argmax_W P(W \\mid X) $$\n",
      "\n",
      "The aim of the DNN is to model the **posterior probabilities** over HMM states.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_12.png)\n",
      "\n",
      "Some considerations on the HMM-DNN framework:\n",
      "- we usually take a large number of hidden layers\n",
      "- the inputs features typically are extracted from large windows (up to 1-2 seconds) to have a large context\n",
      "- early stopping can be used \n",
      "\n",
      "You might have noticed that the training of the DNN produces posterior, whereas the Viterbi Backward-Forward algorithm requires $$ P(X \\mid W) $$ to identify the optimal sequence when training the HMM. Therefore, we use Bayes Rule:\n",
      "\n",
      "$$ P(X \\mid W) = \\frac{P(W \\mid X) P(X)}{P(W)} $$\n",
      "\n",
      "The probability of the acoustic feature $$ P(X) $$ is not known, but it just scales all the likelihoods by the same factor, and therefore does not modify the alignment.\n",
      "\n",
      "The training of HMM-DNN architectures is based:\n",
      "- either on the original hybrid HMM-DNN, using EM, where:\n",
      "\t- E-step keeps DNN and HMM parameters constant and estimates the DNN outputs to produce scaled likelihoods\n",
      "\t- M-step re-trains the DNN parameters on the new targets from E-step\n",
      "- either using REMAP, with a similar architecture, except that the states priors are also given as inputs to the DNN\n",
      "\n",
      "### **3. HMM-DNN vs. HMM-GMM**\n",
      "\n",
      "Here is a brief summary of the pros and cons of HMM/DNN and HMM/GMM:\n",
      "\n",
      "| HMM/DNN | HMM/GMM |\n",
      "|--------------------------------------------------------|--------------------------------------------------------------------------------|\n",
      "| Considers short term correlation | Assumes no correlation in inputs |\n",
      "| No probability distribution function assumption | Assumes GMMs as PDFs |\n",
      "| Discriminative training in the generated distributions | No discriminative training in the generated distributions (can be overlapping) |\n",
      "| Discriminative acoustic model at frame level | Poor discrimination (Maximum Likelihood instead of Maximum A Posteriori) |\n",
      "| Higher performance | Lower performance |\n",
      "\n",
      "### **4. End-to-end models**\n",
      "\n",
      "In End-to-end models, the steps of feature extraction and phoneme prediction are combined:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_13.png)\n",
      "\n",
      "This concludes the part on acoustic modeling.\n",
      "\n",
      "## Pronunciation\n",
      "\n",
      "In small vocabulary sizes, it is quite easy to collect a lot of utterances for each word, and the HMM-GMM or HMM-DNN training is efficient. However, \"statistical modeling requires a sufficient\n",
      "number of examples to get a good estimate of the relationship between speech input and the parts of words\". In large-vocabulary tasks, we might collect 1 or even 0 training examples. t. Thus, it is not feasible to train a model for each word, and we need to share information across words, based on the pronunciation.\n",
      "\n",
      "$$ W^{\\star} = argmax_W P(W \\mid X) $$\n",
      "\n",
      "We consider words are being sequences of states $$ Q $$.\n",
      "\n",
      "$$ W^{\\star} = argmax_W P(X \\mid Q, W) P(Q, W) $$\n",
      "\n",
      "$$ W^{\\star} \\approx argmax_W P(X \\mid Q) \\sum_Q P(Q \\mid W) P(W) $$\n",
      "\n",
      "$$ W^{\\star} \\approx argmax_W max_Q P(X \\mid Q) P(Q \\mid W) P(W) $$\n",
      "\n",
      "Where $$ P(Q \\mid W) $$ is the **pronunciation model**.\n",
      "\n",
      "The pronunciation dictionary is written by human experts, and defined in the IPA. The pronunciation of words is typically stored in a lexical tree, a data structure that allows us to share histories between words in the lexicon.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_15.png)\n",
      "\n",
      "When decoding a sequence in prediction, we must identify the most likely path in the tree based on the HMM-DNN output.\n",
      "\n",
      "In ASR, most recent approaches are:\n",
      "- either end to end\n",
      "- or at the character level\n",
      "\n",
      "In both approaches, we do not care about the full pronunciation of the words. Grapheme-to-phoneme (G2P) models try to learn automatically the pronunciation of new words.\n",
      "\n",
      "## Language Modeling\n",
      "\n",
      "Let's get back to our ASR base equation:\n",
      "\n",
      "$$ W^{\\star} = argmax_W P(W \\mid X) $$\n",
      "\n",
      "$$ W^{\\star} = argmax_W P(X \\mid W) P(W) $$\n",
      "\n",
      "The language model is defined as $$ P(W) $$. It assigns a probability estimate to word sequences, and defines:\n",
      "- what the speaker may say\n",
      "- the vocabulary\n",
      "- the probability over possible sequences, by training on some texts\n",
      "\n",
      "The contraint on $$ P(W) $$ is that $$ \\sum_W P(W) = 1 $$.\n",
      "\n",
      "$$ W^{\\star} = argmax_W P(X \\mid Q, W) P(Q, W) $$\n",
      "\n",
      "In statistical language modeling, we aim to disambiguate sequences such as:\n",
      "\n",
      "\"recognize speech\", \"wreck a nice beach\"\n",
      "\n",
      "The maximum likelihood estimation of a sequence is given by:\n",
      "\n",
      "$$ P(w_i \\mid w_1, ..., w_{i-1}) = \\frac{C(w_1, ..., w_i)}{\\sum_v C(w_1, ..., w_{i-1} v)} $$ \n",
      "\n",
      "Where $$ C(w_1, ..., w_i) $$ is the observed count in the training data. For example:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_16.png)\n",
      "\n",
      "We call this ratio the **relative frequency**. The probability of a whole sequence is given by the **chain rule** of probabilities:\n",
      "\n",
      "$$ P(w_1, ..., w_N) = \\prod_{k=1}^N (w_k \\mid w_{k-1}) $$\n",
      "\n",
      "This approach seems logic, but the longer the sequence, the most likely it will be that we encounter 0's, hence bringing the probability of the whole sequence at 0.\n",
      "\n",
      "What solutions can we apply?\n",
      "- smoothing: redistribute the probability mass from observed to unobserved events (e.g Laplace smoothing, Add-k smoothing)\n",
      "- backoff: explained below\n",
      "\n",
      "### **1. N-gram language model**\n",
      "\n",
      "But one of the most popular solution is the **n-gram model**. The idea behind the n-gram model is to truncate the word history to the last 2, 3, 4 or 5 words, and therefore approximate the history of the word:\n",
      "\n",
      "$$ P(w_i \\mid h) = P(w_i \\mid w_{i-n+1}, ..., w_{i-1}) $$\n",
      "\n",
      "We take $$ n $$ as being 1 (unigram), 2 (bigram), 3 (trigram)...\n",
      "\n",
      "Let us now discuss some practical implementation tricks:\n",
      "- we compute the log of the probabilities, rather than the probabilities themselves (to avoid floating point approximation to 0)\n",
      "- for the first word of a sequence, we need to define **pseudo-words** as being the first 2 missing words for the trigram: $$ P(I \\mid <s><s>) $$ \n",
      "\n",
      "With N-grams, it is possible that we encounter unseen N-grams in prediction. There is a technique called **backoff** that states that if we miss the trigram evidence, we use the bigram instead, and if we miss the bigram evidence, we use the unigram instead...\n",
      "\n",
      "Another approach is **linear interpolate**, where we combine different order n-grams by linearly interpolating all the models:\n",
      "\n",
      "$$ P(w_n \\mid w_{n-2} w_{n-1}) =  \\lambda_1 P(w_n \\mid w_{n−2} w_{n−1}) + \\lambda_2 P(w_n \\mid w_{n−1}) + \\lambda_3 P(w_n) $$\n",
      "\n",
      "### **2. Language models evaluation metrics**\n",
      "\n",
      "There are 2 types of evaluation metrics for language models:\n",
      "- *extrinsic evaluation*, for which we embed the language model in an application and see by which factor the performance is improved\n",
      "- *intrinsic evaluation* that measures the quality of a model independent of any application\n",
      "\n",
      "Extrinsic evaluations are often heavy to implement. Hence, when focusing on intrinsic evaluations, we:\n",
      "- split the dataset/corpus into train and test (and development set if needed)\n",
      "- learn transition probabilities from the trainig set\n",
      "- use the **perplexity** metric to evaluate the language model on the test set\n",
      "\n",
      "We could also use the raw probabilities to evaluate the language model, but the perpeplixity is defined as the inverse probability of the test set, normalized by the number of words. For example, for a bi-gram model, the perpeplexity (noted PP) is defined as:\n",
      "\n",
      "$$ PP(W) = \\sqrt[^N]{ \\prod_{i=1}^{N} \\frac{1}{P(w_i \\mid w_{i-1})}} $$\n",
      "\n",
      "The lower the perplexity, the better\n",
      "\n",
      "### **3. Limits of language models**\n",
      "\n",
      "Language models are trained on a closed vocabulary. Hence, when a new unknown word is met, it is said to be **Out of Vocabulary** (OOV).\n",
      "\n",
      "### **4. Deep learning language models**\n",
      "\n",
      "More recently in Natural Language Processing, neural network-based language models have become more and more popular. Word embeddings project words into a continuous space $$ R^d $$, and respect topological properties (semantics and morpho-syntaxic).\n",
      "\n",
      "Recurrent neural networks and LSTMs are natural candidates when learning such language models.\n",
      "\n",
      "## Decoding\n",
      "\n",
      "The training is now done. The final step to cover is the decoding, i.e. the predictions to make when we collect audio features and want to produce transcript.\n",
      "\n",
      "We need to find:\n",
      "\n",
      "$$ W^{\\star} = argmax_W P(X \\mid W) P(W) I^{\\mid W \\mid} $$\n",
      "\n",
      "However, exploring the whole spact, especially since the Language Model $$ P(W) $$ has a really large scale factor, can be incredibly long.\n",
      " \n",
      "One of the solutions is to explore the **Beam Search**. The Beam Search algorithm greatly reduces the scale factor within a language model (whether N-gram based or Neural-network-based). In Beam Search, we:\n",
      "- identify the probability of each word in the vocabulary for the first position, and keep the top K ones (K is called the Beam width)\n",
      "- for each of the K words, we compute the conditional probability of observing each of the second words of the vocabulary\n",
      "- among all produced probabilities, we keep only the top K ones\n",
      "- and we move on to the third word...\n",
      "\n",
      "Let us illustrate this process the following way. We want to evaluate the sequence that is the most likely. We first compute the probability of the different words of the vocabulary to be the starting word of the sentence:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_18.png)\n",
      "\n",
      "Here, we fix the beam width to 2, meaning that we only select the 2 most likely words to start with. Then, we move on to the next word, and compute the probability of observing it using conditional probability in the language model: $$ P(w_2, w_1 \\mid W) = P(w_1 \\mid W) P(w_2 \\mid w_1, W) $$. We might see that a potential candidate, e.g. \"The\", when selecting the top 2 candidates second words among all possible words, is not a possible path anymore. In that case, we narrow the search, since we know that the first must must be \"a\".\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_19.png)\n",
      "\n",
      "And so on... Another approach to decoding is the Weighted Finite State Transducers (I'll make an article on that).\n",
      "\n",
      "## Summary of the ASR pipeline\n",
      "\n",
      "In their paper [\"Word Embeddings for Speech Recognition\"](https://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/42543.pdf), Samy Bengio and Georg Heigold present a good summary of a modern ASR architecture:\n",
      "- Words are represented through lexicons as phonemes\n",
      "- Typically, for context, we cluster triphones\n",
      "- We then assume that these triphones states were in fact HMM states\n",
      "- And the the observations each HMM state generates are produced by DNNs or GMMs\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_17.png)\n",
      "\n",
      "# End-to-end approach\n",
      "\n",
      "Alright, this article is already long, but we're almost done. So far, we mostly covered historical statistical approaches. These approaches work very well. However, most recent papers and implementations focus on end-to-end approaches, where:\n",
      "- we encode $$ X $$ as a sequence of contexts $$ C $$\n",
      "- we decode $$ C $$ into a sequence of words $$ W $$ \n",
      "\n",
      "These approaches, also called encoder-decoder, are part of sequence-to-sequence models. Sequence to sequence models learn to map a sequence of inputs to a sequence of outputs, even though their length might differ. This is widely used in Machine Translation for example.\n",
      "\n",
      "As illustrated below, the Encoder reduces the input sequence to a encoder vector through a stack of RNNs, and the decoder vector uses this vector as an input.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_20.jpg)\n",
      "\n",
      "I will write more about End-to-end models in another article.\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "This is all for this quite long introduction to automatic speech recognition. After a brief introduction to speech production, we covered historical approaches to speech recognition with HMM-GMM and HMM-DNN approaches. We also mentioned the more recent end-to-end approaches. If you want to improve this article or have a question, feel free to leave a comment below :)\n",
      "\n",
      "References:\n",
      "- [\"Automatic speech recognition\" by Gwénolé Lecorvé from the Research in Computer Science (SIF) master](http://people.irisa.fr/Gwenole.Lecorve/lectures/ASR.pdf)\n",
      "- EPFL Statistical Sequence Processing course\n",
      "- [Stanford CS224S](https://www.youtube.com/watch?v=WSBZ0hBJn7E)\n",
      "- [Rasmus Robert HMM-DNN](https://mycourses.aalto.fi/pluginfile.php/426574/mod_folder/content/0/Rasmus_Robert_DNN.pdf?forcedownload=0)\n",
      "- [A Tutorial on Pronunciation Modeling for Large Vocabulary Speech Recognition](https://link.springer.com/chapter/10.1007/978-3-540-45115-0_3)\n",
      "- [N-gram Language Models, Stanford](https://web.stanford.edu/~jurafsky/slp3/3.pdf)\n",
      "- [Andrew Ng's Beam Search explanation](https://www.youtube.com/watch?v=RLWuzLLSIgw)\n",
      "- [Encoder Decoder model](https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346)\n",
      "- [Automatic Speech Recognition Introduction, University of Edimburgh](http://www.inf.ed.ac.uk/teaching/courses/asr/2019-20/asr01-intro.pdf)\n",
      "---\n",
      "title: How to run a Zeppelin notebook on AWS EMR?\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Amazon Web Services\"\n",
      "---\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/zep_emr.jpg)\n",
      "\n",
      "We have already seen how to run a Zeppelin notebook locally. Most of the time, your notebook will include dependencies (such as AWS connectors to download data from your S3 bucket), and in such case, you might want to use an EMR. Amazon EMR provides a managed Hadoop framework that makes it easy, fast, and cost-effective to process vast amounts of data across dynamically scalable Amazon EC2 instances.\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "Important notice: EMR instances are fully managed and configured. Once launched, EMR instances cannot be terminated without losing all data attached to it. EMR can typically be used to build an ETL (extract, transform, load) to download and transform data from a given source, and later on load the data in a database.\n",
      "\n",
      "## Launching an EMR instance\n",
      "\n",
      "### 1. Key Pair\n",
      "\n",
      "The first step is to create a key pair. \"Amazon uses public-key cryptography to encrypt and decrypt login information. Public–key cryptography uses a public key to encrypt a piece of data, such as a password, then the recipient uses the private key to decrypt the data. The public and private keys are known as a key pair\". \n",
      "\n",
      "Log into your AWS Console and click on EC2 (or click here) : <span style=\"color:blue\">[https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#KeyPairs:sort=keyName](https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#KeyPairs:sort=keyName)</span>\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/EMR0.jpg){:height=\"35%\" width=\"35%\"}\n",
      "\n",
      "Scroll down the side menu to Network and Security and click on Key Pairs :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/key.jpg){:height=\"30%\" width=\"30%\"}\n",
      "\n",
      "Then, create a key pair and give it a name :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/key2.jpg){:height=\"45%\" width=\"45%\"}\n",
      "\n",
      "The keypair .pem file will automatically be downloaded. Make sure to save the file!\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/key3.jpg){:height=\"30%\" width=\"30%\"}\n",
      "\n",
      "### 2. Create New Group\n",
      "\n",
      "An IAM group is a collection of IAM users. Groups let you specify permissions for multiple users, which can make it easier to manage the permissions for those users. For example, you could have a group called Admins and give that group the types of permissions that administrators typically need. Any user in that group automatically has the permissions that are assigned to the group.\n",
      "\n",
      "Your first step here will be to connect to your IAM section from the AWS console :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/iam1.jpg){:height=\"35%\" width=\"35%\"}\n",
      "\n",
      "Then, click on Groups, \"Create New Group\" and add a name :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/iam2.jpg){:height=\"60%\" width=\"60%\"}\n",
      "\n",
      "Select a policy to attach. Here, the AdministratorAccess.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/iam3.jpg){:height=\"90%\" width=\"90%\"}\n",
      "\n",
      "Review the information and confirm the group creation.\n",
      "\n",
      "### 3. Add a user to your group\n",
      "\n",
      "From the IAM menu, click now on \"User\" and add a new user. Give it a name, and allow programmatic access.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/iam4.jpg)\n",
      "\n",
      "Add your newly created user to the group you created previously :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/iam5.jpg)\n",
      "\n",
      "You can eventually add tags at the next step. Confirm the user creation. \n",
      "\n",
      "! Make sure to save the CSV file. This is your only chance to download this file. The file contains the private and public key for your user. Those credentials will be useful when you'll interact in Spark-Scala with AWS services (e.g S3 bucket).\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/iam6.jpg)\n",
      "\n",
      "The user we created has admin rights. It will be useful in our case, but for security reasons, it is not advised to work with admin accounts. \n",
      "\n",
      "### 4. Start the instance\n",
      "\n",
      "Then, log in to your AWS management console : <span style=\"color:blue\">[https://console.aws.amazon.com/console/](https://console.aws.amazon.com/console/)</span>\n",
      "\n",
      "In the Analytics section, click on EMR :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/EMR1.v){:height=\"35%\" width=\"35%\"}\n",
      "\n",
      "Click on \"Create Cluster\" :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/EMR2.jpg){:height=\"35%\" width=\"35%\"}\n",
      "\n",
      "Make sure to select the configuration that includes Spark :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/EMR3.jpg){:height=\"70%\" width=\"70%\"}\n",
      "\n",
      "And select your key pair :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/key4.jpg){:height=\"70%\" width=\"70%\"}\n",
      "\n",
      "Wait a few minutes for your instance to start. \n",
      "\n",
      "At that point, the instance we created does not allow for SSH connection. The next step will allow us to allow SSH connection and to redirect the port of your AWS machine to a local one later on. \n",
      "\n",
      "### 5. Allow SSH connection\n",
      "\n",
      "In the \"Security and access\" section, click on the link attached to Security groups for Master.\n",
      "![image](https://maelfabien.github.io/assets/images/ssh1.jpg){:height=\"70%\" width=\"70%\"}\n",
      "\n",
      "Then select the group \"Master group for Elastic MapReduce\" and edit inbound rule :\n",
      "![image](https://maelfabien.github.io/assets/images/ssh2.jpg){:height=\"70%\" width=\"70%\"}\n",
      "\n",
      "Then add a rule, and allow SSH from anywhere :\n",
      "![image](https://maelfabien.github.io/assets/images/ssh3.jpg){:height=\"70%\" width=\"70%\"}\n",
      "\n",
      "If you get an error, add the sources separately. First a security rule with source ``` 0.0.0.0/0 ``` , and then another one with ``` ::/0 ```.\n",
      "\n",
      "Once the SSH connection has been allowed, we will be able to redirect the different services to our local ports when establishing the SSH connection. The different services pre-configured on your EMR instance can be accessed through the following ports :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ssh4.jpg){:height=\"50%\" width=\"50%\"}\n",
      "\n",
      "## 6. Connect to your EMR instance\n",
      "\n",
      "In your terminal :\n",
      "\n",
      "``` bash\n",
      "ssh -L 8891:127.0.0.1:8890 -i Test.pem hadoop@ec2-XXX.compute-1.amazonaws.com \n",
      "```\n",
      "\n",
      "The command ``` -L ``` redirects the port  ``` 127.0.0.1:8890 ```  of your EMR instance to your local port 8891. Make sure that you are in the folder that contains your keypair  ``` Test.pem ``` or that you indicate the complete path to the key pair.\n",
      "\n",
      "If your connection is successful, you should see something like this :\n",
      "```bash\n",
      "\n",
      "Last login: Thu Dec 13 09:48:13 2018\n",
      "\n",
      "__|  __|_  )\n",
      "_|  (     /   Amazon Linux AMI\n",
      "___|\\___|___|\n",
      "\n",
      "https://aws.amazon.com/amazon-linux-ami/2018.03-release-notes/\n",
      "20 package(s) needed for security, out of 26 available\n",
      "Run \"sudo yum update\" to apply all updates.\n",
      "\n",
      "EEEEEEEEEEEEEEEEEEEE MMMMMMMM           MMMMMMMM RRRRRRRRRRRRRRR    \n",
      "E::::::::::::::::::E M:::::::M         M:::::::M R::::::::::::::R   \n",
      "EE:::::EEEEEEEEE:::E M::::::::M       M::::::::M R:::::RRRRRR:::::R \n",
      "E::::E       EEEEE M:::::::::M     M:::::::::M RR::::R      R::::R\n",
      "E::::E             M::::::M:::M   M:::M::::::M   R:::R      R::::R\n",
      "E:::::EEEEEEEEEE   M:::::M M:::M M:::M M:::::M   R:::RRRRRR:::::R \n",
      "E::::::::::::::E   M:::::M  M:::M:::M  M:::::M   R:::::::::::RR   \n",
      "E:::::EEEEEEEEEE   M:::::M   M:::::M   M:::::M   R:::RRRRRR::::R  \n",
      "E::::E             M:::::M    M:::M    M:::::M   R:::R      R::::R\n",
      "E::::E       EEEEE M:::::M     MMM     M:::::M   R:::R      R::::R\n",
      "EE:::::EEEEEEEE::::E M:::::M             M:::::M   R:::R      R::::R\n",
      "E::::::::::::::::::E M:::::M             M:::::M RR::::R      R::::R\n",
      "EEEEEEEEEEEEEEEEEEEE MMMMMMM             MMMMMMM RRRRRRR      RRRRRR\n",
      "\n",
      "[hadoop@ip-XXX ~]$ \n",
      "```\n",
      "\n",
      "Thanks to the redirection we previously established, you should be able to simply connect to Zeppelin from your local host : <span style=\"color:blue\">[http://localhost:8891/#/](http://localhost:8891/#/)</span> )\n",
      "\n",
      "The port 8891 was chosen quite randomly since 8890 was already used by Jupyter Notebook on my computer.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/zeppelin.jpg){:height=\"70%\" width=\"70%\"}\n",
      "---\n",
      "title: Create Ubuntu VMs with Virtual Box\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "Hadoop runs only on GNU/Linux platforms. Therefore, if you have another OS, you need to install Virtual Box. Virtual Box is a software that lets you create and run Virtual Machines.\n",
      "\n",
      "A virtual machine is a machine that takes part of the resources of your computer (according to initial parameters you chose), and for which you can choose the OS to boot on within the software.\n",
      "\n",
      "## Step 1: Install Virtual Box\n",
      "\n",
      "The first step of this exercise is to install Virtual Box if you don't have a Linux operating system.\n",
      "- Go to: https://www.virtualbox.org/ and download VirtualBox\n",
      "- Follow the installation steps\n",
      "\n",
      "## Step 2: Configure the VM\n",
      "\n",
      "In this part, we'll try to understand how to install a complete Ubuntu VM on your computer. Go to [this link](https://www.ubuntu.com/download/desktop) and download the Ubuntu Desktop file.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/13.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/14.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/15.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/16.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/17.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/18.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/19.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/20.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/21.jpg)\n",
      "\n",
      "Here's what you should see :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/22.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/23.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/24.jpg)\n",
      "\n",
      "Follow the steps until you have to restart the VM. You might need to quit the installation and restart the VM from the menu. You should finally see your user interface :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/25.jpg)\n",
      "\n",
      "> Conclusion: I hope this tutorial was clear and helpful. I'd be happy to answer any question you might have in the comments section.\n",
      "\n",
      "---\n",
      "title: Build a Language Recognition app from scratch\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Markov Processes and HMM\"\n",
      "---\n",
      "\n",
      "In this article, we will build a language recognition app using Markov Chains and likelihood decoding algorithm. If you have not seen my previous articles on this topic, I invite you to check them :)\n",
      "\n",
      "# Language Recognition \n",
      "\n",
      "We aim to build a small web app able to recognize the language of an input text. We will :\n",
      "- Build a transition probabilities matrix from Wikipedia's articles\n",
      "- Find the most \"likely\" language by multiplying the transition probabilities for a given sequence\n",
      "- Identify the highest result to return the language\n",
      "\n",
      "## Build the transition probability matrix\n",
      "\n",
      "Start by importing the following packages :\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Parsing\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "```\n",
      "\n",
      "Then, we'll pick one of the longest articles from Wikipedia: South African Labor Law, and learn transition probabilities from it. Transition probabilities are simply the probabilities, from a given letter (say `s`) to move to another letter (say `o`).\n",
      "\n",
      "```python\n",
      "url = 'https://en.wikipedia.org/wiki/South_African_labour_law'\n",
      "res = requests.get(url)\n",
      "html_page = res.content\n",
      "```\n",
      "\n",
      "We can then parse the content :\n",
      "\n",
      "```python\n",
      "soup = BeautifulSoup(html_page, 'html.parser')\n",
      "text = soup.find_all(text=True)\n",
      "```\n",
      "\n",
      "We will use the following code to clean the content as much as possible :\n",
      "\n",
      "```python\n",
      "output = ''\n",
      "\n",
      "blacklist = [\n",
      "    '[document]',\n",
      "    'noscript',\n",
      "    'header',\n",
      "    'html',\n",
      "    'meta',\n",
      "    'head', \n",
      "    'input',\n",
      "    'script',\n",
      "    '\\n'\n",
      "]\n",
      "\n",
      "for t in text:\n",
      "    if t.parent.name not in blacklist:\n",
      "        output += '{} '.format(t)\n",
      "```\n",
      "\n",
      "The raw text contains many HTML tags, and elements specific to Wikipedia. We need to clean the text a bit :\n",
      "\n",
      "```python\n",
      "def preprocess_text(text) :\n",
      "\n",
      "    text = text.replace('\\n', '').replace('[ edit ]', '').replace(\"\\'\", \"'\")\n",
      "    text = ''.join(c.lower() for c in text if not c.isdigit())\n",
      "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
      "\n",
      "    return text\n",
      "```\n",
      "\n",
      "And apply the function to the clean text :\n",
      "\n",
      "```\n",
      "text = preprocess_text(output)\n",
      "```\n",
      "\n",
      "Then, define two dictionnaries we'll need later on :\n",
      "\n",
      "```\n",
      "dic={1 : ' ', \n",
      "    2 : 'a', \n",
      "    3 : 'b', \n",
      "    4: 'c', \n",
      "    5 : 'd', \n",
      "    6 : 'e', \n",
      "    7: 'f', \n",
      "    8 : 'g', \n",
      "    9 : 'h', \n",
      "    10: 'i', \n",
      "    11: 'j', \n",
      "    12 : 'k', \n",
      "    13 : 'l', \n",
      "    14: 'm', \n",
      "    15 : 'n', \n",
      "    16 : 'o', \n",
      "    17: 'p', \n",
      "    18 : 'q', \n",
      "    19 : 'r' , \n",
      "    20: 's', \n",
      "    21 : 't', \n",
      "    22 : 'u', \n",
      "    23: 'v', \n",
      "    24 : 'w', \n",
      "    25 : 'x' , \n",
      "    26: 'y', \n",
      "    27 : 'z'\n",
      "}\n",
      "\n",
      "dic_2 = {' ' : 0, \n",
      "    'a' : 1, \n",
      "    'b' : 2, \n",
      "    'c' : 3, \n",
      "    'd' : 4, \n",
      "    'e' : 5, \n",
      "    'f' : 6, \n",
      "    'g' : 7, \n",
      "    'h' : 8, \n",
      "    'i' : 9, \n",
      "    'j' : 10, \n",
      "    'k' : 11, \n",
      "    'l' : 12, \n",
      "    'm' : 13, \n",
      "    'n' : 14, \n",
      "    'o' : 15, \n",
      "    'p' : 16, \n",
      "    'q' : 17, \n",
      "    'r' : 18, \n",
      "    's' : 19, \n",
      "    't' : 20, \n",
      "    'u' : 21, \n",
      "    'v' : 22, \n",
      "    'w' : 23, \n",
      "    'x' : 24, \n",
      "    'y' : 25, \n",
      "    'z' : 26\n",
      "}\n",
      "```\n",
      "\n",
      "Alright, we now need to go through the whole text, and compute the number of time we went from one letter to another. I have kept the implementation really simple for explainability purposes. There are several ways to improve this part :\n",
      "\n",
      "```python\n",
      "a = np.zeros(27)\n",
      "b = np.zeros(27)\n",
      "c = np.zeros(27)\n",
      "d = np.zeros(27)\n",
      "e = np.zeros(27)\n",
      "f = np.zeros(27)\n",
      "g = np.zeros(27)\n",
      "h = np.zeros(27)\n",
      "i = np.zeros(27)\n",
      "j = np.zeros(27)\n",
      "k = np.zeros(27)\n",
      "l = np.zeros(27)\n",
      "m = np.zeros(27)\n",
      "n = np.zeros(27)\n",
      "o = np.zeros(27)\n",
      "p = np.zeros(27)\n",
      "q = np.zeros(27)\n",
      "r = np.zeros(27)\n",
      "s = np.zeros(27)\n",
      "t = np.zeros(27)\n",
      "u = np.zeros(27)\n",
      "v = np.zeros(27)\n",
      "w = np.zeros(27)\n",
      "x = np.zeros(27)\n",
      "y = np.zeros(27)\n",
      "z = np.zeros(27)\n",
      "space = np.zeros(27)\n",
      "\n",
      "prev = ' '\n",
      "\n",
      "for char in text:\n",
      "    if prev == ' ':\n",
      "        space[dic_2[char]] += 1\n",
      "    elif prev == 'a' : \n",
      "        a[dic_2[char]] += 1\n",
      "    elif prev == 'b':\n",
      "        b[dic_2[char]] += 1\n",
      "    elif prev == 'c':\n",
      "        c[dic_2[char]] += 1\n",
      "    elif prev == 'd':\n",
      "        d[dic_2[char]] += 1\n",
      "    elif prev == 'e':\n",
      "        e[dic_2[char]] += 1\n",
      "    elif prev == 'f':\n",
      "        f[dic_2[char]] += 1\n",
      "    elif prev == 'g':\n",
      "        g[dic_2[char]] += 1\n",
      "    elif prev == 'h':\n",
      "        h[dic_2[char]] += 1\n",
      "    elif prev == 'i':\n",
      "        i[dic_2[char]] += 1\n",
      "    elif prev == 'j':\n",
      "        j[dic_2[char]] += 1\n",
      "    elif prev == 'k':\n",
      "        k[dic_2[char]] += 1\n",
      "    elif prev == 'l':\n",
      "        l[dic_2[char]] += 1\n",
      "    elif prev == 'm':\n",
      "        m[dic_2[char]] += 1\n",
      "    elif prev == 'n':\n",
      "        n[dic_2[char]] += 1\n",
      "    elif prev == 'o':\n",
      "        o[dic_2[char]] += 1\n",
      "    elif prev == 'p':\n",
      "        p[dic_2[char]] += 1\n",
      "    elif prev == 'q':\n",
      "        q[dic_2[char]] += 1\n",
      "    elif prev == 'r':\n",
      "        r[dic_2[char]] += 1\n",
      "    elif prev == 's':\n",
      "        s[dic_2[char]] += 1\n",
      "    elif prev == 't':\n",
      "        t[dic_2[char]] += 1\n",
      "    elif prev == 'u':\n",
      "        u[dic_2[char]] += 1\n",
      "    elif prev == 'v':\n",
      "        v[dic_2[char]] += 1\n",
      "    elif prev == 'w':\n",
      "        w[dic_2[char]] += 1\n",
      "    elif prev == 'x':\n",
      "        x[dic_2[char]] += 1\n",
      "    elif prev == 'y':\n",
      "        y[dic_2[char]] += 1\n",
      "    elif prev == 'z':\n",
      "        z[dic_2[char]] += 1\n",
      "\n",
      "    prev = char\n",
      "```\n",
      "\n",
      "At that point, we have raw number which we need to transform into probabilities :\n",
      "\n",
      "```python\n",
      "a = a / np.sum(a)\n",
      "b = b / np.sum(b)\n",
      "c = c / np.sum(c)\n",
      "d = d / np.sum(d)\n",
      "e = e / np.sum(e)\n",
      "f = f / np.sum(f)\n",
      "g = g / np.sum(g)\n",
      "h = h / np.sum(h)\n",
      "i = i / np.sum(i)\n",
      "j = j / np.sum(j)\n",
      "k = k / np.sum(k)\n",
      "l = l / np.sum(l)\n",
      "m = m / np.sum(m)\n",
      "n = n / np.sum(n)\n",
      "o = o / np.sum(o)\n",
      "p = p / np.sum(p)\n",
      "q = q / np.sum(q)\n",
      "r = r / np.sum(r)\n",
      "s = s / np.sum(s)\n",
      "t = t / np.sum(t)\n",
      "u = u / np.sum(u)\n",
      "v = v / np.sum(v)\n",
      "w = w / np.sum(w)\n",
      "x = x / np.sum(x)\n",
      "y = y / np.sum(y)\n",
      "z = z / np.sum(z)\n",
      "space = space / np.sum(space)\n",
      "```\n",
      "\n",
      "To retrieve the final matrix, we can declare :\n",
      "\n",
      "```python\n",
      "trans_eng = np.matrix([space, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z])\n",
      "```\n",
      "\n",
      "I have summarized this into a function. Here is the whole code needed :\n",
      "\n",
      "```python\n",
      "global dic\n",
      "global dic_2\n",
      "\n",
      "dic={1 : ' ', \n",
      "    2 : 'a', \n",
      "    3 : 'b', \n",
      "    4: 'c', \n",
      "    5 : 'd', \n",
      "    6 : 'e', \n",
      "    7: 'f', \n",
      "    8 : 'g', \n",
      "    9 : 'h', \n",
      "    10: 'i', \n",
      "    11: 'j', \n",
      "    12 : 'k', \n",
      "    13 : 'l', \n",
      "    14: 'm', \n",
      "    15 : 'n', \n",
      "    16 : 'o', \n",
      "    17: 'p', \n",
      "    18 : 'q', \n",
      "    19 : 'r' , \n",
      "    20: 's', \n",
      "    21 : 't', \n",
      "    22 : 'u', \n",
      "    23: 'v', \n",
      "    24 : 'w', \n",
      "    25 : 'x' , \n",
      "    26: 'y', \n",
      "    27 : 'z'\n",
      "}\n",
      "\n",
      "dic_2 = {' ' : 0, \n",
      "    'a' : 1, \n",
      "    'b' : 2, \n",
      "    'c' : 3, \n",
      "    'd' : 4, \n",
      "    'e' : 5, \n",
      "    'f' : 6, \n",
      "    'g' : 7, \n",
      "    'h' : 8, \n",
      "    'i' : 9, \n",
      "    'j' : 10, \n",
      "    'k' : 11, \n",
      "    'l' : 12, \n",
      "    'm' : 13, \n",
      "    'n' : 14, \n",
      "    'o' : 15, \n",
      "    'p' : 16, \n",
      "    'q' : 17, \n",
      "    'r' : 18, \n",
      "    's' : 19, \n",
      "    't' : 20, \n",
      "    'u' : 21, \n",
      "    'v' : 22, \n",
      "    'w' : 23, \n",
      "    'x' : 24, \n",
      "    'y' : 25, \n",
      "    'z' : 26\n",
      "    }\n",
      "\n",
      "def preprocess_text(text) :\n",
      "\n",
      "    text = text.replace('\\n', '').replace('[ edit ]', '').replace(\"\\'\", \"'\")\n",
      "    text = ''.join(c.lower() for c in text if not c.isdigit())\n",
      "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
      "\n",
      "    return text\n",
      "\n",
      "def compute_transition(url_input):\n",
      "\n",
      "    url = url_input\n",
      "    res = requests.get(url)\n",
      "    html_page = res.content\n",
      "\n",
      "    soup = BeautifulSoup(html_page, 'html.parser')\n",
      "    text = soup.find_all(text=True)\n",
      "\n",
      "    output = ''\n",
      "    blacklist = [\n",
      "        '[document]',\n",
      "        'noscript',\n",
      "        'header',\n",
      "        'html',\n",
      "        'meta',\n",
      "        'head', \n",
      "        'input',\n",
      "        'script',\n",
      "        '\\n'\n",
      "    ]\n",
      "\n",
      "    for t in text:\n",
      "        if t.parent.name not in blacklist:\n",
      "        output += '{} '.format(t)\n",
      "\n",
      "    text = preprocess_text(output)\n",
      "\n",
      "    a = np.zeros(27)\n",
      "    b = np.zeros(27)\n",
      "    c = np.zeros(27)\n",
      "    d = np.zeros(27)\n",
      "    e = np.zeros(27)\n",
      "    f = np.zeros(27)\n",
      "    g = np.zeros(27)\n",
      "    h = np.zeros(27)\n",
      "    i = np.zeros(27)\n",
      "    j = np.zeros(27)\n",
      "    k = np.zeros(27)\n",
      "    l = np.zeros(27)\n",
      "    m = np.zeros(27)\n",
      "    n = np.zeros(27)\n",
      "    o = np.zeros(27)\n",
      "    p = np.zeros(27)\n",
      "    q = np.zeros(27)\n",
      "    r = np.zeros(27)\n",
      "    s = np.zeros(27)\n",
      "    t = np.zeros(27)\n",
      "    u = np.zeros(27)\n",
      "    v = np.zeros(27)\n",
      "    w = np.zeros(27)\n",
      "    x = np.zeros(27)\n",
      "    y = np.zeros(27)\n",
      "    z = np.zeros(27)\n",
      "    space = np.zeros(27)\n",
      "\n",
      "    prev = ' '\n",
      "\n",
      "    for char in text:\n",
      "        if prev == ' ':\n",
      "            space[dic_2[char]] += 1\n",
      "        elif prev == 'a' : \n",
      "            a[dic_2[char]] += 1\n",
      "        elif prev == 'b':\n",
      "            b[dic_2[char]] += 1\n",
      "        elif prev == 'c':\n",
      "            c[dic_2[char]] += 1\n",
      "        elif prev == 'd':\n",
      "            d[dic_2[char]] += 1\n",
      "        elif prev == 'e':\n",
      "            e[dic_2[char]] += 1\n",
      "        elif prev == 'f':\n",
      "            f[dic_2[char]] += 1\n",
      "        elif prev == 'g':\n",
      "            g[dic_2[char]] += 1\n",
      "        elif prev == 'h':\n",
      "            h[dic_2[char]] += 1\n",
      "        elif prev == 'i':\n",
      "            i[dic_2[char]] += 1\n",
      "        elif prev == 'j':\n",
      "            j[dic_2[char]] += 1\n",
      "        elif prev == 'k':\n",
      "            k[dic_2[char]] += 1\n",
      "        elif prev == 'l':\n",
      "            l[dic_2[char]] += 1\n",
      "        elif prev == 'm':\n",
      "            m[dic_2[char]] += 1\n",
      "        elif prev == 'n':\n",
      "            n[dic_2[char]] += 1\n",
      "        elif prev == 'o':\n",
      "            o[dic_2[char]] += 1\n",
      "        elif prev == 'p':\n",
      "            p[dic_2[char]] += 1\n",
      "        elif prev == 'q':\n",
      "            q[dic_2[char]] += 1\n",
      "        elif prev == 'r':\n",
      "            r[dic_2[char]] += 1\n",
      "        elif prev == 's':\n",
      "            s[dic_2[char]] += 1\n",
      "        elif prev == 't':\n",
      "            t[dic_2[char]] += 1\n",
      "        elif prev == 'u':\n",
      "            u[dic_2[char]] += 1\n",
      "        elif prev == 'v':\n",
      "            v[dic_2[char]] += 1\n",
      "        elif prev == 'w':\n",
      "            w[dic_2[char]] += 1\n",
      "        elif prev == 'x':\n",
      "            x[dic_2[char]] += 1\n",
      "        elif prev == 'y':\n",
      "            y[dic_2[char]] += 1\n",
      "        elif prev == 'z':\n",
      "            z[dic_2[char]] += 1\n",
      "\n",
      "        prev = char\n",
      "\n",
      "    a = a / np.sum(a)\n",
      "    b = b / np.sum(b)\n",
      "    c = c / np.sum(c)\n",
      "    d = d / np.sum(d)\n",
      "    e = e / np.sum(e)\n",
      "    f = f / np.sum(f)\n",
      "    g = g / np.sum(g)\n",
      "    h = h / np.sum(h)\n",
      "    i = i / np.sum(i)\n",
      "    j = j / np.sum(j)\n",
      "    k = k / np.sum(k)\n",
      "    l = l / np.sum(l)\n",
      "    m = m / np.sum(m)\n",
      "    n = n / np.sum(n)\n",
      "    o = o / np.sum(o)\n",
      "    p = p / np.sum(p)\n",
      "    q = q / np.sum(q)\n",
      "    r = r / np.sum(r)\n",
      "    s = s / np.sum(s)\n",
      "    t = t / np.sum(t)\n",
      "    u = u / np.sum(u)\n",
      "    v = v / np.sum(v)\n",
      "    w = w / np.sum(w)\n",
      "    x = x / np.sum(x)\n",
      "    y = y / np.sum(y)\n",
      "    z = z / np.sum(z)\n",
      "    space = space / np.sum(space)\n",
      "\n",
      "    return np.matrix([space, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z])\n",
      "```\n",
      "\n",
      "We can then pick long articles in English, French and Italian for example :\n",
      "\n",
      "```python\n",
      "trans_eng = compute_transition('https://en.wikipedia.org/wiki/South_African_labour_law')\n",
      "trans_fr = compute_transition('https://fr.wikipedia.org/wiki/Histoire_du_m%C3%A9tier_de_plombier')\n",
      "trans_it = compute_transition('https://it.wikipedia.org/wiki/Storia_d%27Italia')\n",
      "```\n",
      "\n",
      "The transition matrices are now computed! Let's move to the language recognition part.\n",
      "\n",
      "## Identify the language\n",
      "\n",
      "We will now try to identify the language based on the transition likelihood. All we need to do is, for each language, roll back identify the transition probability from one letter to another, and return the most likely language.\n",
      "\n",
      "```python\n",
      "def rec_language(dic, dic_2, bi_eng, bi_fr, bi_it, seq) :\n",
      "\n",
      "    seq = preprocess_text(seq)\n",
      "\n",
      "    key_0 = 0\n",
      "    trans_eng = 1\n",
      "    trans_fra = 1\n",
      "    trans_it = 1\n",
      "\n",
      "    for letter in seq :\n",
      "    \n",
      "    # If unknown character missed by pre-processing\n",
      "        try :\n",
      "            key_1 = dic_2[letter]\n",
      "\n",
      "            trans_eng = trans_eng * bi_eng[key_0, key_1]\n",
      "            trans_fra = trans_fra * bi_fr[key_0, key_1]\n",
      "            trans_it = trans_it * bi_it[key_0, key_1]\n",
      "\n",
      "            key_0 = dic_2[letter]\n",
      "        except :\n",
      "            continue\n",
      "\n",
      "    if trans_eng > trans_fra and trans_eng > trans_it :\n",
      "        print(\"It's English !\")\n",
      "    elif trans_fra > trans_eng and trans_fra > trans_it :\n",
      "        print(\"It's French !\") \n",
      "    else :\n",
      "        print(\"It's Italian !\")\n",
      "```\n",
      "\n",
      "We can now try it in some sentences! First, a french sentence :\n",
      "\n",
      "```python\n",
      "rec_language(dic, dic_2, trans_eng, trans_fr, trans_it, \"Quel beau temps aujourd'hui !\")\n",
      "```\n",
      "\n",
      "Returns : \n",
      "```\n",
      "It's French !\n",
      "```\n",
      "\n",
      "Then, in English :\n",
      "\n",
      "```python\n",
      "rec_language(dic, dic_2, trans_eng, trans_fr, trans_it, 'What a nice weather today !')\n",
      "```\n",
      "\n",
      "Returns :\n",
      "\n",
      "```\n",
      "It's English !\n",
      "```\n",
      "\n",
      "And in italian : \n",
      "\n",
      "```python\n",
      "rec_language(dic, dic_2, trans_eng, trans_fr, trans_it, 'Che bello tempo fa oggi !')\n",
      "```\n",
      "\n",
      "Returns :\n",
      "\n",
      "```\n",
      "It's Italian !\n",
      "```\n",
      "\n",
      "## Potential improvements \n",
      "\n",
      "We fetched the transition probabilities from single articles in Wikipedia. To develop a more robust solution, we should consider a large input corpus.\n",
      "\n",
      "The text pre-processing is not perfect, and we should add some more features to it.\n",
      "\n",
      "Finally, we tested only 3 languages, but we could generalize the solution we have developed to other languages.\n",
      " \n",
      "# Standalone App with Voilà\n",
      " \n",
      " You might have heard of Voilà that lets you run your Jupyter Notebooks as standalone apps. Let's try this out!\n",
      " \n",
      " Start by installing Voilà :\n",
      " \n",
      " ```\n",
      " pip install voila\n",
      " ```\n",
      " \n",
      " Then, in the notebook, create an interactive widget :\n",
      " \n",
      " ```python\n",
      " from ipywidgets import widgets\n",
      " from ipywidgets import interact, interactive, fixed, interact_manual\n",
      " \n",
      " def reco_interactive(x):\n",
      "    return rec_language(dic, dic_2, trans_eng, trans_fr, trans_it, x)\n",
      " ```\n",
      " \n",
      " And run the interactive cell :\n",
      " \n",
      " ```python\n",
      " interact(reco_interactive, x='Hi there!');\n",
      " ```\n",
      " \n",
      " You should see something like this :\n",
      " \n",
      " ![image](https://maelfabien.github.io/assets/images/voila.jpg)\n",
      " \n",
      " Then, to create an app from it, simply run from your terminal, in the notebook's folder :\n",
      " \n",
      " ```\n",
      " voila notebook.ipynb\n",
      " ```\n",
      " \n",
      " You'll have access to a webpage where the interactive widget works as a standalone app!\n",
      " \n",
      "  ![image](https://maelfabien.github.io/assets/images/voila_2.jpg)\n",
      " \n",
      "> **Conclusion** : Although text embedding and deep learning seem to be everywhere nowadays, simple approaches like likelihood Decoding algorithm and Markov Chains can bring value if we're looking for a light, fast and explainable solution (think about including this in a smartphone's software for example). I hope this was useful, and don't hesitate to comment if you have any question.\n",
      "\n",
      "---\n",
      "title: A No-SQL Big Data project from scratch\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "The GDELT Project monitors the world's broadcast, print, and web news from nearly every corner of every country in over 100 languages and identifies the people, locations, organizations, themes, sources, emotions, counts, quotes, images and events driving our global society every second of every day, creating a free open platform for computing on the entire world. With new files uploaded every 15 minutes, GDELT data bases contain more than 700 Gb of zipped data for the single year 2018.\n",
      "\n",
      "In order to be able to work with a large amount of data, we have chosen to work with the following architecture :\n",
      "- NoSQL : Cassandra\n",
      "- AWS : EMR to transfer the data to Cassandra, and EC2 for the resiliency for the requests\n",
      "- Visualization : A Zeppelin Notebook\n",
      "\n",
      "Contributors : Raphael Lederman, Anatoli De Bradke, Alexandre Bec, Anthony Houdaille, Thomas Binetruy\n",
      "\n",
      "The Github of the project can be found here :\n",
      "\n",
      "<div class=\"github-card\" data-github=\"maelfabien/Cassandra-GDELT-Queries\" data-width=\"100%\" data-height=\"\" data-theme=\"default\"></div>\n",
      "<script src=\"//cdn.jsdelivr.net/github-cards/latest/widget.js\"></script>\n",
      "\n",
      "<br>\n",
      "\n",
      "## 0. Articles\n",
      "\n",
      "I've made a series of articles regarding this work. Here the links in order :\n",
      "- [Install and run Zeppelin Locally](https://maelfabien.github.io/bigdata/zeppelin_local/)\n",
      "- [Install and run Zeppelin on AWS EMR](https://maelfabien.github.io/bigdata/zeppelin_emr/)\n",
      "- [Working with Amazon S3 buckets](https://maelfabien.github.io/bigdata/storage/)\n",
      "- [Launch and access an AWS EC2 Cluster](https://maelfabien.github.io/bigdata/EC2/)\n",
      "- [Install Apache Cassandra on an AWS EC2 Cluster](https://maelfabien.github.io/bigdata/EC2_Cassandra/)\n",
      "- [Install Zookeeper on EC2 instances](https://maelfabien.github.io/bigdata/ZK/)\n",
      "- [Install Apache Spark on EC2 instances](https://maelfabien.github.io/bigdata/Spark/)\n",
      "- [Big (Open)  Data , the GDELT Project](https://maelfabien.github.io/bigdata/zeppelin-GDELT/)\n",
      "- [Build an ETL in Scala for GDELT Data](https://maelfabien.github.io/bigdata/Scala/)\n",
      "- [Move Scala Dataframes to Cassandra](https://maelfabien.github.io/bigdata/Scala_Cassandra/)\n",
      "\n",
      "## 1. The data\n",
      "\n",
      "- [Description of the data Mentions and Events](http://data.gdeltproject.org/documentation/GDELT-Event_Codebook-V2.0.pdf)\n",
      "- [Description of the Graph of Events GKG](http://data.gdeltproject.org/documentation/GDELT-Global_Knowledge_Graph_Codebook-V2.1.pdf)\n",
      "\n",
      "![images](https://maelfabien.github.io/assets/images/data.png)\n",
      "\n",
      "A event is defined as an action that an actor (Actor1) takes on another actor (Actor2). A mention is an article or any source that talks about an event. The GKG database reflects the events that took place in the world, ordered by theme, type of event and location.\n",
      "\n",
      "The conceptual model of the data is the following :\n",
      "![alt text](https://maelfabien.github.io/assets/images/concept.png)\n",
      "\n",
      "## 2. Architecture\n",
      "\n",
      "The architecture we have chosen is the following :\n",
      "![alt text](https://maelfabien.github.io/assets/images/archi.png)\n",
      "\n",
      "Our architecture is composed by one cluster EMR (1 master and 5 slaves) and one cluster EC2 (8 instances).\n",
      "\n",
      "In our 8 EC2 instances we have : \n",
      "- 2 Masters nodes with apache-Spark-2.3.2 and apache-Zeppelin-0.8.0\n",
      "- 5 Slaves nodes with apache-Spark-2.3.2 and apache-cassandra-3.11.2, including zookeeper installed on 2 of these nodes.\n",
      "- The last one is a node created for the resilience of the Master. We Installed zookeeper in it. \n",
      "\n",
      "The Slaves resilience is automatically handled by the master Spark. The Masters resilience is handled by Zookeper. For the Zookeeper, refer to the [ReadMe of the dedicated Github folder](https://github.com/maelfabien/gdelt/tree/master/Zookeeper)\n",
      "\n",
      "The cluster EMR is used to transfer data from S3 to our Cassandra nodes on EC2. The reason for this architecture is that our EC2 Spark instaces could not connect to S3 due to issues with package dependencies. For Cassandra, refer to the [ReadMe the dedicated Github folder](https://github.com/maelfabien/gdelt/tree/master/Cassandra)\n",
      "\n",
      "We do not find any solution :\n",
      "[link](https://docs.hortonworks.com/HDPDocuments/HDCloudAWS/HDCloudAWS-1.8.0/bk_hdcloud-aws/content/s3-trouble/index.html)\n",
      "\n",
      "![alt text](https://maelfabien.github.io/assets/images/hortonworks.png)\n",
      "\n",
      "When all data are on our Casandra nodes, we shutdown the EMR cluster. We run our Spark-SQL request on Zeppelin.\n",
      "\n",
      "![alt text](https://maelfabien.github.io/assets/images/spark.png)\n",
      "\n",
      "## 3. Data Preparation\n",
      "\n",
      "Import the necessary packages :\n",
      "\n",
      "```scala\n",
      "// Imports\n",
      "import sys.process._\n",
      "import java.net.URL\n",
      "import java.io.File\n",
      "import java.io.File\n",
      "import java.nio.file.{Files, StandardCopyOption}\n",
      "import java.net.HttpURLConnection \n",
      "import org.apache.spark.sql.functions._\n",
      "import sqlContext.implicits._\n",
      "import org.apache.spark.input.PortableDataStream\n",
      "import java.util.zip.ZipInputStream\n",
      "import java.io.BufferedReader\n",
      "import java.io.InputStreamReader\n",
      "import org.apache.spark.sql.SQLContext\n",
      "import com.amazonaws.services.s3.AmazonS3Client\n",
      "import com.amazonaws.auth.BasicAWSCredentials\n",
      "import org.apache.spark.sql.cassandra._\n",
      "import com.datastax.spark.connector._\n",
      "import org.apache.spark.sql.types.IntegerType\n",
      "```\n",
      "\n",
      "The ZIP files are extracted from the GDELT website following this procedure :\n",
      "\n",
      "1. Define a file downloading function\n",
      "\n",
      "```scala\n",
      "def fileDownloader(urlOfFileToDownload: String, fileName: String) = {\n",
      "    val url = new URL(urlOfFileToDownload)\n",
      "    val connection = url.openConnection().asInstanceOf[HttpURLConnection]\n",
      "    connection.setConnectTimeout(5000)\n",
      "    connection.setReadTimeout(5000)\n",
      "    connection.connect()\n",
      "\n",
      "    if (connection.getResponseCode >= 400)\n",
      "        println(\"error\")\n",
      "    else\n",
      "        url #> new File(fileName) !!\n",
      "}\n",
      "```\n",
      "\n",
      "Grab the list of URLs to download the ZIP files from, from the english and the translated documents.\n",
      "\n",
      "```scala\n",
      "// Download locally the list of URL\n",
      "fileDownloader(\"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\", \"/tmp/masterfilelist.txt\") // save the list file to the Spark Master\n",
      "fileDownloader(\"http://data.gdeltproject.org/gdeltv2/masterfilelist-translation.txt\", \"/tmp/masterfilelist_translation.txt\") \n",
      "```\n",
      "\n",
      "Then, put the file that contains the list of the files to download into the S3 bucket :\n",
      "\n",
      "```scala\n",
      "awsClient.putObject(\"fabien-mael-telecom-gdelt2018\", \"masterfilelist.txt\", new File(\"/tmp/masterfilelist.txt\") )\n",
      "awsClient.putObject(\"fabien-mael-telecom-gdelt2018\", \"masterfilelist_translation.txt\", new File( \"/tmp/masterfilelist_translation.txt\") )\n",
      "```\n",
      "\n",
      "We will focus only on year 2018 :\n",
      "\n",
      "```scala\n",
      "val list_csv = spark.read.format(\"csv\").option(\"delimiter\", \" \").\n",
      "    csv(\"s3a://fabien-mael-telecom-gdelt2018/masterfilelist.txt\").\n",
      "    withColumnRenamed(\"_c0\",\"size\").\n",
      "    withColumnRenamed(\"_c1\",\"hash\").\n",
      "    withColumnRenamed(\"_c2\",\"url\")\n",
      "val list_2018_tot = list_csv.where(col(\"url\").like(\"%/2018%\"))\n",
      "```\n",
      "\n",
      "We download all the data of 2018 for the English URLs :\n",
      "\n",
      "```scala\n",
      "list_2018_tot.select(\"url\").repartition(100).foreach( r=> {\n",
      "    val URL = r.getAs[String](0)\n",
      "    val fileName = r.getAs[String](0).split(\"/\").last\n",
      "    val dir = \"/mnt/tmp/\"\n",
      "    val localFileName = dir + fileName\n",
      "    fileDownloader(URL,  localFileName)\n",
      "    val localFile = new File(localFileName)\n",
      "    AwsClient.s3.putObject(\"fabien-mael-telecom-gdelt2018\", fileName, localFile )\n",
      "    localFile.delete()\n",
      "})\n",
      "\n",
      "```\n",
      "\n",
      "We duplicate this task for the translation data. Then, we need to create four data frames : \n",
      "- Mentions in english\n",
      "- Events in english\n",
      "- Mentions translated\n",
      "- Events translated\n",
      "\n",
      "This is done the following way :\n",
      "\n",
      "```scala\n",
      "val mentionsRDD_trans = sc.binaryFiles(\"s3a://fabien-mael-telecom-gdelt2018/201801*translation.mentions.CSV.zip\"). // charger quelques fichers via une regex\n",
      "    flatMap {  // decompresser les fichiers\n",
      "        case (name: String, content: PortableDataStream) =>\n",
      "            val zis = new ZipInputStream(content.open)\n",
      "            Stream.continually(zis.getNextEntry).\n",
      "            takeWhile{ case null => zis.close(); false\n",
      "                case _ => true }.\n",
      "            flatMap { _ =>\n",
      "                val br = new BufferedReader(new InputStreamReader(zis))\n",
      "                Stream.continually(br.readLine()).takeWhile(_ != null)\n",
      "            }\n",
      "    }\n",
      "    \n",
      "val mentionsDF_trans = mentionsRDD_trans.map(x => x.split(\"\\t\")).map(row => row.mkString(\";\")).map(x => x.split(\";\")).toDF()\n",
      "```\n",
      "\n",
      "In order to reach fast responding queries, we create several smaller data frames for the different queries we later on build. For example :\n",
      "\n",
      "```scala\n",
      "// Mentions\n",
      "val mentions_trans_1 = mentionsDF_trans.withColumn(\"_tmp\", $\"value\").select(\n",
      "    $\"_tmp\".getItem(0).as(\"globaleventid\"),\n",
      "    $\"_tmp\".getItem(14).as(\"language\")\n",
      "    )\n",
      "val mentions_1 = mentionsDF.withColumn(\"_tmp\", $\"value\").select(\n",
      "    $\"_tmp\".getItem(0).as(\"globaleventid\"),\n",
      "    $\"_tmp\".getItem(14).as(\"language\")\n",
      "    )\n",
      "\n",
      "// Events \n",
      "val events_trans_1 = exportDF_trans.withColumn(\"_tmp\", $\"value\").select(\n",
      "    $\"_tmp\".getItem(0).as(\"globaleventid\"),\n",
      "    $\"_tmp\".getItem(1).as(\"day\"),\n",
      "    $\"_tmp\".getItem(33).as(\"numarticles\"),\n",
      "    $\"_tmp\".getItem(53).as(\"actioncountry\")\n",
      "    )\n",
      "val events_1 = exportDF.withColumn(\"_tmp\", $\"value\").select(\n",
      "    $\"_tmp\".getItem(0).as(\"globaleventid\"),\n",
      "    $\"_tmp\".getItem(1).as(\"day\"),\n",
      "    $\"_tmp\".getItem(33).as(\"numarticles\"),\n",
      "    $\"_tmp\".getItem(53).as(\"actioncountry\")\n",
      "    )\n",
      "\n",
      "// Join english and translated data\n",
      "val df_events_1 = events_1.union(events_trans_1)\n",
      "val df_mentions_1 = mentions_1.union(mentions_trans_1)\n",
      "\n",
      "// Join events and mentions\n",
      "val df_1 = df_mentions_1.join(df_events_1,\"GlobalEventID\")\n",
      "```\n",
      "\n",
      "We can later on build the Cassandra tables that will allow us transfer the spark dataframes :\n",
      "\n",
      "```scala\n",
      "%cassandra\n",
      "CREATE TABLE q1_1(\n",
      "day int,\n",
      "language text,\n",
      "actioncountry text,\n",
      "numarticles int,\n",
      "PRIMARY KEY (day, language, actioncountry));\n",
      "```\n",
      "\n",
      "Finally, we can write the dataframe to Cassandra. Therefore, the lazy evaluation will take place before we add requests to the data base.\n",
      "\n",
      "```scala\n",
      "df_1.write.cassandraFormat(\"q1_1\", \"gdelt_datas\").save()\n",
      "val df_1_1 = spark.read.cassandraFormat(\"q1_1\", \"gdelt_datas\").load()\n",
      "df_1_1.createOrReplaceTempView(\"q1_1\")\n",
      "```\n",
      "\n",
      "The requests are then simple to make :\n",
      "\n",
      "```scala \n",
      "z.show(spark.sql(\"\"\" SELECT * FROM q1_1 ORDER BY NumArticles DESC LIMIT 10 \"\"\"))\n",
      "```\n",
      "\n",
      "![alt text](https://maelfabien.github.io/assets/images/q1_2.png)\n",
      "\n",
      "Zeppelin also has a great feature of Map Visualization. In order to activate it, you need to activate the Helium Zeppelin Leaflet in Zeppelin.\n",
      "\n",
      "![alt text](https://maelfabien.github.io/assets/images/helium.png)\n",
      "\n",
      "Simply click on Helium from the interpreter menu, and activate \"Zeppelin Leaflet\". A new option on the Zeppelin show request will then appear :\n",
      "\n",
      "![alt text](https://maelfabien.github.io/assets/images/map.png)\n",
      "\n",
      "## 4. Exploration\n",
      "\n",
      "We will present in this section the results of the analsis we lead on the Cassandra tables :\n",
      "\n",
      "1. Number of articles by day and language\n",
      "![alt text](https://maelfabien.github.io/assets/images/q1_1.png)\n",
      "\n",
      "2. Number of articles by day\n",
      "![alt text](https://maelfabien.github.io/assets/images/q1_2.png)\n",
      "\n",
      "3. Number of articles by language\n",
      "![alt text](https://maelfabien.github.io/assets/images/q1_3.png)\n",
      "\n",
      "4. Countries which received the most negative articles \n",
      "![alt text](https://maelfabien.github.io/assets/images/q3_1.png)\n",
      "\n",
      "5. Actors or countries that divide the most \n",
      "![alt text](https://maelfabien.github.io/assets/images/q4_1.png)\n",
      "\n",
      "6. Evolution of the relation two countries (Here USA and Israel)\n",
      "![alt text](https://maelfabien.github.io/assets/images/q5_1.png)\n",
      "\n",
      "\n",
      "## 5. Performance\n",
      "\n",
      "The total zipped files (Mentions, Events, GKG) reached 698.9 Gb on our S3 bucket. \n",
      "\n",
      "```\n",
      "aws s3 ls --summarize --human-readable --recursive s3://fabien-mael-telecom-gdelt2018\n",
      "```\n",
      "\n",
      "![alt text](https://maelfabien.github.io/assets/images/total.png)\n",
      "\n",
      "In order to write the files into Cassandra, for 1 month of data, the requests usually took around 20 minutes. Then, once loaded, the different requests could be loaded within 1 to 10 seconds at most.\n",
      "\n",
      "## 6. Resiliency\n",
      "\n",
      "By killing a worker from AWS :\n",
      "![alt text](https://maelfabien.github.io/assets/images/worker.png)\n",
      "\n",
      "The resiliency should be observed. The requests should always be able to run due to the replication factor of Cassandra.\n",
      "\n",
      "Also, if one of the Spark master nodes fail, Zookeeper elects a new leader node and workers switch to the new leader node.\n",
      "\n",
      "## 7. Budget\n",
      "\n",
      "One should take into account the budget of implementing such structure. The costs for our project was the following :\n",
      "- S3 storage and EMR to load the data in Cassandra : 150$\n",
      "- EC2 instances : 60$\n",
      "\n",
      "For an overall budget of 210$. The architecture was up for 3 days.\n",
      "\n",
      "## 8. Potential improvements\n",
      "\n",
      "Some recent projects on the GDELT Project include :\n",
      "- a streaming architecture updating the data every 15 minutes\n",
      "- a web interface using CartoDB\n",
      "- a further exploration : Time series, ML, DL\n",
      "- automate deployment (Ansible, Docker...)\n",
      "\n",
      "---\n",
      "title: Word Embedding with Skip-Gram Word2Vec\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Natural Language Processing\"\n",
      "---\n",
      "\n",
      "To analyze text and run algorithms on it, we need to embed the text. The notion of embedding simply means that we'll convert the input text into a set of numerical vectors that can be used into algorithms. In this article, we'll focus on Word2Vec, a state of the art embedding method, that embeds each word individually.\n",
      "\n",
      "For the sake of clarity, we'll call a document a simple text, and each document is made of words, which we'll call terms.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "The techniques we are about to cover were first introduced by Google in 2013. The original paper can be found [here](https://arxiv.org/pdf/1301.3781.pdf). Among the authors, you'll probably recognize Jeff Dean or Tomas Mikolov.\n",
      "\n",
      "# I. Why should we embed words?\n",
      "\n",
      "Embedding, as stated above, is used to create feature vectors from words. The idea is to encode their meaning and allow us to calculate a similarity score for any pair of words for example.\n",
      "\n",
      "What can we do with an embedded corpus?\n",
      "- determine similarity scores between documents for a search engine, using cosine similarity for example\n",
      "- identify topics of the documents\n",
      "- build a recommender system to suggest other movies based on the synopsis\n",
      "- machine translation to identify that \"Au revoir\" and \"Goodbye\" actually mean the same thing\n",
      "\n",
      "# II. General Word Embedding Principles\n",
      "\n",
      "In Word Embedding in general, we want a model to learn to associate a vector to a word but embedding the semantic links between words, using the word context.\n",
      "\n",
      "The main hypothesis between word embedding is **the distributional semantics**. We suppose that 2 words occurring in the same context have semantic proximity.\n",
      "\n",
      "We need as an input a large dataset of texts, and by training a model, we expect to get for each word of the vocabulary, a vector of a pre-determined size, say 300, that looks like this :\n",
      "\n",
      "```\n",
      "dog = [0.2854, 0.8711, -0.5217, 0.1281, ...] \n",
      "```\n",
      "\n",
      "We generally use Neural Networds to encode those dimensions, in methods such as Word2Vec. These representations are very good at encoding dimensions of similarity. Indeed, in the embedding space, the following relations should more or less hold :\n",
      "- Syntactically : $$ x_{orange} - x_{oranges} = x_{train} - x_{trains} = x_{phone} - x_{phones} $$\n",
      "- Semantically : $$ x_{queen} - x_{woman} = x_{king} - x_{man} $$\n",
      "\n",
      "The context of a word is the set of the C surrounding words. Words on the left of the word we'd like to embed are called the left context. Words on the right are called the right context. The order of the words within the context has however no importance.\n",
      "\n",
      "For example, say that we want to embed the word \"dog\", and encounter the following sentence :\n",
      "\n",
      "`I am walking my dog with Julie`\n",
      "\n",
      "If we take a 2 left-right context, i.e 2 words on the left, and 2 words on the right, the context will be :\n",
      "\n",
      "{Walking, my, with, Julie}\n",
      "\n",
      "Within the context of the embedding of the dog, the words Walking and Julie will be relevant.\n",
      "\n",
      "# III. The Word2Vec Skip-gram model\n",
      "\n",
      "Word2Vec works pretty much as an auto-encoder. We will train on one side a neural network to perform a certain task on one side, and on the other side to undo it to get back to the original result. \n",
      "\n",
      "There are two variants of the Word2Vec paradigm – skip-gram and CBOW. The skip-gram variant takes a target word and tries to predict the surrounding context words, while the CBOW (continuous bag of words) variant takes a set of context words and tries to predict a target word. We'll start by focusing on the Skip-gram model, and develop the C-BoW in a further article.\n",
      "\n",
      "## General idea\n",
      "\n",
      "First of all, we need to define the list of vocabulary. The task we'll teach our model to do is to learn the probability that two words are \"nearby\" within a text. For each \"target\" word among our vocabulary, we do that by :\n",
      "- defining a window size, i.e. the context of the word\n",
      "- picking randomly a word within the window size around the target word\n",
      "- counting the number of times they appear together\n",
      "- applying softmax activation to create probabilities\n",
      "\n",
      "The output probabilities are a measure of how likely it is that these 2 words appear together.\n",
      "\n",
      "We give the network as an entry the pairs of words. To add randomness into the process, the window size in the training process is chosen randomly, between 1 and the specified window size.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_3.jpg)\n",
      "\n",
      "If we have 5'000 words of vocabulary and we fit our model on it, the output of the network should be a 5'000 * 5'000 matrix, and within each position the probability that these words appear together.\n",
      "\n",
      "## Model details\n",
      "\n",
      "The model is made of a single hidden layer and an output layer with a softmax classifier.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/skp_gr.png)\n",
      "\n",
      "- The input of the model is a vector that has the size of the vocabulary, 0's everywhere except for a 1 at the position of the word we'd like to embed.\n",
      "\n",
      "- The output of the model is a softmax layer that has the size of the vocabulary, and for each word, the probability that this word will be observed in the context of the input word. Softmax transforms to map the output as probabilities that sum to 1 :\n",
      "\n",
      "$$ soft_x = \\frac {e^x} {\\sum e^x} $$\n",
      "\n",
      "- The hidden layer helps to choose the size of the vectors we'll later be using. If we set the hidden layer size to 500, the hidden layer will be a weight matrix of the size of the vocabulary, say 15'000 rows times 500 columns. The hidden layer in this model is mainly operating a lookup-table, i.e selecting the row corresponding to the word vector. Google, for its Word2Vec, has used 300 dimensions for example. \n",
      "\n",
      "By multiplying a feature vector of \"dog\" with an output layer of \"walking\", what we're computing here is the probability that if we pick a word randomly around \"dog\", this word is \"walking\" for example. \n",
      "\n",
      "If two words are different be happen in similar contexts, our model will learn similar word vectors for these words, i.e. \"engine\" and \"transmission\".\n",
      "\n",
      "All we need to do once the model has been trained is to drop the output layer. Indeed, we are only interested in the vector representation of the words, and as in auto-encoder, the second part of the network will not be used.\n",
      "\n",
      "## Limits of the Skip-gram\n",
      "\n",
      "The Skip-Gram model must compute a huge number of weights. For a vocabulary size of 10'000 words and 500 features, we would have 5 million weights in the hidden layer and the output layer. We also need a huge number of training data (typically counting in billions at that point). Since companies like Google train those models on the entire Web (or close), the number of weights to compute is just enormous. \n",
      "\n",
      "It has been shown recently that training a single Word Embedding Model can produce as much Co2 as 5 cars in their entire lifetime. See [this article](https://www.technologyreview.com/s/613630/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/) for more information.\n",
      "\n",
      "The training becomes pretty much impossible, which is the reason why the authors of Word2Vec have developed a second version called Continuous Bag-Of-Words that contains several tweaks to make the training faster.\n",
      "\n",
      "## Performance improvements of Word2Vec Skip-Gram model\n",
      "\n",
      "### Subsampling frequent words\n",
      "\n",
      "When we create training samples :\n",
      "- words such as \"the\", \"my\", \"a\"... don't bring much information\n",
      "- we'll end up with too many samples containing these stop words\n",
      "\n",
      "The authors propose a sampling rate which states whether we should keep a word or not. $$ P(w_i) $$ describes the probability of keeping a word :\n",
      "\n",
      "$$ P(w_i) = ( \\sqrt{ \\frac {z(w_i)} {0.001}} + 1 ) \\times \\frac {0.001} {z(w_i)} $$\n",
      "\n",
      "Where $$ z(w_i) $$ is the fraction of the total words in the corpus that the word $$ w_i $$ represents.\n",
      "\n",
      "The value 0.001 is proportional to the fraction of the most frequent words we'll remove. By training our model on the whole of Wikipedia, setting the sample to 0.001 would only remove 27 words. However, these 27 words represent up to 33% of all words on Wikipedia. These words include :\n",
      "- the\n",
      "- of\n",
      "- and\n",
      "- in\n",
      "- to\n",
      "- was\n",
      "- is \n",
      "- ...\n",
      "\n",
      "By subsampling, we can reduce this 33% ratio to 22%, since the probabilities to keep those words are rather low.\n",
      "\n",
      "### Context Position Weighting\n",
      "\n",
      "In the most Word2Vec most standard implementation, we randomly shrink the size of the context window.  We want to weight the context words according to how close they are to the target word. This is done in the random shrinking. Indeed, for different maximum window sizes, the percentage that a given box is included in a context is the following :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/window.jpg)\n",
      "\n",
      "### Negative sampling\n",
      "\n",
      "By training our neural network, we adjust the neuron weights for each input data. However, we have to modify **all** weights at each step, i.e. billions of weight for each training sample. \n",
      "\n",
      "In negative sampling, we only modify a small percentage of the weights at each step. Indeed, we will modify the weights of \"negative samples\", i.e a list of 5 to 20 words that we will take as inputs along with the target word. \n",
      "\n",
      "We want the neuron to output 0 for all the negative samples that we added to our target word. By doing this, instead of selecting the whole vocabulary size, we select typically 10+1=11 words out of a vocabulary size of 10'000 for example. This is a drastic drop in the overall amount of computations required.\n",
      "\n",
      "We are doing kind of a random pick of the words to select in the negative samples, with slight modification since we raise an element to a certain power (this was chosen empirically in the original paper).\n",
      "\n",
      "$$ P(w_i) = \\frac {f(w_i)^{\\frac{3}{4}}} {\\sum_j(f(w_j)^{\\frac{3}{4}})} $$\n",
      "\n",
      "Where $$ f(w_i) $$ is the number of times a given word appears in the corpus, and the denominator is the total number of words in the corpus.\n",
      "\n",
      "> **Conclusion** : And this is it. We have covered the main concepts of the Skip-Gram version of Word2Vec. I hope this introduction to Bag-Of-Words in NLP was helpful. Don't hesitate to drop a comment if you have a comment.\n",
      "\n",
      "Sources :\n",
      "- NLP class at Telecom ParisTech\n",
      "- The Inner Workings of Word2Vec by Chris McCormick, which I highly recommend\n",
      "\n",
      "---\n",
      "title: Predicting the song of the year 2019 (1/3)\n",
      "layout: post\n",
      "subtitle : \"Hackathon\"\n",
      "---\n",
      "\n",
      "Earlier this year, at Telecom ParisTech, we had the opportunity to participate in a 10 hours Hackathon. \n",
      "The subject was pretty open: What will be the hit song of 2019?\n",
      "\n",
      "We were working in groups of 5 and had 10 hours to hand in a Jupyter Notebook. I wanted to share this experience, and especially the way we organized the group project, and the outcomes. \n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "## The first 30 minutes\n",
      "\n",
      "The first 30 minutes are the most important of the whole day. The questions you should answer in those first 30 minutes are the following :\n",
      "- which data sets are you going to use?\n",
      "- who will be responsible for what?\n",
      "- are your data sources sufficient? Should you diversify?\n",
      "- how will you break down the project?\n",
      "\n",
      "This hackathon was centered on the storytelling we could build around our results, the prediction accuracy being only secondary. Therefore, we decided to break down the project in key questions, find a dataset that would support each or several questions. We planned to answer the following questions :\n",
      "- Which country should you focus on?\n",
      "- Who should you sing with?\n",
      "- What should your nationality be?\n",
      "- What kind of music should you play?\n",
      "- What kind of lyrics should you write?\n",
      "- How many words should you pronounce in a single song?\n",
      "- Should you repeat words?\n",
      "- What are the main words you should use?\n",
      "- Should you write a positive or a negative song?\n",
      "- What kind of mood should you express?\n",
      "- What are the most important musical features of a top hit?\n",
      "- Are we able to build a prediction from the features we extracted?\n",
      "- Making a video clip on YouTube, worth it?\n",
      "- When should the song be released?\n",
      "\n",
      "A few imports before we start... (not all of them are useful at this step). \n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import IPython.display as ipd\n",
      "import requests\n",
      "import lyricsgenius as genius\n",
      "from glob import glob\n",
      "import os.path as op\n",
      "from nltk.corpus import stopwords\n",
      "import re\n",
      "import itertools\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "from sklearn import model_selection\n",
      "from sklearn.svm import LinearSVC\n",
      "import pickle\n",
      "from bs4 import BeautifulSoup\n",
      "import seaborn as sns\n",
      "from matplotlib import rc\n",
      "from sklearn import linear_model\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.linear_model import LassoCV\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn import metrics\n",
      "import requests, json, logging\n",
      "import pandas as pd\n",
      "import base64\n",
      "import six\n",
      "from sklearn import  linear_model\n",
      "```\n",
      "\n",
      "## Which country should you focus on?\n",
      "\n",
      "We tried to answer this question by looking at the total number of streams in the 10 regions of the world that listen to the most music. We downloaded data of streams from the Spotify Top 200 charts: https://spotifycharts.com/regional\n",
      "\n",
      "Load datas from the Spotify Top Charts : https://spotifycharts.com/regional\n",
      "```python\n",
      "folder = \"**********/Hackathon/Total/\"\n",
      "onlyfiles = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
      "print(\"Working with {0} csv files\".format(len(onlyfiles)))\n",
      "```\n",
      "\n",
      "We then gathered all files in a single data frame :\n",
      "```python\n",
      "data = []\n",
      "\n",
      "for file in onlyfiles :\n",
      "    if file != '.DS_Store' :\n",
      "        df = pd.read_csv(folder + file, skiprows=1)\n",
      "        df['country'] = file[9:11]\n",
      "        df['week'] = file[19:20]\n",
      "        data.append(df)\n",
      "\n",
      "data = pd.concat(data, axis=0)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/tabh1.jpg){:height=\"100%\" width=\"100%\"}\n",
      "\n",
      "We now need to group some rows, as you may have understood that the data are ordered per region per week. \n",
      "\n",
      "```python\n",
      "data_country = data.groupby(['country'])[[\"Streams\"]].sum().sort_values('Streams', ascending=True)\n",
      "```\n",
      "\n",
      "```python\n",
      "data_artists = data.groupby(['Artist'])[[\"Streams\"]].sum().sort_values('Streams', ascending=False)\n",
      "```\n",
      "\n",
      "I like using Plotly for this kind of tasks, it allows a pretty visual representation of the data.\n",
      "```python\n",
      "trace0 = go.Bar(\n",
      "    x=data_country.index,\n",
      "    y=data_country['Streams'],\n",
      "    text=data_country['Streams'],\n",
      "    marker=dict(\n",
      "        color='rgb(158,202,225)',\n",
      "        line=dict(\n",
      "            color='rgb(8,48,107)',\n",
      "            width=1.5,\n",
      "        )\n",
      "    ),\n",
      "    opacity=0.6\n",
      ")\n",
      "\n",
      "data = [trace0]\n",
      "layout = go.Layout(\n",
      "    title='Regions of the world that consume most music',\n",
      ")\n",
      "\n",
      "fig1 = go.Figure(data=data, layout=layout)\n",
      "py.iplot(fig1, filename='text-hover-bar')\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/US.jpg){:height=\"100%\" width=\"100%\"}\n",
      "\n",
      "It looks pretty clear from this graph that the US market is by far the most important in terms of monthly music streams. Over the past month, the US market has consumed as much music streaming as Germany, Great Britain, France, Australia, Netherlands, Canada, and Japan together.\n",
      "\n",
      " We were pretty short on time, but it could be interesting to take a look at the streams per person in all those regions.\n",
      "\n",
      "## Who should you sing with?\n",
      "\n",
      "Plotting the same thing, but for the `data_artists` table heads :\n",
      "![image](https://maelfabien.github.io/assets/images/artist.jpg){:height=\"100%\" width=\"100%\"}\n",
      "\n",
      "The top trending artist at the time this article is written is XXXTentacion. However, the young rapper passed away this summer, and his latest hits were revealed.\n",
      "\n",
      "Otherwise, the next trending artists are :\n",
      "- XXX Tentacion\n",
      "- Post Malone\n",
      "- Khalid\n",
      "- Ariana Grande\n",
      "- Juice WRLD\n",
      "- Lil Baby\n",
      "- Drake\n",
      "- ...\n",
      "\n",
      "## What should your nationality be?\n",
      "\n",
      "We can expect that in some countries, the probability to become a top trending artist is higher than in others. Once again, we could have controlled for the population of each country to get a probability instead of the raw value.\n",
      "\n",
      "Based on this list: https://www.thefamouspeople.com/singers.php, we have scrapped the most relevant data regarding the nationality of the most famous artists.\n",
      "\n",
      "```python\n",
      "def _handle_request_result_and_build_soup(request_result):\n",
      "    if request_result.status_code == 200:\n",
      "    html_doc =  request_result.text\n",
      "    soup = BeautifulSoup(html_doc,\"html.parser\")\n",
      "    \n",
      "    return soup\n",
      "\n",
      "def _convert_string_to_int(string):\n",
      "    if \"K\" in string:\n",
      "        string = string.strip()[:-1]\n",
      "        return float(string.replace(',','.'))*1000\n",
      "    else:\n",
      "        return int(string.strip())\n",
      "\n",
      "def get_all_links_for_query(query):\n",
      "    url = website + \"/rechercher/\"\n",
      "    res = requests.post(url, data = {'q': query })\n",
      "    soup = _handle_request_result_and_build_soup(res)\n",
      "    specific_class = \"c-article-flux__title\"\n",
      "    all_links = map(lambda x : x.attrs['href'] , soup.find_all(\"a\", class_= specific_class))\n",
      "\n",
      "    return all_links\n",
      "\n",
      "def get_share_count_for_page(page_url):\n",
      "    res = requests.get(page_url)\n",
      "    soup = _handle_request_result_and_build_soup(res)\n",
      "    specific_class = \"c-sharebox__stats-number\"\n",
      "    share_count_text = soup.find(\"span\", class_= specific_class).text\n",
      "    \n",
      "    return  _convert_string_to_int(share_count_text)\n",
      "\n",
      "\n",
      "def get_popularity_for_people(query):  \n",
      "    url_people = get_all_links_for_query(query)\n",
      "    results_people = []\n",
      "\n",
      "    for url in url_people:\n",
      "        results_people.append(get_share_count_for_page(website_prefix + url))\n",
      "\n",
      "    return sum(results_people)\n",
      "\n",
      "def get_name_nationality(page_url):\n",
      "    res = requests.get(page_url)\n",
      "    soup = _handle_request_result_and_build_soup(res)\n",
      "    specific_class = \"btn btn-primary btn-sm btn-block btn-block-margin\"\n",
      "    share_count_text = soup.find(\"a\", class_= specific_class).text\n",
      "\n",
      "    return  share_count_text\n",
      "```\n",
      "```python\n",
      "artists_dict = {}\n",
      "\n",
      "for i in range(1, 17):\n",
      "    website = 'https://www.thefamouspeople.com/singers.php?page='+str(i)\n",
      "\n",
      "    res = requests.get(website)\n",
      "    specific_class = \"btn btn-primary btn-sm btn-block btn-block-margin\"\n",
      "    soup = _handle_request_result_and_build_soup(res)\n",
      "    classes = soup.find_all(\"a\", class_= specific_class)\n",
      "\n",
      "    for i in classes:\n",
      "        mini_array = i.text[:-1].split('(')\n",
      "            artists_dict[mini_array[0]]=mini_array[1]\n",
      "\n",
      "artists_df = pd.DataFrame.from_dict(artists_dict, orient='index', columns=['Country'])\n",
      "artists_df.head(n=10)\n",
      "```\n",
      "Plotting the results, we get :\n",
      "![image](https://maelfabien.github.io/assets/images/nationality.jpg){:height=\"100%\" width=\"100%\"}\n",
      "\n",
      "The result is quite coherent with the previous one. Being American should help once again.\n",
      "\n",
      "## What kind of music should you play?\n",
      "\n",
      "For this question, we relied on Google Trends and extracted a CSV file over the past 5 years comparing the research interest in the most popular music styles. \n",
      "\n",
      "```python\n",
      "genres = pd.read_csv('genres.csv', skiprows=1)\n",
      "genres.plot(figsize=(15,10))\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/trends.jpg){:height=\"100%\" width=\"100%\"}\n",
      "\n",
      "Rap music has over the past years invaded the music industry, and Google Trends reflects it pretty well.\n",
      "\n",
      "> **Conclusion **: This is all for the first part of this Hackathon story. If you liked it, please check the second part!\n",
      "---\n",
      "title: Introduction to Digital Signal Processing\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Signal Processing\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "These notes are a summary of \"Understanding Digital Signal Processing\" by Richard G. Lyons. I am also relying on the excellent series on Youtube by Iman: Signal Processing 101.\n",
      "\n",
      "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KZd68xgasIU\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
      "\n",
      "# I. Complex Numbers\n",
      "\n",
      "Complex numbers are made of a real and an imaginary part. There are two\n",
      "> A signal is a time-varying physical process. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/audio_6.png)\n",
      "\n",
      "---\n",
      "title: Key Concepts of Time Series\n",
      "layout: post\n",
      "tags: [statistics]\n",
      "subtitle : \"Time Series\"\n",
      "---\n",
      "\n",
      "In this article, we'll introduce the key concepts related to time series.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "{% highlight matlab %}\n",
      "{% endhighlight %}\n",
      "\n",
      "We'll be using the same data set as in the previous article: Open Power System Data ([OPSD](https://open-power-system-data.org/)) for Germany. The data can be downloaded [here](https://raw.githubusercontent.com/jenfly/opsd/master/opsd_germany_daily.csv)\n",
      "\n",
      "Start by importing the following packages :\n",
      "\n",
      "```python\n",
      "### General import\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn import preprocessing\n",
      "import statsmodels.api as sm\n",
      "\n",
      "### Time Series\n",
      "from statsmodels.tsa.ar_model import AR\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from pandas.tools.plotting import autocorrelation_plot\n",
      "from statsmodels.tsa.arima_model import ARIMA\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "#from statsmodels.tsa.sarimax_model import SARIMAX\n",
      "\n",
      "### LSTM Time Series\n",
      "from keras.models import Sequential  \n",
      "from keras.layers import Dense  \n",
      "from keras.layers import LSTM  \n",
      "from keras.layers import Dropout \n",
      "from sklearn.preprocessing import MinMaxScaler  \n",
      "```\n",
      "\n",
      "Then, load the data :\n",
      "```python\n",
      "df = pd.read_csv('opsd_germany_daily.csv', index_col=0)\n",
      "df.head(10)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_2.jpg)\n",
      "\n",
      "Then, make sure to transform the dates into `datetime` format in pandas :\n",
      "\n",
      "```python\n",
      "df.index = pd.to_datetime(df.index)\n",
      "```\n",
      "\n",
      "# I. Key concepts and definitions\n",
      "\n",
      "## 1. Auto-correlation\n",
      "\n",
      "The auto-correlation $$ \\rho $$ is defined as the correlation of the series over time, i.e how much the value at time $$ t $$ depends on the value at time $$ t-j $$ for all $$ j $$.\n",
      "\n",
      "- The auto-correlation $$ \\rho $$ of order 1 is : $$ Corr(y_t, y_{t-1}) $$\n",
      "- The auto-correlation $$ \\rho $$ of order j is : $$ Corr(y_t, y_{t-j}) $$\n",
      "- The auto-covariance $$ \\rho $$ of order 1 is : $$ Cov(y_t, y_{t-1}) $$\n",
      "- The auto-covariance $$ \\rho $$ of order j is : $$ Cov(y_t, y_{t-j}) $$\n",
      "\n",
      "Empirically, the auto-correlation can be estimated by the sample auto-correlation :\n",
      "\n",
      "$$ r_j = \\frac {Cov^e (y_t, y_{t-j})} {Var^e(y_t)} $$ \n",
      "\n",
      "Where : $$ Cov^e = \\frac {1} {T} \\sum_{t-j+1} (y_t - \\bar{y_{j+1,T}} )  (y_{t-j} - \\bar{y_{1,T-j}}) $$\n",
      "\n",
      "To plot the auto-correlation and the partial auto-correlation, we can use `statsmodel` package :\n",
      "\n",
      "```python\n",
      "fig, axes = plt.subplots(1, 2, figsize=(15,8))\n",
      "\n",
      "fig = sm.graphics.tsa.plot_acf(df['Consumption'], lags=400, ax=axes[0])\n",
      "fig = sm.graphics.tsa.plot_pacf(df['Consumption'], lags=400, ax=axes[1])\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_18.jpg)\n",
      "\n",
      "We observe a clear trend. The value od consumption at time $$ t $$ is negatively correlated with the values 180 days ago, and positively correlated with the values 360 days ago.\n",
      "\n",
      "## 2. Partial Auto-correlation\n",
      "\n",
      "The partial autocorrelation function (PACF) gives the partial correlation of a stationary time series with its own lagged values, regressed the values of the time series at all shorter lags. It is a regression of the series against its past lags. \n",
      "\n",
      "How can we correct auto-correlation ? Take for example : \n",
      "\n",
      "$$ y_{t-1} = \\beta_0 + \\beta_1 X_{t-1} + u_{t-1} $$\n",
      "\n",
      "$$ y_{t} = \\beta_0 + \\beta_1 X_{t} + u_{t} $$\n",
      "\n",
      "Therefore, if you substract the first to the second with a coefficient equal to the auto-correlation $$ \\rho $$ :\n",
      "\n",
      "$$ y_t - \\rho y_{t-1} = (1-\\rho) \\beta_0 + \\beta_1 (X_t - \\rho X_{t-1}) + e_t $$ for $$ t ≥ 2 $$\n",
      "\n",
      "Therefore, if we want to make a regression without auto-correlation :\n",
      "\n",
      "$$ \\hat{y_{t}} =  (1-\\rho) \\beta_0 + \\beta_1 \\hat{X_t} + e_t $$\n",
      "\n",
      "Why would we want to remove the auto-correlation?\n",
      "- to derive the OLS estimator of the parameters $$ \\beta_1 $$ for example\n",
      "- because there is a bias otherwise since $$ u_t $$ would depend on $$ u_{t-1} $$\n",
      "\n",
      "## 3. Stationarity\n",
      "\n",
      "**Stationarity** of a time series is a desired property, reached when the joint distribution of $$ y_s, y_{s+1}, y_{s+2}... $$ does not depend on $$ s $$. In other words, the future and the present should be quite similar. Stationary time series do therefore not have underlying trends or seasonal effect. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_19.jpg)\n",
      "\n",
      "What kind of events makes a series non-stationary?\n",
      "- a trend, i.e increasing sales over time\n",
      "- a seasonality, i.e more sales during the summertime than wintertime\n",
      "\n",
      "> We usually want our series to be stationary even before applying any predictive model! \n",
      "\n",
      "How can we test if a time series is stationary?\n",
      "- look at the plots (as above)\n",
      "- look at summary statistics and box plots as in the previous article. A simple trick is to cut the data set in 2, look at mean and variance for each split, and plot the distribution of values for both splits.\n",
      "- perform statistical tests, using the (Augmented) Dickey-Fuller test\n",
      "\n",
      "### Unit roots\n",
      "\n",
      "Let's cover into more details the Dickey-Fuller test. To do so, we need to introduce the notion of *unit root*. A unit root is a stochastic trend in a time series, sometimes called a random walk with drift. If a series has a unit root, it makes it unpredictable due to a systematic pattern.\n",
      "\n",
      "Let's consider an autoregressive (we'll dive deeper later in to this) :\n",
      "\n",
      "$$ y_t = a_1 y_{t-1} + a_2 y_{t-2} + ... + a_p y_{t-p} + \\epsilon_t $$\n",
      "\n",
      "We define the characteristic equation as :\n",
      "\n",
      "$$ m^p - m^{p-1}a_1 - m^{p-2}a_2 - ... - a_p = 0 $$. If $$ m = 1 $$ is a root to this equation, then the process is said to have a unit root. Equivalently, the process is said to be integrated of order 1 : $$ I(1) $$.\n",
      "\n",
      "In other words, there is a unit root if the previous values keep having a 1:1 impact on the current value. If we consider a simple autoregressive model AR(1) : $$ y_t = a_1 y_{t-1} + \\epsilon_t $$, the process has a unit root when $$ a_1 = 1 $$.\n",
      "\n",
      "If a process has a unit root, then it is non-stationary, i.e the moments of the process depend on $$ t $$.\n",
      "\n",
      "A process is a weakly dependent process, also called integrated of order 0 ( $$ I(0) $$ ) if taking the first different of the model is enough to make the series stationary : \n",
      "\n",
      "$$ \\Delta y_t = y_t - y_{t-1} $$\n",
      "\n",
      "### Dickey-Fuller Test\n",
      "\n",
      "The Dicker Fuller test is used to assess if a unit root is present in an autoregressive process : \n",
      "\n",
      "$$ H_0 : $$ There is a unit root and the process is **not** stationary.\n",
      "\n",
      "$$ H_1 : $$ There is no unit root and the process is stationary.\n",
      "\n",
      "For example, in an AR(1) model where $$ y_t = \\alpha + \\rho y_{t-1} + e_t $$, the hypothesis are :\n",
      "\n",
      "$$ H_0 : \\rho = 1 $$\n",
      "\n",
      "$$ H_1 : \\rho < 1 $$\n",
      "\n",
      "The hypothesis $$ H_1 > 1 $$ would mean an explosive process, and is therefore not considered. When $$ \\mid \\rho \\mid < 1 $$, then $$ Corr(y_t, y_{t-h}) = \\rho^h → 0 $$. \n",
      "\n",
      "In practice, we consider the following equation :\n",
      "\n",
      "$$ \\Delta y_t = \\alpha + \\theta y_{t-1} + e_t $$\n",
      "\n",
      "We have $$ \\theta = \\rho -1 $$ and test $$ H_0 : \\theta = 0 $$.\n",
      "\n",
      "### Augmented Dickey-Fuller Test\n",
      "\n",
      "The Augmented Dickey-Fuller Test (ADF) is an augmented version of the Dickey-Fuller test in the sense that it can test for a more complex set of time series models. For example, consider an ADF on an AR(p) process :\n",
      "\n",
      "$$ \\Delta_t = \\alpha + \\theta y_{t-1} + \\gamma_1 \\Delta y_{t-1} + ... + \\gamma_p \\Delta y_{t-p} + \\epsilon_t $$\n",
      "\n",
      "And the null hypothesis : $$ H_0 : \\theta = 0 $$.\n",
      "\n",
      "## 4. Ergodicity\n",
      "\n",
      "**Ergodicity** is the process by which we forget the initial conditions. This is reached when auto-correlation of order $$ k $$ tends to $$ 0 $$ as $$ k $$ tends to $$ \\infty $$.\n",
      "\n",
      "According to the ergodicity theorem, when a time series is strictly stationary and erdogic, and $$ E(Y_T) < \\infty $$ when $$ T → \\infty $$, then $$ \\frac {1} {n} \\sum_i y_t → E(Y_T) $$ \n",
      "\n",
      "## 5. Exogeneity\n",
      "\n",
      "**Exogeneity** describes the relation between the residuals and the explanatory variables. The exogeneity is said to be strict if :\n",
      "\n",
      "$$ y_t = \\beta_0 + \\beta_1 X_{t-1} + ... + \\beta_k X_{tk} + u_t $$ and $$ E(u_t \\mid X) = 0 $$ for all t.\n",
      "\n",
      "The exogenity is said to be contemporary when : \n",
      "\n",
      "$$ E(u_t \\mid X_{t-1}, ..., X_{t-k}) = E(u_t \\mid X_t) = 0 $$ which is a weaker assumption, but satisfies the consistency hypothesis. \n",
      "\n",
      "## 6. Long term effect\n",
      "\n",
      "Let's consider again the model : $$ y_t = \\beta_0 + \\beta_1 X_{t-1} + ... + \\beta_k X_{tk} + u_t $$. In that case, we can estimate the long term effect as :\n",
      "\n",
      "$$ LRP = \\beta_0 + \\beta_1 + ... + \\beta_q $$\n",
      "\n",
      "We can test the Granger causality using a Fisher test :\n",
      "\n",
      "$$ H_0 : \\beta_0 = \\beta_1 = ... = \\beta_q = 0 $$. Under this hypothesis, no past value of $$ X $$ would allow to predict $$ Y $$.\n",
      "\n",
      "> **Conclusion** : I hope you found this article useful. Don't hesitate to drop a comment if you have a question.\n",
      "\n",
      "---\n",
      "title: Predicting the song of the year 2019 (3/3)\n",
      "layout: post\n",
      "subtitle : \"Hackathon\"\n",
      "---\n",
      "\n",
      "Let's dive deeper into the features a hit song should have. We are using the Spotify API for this analysis.\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "## The data\n",
      "\n",
      "Here we send requests to the Spotify API to retrieve specific information on songs. The features retrieved from the API are the following.\n",
      "\n",
      "- duration_ms: The duration of the track in milliseconds.\n",
      "- key: The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.\n",
      "- mode: Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\n",
      "- time_signature: An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure).\n",
      "- acousticness: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic. The distribution of values for this feature looks like this: Acousticness distribution\n",
      "- danceability: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable. The distribution of values for this feature looks like this: Danceability distribution\n",
      "- energy: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy. The distribution of values for this feature looks like this: Energy distribution\n",
      "- instrumentalness : Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater the likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0. The distribution of values for this feature looks like this: Instrumentalness distribution\n",
      "- liveness: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides a strong likelihood that the track is live. The distribution of values for this feature looks like this: Liveness distribution\n",
      "- loudness: The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 dB. The distribution of values for this feature looks like this: Loudness distribution\n",
      "- speechiness : Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audiobook, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. The distribution of values for this feature looks like this: Speechiness distribution\n",
      "- valence: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry). The distribution of values for this feature looks like this: Valence distribution tempo float The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, the tempo is the speed or pace of a given piece and derives directly from the average beat duration. The distribution of values for this feature looks like this: Tempo distribution\n",
      "- tempo: The overall estimated tempo of the section in beats per minute (BPM). In musical terminology, the tempo is the speed or pace of a given piece and derives directly from the average beat duration.\n",
      "- key : The estimated overall key of the section. The values in this field ranging from 0 to 11 mapping to pitches using standard Pitch Class notation (E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on). If no key was detected, the value is -1.\n",
      "- mode: integer Indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. This field will contain a 0 for “minor”, a 1 for “major”, or a -1 for no result. Note that the major key (e.g. C major) could more likely be confused with the minor key at 3 semitones lower (e.g. A minor) as both keys carry the same pitches.\n",
      "- mode_confidence: The confidence, from 0.0 to 1.0, of the reliability of the mode.\n",
      "- time_signature : An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of “3/4”, to “7/4”.\n",
      "\n",
      "```python\n",
      "import requests, json, logging\n",
      "import pandas as pd\n",
      "import base64\n",
      "import six\n",
      "\n",
      "def get_info(song_name = 'africa', artist_name = 'toto', req_type = 'track'):\n",
      "    client_id = '***'\n",
      "    client_secret = '***'\n",
      "    auth_header = {'Authorization' : 'Basic %s' % base64.b64encode(six.text_type(client_id + ':' +                              client_secret).encode('ascii')).decode('ascii')}\n",
      "    r = requests.post('https://accounts.spotify.com/api/token', headers = auth_header, data= {'grant_type': 'client_credentials'})\n",
      "    token = 'Bearer {}'.format(r.json()['access_token'])\n",
      "    headers = {'Authorization': token, \"Accept\": 'application/json', 'Content-Type': \"application/json\"}\n",
      "\n",
      "    payload = {\"q\" : \"artist:{} track:{}\".format(artist_name, song_name), \"type\": req_type, \"limit\": \"1\"}\n",
      "\n",
      "    res = requests.get('https://api.spotify.com/v1/search', params = payload, headers = headers)\n",
      "    res = res.json()['tracks']['items'][0]\n",
      "    year = res['album']['release_date'][:4]\n",
      "    month = res['album']['release_date'][5:7]\n",
      "    day = res['album']['release_date'][8:10]\n",
      "    artist_id = res['artists'][0]['id']\n",
      "    artist_name = res['artists'][0]['name'].lower()\n",
      "    song_name = res['name'].lower()\n",
      "    track_id = res['id']\n",
      "    track_pop = res['popularity']\n",
      "\n",
      "    res = requests.get('https://api.spotify.com/v1/audio-analysis/{}'.format(track_id), headers = headers)\n",
      "    res = res.json()['track']\n",
      "    duration = res['duration']\n",
      "    end_fade = res['end_of_fade_in']\n",
      "    key = res['key']\n",
      "    key_con = res['key_confidence']\n",
      "    mode = res['mode']\n",
      "    mode_con = res['mode_confidence']\n",
      "    start_fade = res['start_of_fade_out']\n",
      "    temp = res['tempo']\n",
      "    time_sig = res['time_signature']\n",
      "    time_sig_con = res['time_signature_confidence']\n",
      "\n",
      "    res = requests.get('https://api.spotify.com/v1/audio-features/{}'.format(track_id), headers = headers)\n",
      "    res = res.json()\n",
      "    acousticness =  res['acousticness']\n",
      "    danceability = res['danceability']\n",
      "    energy = res['energy']\n",
      "    instrumentalness = res['instrumentalness']\n",
      "    liveness = res['liveness']\n",
      "    loudness = res['loudness']\n",
      "    speechiness = res['speechiness']\n",
      "    valence = res['valence']\n",
      "\n",
      "    res = requests.get('https://api.spotify.com/v1/artists/{}'.format(artist_id), headers = headers)\n",
      "    artist_hot = res.json()['popularity']/100\n",
      "\n",
      "    return pd.Series([artist_name, song_name, duration, key,mode,temp,artist_hot,end_fade, start_fade, mode_con,key_con,time_sig,time_sig_con,acousticness,danceability,energy ,instrumentalness,liveness,loudness,speechiness,valence, year, month, day, track_pop], index = ['artist_name', 'song_name', 'duration','key','mode','tempo','artist_hotttnesss','end_of_fade_in','start_of_fade_out','mode_confidence','key_confidence','time_signature','time_signature_confidence','acousticness','danceability','energy' ,'instrumentalness','liveness','loudness','speechiness','valence','year','month', 'day', 'track_popularity'])\n",
      "```\n",
      "This function tests if a song request through the API is successful.\n",
      "\n",
      "```python\n",
      "def test(song_name = 'africa', artist_name = 'toto', req_type = 'track'):\n",
      "    client_id = '***'\n",
      "    client_secret = '***'\n",
      "    auth_header = {'Authorization' : 'Basic %s' % base64.b64encode(six.text_type(client_id + ':' + client_secret).encode('ascii')).decode('ascii')}\n",
      "    r = requests.post('https://accounts.spotify.com/api/token', headers = auth_header, data= {'grant_type': 'client_credentials'})\n",
      "    token = 'Bearer {}'.format(r.json()['access_token'])\n",
      "    headers = {'Authorization': token, \"Accept\": 'application/json', 'Content-Type': \"application/json\"}\n",
      "\n",
      "    payload = {\"q\" : \"artist:{} track:{}\".format(artist_name, song_name), \"type\": req_type, \"limit\": \"1\"}\n",
      "\n",
      "    res = requests.get('https://api.spotify.com/v1/search', params = payload, headers = headers)\n",
      "    if not res.json()['tracks']['items']:\n",
      "        return False\n",
      "    else:\n",
      "        return True\n",
      "```\n",
      "\n",
      "This part of the code iterates over our dataset of hit songs (.csv file) to request the Spotify API and retrieve the audio features. Everything is gathered in a single data frame, and we create a feature by combining the mode confidence and the mode. We then proceeded to some data cleaning :\n",
      "\n",
      "```python\n",
      "song_list = pd.read_csv('/Users/raphaellederman/Downloads/Tracks_Hackathon_treated (4).csv', sep = ';')\n",
      "print(type(song_list['Track Name']))\n",
      "\n",
      "rows= []\n",
      "features = ['artist_name', 'song_name', 'duration','key','mode','tempo','artist_hotttnesss','end_of_fade_in','start_of_fade_out','mode_confidence','key_confidence','time_signature','time_signature_confidence','acousticness','danceability','energy' ,'instrumentalness','liveness','loudness','speechiness','valence','year','month', 'day', 'track_popularity']\n",
      "\n",
      "for index, row in song_list.iterrows():\n",
      "    print(row['Track Name'].replace('\\'','') + ' - ' + row['Artist'])\n",
      "    if test(row['Track Name'].replace('\\'',''), row['Artist'], req_type = 'track') == True :\n",
      "        rows.append(get_info(row['Track Name'].replace('\\'',''), row['Artist'], req_type = 'track'))\n",
      "\n",
      "data = pd.DataFrame(rows, columns=features)\n",
      "data['mode_confidence'] = np.where(data['mode'] == 1, data['mode']* data['mode_confidence'], (data['mode']- 1)* data['mode_confidence'])\n",
      "data = data.drop('mode', axis=1)\n",
      "\n",
      "data_songs = data_songs.reset_index().drop(['index', 'artist_name', 'song_name'], axis=1).replace('', np.nan).dropna()\n",
      "```\n",
      "## Exploratory data analysis\n",
      "\n",
      "One of the best way to understand how features are related in a music is to build the correlation matrix :\n",
      "\n",
      "```python\n",
      "import seaborn as sn\n",
      "fig = plt.figure(figsize=(20, 20))\n",
      "sn.heatmap(data.corr(),  annot=True)\n",
      "plt.title('Correlation of every features', fontsize=20)\n",
      "```\n",
      "![image](https://maelfabien.github.io/assets/images/corr.jpg){:height=\"100%\" width=\"100%\"}\n",
      "\n",
      "We are trying to predict how likely the artist is to be considered as a hot artist in 2019. Therefore, we focus on the variables that have a correlation of more than 0.1 or less than -0.1 with the artist Hotness.\n",
      "\n",
      "```python\n",
      "features = [ 'duration', 'tempo', 'danceability', 'end_of_fade_in', 'start_of_fade_out','energy', 'speechiness','valence','track_popularity']\n",
      "i=1\n",
      "\n",
      "for feature in features : \n",
      "    plt.figure(figsize=(15,15))\n",
      "    plt.subplot(3,3, i )\n",
      "    plt.scatter(data['artist_hotttnesss'], data[feature])\n",
      "    plt.title(\"correlation between artist_hotttnesss and \" + feature)\n",
      "    i+=1\n",
      "```\n",
      "\n",
      "I won't display all the graphs here, but one of the interesting relations is between artist hotness and the speechiness. This relation pretty much illustrates the popularity of rap artists. Indeed, rap tends to be a quite speechy music style. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/speechiness.jpg){:height=\"100%\" width=\"100%\"}\n",
      "\n",
      "## The basic features\n",
      "\n",
      "We tend to believe that taking the most popular features of each hit song of 2018 should be a pretty good start to make your next song a hit. If we focus on some features :\n",
      "\n",
      "`data.describe()`\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/features.jpg){:height=\"100%\" width=\"100%\"}\n",
      "\n",
      "Among the most important features, we observe that the idea tempo should be around 120 BPM, and the ideal duration should be around 213 seconds.\n",
      "\n",
      "## Feature importance\n",
      "\n",
      "We used a Random Forest Regressor to evaluate feature importance when predicting the track popularity :\n",
      "```python\n",
      "\n",
      "features_columns = [col for col in data.drop(\"track_popularity\", axis = 1).columns]\n",
      "X = data[features_columns].apply(pd.to_numeric, errors='coerce')\n",
      "y = data['track_popularity'].apply(pd.to_numeric, errors='coerce')\n",
      "\n",
      "# Split the data in order to compute the accuracy score\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
      "```\n",
      "\n",
      "We can construct a plot showing the importance of the features:\n",
      "```python\n",
      "\n",
      "rnd_clf = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42)\n",
      "rnd_clf.fit(X_train, y_train)\n",
      "\n",
      "importances_rf = rnd_clf.feature_importances_\n",
      "indices_rf = np.argsort(importances_rf)\n",
      "\n",
      "n = len(indices_rf)\n",
      "sorted_features_rf = [0] * n;  \n",
      "for i in range(0,n): \n",
      "sorted_features_rf[indices_rf[i]] = features_columns[i] \n",
      "\n",
      "plt.figure(figsize=(140,120) )\n",
      "plt.title('Random Forest Features Importance')\n",
      "plt.barh(range(len(indices_rf)), importances_rf[indices_rf], color='b', align='center')\n",
      "plt.yticks(range(len(indices_rf)), sorted_features_rf)\n",
      "plt.xlabel('Relative Importance')\n",
      "plt.tick_params(axis='both', which='major', labelsize=100)\n",
      "plt.tick_params(axis='both', which='minor', labelsize=100)\n",
      "\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/importance.jpg){:height=\"100%\" width=\"100%\"}\n",
      "\n",
      "At that point, we were pretty much running short on time. We tried several regressors to predict of song popularity. The one that happened to work best was the XGBoost, but the overall training set was too small (which headed a 26% r-squared only...).\n",
      "\n",
      "```python\n",
      "clf = xgboost.XGBRegressor(colsample_bytree = 0.44, n_estimators=30000, learning_rate=0.07,max_depth=9,alpha = 5)\n",
      "model = clf.fit(X_train.drop(worst_features, axis=1), y_train)\n",
      "pred = model.predict(X_test.drop(worst_features, axis=1))\n",
      "score_rf = metrics.r2_score(y_test, pred)\n",
      "print(score_rf)\n",
      "```\n",
      "\n",
      "## Release time \n",
      "\n",
      "At that point, we had all the ingredients of the perfect song, but when should the song be released? Spotify's API gave us access to the release date of the hits in its charts. \n",
      "\n",
      "```python \n",
      "s = pd.Series(data['month']).dropna()\n",
      "fig, ax = plt.subplots(figsize = (12,12))\n",
      "ax.hist(s, alpha=0.8, color='blue', bins = 25)\n",
      "ax.xaxis.set_ticks(range(13))\n",
      "ax.xaxis.set_ticklabels( [' ','Janvier', 'Fevrier', 'Mars', 'Avril', 'Mai', 'Juin', 'Juillet', 'Aout', 'Septembre', 'Octobre', 'Novembre', 'Decembre'])\n",
      "plt.title(\"Historgram of the number of hit by month\")\n",
      "```\n",
      "![image](https://maelfabien.github.io/assets/images/release.jpg){:height=\"100%\" width=\"100%\"}\n",
      "\n",
      "March looks great to launch your next hit ;)\n",
      "\n",
      "> **Conclusion **: Alright, this is all we had time to cover in this 10 hours Hackathon. Of course, we wanted to explore many more axis. At some point, we had to start focusing on the report to hand it and start preparing our pitch. I hope you enjoyed this series of articles!\n",
      "---\n",
      "title: Interpretability and explainability (1/2)\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Better ML\"\n",
      "search: false\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "In the previous blog post [\"Complexity vs. explainability\"](https://www.explorium.ai/complexity-vs-explainability/), we highlighted the tradeoff between increasing the model's complexity and loosing explainability. In this article, we will continue our discussion and cover the notions of interpretability and explainability in machine learning.\n",
      "\n",
      "Machine Learning interpretability and explainability are becoming essential in solutions we build nowadays. In fields such as healthcare or banking, interpretability and explainability could for example help overcome some legal constraints. In solutions that support a human decision, it is essential to establish a trust relationship and explain the outcome and the internal mechanics of an algorithm. The whole idea behind interpretable and explainable ML is to avoid the black box effect.\n",
      "\n",
      "Christoph Molnar has recently published an excellent book on this topic : [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/).\n",
      "\n",
      "First of all, let's define the difference between machine learning explainability and interpretability :\n",
      "- **Interpretability** is linked to the model. A model is said to be interpretable if we can interpret directly the impact of its parameters on the outcome. Among interpretable models, one can for example mention : Linear and logistic regression, Lasso and Ridge regressions, Decision trees, etc.\n",
      "- **Explainability** can be applied to any model, even models that are not interpretable. Explainability is the extent to which we can interpret the outcome and the internal mechanics of an algorithm. \n",
      "\n",
      "In this article, we will be using the [UCI Machine learning repository Breast Cancer](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29) data set. It is also available on [Kaggle](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data/downloads/breast-cancer-wisconsin-data.zip/2). Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. There are 30 features, including the radius of the tumor, the texture, the perimeter... Our task will be to perform a binary classification of the tumor, that is either malignant (M) or benign (B). \n",
      "\n",
      "Start off by importing the packages :\n",
      "\n",
      "```python\n",
      "# Handle data and plot\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Interpretable models\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "import statsmodels.api as sm\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.tree import export_graphviz\n",
      "import graphviz\n",
      "```\n",
      "\n",
      "Then, read the data and apply a simply numeric transformation of the label (\"M\" or \"B\").\n",
      "\n",
      "```python\n",
      "df = pd.read_csv('data.csv').drop(['id', 'Unnamed: 32'], axis=1)\n",
      "\n",
      "def to_category(diag):\n",
      "    if diag == \"M\" :\n",
      "        return 1\n",
      "    else :\n",
      "        return 0\n",
      "\n",
      "df['diagnosis'] = df['diagnosis'].apply(lambda x : to_category(x))\n",
      "df.head()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/df_head.jpg)\n",
      "\n",
      "```python\n",
      "X = df.drop(['diagnosis'], axis=1)\n",
      "y = df['diagnosis']\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
      "```\n",
      "\n",
      "It is a great exercise to work on interpretability and explainability of models in the healthcare sector, since performing such work could typically be required by authorities.\n",
      "\n",
      "In the next sections, we will cover the main interpretable models, their advantages, their limitations and examples. In the next article, we will explore explainability methods, as well as examples for each method.\n",
      "\n",
      "# Interpretable models\n",
      "\n",
      "## 1. Linear Regression\n",
      "\n",
      "Linear regression is probably the most basic regression model and takes the following form:\n",
      "\n",
      "$$ Y_i = {\\beta}_0 + {\\beta}_1{X}_{1i} + {\\beta}_2{X}_{2i} + {\\beta}_3{X}_{3i} + ... + {\\epsilon}_i $$. \n",
      "\n",
      "This simple equation states the following :\n",
      "- suppose we have $$ n $$ observations of a dataset and we pick the $$ i^{th} $$.\n",
      "- $$ Y_i $$ is the target, e.g. the diagnosis of the breast tissue.\n",
      "- $$ {X}_{1i} $$ is the $$ i^{th} $$ observation of the first feature, e.g. the radius of the tumor.\n",
      "- $$ {X}_{2i} $$ is the $$ i^{th} $$ observation of the second feature, e.g. the texture of the tumor.\n",
      "- ...\n",
      "- $$ {\\beta}_0 $$ is called the intercept, it is a constant term.\n",
      "- $$ {\\beta}_1 $$ is the coefficient associated with $$ X_{1i} $$ . It describes the weight of $$ {X}_{1i} $$ on the final output.\n",
      "- ...\n",
      "- $$ {\\epsilon} $$ is the noise of the model. The data we observe rarrly stand on a straight line or on a hyperplane.\n",
      "\n",
      "We can fit the linear regression using the `statsmodel` package :\n",
      "\n",
      "```python\n",
      "model = sm.OLS(y_train, X_train).fit()\n",
      "model.summary()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/stats.jpg)\n",
      "\n",
      "The `statsmodel` summary gives direct access to the coefficients, the standard errors, the t-statistics and the p-values for each feature.\n",
      "\n",
      "### Interpretability of Linear Regression\n",
      "\n",
      "- The coefficients of a linear regression are directly interpretable. At stated above, each coefficient describes the effect on the output of a change of 1 unit of a given input. \n",
      "- The importance of a feature can be seen as the absolute value of the t-statistic value. A feature is important if its coefficient is high and the variance around this estimate is low. \n",
      "\n",
      "$$ t_{\\hat{\\beta_j}} = \\frac{\\hat{\\beta_j}}{SE(\\hat{\\beta_j})} $$\n",
      "\n",
      "- In a binary classification task, each coefficient can be seen as a percentage of contribution to a class or another.\n",
      "- The variance explained by the model can be explained by the $$ R^2 $$ coefficient, displayed in the summary above.\n",
      "- We can use confidence intervals and tests for coefficient values : \n",
      "\n",
      "```python\n",
      "model.conf_int()\n",
      "```\n",
      "\n",
      "|  | 0 | 1 |\n",
      "| radius_mean | -0.854929  | -0.071102 |\n",
      "| texture_mean | -0.007799 | 0.027502 |\n",
      "| perimeter_mean | -0.028758 | 0.083970 |\n",
      "| ...  |  ...  | ... |\n",
      "\n",
      "- We are guaranteed to find the best coefficients by OLS properties\n",
      "\n",
      "To illustrate the interpretability of the Linear Regression, we can plot the coefficient's values and standard errors. This graph was inspired by the excellent work of [Zhiya Zuo](https://zhiyzuo.github.io/Python-Plot-Regression-Coefficient/). Start by computing an error term equal to the difference between the parameter's value and the lower confidence interval bound for this parameter, and build a single table with the coefficient, the error term and the name of the variable.\n",
      "\n",
      "```python\n",
      "err = model.params - model.conf_int()[0]\n",
      "coef_df = pd.DataFrame({'coef': model.params.values[1:], #drop the intercept\n",
      "    'err': err.values[1:], \n",
      "    'varname': err.index.values[1:]\n",
      "})\n",
      "```\n",
      "\n",
      "Then, plot the graph :\n",
      "\n",
      "```python\n",
      "coef_df.plot(y='coef', x='varname', kind='bar', color='none', yerr='err', legend=False, figsize=(12,8))\n",
      "plt.scatter(x=np.arange(coef_df.shape[0]), s=100, y=coef_df['coef'], color='blue')\n",
      "plt.axhline(y=0, linestyle='--', color='black', linewidth=1)\n",
      "plt.title(\"Coefficient and Standard error\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/coef_lin.jpg)\n",
      "\n",
      "This graph displays for each feature, the coefficient value as well as the standard error around this coefficient. The `smoothness_se` seems to be one of the most important feature in this linear regression framework.\n",
      "\n",
      "### Limitations of Linear Regression\n",
      "\n",
      "- Linear regression is a basic model. It is rare to observe linear relationships in the data, and the linear regression is rarely performing well.\n",
      "- Moreover, when it comes to classification tasks, the linear regression is risky to apply, since a line or an hyperplane cannot constraint the output between 0 and 1. We prefer to apply the Logistic Regression in such case.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/log_1.jpg)\n",
      "\n",
      "We can also illustrate the second limitation by plotting the predictions sorted by value :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(np.sort(y_pred))\n",
      "plt.axhline(0.5, c='r')\n",
      "plt.title(\"Predictions\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/pred.jpg)\n",
      "\n",
      "the output is not mapped between 0 and 1 systematically. Setting the threshold to 0.5 seems indeed to be an arbitrary choice.\n",
      "\n",
      "We can show that modifying the threshold that we consider for classifying in one class or another has a large effect on the accuracy :\n",
      "\n",
      "```python\n",
      "def classify(pred, thr = 0.5) :\n",
      "    if pred < thr :\n",
      "        return 0\n",
      "    else :\n",
      "        return 1\n",
      "\n",
      "accuracy = []\n",
      "for thr in np.linspace(0,1,100): \n",
      "    y_pred_class = y_pred.apply(lambda x: classify(x, thr)) \n",
      "    accuracy.append(accuracy_score(y_pred_class, y_test))\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(accuracy)\n",
      "plt.title(\"Acccuracy depending on threshold\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/pred_2.jpg)\n",
      "\n",
      "The maximum accuracy is reached for a threshold of 40.4% : \n",
      "\n",
      "`np.linspace(0,1,100)[np.argmax(accuracy)]`\n",
      "\n",
      "For this threshold, the accuracy achieved is 0.9385. Although the linear regression remains interesting for interpretability purposes, it is not optimal to tune the threshold on the predictions. We tend to use logistic regression instead.\n",
      "\n",
      "## 2. Logistic Regression\n",
      "\n",
      "The logistic regression using the logistic function to map the output between 0 and 1 for binary classification purposes. The function is defined as :\n",
      "\n",
      "$$ Sig(z) = \\frac {1} {1 + e^{-z}} $$\n",
      "\n",
      "In this plot, we represent both a sigmoid function and the inputs we feed it :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images//log_2.jpg)\n",
      "\n",
      "In the logistic regression model, instead of a linear relation between the input and the output, the relation is the following :\n",
      "\n",
      "$$ P(Y=1) = \\frac {1} {1 + exp^{-(\\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p)}} $$\n",
      "\n",
      "How can we interpret the partial effect of $$ X_1 $$ on $$ Y $$ for example ? Well, the weights in the logistic regression **cannot** be interpreted as for linear regression. We need to use the logit transform :\n",
      "\n",
      "$$ \\log( \\frac {P(y=1)} {1-P(y=1)} ) = \\log ( \\frac {P(y=1)} {P(y=0)} ) $$ \n",
      "\n",
      "$$ = odds = \\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k $$\n",
      "\n",
      "We define the this ratio as the \"odds\". Therefore, to estimate the impact of $$ X_j $$ increasing by 1 unit, we can compute it this way :\n",
      "\n",
      "$$ \\frac {odds_{X_{j+1}}} {odds} = \\frac {exp^{\\beta_0 + \\beta_1 X_1 + ... + \\beta_j (X_j + 1) + ... + \\beta_k X_k}} {exp^{\\beta_0 + \\beta_1 X_1 + ... + \\beta_j X_j + ... + \\beta_k X_k}} $$\n",
      "\n",
      "$$ = exp^{\\beta_j (X_j + 1) - \\beta_j X_j} = exp^{\\beta_j} $$\n",
      "\n",
      "A change in $$ X_j $$ by one unit increases the log odds ratio by $$ exp^{\\beta_j} $$. In other words, an increase in the log-odds ratio is proportional to classifying a bit more in class 1 rather than to class 0, according to an exponential factor in $$ \\beta_j $$.\n",
      "\n",
      "The implementation is straight forward in Python using scikit-learn. \n",
      "\n",
      "```python\n",
      "lr = LogisticRegression()\n",
      "lr.fit(X_train, y_train)\n",
      "y_pred = lr.predict(X_test)\n",
      "y_proba = lr.predict_proba(X_test)\n",
      "print(accuracy_score(y_pred, y_test))\n",
      "```\n",
      "\n",
      "`0.9473684210526315`\n",
      "\n",
      "### Interpretability of Logistic Regression\n",
      "\n",
      "With the logistic regression, we keep most of the advantages of the linear regression. For example, we can plot the value of the coefficients :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.barh(X.columns,lr.coef_[0])\n",
      "plt.title(\"Coefficient values\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/pred_3.jpg)\n",
      "\n",
      "An increase in the `concavity_worst` is more likely to lead to a malignant tumor, whereas an increase in te `radius_mean` is more likely to lead to a benign tumor. It is a model meant for binary classification, so the prediction probabilities are sent between 0 and 1.\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(np.sort(y_proba[:,0]))\n",
      "plt.axhline(0.5, c='r')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/pred_4.jpg)\n",
      "\n",
      "### Limitations of Logistic Regression\n",
      "\n",
      "Just like linear regression, the model remains quite limited in terms of performance, although a good regularization can offer decent performance. The coefficients are not as easily interpretable as for the linear regression. There is a tradeoff to make when choosing these kind of models, and they are often used in customer classification for car rental companies or in banking industry for example.\n",
      "\n",
      "## 3. Decision Trees\n",
      "\n",
      "Linear regression and logistic regression cannot model interactions between features. The Classification And Regression Trees (CART) algorithm is the most simple and popular tree algorithm, and models a simple interaction between features.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/dt.png)\n",
      "\n",
      "To build the tree, we choose each time the feature that splits our data the best way possible. How do we measure the qualitiy of a split ? We apply criteria such as the cross-entropy or Gini impurity. We stop the development of the tree when splitting a node does not lower the impurity.\n",
      "\n",
      "To implement decision trees in Python, we can use scikit-learn :\n",
      "\n",
      "```python\n",
      "clf = DecisionTreeClassifier(max_depth=3)\n",
      "clf.fit(X_train, y_train)\n",
      "y_pred = clf.predict(X_test)\n",
      "accuracy_score(y_pred, y_test)\n",
      "```\n",
      "\n",
      "`0.9210526315789473`\n",
      "\n",
      "\n",
      "### Interpretability of CART algorithm\n",
      "\n",
      "By growing the depth of the tree, we add \"AND\" conditions. For a new instance, the feature 1 is larger than `a` **and** the feature 3 is smaller than `b` **and** the feature 2 equals `c`.\n",
      "\n",
      "CART algorithm offers a nice way to compute the importance of each feature in the model. We measure the importance of a Gini index by the extent to which the chosen citeria has been decreased when creating a new node on the given feature.\n",
      "\n",
      "The tree offers a natural interpretability, and can be represented visually :\n",
      "\n",
      "```python\n",
      "export_graphviz(clf, out_file=\"tree.dot\")\n",
      "with open(\"tree.dot\") as f:\n",
      "dot_graph = f.read()\n",
      "\n",
      "graphviz.Source(dot_graph)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/pred_5.jpg)\n",
      "\n",
      "### Limitations of CART algorithm\n",
      "\n",
      "CART algorithms fails to represent linear relationships between the input and the output. It easily overfits and gets quite deep if we don't crontrol the model. For this reason, tree based ensemble models such as Random Forest have been developped.\n",
      "\n",
      "## 4. Other models\n",
      "\n",
      "There are other models that are by construction interpretable :\n",
      "- K-Nearest Neighbors (KNN)\n",
      "- Generalized Linear Models (GLMs)\n",
      "- Lasso and Ridge Regressions\n",
      "- Stochastic processses such as Poisson processes if you want to model arrival rates or goals in a football match\n",
      "- ...\n",
      "\n",
      "> We have covered in this article the motivation for interpretable and explainable machine learning, as well as the main interpretable models. In the next article, we will see how to explain the outcomes of black-box models through model explainability.\n",
      "\n",
      "If you'd like to read more on this topic, make sure to check these references :\n",
      "- [Interpretable ML Book](https://christophm.github.io/interpretable-ml-book)\n",
      "- [Zhiya Zuo's blog](https://zhiyzuo.github.io/Python-Plot-Regression-Coefficient/).\n",
      "\n",
      "---\n",
      "title: Installing external libraries\n",
      "layout: post\n",
      "tags: [da_tuto]\n",
      "subtitle : \"Data Analysis Basics\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# PyPi and Anaconda\n",
      "\n",
      "When you'll dive deeper in the use of Python, you will quickly notice that there are lots of **libraries**, i.e. open source projects you can install, which handle a lot for your and avoid having to implement everything from scratch each time. \n",
      "\n",
      "Among them, we can mention:\n",
      "- NumPy: array manipulation and computation\n",
      "- Pandas: loading data in dataframes\n",
      "- SciPy: scientific package (signal processing...)\n",
      "- Matplotlib: plotting graphs\n",
      "- Scikit-learn: machine learning\n",
      "- NLTK: natural language processing\n",
      "\n",
      "To install these packages, there are 2 options:\n",
      "- either clone the repository on Github. This is advised only if the second option is not available\n",
      "- install the package via PyPi or Anaconda.\n",
      "\n",
      "PiPy and Anaconda are distributions. They allow you to install Python packages in a single command line, and store the code in the right directory of your computer.\n",
      "\n",
      "To install a package from [PyPi](https://pypi.org/), simply type in your terminal :\n",
      "\n",
      "```bash\n",
      "pip install package_name\n",
      "```\n",
      "\n",
      "This will run the whole installation process for you. Then, if you want to see the list of all packages you have installed up to date, use:\n",
      "\n",
      "```bash\n",
      "pip list\n",
      "```\n",
      "\n",
      "Which heads something like:\n",
      "\n",
      "```bash\n",
      "Package                            Version     \n",
      "---------------------------------- ------------\n",
      "absl-py                            0.8.1       \n",
      "address                            0.1.1       \n",
      "alabaster                          0.7.12      \n",
      "allennlp                           0.9.0       \n",
      "altair                             3.2.0       \n",
      "anaconda-client                    1.7.2       \n",
      "anaconda-navigator                 1.9.7     \n",
      "...  \n",
      "```\n",
      "\n",
      "Anaconda does a similar job when it comes to package installing. However, they do have a higher control over the quality of the packages they distribute, and therefore some implementations that rely on Anaconda run faster than other implementations that use PyPi.\n",
      "\n",
      "To install a package with Anaconda, simply type:\n",
      "\n",
      "```bash\n",
      "conda install package_name\n",
      "```\n",
      "\n",
      "To see all the packages in your environment using `conda` command, you can type:\n",
      "\n",
      "```bash\n",
      "conda list\n",
      "```\n",
      "\n",
      "This will head:\n",
      "\n",
      "```bash\n",
      "# Name                    Version                   Build  Channel\n",
      "_anaconda_depends         2019.03                  py37_0    anaconda\n",
      "_ipyw_jlab_nb_ext_conf    0.1.0                    py37_0  \n",
      "absl-py                   0.8.1                    pypi_0    pypi\n",
      "...\n",
      "```\n",
      "\n",
      "> If you found the article useful or see ways in which it could be improved, please leave a comment :)\n",
      "---\n",
      "title: Making your code production-ready\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Tips & Tricks\"\n",
      "---\n",
      "\n",
      "As I currently am putting some Python code in production, I though that sharing some tricks and pakcages might be a good idea.\n",
      "\n",
      "# Project Structure\n",
      "\n",
      "Let's first start with an overview of what your project's structure should look like for the most basic use case of publishing it to GitHub for other developers :\n",
      "\n",
      "```\n",
      "- README.md\n",
      "- LICENSE.txt\n",
      "- requirements.txt\n",
      "- app\n",
      "    - files.py\n",
      "    - tests.py\n",
      "```\n",
      "\n",
      "The file `files.py` is your project. The file `test.py` implements unit tests. The file `requirements.txt` specifies any depenencies needed in your project, with the exact version.\n",
      "\n",
      "# Automatically collect your requirements\n",
      "\n",
      "A famous option to save your requirements in a file automatically is to use `pip freeze`. However :\n",
      "- pip freeze only saves the packages that are installed with pip install in your environment.\n",
      "- pip freeze saves all packages in the environment including those that you don't use in your current project. (if you don't have virtualenv)\n",
      "- and sometimes you just need to create requirements.txt for a new project without installing modules.\n",
      "\n",
      "For this reason, using `pipreqs` is better. The GitHub project can be found [here](https://github.com/bndr/pipreqs). After installing the package, simply run :\n",
      "\n",
      "```bash\n",
      "pipreqs /home/project/location\n",
      "```\n",
      "\n",
      "And a `requirements.txt` file containing all dependencies and versions will be created. For example :\n",
      "\n",
      "```\n",
      "spacy==2.0.18\n",
      "pandas==0.23.0\n",
      "numpy==1.15.4\n",
      "```\n",
      "\n",
      "# AutoPEP8\n",
      "\n",
      "PEP8 is a standard for Python programming. It specifies the number of spaces in each case, the number of lines between 2 sections and much more. There are tools to automatically \"clean\" your code so that they fit with the PEP8 format. [AutoPep8](https://github.com/hhatto/autopep8) is the most famous option to do this. Simply run :\n",
      "\n",
      "```bash\n",
      "autopep8 --in-place --aggressive --aggressive <filename>\n",
      "```\n",
      "\n",
      "And your code will be modified accoring to PEP8 standards. AutoPep8 will fix indentation issues, lines that are too long, lists that are too long...\n",
      "\n",
      "---\n",
      "title: Introduction to D3.js\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Data Viz\"\n",
      "---\n",
      "\n",
      "To build viz on the Web, we often use HTML for page content, CSS for aesthetics, JavaScript for interaction, SVG for vector graphics… Web as a platform allows a shared representation of the page called Document Object Model (DOC). Interoperability is however lost due to the encapsulation of the DOM with more specialized forms. \n",
      "\n",
      "A good web viz tool should reach 3 objectives :\n",
      "- **Compatibility **: the tool exists in an ecosystem of related components\n",
      "- **Debugging **: accessible tools must be designed to support debugging when the inevitable occurs. \n",
      "- **Performance **: Visualizations can be greatly enhanced by interaction and animation. Focusing on transformation rather than representation shifts this responsibility to the developer, improving performance while enabling animation and interaction. \n",
      "\n",
      "# D3.js\n",
      "\n",
      "D3 is a JavaScript library for creating graphs and interactive visualization tools on the web.\n",
      "\n",
      "D3 stands for **D**ata **D**riven **D**ocument. D3 answers those 3 objectives. D3’s core contribution is a visualization “kernel” rather than a framework. For high-level capability, D3 includes a collection of helper modules that sit on top of the selection-based kernel; these modules are heavily influenced by prior visualization systems, including Protovis.\n",
      "\n",
      "In D3, we need to think about 2 primary things :\n",
      "- the data\n",
      "- the document \n",
      "\n",
      "Regarding the data, we need to :\n",
      "- Load and parse the data,\n",
      "- Choose an internal data structure to represent them.\n",
      "\n",
      "For the document, we need to:\n",
      "- Choose a representational structure,\n",
      "- Define a mapping from the data to that structure.\n",
      "\n",
      "As stated above, D3 allows to create interactive visualization tools, so the final step could be to make the data interactive.\n",
      "\n",
      "Since D3 is a web-based solution, we'll mainly be working with HTML, CSS, JavaScript, and SVG. I'll assume that you have at least a basic understanding of the roles of HTML, CSS, and JavaScript.\n",
      "\n",
      "## SVG\n",
      "\n",
      "There are two primary ways to draw directly to a web page: SVG and the HTML Canvas. In D3, we mostly use SVG. SVG stands for Structured Vector Graphics. SVG can be embedded directly in an HTML page.\n",
      "\n",
      "```\n",
      "<svg width=\"200\" height=\"200\">\n",
      "    <rect x=\"10\" y=\"10\" width=\"125\" height=\"125\" style=\"fill: red; stroke:black;\">\n",
      "</svg>\n",
      "```\n",
      "\n",
      "<svg width=\"200\" height=\"200\">\n",
      "    <rect x=\"10\" y=\"10\" width=\"125\" height=\"125\" style=\"fill: red; stroke:black;\">\n",
      "</svg>\n",
      "\n",
      "<br>\n",
      "\n",
      "In SVG, the most common shapes are :\n",
      "- `<rect>`\n",
      "- `<circle>`\n",
      "- `<ellipse>`\n",
      "- `<path>`\n",
      "\n",
      "## Project Architecture\n",
      "\n",
      "There are 2 ways to integrate D3 in a webpage :\n",
      "- by calling the library's remotely (through an URL)\n",
      "- by importing the D3 library and putting it into your `js` folder\n",
      "\n",
      "- Project\n",
      "    - css/\n",
      "    - data/\n",
      "    - index.html\n",
      "    - js/\n",
      "        - d3\n",
      "        - d3.js\n",
      "        - d3.min.js\n",
      "        - LICENSE\n",
      "\n",
      "## D3 Integration\n",
      "\n",
      "To indicate our project to use the d3.js library, we simply need to add this line in the header of `index.html` :\n",
      "\n",
      "```\n",
      "<script type=\"text/javascript\" src=\"js/d3/d3.js\"></script>\n",
      "```\n",
      "\n",
      "## How to create a graph?\n",
      "\n",
      "Building a graph in D3 can be done in a few steps. First of all, you need to create a new `.js` file that will contain the code of the graph.\n",
      "\n",
      "Then, you need to call that file from the HTML page, in the **body** section :\n",
      "\n",
      "```\n",
      "<script type=\"text/javascript\" src=\"js/myfile.js\"></script>\n",
      "```\n",
      "\n",
      "We start by creating an empty canvas in the file `myfile.js` :\n",
      "\n",
      "```\n",
      "const w = 600\n",
      "const h = 600\n",
      "\n",
      "//Create the SVG\n",
      "let svg = d3.select(\"body\")\n",
      "            .append(\"svg\")\n",
      "                .attr(\"width\", w)\n",
      "                .attr(\"height\",h);\n",
      "```\n",
      "\n",
      "We create a global variable, `d3` that we will use to interact with the D3 system. If we run this part and inspect the code of the HTML page, we should see something that has the following structure :\n",
      "\n",
      "```\n",
      "<body>\n",
      "    <svg></svg>\n",
      "</body>\n",
      "```\n",
      "\n",
      "And more specifically :\n",
      "\n",
      "```\n",
      "<body>\n",
      "    <svg width=\"600\" height=\"600\"></svg> \n",
      "</body>\n",
      "```\n",
      "\n",
      "We then need to load the data (in this case a TSV file) :\n",
      "\n",
      "```\n",
      "d3.tsv(\"data/myfile.tsv\")\n",
      "    .get( (error, rows) => {\n",
      "        console.log(\"Loaded \" + rows.length + \" rows\");\n",
      "}\n",
      "```\n",
      "\n",
      "The `.get()` method tells us to start loading the file and should call a function. At that moment, all the values are by default strings. We need to convert the data to match their true type :\n",
      "\n",
      "```\n",
      "d3.tsv(\"data/myfile.tsv\")\n",
      "    .row( (d, i) => {\n",
      "        return {\n",
      "            intCol: +d.intCol,\n",
      "            longCol: +d[\"Column Name Long\"],\n",
      "            stringCol: d.stringCol,\n",
      "        };\n",
      "    })\n",
      "    .get( (error, rows) => {\n",
      "        console.log(\"Loaded \" + rows.length + \" rows\");\n",
      "}\n",
      "```\n",
      "\n",
      "Using the row method, we specify a type for the input data. Here's what's happening :\n",
      "- we call each column using `d`, a datum (JavaScript dictionary) whose keys are column labels from our TSV files\n",
      "- we put a `+` in from of columns whose type is numeric\n",
      "- we can also call columns whose name contains spaces\n",
      "- we don't put anything in front of string columns since it's the default type\n",
      "\n",
      "## Enter, update, exit\n",
      "\n",
      "We now need to define what happens on 3 events :\n",
      "- `enter` whenever a new data entry is created\n",
      "- `update` when a value changes\n",
      "- `exit` when a value is deleted\n",
      "\n",
      "The Enter part might correspond to the web page being shown (this is most often the case). So far, let's not consider the Update or Exit.\n",
      "\n",
      "We will now create a small rectangle for each \"x:y\" couple in our dataset.\n",
      "\n",
      "```\n",
      "let dataset = [];\n",
      "\n",
      "function draw() {\n",
      "\n",
      "    svg.selectAll(\"rect\")\n",
      "        .data(dataset)\n",
      "        .enter()\n",
      "        .append(\"rect\")\n",
      "```\n",
      "\n",
      "This function tells us to append new \"rects\" to our canvas and what function to apply to all \"rects\". We then need to add attributes to this function, including which data it should map on the x-axis and the y-axis.\n",
      "\n",
      "```\n",
      "let dataset = [];\n",
      "\n",
      "function draw() {\n",
      "\n",
      "    svg.selectAll(\"rect\")\n",
      "        .data(dataset)\n",
      "        .enter()\n",
      "        .append(\"rect\")\n",
      "        .attr(\"x\", (d) => d.thisColumn)\n",
      "        .attr(\"y\", (d) => d.thisOtherColumn)\n",
      "```\n",
      "\n",
      "One of the issues at that point is that the data might need to be scaled between the input width and length (600x600) we specified.\n",
      "\n",
      "```\n",
      "let dataset = [];\n",
      "\n",
      "var x = d3.scaleLinear()\n",
      "    .domain(d3.extent(rows, (d) => d.thisColumn))\n",
      "    .range([0,w]);\n",
      "\n",
      "var y = d3.scaleLinear()\n",
      "    .domain(d3.extent(rows, (d) => d.thisOtherColumn))\n",
      "    .range([h,0]);\n",
      "\n",
      "function draw() {\n",
      "\n",
      "    svg.selectAll(\"rect\")\n",
      "        .data(dataset)\n",
      "        .enter()\n",
      "        .append(\"rect\")\n",
      "        .attr(\"x\", (d) => d.thisColumn)\n",
      "        .attr(\"y\", (d) => d.thisOtherColumn)\n",
      "        \n",
      "}\n",
      "```\n",
      "\n",
      "To summarize, at that point, we created the Canvas, loaded the data, scaled them, and mapped them in a draw function. All that is left for this example is to call the draw function at the end of `myfile.js`, or right after the get method.\n",
      "\n",
      "```\n",
      "draw()\n",
      "```\n",
      "\n",
      "I've made a small demo of what D3 can be used for at a quite basic level :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/map_d3.jpg)\n",
      "\n",
      "The tool can be accessed [here](https://maelfabien.github.io/viz), and the code [here](https://github.com/maelfabien/maelfabien.github.io/blob/master/assets/js/hello-france.js).\n",
      "\n",
      "> **Conclusion** : That's it ! I hope this introduction to D3 was interesting. I have so far used D3 on 2-3 projects, and even though some things are tricky to understand, it's a powerful tool. I highly recommend checking D3's documentation, it's fantastic, well illustrated and well written.\n",
      "\n",
      "\n",
      "---\n",
      "title: Create a streaming data pipeline with Cloud DataFlow \n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "---\n",
      "\n",
      "In this project, we will analyze data from a taxi business. The aim of the lab is to :\n",
      "- Connect to a streaming data Topic in Cloud Pub/sub\n",
      "- Ingest streaming data with Cloud Dataflow\n",
      "- Load streaming data into BigQuery\n",
      "- Analyze and visualize the results with DataStudio\n",
      "\n",
      "First, we need to confirm that the needed APIs are enabled. On the console, in the side menu, click on \"API & Services\". You should see Google Cloud Pub/Sub API and Dataflow API already enabled. If not, enable them.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_118.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_119.jpg)\n",
      "\n",
      "In this lab, messages published into Pub/Sub will be aggregated and stored in BigQuery. Therefore, we need to create a BigQuery Dataset.\n",
      "\n",
      "Open a shell, and type the following command :\n",
      "\n",
      "```\n",
      "bq mk taxirides\n",
      "```\n",
      "\n",
      "Then, to create the table, type this command :\n",
      "\n",
      "```\n",
      "bq mk \\\n",
      "--time_partitioning_field timestamp \\\n",
      "--schema ride_id:string,point_idx:integer,latitude:float,longitude:float,\\\n",
      "timestamp:timestamp,meter_reading:float,meter_increment:float,ride_status:string,\\\n",
      "\n",
      "passenger_count:integer -t taxirides.realtime\n",
      "```\n",
      "\n",
      "It creates an empty schema for `taxirides.realtime` table in which we will stream later.\n",
      "\n",
      "We will then use Cloud Storage to provide working space for our Cloud Dataflow pipeline. To do so, open the Storage tab in the menu, and create a bucket whose name is the same as your project's name :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_122.jpg)\n",
      "\n",
      "Then, we will set up a streaming data pipeline to read sensor data from Pub/Sub, compute the maximum temperature within a time window, and write this out to BigQuery.\n",
      "\n",
      "From the navigation menu, click on DataFlow :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_123.jpg)\n",
      "\n",
      "Then, click on \"Create job from template\" :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_124.jpg)\n",
      "\n",
      "- Under Cloud Dataflow template, select the Cloud Pub/Sub Topic to BigQuery template.\n",
      "- Under Cloud Pub/Sub input topic, enter `projects/pubsub-public-data/topics/taxirides-realtime`\n",
      "- Under BigQuery output table, enter `<myprojectid>:taxirides.realtime`\n",
      "- Under Temporary Location, enter `gs://<mybucket>/tmp/`\n",
      "\n",
      "For example :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_125.jpg)\n",
      "\n",
      "Click on Run job, and you'll see that your pipeline is now running! The main steps in this template are :\n",
      "- Read topic\n",
      "- Convert message to tables\n",
      "- Write records\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_126.jpg)\n",
      "\n",
      "After a few minutes, all services will be up and running, and you should see information on the rate of data coming in :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_127.jpg)\n",
      "\n",
      "We can now analyze the data in BigQuery :\n",
      "\n",
      "```\n",
      "SELECT * FROM taxirides.realtime LIMIT 10\n",
      "````\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_128.jpg)\n",
      "\n",
      "We can also perform aggregations on the stream for reporting :\n",
      "\n",
      "```\n",
      "WITH streaming_data AS (\n",
      "\n",
      "SELECT\n",
      "    timestamp,\n",
      "    TIMESTAMP_TRUNC(timestamp, HOUR, 'UTC') AS hour,\n",
      "    TIMESTAMP_TRUNC(timestamp, MINUTE, 'UTC') AS minute,\n",
      "    TIMESTAMP_TRUNC(timestamp, SECOND, 'UTC') AS second,\n",
      "    ride_id,\n",
      "    latitude, \n",
      "    longitude,\n",
      "    meter_reading,\n",
      "    ride_status,\n",
      "    passenger_count\n",
      "FROM\n",
      "    taxirides.realtime\n",
      "WHERE ride_status = 'dropoff'\n",
      "ORDER BY timestamp DESC\n",
      "LIMIT 100000\n",
      ")\n",
      "\n",
      "# calculate aggregations on stream for reporting:\n",
      "SELECT \n",
      "    ROW_NUMBER() OVER() AS dashboard_sort,\n",
      "    minute,\n",
      "    COUNT(DISTINCT ride_id) AS total_rides,\n",
      "    SUM(meter_reading) AS total_revenue,\n",
      "    SUM(passenger_count) AS total_passengers\n",
      "FROM streaming_data\n",
      "GROUP BY minute, timestamp\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_129.jpg)\n",
      "\n",
      "It shows total revenue, customers and rides every minute.\n",
      "\n",
      "We can now explore this table in Data Studio. Click on \"Open with Data Studio\", and we'll build a small dashboard to display information :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_130.jpg)\n",
      "\n",
      "Select the Bar chart option. Set minute as the dimensions as well as dashboard sort, and use the number of passengers, ride, and revenu as metrics. \n",
      "\n",
      "The data is updated every minute, just click on the button \"Refresh Data\" whenever you want to visualize the latest data.\n",
      "\n",
      "If you want to stop the streaming, stop the job from DataFlow :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_131.jpg)\n",
      "\n",
      "And choose to either cancel or drain the ingestion :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_132.jpg)\n",
      "\n",
      "---\n",
      "title: Introduction to Graphs\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Graph Analysis and Graph Learning\"\n",
      "---\n",
      "\n",
      "You might have already heard of graph analysis previously. In this article, we'll cover the basic graph notions.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "For what comes next, open a Jupyter Notebook and import the following packages :\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import random\n",
      "import networkx as nx\n",
      "from IPython.display import Image\n",
      "import matplotlib.pyplot as plt\n",
      "```\n",
      "\n",
      "If you have not already installed the `networkx` package, simply run :\n",
      "\n",
      "```bash\n",
      "pip install networkx\n",
      "```\n",
      "\n",
      "The following articles will be using the latest version  `2.x` of  `networkx`. NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\n",
      "\n",
      "## What is a Graph?\n",
      "\n",
      "A graph is a collection of nodes that are interconnected. For example, a very simple graph could be :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_1.jpg)\n",
      "\n",
      "Graphs can be used to represent :\n",
      "- social networks\n",
      "- web pages\n",
      "- biological networks\n",
      "- ...\n",
      "\n",
      "What can we do on a graph?\n",
      "- study topology and connectivity\n",
      "- community detection\n",
      "- identification of central nodes\n",
      "- ...\n",
      "\n",
      "In our notebook, let's import our first pre-built graph :\n",
      "\n",
      "```python\n",
      "# Load the graph\n",
      "G_karate = nx.karate_club_graph()\n",
      "# Find key-values for the graph\n",
      "pos = nx.spring_layout(G_karate)\n",
      "# Plot the graph\n",
      "nx.draw(G_karate, cmap = plt.get_cmap('rainbow'), with_labels=True, pos=pos)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_2.jpg)\n",
      "\n",
      "What does this graph represent?\n",
      "\"A social network of a karate club was studied by Wayne W. Zachary for a period of three years from 1970 to 1972. The network captures 34 members of a karate club, documenting links between pairs of members who interacted outside the club. During the study, a conflict arose between the administrator \"John A\" and instructor \"Mr. Hi\" (pseudonyms), which led to the split of the club into two. Half of the members formed a new club around Mr. Hi; members from the other part found a new instructor or gave up karate. Based on collected data Zachary correctly assigned all but one member of the club to the groups they joined after the split.\"\n",
      "\n",
      "## Basic graph notions\n",
      "\n",
      "A *graph* $$ G = (V,E) $$ is made of a set of :\n",
      "- nodes, also called verticles, $$ V = {1,...,n} $$\n",
      "- edges $$ E ⊆ V \\times V $$ \n",
      "\n",
      "- An *edge* $$ (i,j) ∈ E $$ links nodes $$ i $$  and $$ j $$. $$ i $$  and $$ j $$ are said to be neighbors.\n",
      "- A *degree* of a node is its number of neighbors\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_3.jpg)\n",
      "\n",
      "- A graph is *complete* is all nodes have $$ n-1 $$ neighbors, i.e all nodes are connected in every possible way.\n",
      "- A *path* from $$ i $$ to $$ j $$ is a sequence of edges that goes from $$ i $$ to $$ j $$. This path has a *length* equal to the number of edges\n",
      "- The *diameter* of a graph is the length of the longest path among all the shortest path that link any two nodes\n",
      "\n",
      "For example, in this case, we can compute some of the shortest paths to link any two nodes. The diameter would typically be 3 since the is no pair of nodes such that the shortest way to link them is longer than 3.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_4.jpg)\n",
      "\n",
      "- The shortest path between two nodes is called the *geodesic* path.\n",
      "- If all the nodes can be reached from  each other by a given path, they form a *connected component*\n",
      "- A graph is *connected* is it has a single connected component\n",
      "\n",
      "For example, here is a graph with 2 different connected components :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_5.jpg)\n",
      "\n",
      "- A graph is *directed* if edges are ordered pairs. In this case, the \"in-degree\" of $$ i $$ is the number of incoming edges to $$ i $$, and the \"out-degree\" is the number of outgoing edges from $$ i $$.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_6.jpg)\n",
      "\n",
      "- A graph is *cyclic* if there are paths through relationships and nodes where you walk from and back to a particular node.\n",
      "- A graph is *weighted* if we assign weights to either nodes or relationships.\n",
      "- A graph is *sparse* if the number of relationships is large compared to nodes.\n",
      "\n",
      "To summarize :\n",
      "![image](https://maelfabien.github.io/assets/images/graph_summary.jpg)\n",
      "\n",
      "Let's now see how to retrieve this information from a graph in Python :\n",
      "\n",
      "```python\n",
      "n=34\n",
      "\n",
      "G_karate.degree()\n",
      "```\n",
      "\n",
      "The attribute `.degree()` returns the list of the number of degrees (neighbors) for each node of the graph :\n",
      "\n",
      "```python\n",
      "DegreeView({0: 16, 1: 9, 2: 10, 3: 6, 4: 3, 5: 4, 6: 4, 7: 4, 8: 5, 9: 2, 10: 3, 11: 1, 12: 2, 13: 5, 14: 2, 15: 2, 16: 2, 17: 2, 18: 2, 19: 3, 20: 2, 21: 2, 22: 2, 23: 5, 24: 3, 25: 3, 26: 2, 27: 4, 28: 3, 29: 4, 30: 4, 31: 6, 32: 12, 33: 17})\n",
      "```\n",
      "\n",
      "Then, isolate the values of the degrees :\n",
      "```python\n",
      "# Isolate the sequence of degrees\n",
      "degree_sequence = list(G_karate.degree())\n",
      "````\n",
      "\n",
      "Compute the number of edges, but also metrics on the degree sequence :\n",
      "\n",
      "```python\n",
      "nb_nodes = n\n",
      "nb_arr = len(G_karate.edges())\n",
      "\n",
      "avg_degree = np.mean(np.array(degree_sequence)[:,1])\n",
      "med_degree = np.median(np.array(degree_sequence)[:,1])\n",
      "\n",
      "max_degree = max(np.array(degree_sequence)[:,1])\n",
      "min_degree = np.min(np.array(degree_sequence)[:,1])\n",
      "```\n",
      "\n",
      "Finally, print all this information :\n",
      "\n",
      "```python\n",
      "print(\"Number of nodes : \" + str(nb_nodes))\n",
      "print(\"Number of edges : \" + str(nb_arr))\n",
      "\n",
      "print(\"Maximum degree : \" + str(max_degree))\n",
      "print(\"Minimum degree : \" + str(min_degree))\n",
      "\n",
      "print(\"Average degree : \" + str(avg_degree))\n",
      "print(\"Median degree : \" + str(med_degree))\n",
      "```\n",
      "\n",
      "This heads :\n",
      "```\n",
      "Number of nodes: 34\n",
      "Number of edges: 78\n",
      "Maximum degree: 17\n",
      "Minimum degree: 1\n",
      "Average degree: 4.588235294117647\n",
      "Median degree: 3.0\n",
      "```\n",
      "\n",
      "On average, each person in the graph is connected to 4.6 persons.\n",
      "\n",
      "We can also plot the histogram of the degrees :\n",
      "\n",
      "```python\n",
      "degree_freq = np.array(nx.degree_histogram(G_karate)).astype('float')\n",
      "\n",
      "plt.figure(figsize=(12, 8))\n",
      "plt.stem(degree_freq)\n",
      "plt.ylabel(\"Frequence\")\n",
      "plt.xlabel(\"Degre\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_7.jpg)\n",
      "\n",
      "We will, later on, see that the histograms of degrees are quite important to determine the kind of graph we are looking at.\n",
      "\n",
      "## How are graphs stored?\n",
      "\n",
      "You might now wonder how we can store complex graph structures?\n",
      "\n",
      "There are 3 ways to store graphs, depending on the usage we want to make of it :\n",
      "- at an edge list :\n",
      "\n",
      "```\n",
      "1   2\n",
      "1   3\n",
      "1   4\n",
      "2   3\n",
      "3   4\n",
      "...\n",
      "```\n",
      "\n",
      "We store the ID of each pair of nodes linked by an edge.\n",
      "\n",
      "- using the adjacency matrix, usually loaded in memory : $$ A ∈ R^{n \\times n} $$, $$ A_{i,j} = 1 $$ if $$ (i,j) ∈ E $$, else 0\n",
      "\n",
      "$$ A = \\begin{pmatrix} 0 & 1 & 1 \\\\ 0 & 0 & 1 \\\\ 1 & 1 & 0 \\end{pmatrix} $$\n",
      "\n",
      "For each possible pair in the graph, set it to 1 if the 2 nodes are linked by an edge. $$ A $$ is symmetric if the graph is undirected.\n",
      "\n",
      "- using adjacency lists :\n",
      "\n",
      "```\n",
      "1 : [2,3, 4]\n",
      "2 : [1,3]\n",
      "3:  [2, 4]\n",
      "...\n",
      "```\n",
      "\n",
      "The best representation will depend on the usage and available memory. Graphs can usually be stored as `.txt` files.\n",
      "\n",
      "Some extensions of graphs might include :\n",
      "- weighted edges\n",
      "- labels on nodes/edges\n",
      "- feature vectors associated with nodes/edges\n",
      "\n",
      "In the next article, we'll explore the graph analysis basics!\n",
      "\n",
      "> **Conclusion** : I hope that this article introduced clearly the basis of graphs and that it does now seem clear to you. Don't hesitate to drop a comment if you have any question.\n",
      "---\n",
      "title: Planning by Dynamic Programming \n",
      "layout: post\n",
      "tags: [RL]\n",
      "subtitle : \"Advanced AI\"\n",
      "---\n",
      "\n",
      "*David Silver's YouTube series on Reinforcement Learning, Episode 3*. \n",
      "\n",
      "The full lesson is the following:\n",
      "\n",
      "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Nd1-UUMVfz4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "We have so far formulated the Bellman Equation, but we did not cover ways to solve it. In this article, we will explore Dynamic Programming, and more specifically:\n",
      "- policy evaluation\n",
      "- policy iteration\n",
      "- value iteration\n",
      "\n",
      "# Dynamic Programming\n",
      "\n",
      "**Dynamic Programming** is a very general solution method for problems which have two properties :\n",
      "- Optimal substructure :\n",
      "    - principle of optimality applies\n",
      "    - optimal solution can be decomposed into subproblems\n",
      "- Overlapping subproblems :\n",
      "    - subproblems recur many times\n",
      "    - solutions can be cached and reused\n",
      "    \n",
      "Markov Decision Processes satisfy both of these properties. The Bellman equation gives a recursive decomposition. The values function stores and reuses solutions. \n",
      "\n",
      "Dynamic Programming assumes full knowledge of the MDP. We know the dynamics and the reward. It is used for **planning** in an MDP, and it's not a full Reinforcement Learning problem.\n",
      "\n",
      "**For Prediction** :\n",
      "\n",
      "- The *input* takes the form of an MDP $$ (S, A, P, R, \\gamma) $$ and a policy $$ \\pi $$, or an MRP $$ (S, P^{\\pi}, R^{\\pi}, \\gamma) $$. \n",
      "- The *output* is a value function $$ v_{pi} $$.\n",
      "\n",
      "**For Control** :\n",
      "\n",
      "- The *input* takes the form of an MDP $$ (S, A, P, R, \\gamma) $$ and a policy $$ \\pi $$.\n",
      "- The *output* is the optimal value function $$ v_{*} $$, and an optimal policy $$ \\pi_{*} $$.\n",
      "\n",
      "Dynamic Programming is used in :\n",
      "- Scheduling algorithms (sequence alignment)\n",
      "- Graph algorithms (shortest path)\n",
      "- Graphical models (Viterbi)\n",
      "- Bioinformatics (lattice models)\n",
      "- ...\n",
      "\n",
      "# Policy Evaluation\n",
      "\n",
      "If we are given an MDP and a policy (e.g always go straight ahead), how can we evaluate this policy $$ \\pi $$ ? \n",
      "\n",
      "> We apply the Bellman expectation backup iteratively.\n",
      "\n",
      "We start off with an arbitrary value function $$ v_1 $$. We then do a 1-step look-ahead to figure out $$ v_2 $$. Then, we iterate many times until $$ v_{\\pi} $$.\n",
      "\n",
      "$$ v_1 → v_2 →\\cdots → v_{\\pi} $$\n",
      "\n",
      "We use **synchronous backups** :\n",
      "- At each iteration $$ k + 1 $$ :\n",
      "    - For all states $$ s \\in S $$\n",
      "    - Update $$ v_{k+1}(s) $$ from $$ v_k(s') $$\n",
      "    - where $$ s' $$ is a successor state of $$ s $$\n",
      "\n",
      "The convergence of this process can also be proven.\n",
      "\n",
      "We can represent Policy Evaluation graphically as in the previous article :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_12.png)\n",
      "\n",
      "$$ v_{k+1}(s) = \\sum_{a \\in A} \\pi (a \\mid s) (R_s^a + \\gamma \\sum_{s' \\in S} P_{ss'}^a v_{k}(s')) $$ \n",
      "\n",
      "Which can be re-written under matrix form :\n",
      "\n",
      "$$ v^{k+1} = R^{\\pi} + \\gamma P^{\\pi} v^k $$\n",
      "\n",
      "Let us now illustrate this concept with a simple grid. The aim of the agent is to reach one of the terminal states (either upper left or lower right). The reward is $$ -1 $$ until the terminal state is reached.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_19.png)\n",
      "\n",
      "We suppose that the agent follows a uniform random policy, and has $$ \\frac{1}{4} $$ chance to pick each action (Up, Down, Left, Right) :\n",
      "\n",
      "$$ \\pi(n \\mid .) = \\pi(e \\mid .) = \\pi(s \\mid .) = \\pi(w \\mid .) = 0.25 $$\n",
      "\n",
      "Here is what a the greedy policy would look like at step $$ 0 $$ of this random policy :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_20.png)\n",
      "\n",
      "The initial estimate of the value function is to put zeros everywhere. Then, let's move on by 1 step. For each cell (except for the terminal states), we set the value as a weighted average of the reward we get by following each action. Since the reward is $$ -1 $$ at each cell expect for the terminal states, the average is equal to -1 everywhere.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_21.png)\n",
      "\n",
      "Let's now move on by 1 step :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_22.png)\n",
      "\n",
      "We still get the immediate reward of $$ -1 $$ for moving, but we add to this amount the value of the cell at the previous step. Threfore, by following this principle by which we add the celle value to the reward we get, when we moved from step 0 to step 1, the value of the cells right next to a terminal state is given by:\n",
      "\n",
      "$$ 0.25 * (-1 + 0) + 0.25 * (-1 + 0) + 0.25 * (-1 + 0) +0.25 * (-1 + 0) = -1 $$\n",
      "\n",
      "If we move by a final step, to $$ K = 3 $$ :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_23.png)\n",
      "\n",
      "At that point, we have reached the optimal policy. There is no longer need to iterate.\n",
      "\n",
      "We now have a way to find the value of a given policy iteratively. How do we then identify the optimal policy ?\n",
      "\n",
      "# Policy Iteration\n",
      "\n",
      "Given a policy $$ \\pi $$, how can we improve the policy we used to have ?\n",
      "- First, evaluate this policy : $$ v_{\\pi}(s) = E(R_{t+1} + \\gamma R_{t+2} + \\cdots \\mid S_t = s) $$\n",
      "- Then, improve the policy greedily : $$ \\pi^{'} = greedy(v_{\\pi}) $$\n",
      "\n",
      "The *greedy* part means that we systematically optimize over the next action by taking into account what will happen if we take this action.\n",
      "\n",
      "In general, we need a lot of improvements and evaluations. This process of **policy iteration** always converges to $$ \\pi^{*} $$.\n",
      "\n",
      "The process can be represented as such :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_24.png)\n",
      "\n",
      "Let us consider a deterministic policy : $$ a = \\pi(s) $$. We can improve the policy by acting greedily :\n",
      "\n",
      "$$ {\\pi}^{'}(s) = argmax_{a \\in A}q_{\\pi}(s, a) $$\n",
      "\n",
      "This improves the value from any state $$ s $$ over one step (at lease) :\n",
      "\n",
      "$$ q_{\\pi}(s, \\pi^{'}(s)) = max_{a \\in A}q_{\\pi}(s, a) ≥ q_{\\pi}(s, \\pi(s)) = v_{\\p}(s)i $$\n",
      "\n",
      "Therefore, the value function is improved over time :\n",
      "\n",
      "$$ v_{\\pi}(s) ≤ q_{\\pi}(s, \\pi^{'}(s)) = E_{\\pi^{'}}(R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\mid S_t = s) $$\n",
      "$$ ≤ E_{\\pi^{'}}(R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, \\pi^{'}(S_{t+1})) \\mid S_t = s) $$\n",
      "$$ ≤ E_{\\pi^{'}}(R_{t+1} + \\gamma R_{t+2} + \\gamma^2 q_{\\pi}(S_{t+2}, \\pi^{'}(S_{t+2})) \\mid S_t = s) $$\n",
      "$$ ≤ E_{\\pi^{'}}(R_{t+1} + \\gamma R_{t+2} + \\cdots \\mid S_t = s) = v_{\\pi^{'}}(s) $$\n",
      "\n",
      "What if the improvements stop, i.e :\n",
      "\n",
      "$$ q_{\\pi}(s, \\pi^{'}(s)) = max_{a \\in A} q_{\\pi}(s,a) = q_{\\pi}(s, \\pi(s)) = v_{\\pi}(s) $$\n",
      "\n",
      "Have we reached the optimal state? Or a we stuck? We actually satisfy the Bellman optimality equation :\n",
      "\n",
      "$$ v_{\\pi}(s) = max_{a \\in A} q_{\\pi}(s,a) $$\n",
      "\n",
      "Therefore, $$ v_{\\pi}(s) = v_{*}(s) $$ for all $$ s \\in S $$.\n",
      "\n",
      "And $$ \\pi $$ is the optimal policy.\n",
      "\n",
      "> Policy iteration solves MDPs. If the iteration ever stops, it means that the optimal policy has been identified.\n",
      "\n",
      "## Modified Policy Iteration\n",
      "\n",
      "Sometimes, we do not need the policy iteration to converge to $$ v_{\\pi} $$ exactly, since this process might be really long. We can therefore :\n",
      "- introduce a stopping criteria over an $$ \\epsilon $$ convergence\n",
      "- stop after $$ k $$ iterations\n",
      "\n",
      "This is called the modified policy iteration.\n",
      "\n",
      "# Value Iteration\n",
      "\n",
      "> Principle of optimality : A policy $$ \\pi(a \\mid s) $$ achieves the optimal value from state $$ s $$, $$ v_{\\pi}(s) = v_{*}(s) $$ if and only if for any state $$ s^{'} $$ reachable from $$ s $$, $$ \\pi $$ achieves the optimal value state $$ s^{'} $$ : $$ v_{\\pi}(s') = v_{*}(s') $$\n",
      "\n",
      "Suppose that we know the solution to subproblems $$ v_{*}(s') $$. Using Bellman, the solution $$ v_{*}(s) $$ can be found by one-step lookahead :\n",
      "\n",
      "$$ v_{*}(s) = max_{a \\in A} R_s^a + \\gamma \\sum_{s' \\in S} P_{ss'}^a v_{*}(s') $$\n",
      "\n",
      "> The idea of value iteration is to apply these updates iteratively.\n",
      "\n",
      "We start with the final rewardds, and work backwards. Suppose that we work on a small gridworld again, where our goal is to reach the blue top-left corner.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_25.png)\n",
      "\n",
      "As before, we suppose that moving has a reward of $$ -1 $$, and reaching the final state has a reward of $$ 0 $$. When we compute the value iteration, we propagate the reward step by step :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_26.png)\n",
      "\n",
      "In value iteration, we must find optimal policy $$ \\pi $$ by iteratively applying Bellman optimality backup : $$ v_1 → v_2 →\\cdots → v_{*} $$\n",
      "\n",
      "It relies on synchronous backups :\n",
      "- At each state $$ k+1 $$\n",
      "- For all states $$ s \\in S $$\n",
      "- Update $$ v_{k+1}(s) $$ from $$ v_k(s') $$\n",
      "\n",
      "Unlike policy iteration, there is no explicit policy. \n",
      "\n",
      "---\n",
      "title: Submitting a first paper to ArXiv\n",
      "layout: post\n",
      "tags: [phd]\n",
      "subtitle : \"Other\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# Why ArXiv?\n",
      "\n",
      "I recently submitted my first conference paper. The paper is called \"Speaker Identification Enhancement using Network Knowledge in Criminal Investigations\", and you can find it [here](https://arxiv.org/abs/2006.02093) if you want.\n",
      "\n",
      "I do not have the response of the conference yet. However, I decided to piublish the paper on ArXiv for several reasons:\n",
      "- It touches a wider audience\n",
      "- It contributes to making research open to all, and not only to those who pay subscriptions to some journals\n",
      "- I read most of the papers on ArXiv, so it's cool to have mine there too :)\n",
      "\n",
      "You should pay attention to the guidelines of the conference you published to. Some of them don't require anything, some of them want you to mention \"Sumbitted to MYCONFERENCE\" as a comment, and some of them forbid to publish it anywhere else while still in review.\n",
      "\n",
      "# From OverLeaf to ArXiv\n",
      "\n",
      "The process of submitting your first ArXiv paper might take longer than you expect. I though that uploading the PDF downloaded from OverLeaf would be enough, but no. ArXiv actually wants the source of your paper, including all of your figures, the .bib references files and the .tex file... \n",
      "\n",
      "Hopefully, if like me, you use Overleaf, there is this magix \"Submit\" button that will help:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/arx_0.png)\n",
      "\n",
      "So first thing first, you need to clean the files that you have in your project:\n",
      "- remove **any** file that is not used\n",
      "- if you use a PDF file at some point, include in your Tex document: ```\\pdfoutput=1```\n",
      "- correct any error in the logs and output files (you can however have warning, no problem with that)\n",
      "\n",
      "Now, once you click on the Submit button, you can select \"ArXiv.org\", and then download the ZIP with submission files. You will end up with a ZIP file that you can now [upload to ArXiv](https://arxiv.org/help/submit).\n",
      "\n",
      "Follow the procedure, and your paper will finally appear as submitted. A few hours/days later, it should be on ArXiv :)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/arx_1.png)\n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "title: A supervised learning approach to predicting nodes betweenness-centrality in time-varying networks\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Criminal Networks\"\n",
      "---\n",
      "\n",
      "I have recently been working on time-varying networks, i.e. networks for which we have timestamps of various interactions between the nodes. This is the case for social networks or criminal networks for example. When we analyze the centrality of the nodes of a graph, it gives us a snapshot at the exact moment of the structure of the graph.\n",
      "\n",
      "However, these networks are time-varying by nature. They evolve, new interactions are being made, new nodes are created... And knowing which nodes are going to be central next month can be a key information in criminal investigations. For this reason, I wanted to spend some time and look at whether one can actually predict the central nodes in the future.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# Dataset\n",
      "\n",
      "I am working on the Enron e-mail dataset, enriched by phone calls that I was able to match. I have overall 1264 events, each event being either an email or a phone call between 2 characters (or more). For each event, I create a row in the training dataset for each node. Overall, my final dataset is made of more than 50'000 rows.\n",
      "\n",
      "The timestamps vary between 2000-08-03 09:10:00 and 2001-01-29 22:21:00. Thus, we have a time period of close to 4 months of events. The first thing that we should look at is the evolution of the centrality of the nodes over time.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/node_evol.png)\n",
      "\n",
      "There seems to be some changes over time over in the order of the major nodes. There also seems to be some dates at which at lot of events were collected, typically because phone calls were not registered continuously.\n",
      "\n",
      "# Feature extraction\n",
      "\n",
      "I turned the node betweenness centrality score prediction into a supervised learning problem. My task will be to predict which node will be central in 1 month from now. This can be useful for police investigations since it does take time to plan when to arrest criminals for example.\n",
      "\n",
      "To build my dataset, for each node, for each date, I am collecting:\n",
      "- the conversation date\n",
      "- the betweenness centrality\n",
      "- the relative degree centrality\n",
      "- the clustering coefficient of the node\n",
      "- the eigen vector centrality\n",
      "- the katz centrality\n",
      "- the load centrality\n",
      "- the harmonic centrality\n",
      "- if the node is in the max clique of the graph\n",
      "- the average clustering of the graph\n",
      "- if the node is in the minimum weighted dominating set\n",
      "\n",
      "In Python, I use NetworkX to implement this feature extraction.\n",
      "\n",
      "```python\n",
      "all_features = []\n",
      "\n",
      "for conv in df.iterrows():\n",
      "    \n",
      "    # If a least 2 characters in the conversation\n",
      "    if len(conv[1]['characters']) >= 2:\n",
      "\n",
      "    \t# Add the edges\n",
      "        for elem in list(itertools.combinations(conv[1]['characters'], 2)):\n",
      "\n",
      "            G.add_edge(elem[0], elem[1])\n",
      "            \n",
      "        # Collect node features\n",
      "        for node in G.nodes():\n",
      "\n",
      "            feature = []\n",
      "            \n",
      "            feature.append(conv[1]['Date'])\n",
      "            feature.append(node)\n",
      "            feature.append(betweenness_centrality(G)[node])\n",
      "            feature.append(G.degree[node]/sum(dict(G.degree).values()))\n",
      "            feature.append(nx.clustering(G)[node])\n",
      "            feature.append(nx.eigenvector_centrality(G)[node])\n",
      "            feature.append(nx.katz_centrality(G)[node])\n",
      "            feature.append(nx.closeness_centrality(G)[node])\n",
      "            feature.append(nx.load_centrality(G)[node])\n",
      "            feature.append(nx.harmonic_centrality(G)[node])\n",
      "\n",
      "            if node in max_clique(G):\n",
      "                feature.append(1)\n",
      "            else:\n",
      "                feature.append(0)\n",
      "            feature.append(average_clustering(G))\n",
      "\n",
      "            if node in min_weighted_dominating_set(G):\n",
      "                feature.append(1)\n",
      "            else:\n",
      "                feature.append(0)\n",
      "                \n",
      "            all_features.append(feature)\n",
      "```\n",
      "\n",
      "I then build a column \"is top 5\" if the node is within the 5 nodes with the highest centrality at the given date. In order to add some additional features, I also append for each node the features at the 5 previous states, and create features that reflect the differences between each state.\n",
      "\n",
      "Then, knowing the situation of the network in 1 month from now, I collect the 5 nodes with the highest centrality at that time, and create a column \"will be top 5\". I must drop the last month of my dataset since it typically would be the period I would neeed to predict on in real life.\n",
      "\n",
      "# Model Performance\n",
      "\n",
      "My dataset is now made of 48744 rows and 125 columns. To create my training and test sets, I must split in time the dataset, in order not to include information from the future. Since there is a class imbalance (is among the five nodes with the highest centrality, and all the other nodes), I chose the F1-score metric. I then compare:\n",
      "- the naive approach of predicting the current centrality \n",
      "- and the model output\n",
      "\n",
      "Talking about the model, I chose an XGBoost with 250 estimators and a max-depth of 6. Since splitting at a random point in time would not be reliable enough, I chose to split to 50 different points in time (every 1000 rows), and plot the results below:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/evol_node.png)\n",
      "\n",
      "# Discussion\n",
      "\n",
      "As we can see, when the XGBoost model sees few training examples, the naive approach clearly outperforms our model. However, with around 6-7'000 training samples, XGBoost clearly outperforms the naive approach. The F1-Score of our prediction is ± 80%, with an accuracy of around 98%, largely over the average F1-Score of the naive approach of 50.6%. \n",
      "\n",
      "We can plot the feature importance of the XGBoost model:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/feat_imp.png)\n",
      "\n",
      "We observe that:\n",
      "- the current load centrality appears to be most important feature to predict betweenness centrality 1 month ahead\n",
      "- other features extracted from the current topology of the network are important\n",
      "- clustering coefficient and relative degree from previous steps are also important\n",
      "- the evolution between relative degrees from one state to another are not as useful features as expected\n",
      "\n",
      "> Overall, predicting the node centrality based on extracted features from the network in a supervised fashion seems to be feasible. The results seem ancouraging and may suggest that investigators in a criminal investigation could use such approach to predict the node centrality one month ahead and plan their surveillance programs accordingly.  \n",
      "\n",
      "---\n",
      "title: Building a Dash Web application for Data Viz and ML\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "I recently had to build a Dash web application to illustrate what Dash-Plotly can do. I chose to present some capabilities regarding Data Viz and Machine Learning. \n",
      "\n",
      "<iframe width=\"700\" height=\"500\" src=\"https://www.youtube.com/embed/UggjszESuUw\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
      "\n",
      "<br>\n",
      "\n",
      "I chose to explore the well-known Iris dataset. I also chose to use only Plotly Express for the visualizations, since the library is a light and well performing tool. There are 2 tabs, the other one is for Machine Learning. The user can select the column to predict, and the columns to use in training. Then, a Support Vector Machine Algorithm is ran on top. Since there are 3 classes, a 3D plot displays the probabilities of belonging to each class. The more separated the probabilities are, the easier it was for the algorithm to split the classes.\n",
      "\n",
      "This could be a first step for a generic ML tool, a bit lit MindsDB. It took few lines of code and an afternoon, and was deployed in minutes with Heroku.\n",
      "---\n",
      "title: Voice Gender Identification\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Signal Processing\"\n",
      "---\n",
      "\n",
      "Can we detect the gender of a voice using ML methods? I recently came across [this](https://appliedmachinelearning.blog/2017/06/14/voice-gender-detection-using-gmms-a-python-primer/) article which I found quite interesting in the way it addresses Gender Identification from vocal recordings. \n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "Voice gender identification relies on three important steps:\n",
      "- Extracting from the training set MFCC features (13 per time window usually)\n",
      "- Train two Gaussian Mixture Models (GMMs) on the feature matrices created (N_records x 13), one for each genre\n",
      "- In prediction, compute the likelihood of each gender using the trained GMMs, and pick the most likely gender\n",
      "\n",
      "The Github repository for this article can be found [here](https://github.com/maelfabien/VoiceGenderDetection/blob/master/README.md).\n",
      "\n",
      "The aim of this project is to build a web application using Streamlit in which a user is able to test the trained algorithm on his or her own voice.\n",
      "\n",
      "# Let's build it\n",
      "\n",
      "## Data and imports\n",
      "\n",
      "⚠️ The dataset has been extracted from [AudioSet](https://research.google.com/audioset/dataset/index.html) and can be downloaded from [here directly](https://drive.google.com/file/d/1g64EswaS5PtwIg-Y0ZmWwvSK1DgYvUuc/view?usp=sharing).\n",
      "\n",
      "\n",
      "Start by importing the libraries that we will need to build this application:\n",
      "\n",
      "```python\n",
      "# Data manipulation\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Feature extraction\n",
      "import scipy\n",
      "import librosa\n",
      "import python_speech_features as mfcc\n",
      "import os\n",
      "from scipy.io.wavfile import read\n",
      "\n",
      "# Model training\n",
      "from sklearn.mixture import GaussianMixture as GMM\n",
      "from sklearn import preprocessing\n",
      "import pickle\n",
      "\n",
      "# Live recording\n",
      "import sounddevice as sd\n",
      "import soundfile as sf\n",
      "```\n",
      "\n",
      "If you have not yet understood or seen the concept of Mel Frequency Cepstral Coefficients (MFCC), I recommend that you take a look at [the article I wrote on the topic of Sound Feature Extraction](https://maelfabien.github.io/machinelearning/Speech9).\n",
      "\n",
      "## Feature Extraction\n",
      "\n",
      "The concept behind this approach to gender detection is really simple. We first create a feature matrix from the training audio recordings. MFCCs are extracted on really small time windows (±20ms), and when you run an MFCC feature extraction using `python_speech_features` or Librosa, it automatically creates a matrix for the whole recording.\n",
      "\n",
      "Knowing that, extracting the MFCC of a audio file is really easy:\n",
      "\n",
      "```python\n",
      "def get_MFCC(sr,audio):\n",
      "    \n",
      "    features = mfcc.mfcc(audio, sr, 0.025, 0.01, 13, appendEnergy = False)\n",
      "    features = preprocessing.scale(features)\n",
      "    \n",
      "    return features\n",
      "```\n",
      "\n",
      "I placed the training data in a folder called AudioSet, in which I have two sub-folders: male_clips and female_clips. We can extract the features of the training set simply by running the function above on all files in the training folder. The problem is however that for the moment, both the train and the test set are in the folder. We must, therefore, split these files in two, and run `get_MFCC` iteratively.\n",
      "\n",
      "```python\n",
      "def get_features(source):\n",
      "    \n",
      "    # Split files\n",
      "    files = [os.path.join(source,f) for f in os.listdir(source) if f.endswith('.wav')]\n",
      "    len_train = int(len(files)*0.8)\n",
      "    train_files = files[:len_train]\n",
      "    test_files = files[len_train:]\n",
      "    \n",
      "    # Train features\n",
      "    features_train = []\n",
      "    for f in train_files:\n",
      "        sr, audio = read(f)\n",
      "        vector = get_MFCC(sr,audio)\n",
      "        if len(features_train) == 0:\n",
      "            features_train = vector\n",
      "        else:\n",
      "            features_train = np.vstack((features_train, vector))\n",
      "            \n",
      "    # Test features  \n",
      "    features_test = []\n",
      "    for f in test_files:\n",
      "        sr, audio = read(f)\n",
      "        vector = get_MFCC(sr,audio)\n",
      "        if len(features_test) == 0:\n",
      "            features_test = vector\n",
      "        else:\n",
      "            features_test = np.vstack((features_test, vector))\n",
      "            \n",
      "    return features_train, features_test\n",
      "```\n",
      "\n",
      "## GMM model training\n",
      "\n",
      "> \"A Gaussian Mixture Model (GMM) is a parametric probability density function represented as a weighted sum of Gaussian component densities. ([source](https://github.com/SuperKogito/Voice-based-gender-recognition))\n",
      "\n",
      "GMMs are commonly used as a parametric model of the probability distribution of continuous measurements or features in a biometric system, such as vocal-tract related spectral features in a speaker recognition system. GMM parameters are estimated from training data using the iterative Expectation-Maximization (EM) algorithm or Maximum A Posteriori(MAP) estimation from a well-trained prior model.\"\n",
      "\n",
      "To apply it to the folder containing the Male recordings, simply use this function, extract the train features and train the Gaussian Mixture Model.\n",
      "\n",
      "```python\n",
      "source = \"AudioSet/male_clips\"\n",
      "features_train_male, features_test_male = get_features(source)\n",
      "gmm_male = GMM(n_components = 8, max_iter = 200, covariance_type = 'diag', n_init = 3)\n",
      "gmm_male.fit(features_train_male)\n",
      "```\n",
      "\n",
      "We can repeat the process for Females:\n",
      "\n",
      "```python\n",
      "source = \"AudioSet/female_clips\"\n",
      "features_train_female, features_test_female =  get_features(source)\n",
      "gmm_female = GMM(n_components = 8, max_iter=200, covariance_type='diag', n_init = 3)\n",
      "gmm_female.fit(features_train_female)\n",
      "```\n",
      "\n",
      "Are these features really differentiable for males and females?\n",
      "\n",
      "We can plot the distribution over the MFCC features for random samples of males and females:\n",
      "\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(15,10))\n",
      "for i in range(1, 430000, 1000):\n",
      "    plt.plot(features_train_male[i], c='b', linewidth=0.5, alpha=0.5)\n",
      "    plt.plot(features_train_female[i], c='r', linewidth=0.5, alpha=0.5)\n",
      "plt.plot(features_male[i+1], c='b', label=\"Male\", linewidth=0.5, alpha=0.5)\n",
      "plt.plot(features_female[i+1], c='r', label=\"Female\", linewidth=0.5, alpha=0.5)\n",
      "plt.legend()\n",
      "plt.title(\"MFCC features for Males and Females\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/mfcc_gender.png)\n",
      "\n",
      "There seem to be slight differences in the features extracted, but the rest of the analysis will tell us more about the separability of these distributions.\n",
      "\n",
      "## Model Evaluation\n",
      "\n",
      "It is now time to evaluate the accuracy of the model on the test features that we kept untouched for the moment. The idea is simply that for a given recording, we estimate the likelihood of each time frame and sum it for the whole recording. Therefore, if the likelihood of a male voice is greater, we return 0 as an answer, and 1 otherwise.\n",
      "\n",
      "```python\n",
      "output = []\n",
      "\n",
      "for f in features_test_male:\n",
      "\n",
      "    log_likelihood_male = np.array(gmm_male.score([f])).sum()\n",
      "    log_likelihood_female = np.array(gmm_female.score([f])).sum()\n",
      "    \n",
      "    if log_likelihood_male > log_likelihood_female:\n",
      "        output.append(0)\n",
      "    else:\n",
      "        output.append(1)\n",
      "```\n",
      "\n",
      "The accuracy for the male test set can be computed as:\n",
      "\n",
      "```python\n",
      "accuracy_male = (1 - sum(output)/len(output))\n",
      "accuracy_male\n",
      "```\n",
      "\n",
      "`0.63148`\n",
      "\n",
      "Similarly, the accuracy for the females reaches 0.63808. \n",
      "\n",
      "Overall, the accuracy is not that high for such a task, and we might need to improve the approach in the next article.\n",
      "\n",
      "## Save models\n",
      "\n",
      "We now suppose that our model is ready to move to production and we re-train it on the whole dataset and save the models:\n",
      "\n",
      "```python\n",
      "def get_features(source):\n",
      "    \n",
      "    files = [os.path.join(source,f) for f in os.listdir(source) if f.endswith('.wav')]\n",
      "    \n",
      "    features = []\n",
      "    for f in files:\n",
      "        sr,audio = read(f)\n",
      "        vector   = get_MFCC(sr,audio)\n",
      "        if len(features) == 0:\n",
      "            features = vector\n",
      "        else:\n",
      "            features = np.vstack((features, vector))\n",
      "\n",
      "    return features\n",
      "\n",
      "source_male = \"test_data/AudioSet/male_clips\"\n",
      "features_male = get_features(source_male)\n",
      "\n",
      "gmm_male = GMM(n_components = 8, max_iter=200, covariance_type='diag', n_init = 3)\n",
      "gmm_male.fit(features_male)\n",
      "\n",
      "source_female = \"test_data/AudioSet/female_clips\"\n",
      "features_female = get_features(source_female)\n",
      "\n",
      "gmm_female = GMM(n_components = 8, max_iter=200, covariance_type='diag', n_init = 3)\n",
      "gmm_female.fit(features_female)\n",
      "\n",
      "# Save models\n",
      "pickle.dump(gmm_male, open(\"male.gmm\", \"wb\" ))\n",
      "pickle.dump(gmm_female, open(\"female.gmm\", \"wb\" ))\n",
      "```\n",
      "\n",
      "## Live Prediction\n",
      "\n",
      "The next step, of course, is to build a live predictor that records 3-5 seconds of an audio sample and classifies it. We use sounddevice for this task, and particularly the rec option.\n",
      "\n",
      "```python\n",
      "def record_and_predict(sr=16000, channels=1, duration=3, filename='pred_record.wav'):\n",
      "    \n",
      "    recording = sd.rec(int(duration * sr), samplerate=sr, channels=channels).reshape(-1)\n",
      "    sd.wait()\n",
      "    \n",
      "    features = get_MFCC(sr,recording)\n",
      "    scores = None\n",
      "\n",
      "    log_likelihood_male = np.array(gmm_male.score(features)).sum()\n",
      "    log_likelihood_female = np.array(gmm_female.score(features)).sum()\n",
      "\n",
      "    if log_likelihood_male >= log_likelihood_female:\n",
      "        return(\"Male\")\n",
      "    else:\n",
      "        return(\"Female\")\n",
      "```\n",
      "\n",
      "To test it in your notebook, simply run :\n",
      "\n",
      "```python\n",
      "record_and_predict()\n",
      "```\n",
      "\n",
      "Leave a comment and tell me how good it works! :)\n",
      "\n",
      "Here's what I noticed while using it. The accuracy on the test remains to improve (63%). When a user plays with his or her voice and tries to imitate the other gender, the GMM gets fooled and predicts the wrong gender. This is also due to the training data that it has seen so far which were extracted from AudioSet and Youtube.\n",
      "\n",
      "# Web application\n",
      "\n",
      "Okay, playing with a notebook is quite easy. But we now need to build a dedicated application for this service. Hopefully, this became really easy with [Streamlit](http://streamlit.io/), a light framework to build interactive applications.\n",
      "\n",
      "I won't dive too much in how Streamlit works (this deserves a dedicated article, coming soon :) ), but here's the code of the application that you should place in `app.py`:\n",
      "\n",
      "```python\n",
      "import streamlit as st\n",
      "\n",
      "# Data manipulation\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Feature extraction\n",
      "import scipy\n",
      "import librosa\n",
      "import python_speech_features as mfcc\n",
      "import os\n",
      "from scipy.io.wavfile import read\n",
      "\n",
      "# Model training\n",
      "from sklearn.mixture import GaussianMixture as GMM\n",
      "from sklearn import preprocessing\n",
      "import pickle\n",
      "\n",
      "# Live recording\n",
      "import sounddevice as sd\n",
      "import soundfile as sf\n",
      "\n",
      "st.title(\"Voice Gender Detection\")\n",
      "st.write(\"This application demonstrates a simple Voice Gender Detection. Voice gender identification relies on three important steps.\")\n",
      "st.write(\"- Extracting from the training set MFCC features (13 usually) for each gender\")\n",
      "st.write(\"- Train a GMM on those features\")\n",
      "st.write(\"- In prediction, compute the likelihood of each gender using the trained GMM, and pick the most likely gender\")\n",
      "\n",
      "\n",
      "st.subheader(\"Ready to try it on your voice?\")\n",
      "\n",
      "st.sidebar.title(\"Parameters\")\n",
      "duration = st.sidebar.slider(\"Recording duration\", 0.0, 10.0, 3.0)\n",
      "\n",
      "def get_MFCC(sr,audio):\n",
      "    \"\"\"\n",
      "    Extracts the MFCC audio features from a file\n",
      "    \"\"\"\n",
      "    features = mfcc.mfcc(audio, sr, 0.025, 0.01, 13, appendEnergy = False)\n",
      "    features = preprocessing.scale(features)\n",
      "    return features\n",
      "\n",
      "def record_and_predict(gmm_male, gmm_female, sr=16000, channels=1, duration=3, filename='pred_record.wav'):\n",
      "    \"\"\"\n",
      "    Records live voice and returns the identified gender\n",
      "    \"\"\" \n",
      "    recording = sd.rec(int(duration * sr), samplerate=sr, channels=channels).reshape(-1)\n",
      "    sd.wait()\n",
      "    \n",
      "    features = get_MFCC(sr,recording)\n",
      "    scores = None\n",
      "\n",
      "    log_likelihood_male = np.array(gmm_male.score(features)).sum()\n",
      "    log_likelihood_female = np.array(gmm_female.score(features)).sum()\n",
      "\n",
      "    if log_likelihood_male >= log_likelihood_female:\n",
      "        return(\"Male\")\n",
      "    else:\n",
      "        return(\"Female\")\n",
      "\n",
      "gmm_male = pickle.load(open('male.gmm','rb'))\n",
      "gmm_female = pickle.load(open('female.gmm','rb'))\n",
      "\n",
      "\n",
      "if st.button(\"Start Recording\"):\n",
      "    with st.spinner(\"Recording...\"):\n",
      "        gender = record_and_predict(gmm_male, gmm_female, duration=duration)\n",
      "        st.write(\"The identified gender is: \" + gender)\n",
      "```\n",
      "\n",
      "To launch the app, you must run the command line `streamlit run app.py`:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gender_app.png)\n",
      "\n",
      "> **Conclusion** : I hope that you enjoyed this article and found the approach useful. It has some severe limitations in terms of accuracy and how the user can trick \n",
      "---\n",
      "title: Predicting the next hit song (Part 1)\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Applied Data Science\"\n",
      "search: false\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "It comes as no surprise that the music industry is tough. When you decide to produce an artist or invest in a marketing campaign for a song, there are many factors to take into account. What if data science could help with this task? What if it could help predict whether a song is going to be a hit or not? \n",
      "\n",
      "# Setting the stage\n",
      "\n",
      "There's no shortage of articles and papers trying to explain why a song became a hit, and the features hit songs share. Here, we will try to go a bit further and build a hit song classifier. To build such a classifier, we'll typically need a lot of data enrichment because there is no single source of data that can help with such a large task. \n",
      "\n",
      "We will use the following sources to help us build the dataset:\n",
      "- Google Trends\n",
      "- Spotify \n",
      "- Billboard\n",
      "- Genius.com\n",
      "\n",
      "We will consider a song a hit if it reaches the top 10 of the most popular songs of the year. Otherwise, it does not count as a hit.\n",
      "\n",
      "In this two-part article, we will implement the following pipeline and build our hit song classifier!\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_22.png)\n",
      "\n",
      "# Data\n",
      "\n",
      "## Trends\n",
      "\n",
      "Let's start by checking the major trends in the music industry using [Google Trends](https://trends.google.com/).\n",
      "\n",
      "We will compare the following musical genres:\n",
      "- Pop\n",
      "- Rock\n",
      "- Rap\n",
      "- Country\n",
      "- Jazz\n",
      "\n",
      "<script type=\"text/javascript\" src=\"https://ssl.gstatic.com/trends_nrtr/1845_RC03/embed_loader.js\"></script> <script type=\"text/javascript\"> trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"/m/064t9\",\"geo\":\"\",\"time\":\"2004-01-01 2019-08-22\"},{\"keyword\":\"/m/06by7\",\"geo\":\"\",\"time\":\"2004-01-01 2019-08-22\"},{\"keyword\":\"/m/01lyv\",\"geo\":\"\",\"time\":\"2004-01-01 2019-08-22\"},{\"keyword\":\"/m/03_d0\",\"geo\":\"\",\"time\":\"2004-01-01 2019-08-22\"},{\"keyword\":\"/m/06bxc\",\"geo\":\"\",\"time\":\"2004-01-01 2019-08-22\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"date=all&q=%2Fm%2F064t9,%2Fm%2F06by7,%2Fm%2F01lyv,%2Fm%2F03_d0,%2Fm%2F06bxc\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"}); </script>\n",
      "\n",
      "Currently, rap is the leading music genre in the world, having taken over other genres such as rock or pop. A geographical analysis can help us understand which countries listen to what music.\n",
      "\n",
      "<script type=\"text/javascript\" src=\"https://ssl.gstatic.com/trends_nrtr/1845_RC03/embed_loader.js\"></script> <script type=\"text/javascript\"> trends.embed.renderExploreWidget(\"GEO_MAP\", {\"comparisonItem\":[{\"keyword\":\"/m/064t9\",\"geo\":\"\",\"time\":\"2004-01-01 2019-08-22\"},{\"keyword\":\"/m/06by7\",\"geo\":\"\",\"time\":\"2004-01-01 2019-08-22\"},{\"keyword\":\"/m/01lyv\",\"geo\":\"\",\"time\":\"2004-01-01 2019-08-22\"},{\"keyword\":\"/m/03_d0\",\"geo\":\"\",\"time\":\"2004-01-01 2019-08-22\"},{\"keyword\":\"/m/06bxc\",\"geo\":\"\",\"time\":\"2004-01-01 2019-08-22\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"date=all&q=%2Fm%2F064t9,%2Fm%2F06by7,%2Fm%2F01lyv,%2Fm%2F03_d0,%2Fm%2F06bxc\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"}); </script>\n",
      "\n",
      "It seems like the US, South Africa and India are strong rap markets, China and Indonesia are strong pop markets, and South America is overall a great rock market. \n",
      "\n",
      "## Top 100\n",
      "\n",
      "First, we will scrape data from the [Billboard Year-End 100 Singles Chart](https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2018). This will be our main data source. This approach has some limitations since we consider that, for any given song in the training data, it will at least hit the top 100 of the world charts. However, if you are trying to sell an ML-based solution to a music label, knowing whether a song will reach the top 10 of the year or remain at the bottom of the ranking has a huge financial impact. \n",
      "\n",
      "The year-end chart is calculated using an inverse point system based on the weekly Billboard charts (100 points for a week at number one, 1 point for a week at number 100, etc), for every year since 1946. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_0.png)\n",
      "\n",
      "Training a classifier using data from 1946 would however make no sense since we need the data to be relevant for the prediction task. Instead, we will focus on data between 2010 and 2018, and make the assumption that the year is not a relevant feature for predicting future hits. \n",
      "\n",
      "## Build the dataset\n",
      "\n",
      "We first need to retrieve the table from Wikipedia for all the years between 2010 and 2018. Open a notebook, and import the following packages :\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import re\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "```\n",
      "\n",
      "We then build a function to handle scraping requests :\n",
      "\n",
      "```python\n",
      "def _handle_request(request_result):\n",
      "    if request_result.status_code == 200:\n",
      "        html_doc =  request_result.text\n",
      "        soup = BeautifulSoup(html_doc,\"html.parser\")\n",
      "        return soup\n",
      "```\n",
      "\n",
      "The table to scrape is of type `table` and has the class: `wikitable sortable jquery-tablesorter`. It can be observed directly from the developer's console :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_1.png)\n",
      "\n",
      "We need to iterate on all the years between 2010 and 2018 :\n",
      "\n",
      "```python\n",
      "artist_array = []\n",
      "\n",
      "for i in range(2010, 2019):\n",
      "\n",
      "    # Iterate over this link and change year\n",
      "    website = 'https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_'+str(i)\n",
      "    \n",
      "    # Get the table\n",
      "    res = requests.get(website)\n",
      "    specific_class = \"wikitable sortable\"\n",
      "    soup = _handle_request(res)\n",
      "    table = soup.find(\"table\", class_= specific_class)\n",
      "    \n",
      "    # Get the body\n",
      "    table_body = table.find('tbody')\n",
      "    \n",
      "    # Get the rows\n",
      "    rows = table_body.find_all('tr')\n",
      "\n",
      "    # For each row\n",
      "    for row in rows:\n",
      "\n",
      "        try :\n",
      "            # Find the ranking\n",
      "            num = row.find_all('th')\n",
      "            num = [ele.text.strip() for ele in num]\n",
      "            \n",
      "            # Assess if the ranking is greater than 1 or not\n",
      "            if int(num[0]) > 10 :\n",
      "                num = 0\n",
      "            else :\n",
      "                num = 1\n",
      "\n",
      "            # Find the title and name of artist\n",
      "            cols = row.find_all('td')\n",
      "            cols = [ele.text.strip() for ele in cols]\n",
      "            \n",
      "            artist_array.append([num, cols[0].replace('\"', ''), cols[1]])\n",
      "\n",
      "        except : \n",
      "            pass\n",
      "```\n",
      "\n",
      "Then, transform this array into a dataframe :\n",
      "\n",
      "```python\n",
      "df = pd.DataFrame(artist_array)\n",
      "df.columns=[\"Hit\", \"Title\", \"Artist\"]\n",
      "df.head(n=10)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_2.png)\n",
      "\n",
      "We now have a good amount of data points. Some songs might be on the charts in two different years so next, we want to keep only the first occurrence of a song to avoid having duplicates in the table:\n",
      "\n",
      "```python\n",
      "df = df.drop_duplicates(subset=[\"Title\", \"Artist\"], keep=\"first\")\n",
      "df.shape\n",
      "```\n",
      "\n",
      "The shape of the data frame is now: `(816, 3)`. Notice that in some cases, the \"Artist\" column contains \"featuring\". We will first create a simple feature where we split the name of the artist column if the word \"featuring\" is present, and add a feature \"featuring\" that is equal to \"1\" if there is a featured artist, and \"0\" elsewhere. \n",
      "\n",
      "```python\n",
      "def featuring(artist):\n",
      "    if \"featuring\" in artist :\n",
      "        return 1\n",
      "    else :\n",
      "        return 0\n",
      "\n",
      "def featuring_substring(artist):\n",
      "    if \"featuring\" in artist :\n",
      "        return artist.split(\"featuring\")[0]\n",
      "    else :\n",
      "        return artist\n",
      "\n",
      "df[\"Featuring\"] = df.apply(lambda row: featuring(row['Artist']), axis=1)\n",
      "df[\"Artist_Feat\"] = df.apply(lambda row: featuring_substring(row['Artist']), axis=1)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_3.png)\n",
      "\n",
      "## Explore the data\n",
      "\n",
      "We can quickly explore the data to observe the most popular artists, the number of hits or the number of features. There is, by construction, an imbalance in the number of hits vs. non-hits :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(10,6))\n",
      "plt.hist(df[\"Hit\"], bins=3)\n",
      "plt.title(\"Non-Hits vs. Hits\")\n",
      "plt.show()\n",
      "````\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_4.png)\n",
      "\n",
      "To assess the performance of a model, we will use the F1-score, which handles unbalanced datasets by providing a harmonic average between the precision and the recall.\n",
      "\n",
      "Is it common for hit songs to feature other artists?\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(10,6))\n",
      "plt.hist(df[\"Featuring\"], bins=3)\n",
      "plt.title(\"No Featuring vs. Featuring\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_5.png)\n",
      "\n",
      "Most hit songs feature other artists! This is an interesting point since in most albums, artists typically have 1 to 3 featurings on a total of 10 to 15 songs.\n",
      "\n",
      "Who are the most popular artists over the years?\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "df['Artist_Feat'].value_counts()[:20].plot(kind=\"bar\")\n",
      "plt.title(\"Most popular artists in the charts\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_6.png)\n",
      "\n",
      "Some artists, like Drake or Ed Sheeran have recorded more hit songs in the time frame considered than any other artist. A song typically has more chances to become a hit if the name of one of those artists is present.\n",
      "\n",
      "## The first model\n",
      "\n",
      "Now, let's build a first \"benchmark\" model that uses the following features :\n",
      "- If there is a featured artist or not\n",
      "- The name of the main artist\n",
      "\n",
      "This model is a naive benchmark and will rely on a simple decision tree.\n",
      "\n",
      "```python\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "```\n",
      "\n",
      "We need to encode the name of the artists into categories to make it understandable for the models we will train :\n",
      "\n",
      "```python\n",
      "le = LabelEncoder()\n",
      "df[\"Artist_Feat_Num\"] = le.fit_transform(df[\"Artist_Feat\"])\n",
      "```\n",
      "\n",
      "Next, we split the `X` and the `y`, and create the train and test sets:\n",
      "\n",
      "```python\n",
      "X = df[[\"Artist_Feat_Num\", \"Featuring\"]]\n",
      "y = df[\"Hit\"]\n",
      "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0) \n",
      "```\n",
      "\n",
      "We will use a simple decision tree classifier with the default parameters as a benchmark :\n",
      "\n",
      "```python\n",
      "dt = DecisionTreeClassifier()\n",
      "dt.fit(X_train, y_train)\n",
      "y_pred = dt.predict(X_test)\n",
      "f1_score(y_pred, y_test)\n",
      "```\n",
      "\n",
      "The resulting F1-score is: `0.066`, which is low. The accuracy is close to 86% since our model tends to predict that the song is systematically not a hit. There is room for better data and better models. \n",
      "\n",
      "# Data enrichment through Spotify\n",
      "\n",
      "Where could we get data from? Well, popular music services like Spotify provide cool APIs that gather a lot of information on artists, albums and tracks. Using external APIs can sometimes be cumbersome. Luckily, there is a great package called [Spotipy](https://spotipy.readthedocs.io/en/latest/#) that does most of the work for us!\n",
      "\n",
      "Spotipy is available on [Github](https://github.com/plamere/spotipy). If you follow the instructions, you will simply have to :\n",
      "- Install the package via PyPI\n",
      "- Create a project from the developer's console of Spotify\n",
      "- Write down your redirect URI and TokenID\n",
      "- Configure the URI and token in the util file of the package\n",
      "\n",
      "Let's move on to the data enrichment. When using Spotipy for the first time, you are required to validate the redirect URI (I have used `http://localhost/` ). \n",
      "\n",
      "```python\n",
      "import sys\n",
      "import spotipy\n",
      "import spotipy.util as util\n",
      "\n",
      "scope = 'user-library-read'\n",
      "\n",
      "if len(sys.argv) > 1:\n",
      "    username = sys.argv[1]\n",
      "else:\n",
      "    print(\"Usage: %s username\" % (sys.argv[0],))\n",
      "sys.exit()\n",
      "\n",
      "token = util.prompt_for_user_token(username, scope)\n",
      "```\n",
      "\n",
      "It will open an external web page. Simply follow it an copy-paste the URL of the page once logged-in.\n",
      "\n",
      "Start the Spotipy session : \n",
      "\n",
      "```python\n",
      "sp = spotipy.Spotify(auth=token)\n",
      "\n",
      "sp.trace = True # turn on tracing\n",
      "sp.trace_out = True # turn on trace out\n",
      "```\n",
      "\n",
      "Spotify's API has a \"search\" feature. Type in the name of an artist or a song (or both combined), and it returns a JSON that contains much of the relevant information needed. We will use information from several levels:\n",
      "- The artist: popularity index and the total number of followers. Notice that these values in the API are the values of today, and therefore take into account some information from the future when you compare it to a song published in 2015 for example.\n",
      "- The album: how many songs were there on the album overall, the date of the release, the number of markets it is available in\n",
      "- The song: Spotify has a number of pre-computed features such as speechiness, loudness, danceability and duration.\n",
      "\n",
      "This will allow us to collect 17 features overall from the Spotify's API ! \n",
      "\n",
      "```python\n",
      "def artist_info(lookup) :\n",
      "\n",
      "    try :\n",
      "        artist = sp.search(lookup)\n",
      "        artist_uri = artist['tracks']['items'][0]['album']['artists'][0]['uri']\n",
      "        track_uri = artist['tracks']['items'][0]['uri']\n",
      "\n",
      "        available_markets = len(artist['tracks']['items'][0]['available_markets'])\n",
      "        release_date = artist['tracks']['items'][0]['album']['release_date']\n",
      "\n",
      "        artist = sp.artist(artist_uri)\n",
      "        total_followers = artist['followers']['total']\n",
      "        genres = artist['genres']\n",
      "        popularity = artist['popularity']\n",
      "\n",
      "        audio_features = sp.audio_features(track_uri)[0]\n",
      "\n",
      "        acousticness = audio_features['acousticness']\n",
      "        danceability = audio_features['danceability']\n",
      "        duration_ms = audio_features['duration_ms']\n",
      "        energy = audio_features['energy']\n",
      "        instrumentalness = audio_features['instrumentalness']\n",
      "        key = audio_features['key']\n",
      "        liveness = audio_features['liveness']\n",
      "        loudness = audio_features['loudness']\n",
      "        speechiness = audio_features['speechiness']\n",
      "        tempo = audio_features['tempo']\n",
      "        time_signature = audio_features['time_signature']\n",
      "        valence = audio_features['valence']\n",
      "\n",
      "        return available_markets, release_date, total_followers, genres, popularity, acousticness, danceability, duration_ms, energy, instrumentalness, key, liveness, loudness, speechiness, tempo, time_signature, valence\n",
      "\n",
      "    except :\n",
      "        return [None]*17\n",
      "```\n",
      "\n",
      "To enhance our chances to identify the song from the search menu, we will create a feature called \"Lookup\" that combines the title of the song and the name of the main artist.\n",
      "\n",
      "```python\n",
      "df['lookup'] = df['Title'] + \" \" + df[\"Artist_Feat\"]\n",
      "```\n",
      "\n",
      "Then, apply the function above to create all columns :\n",
      "\n",
      "```python\n",
      "df['available_markets'], df['release_date'], df['total_followers'], df['genres'], df['popularity'], df['acousticness'], df['danceability'], df['duration_ms'], df['energy'], df['instrumentalness'], df['key'], df['liveness'], df['loudness'], df['speechiness'], df['tempo'], df['time_signature'], df['valence'] = zip(*df['lookup'].map(artist_info))\n",
      "```\n",
      "\n",
      "The new data frame looks like this :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_7.png)\n",
      "\n",
      "We need to make sure that the API sent back relevant information :\n",
      "\n",
      "```python\n",
      "df.shape\n",
      "```\n",
      "\n",
      "`(816,25)`\n",
      "\n",
      "```python\n",
      "df.dropna(how='any').shape\n",
      "```\n",
      "\n",
      "`(814,25)`\n",
      "\n",
      "For two of the input songs, we were not able to retrieve the information from the API. We will simply drop those songs :\n",
      "\n",
      "```python\n",
      "df = df.dropna()\n",
      "```\n",
      "\n",
      "Not all of the features are exploitable as such. Indeed, the release date is under a date format, which cannot be used as a direct input for a predictive model. We initially specified that we wanted our model not to depend on the year. However, the month of release, the day of the month or even the day of the week might be relevant features.\n",
      "\n",
      "```python\n",
      "df['release_date'] = pd.to_datetime(df['release_date'])\n",
      "df['month_release'] = df['release_date'].apply(lambda x: x.month)\n",
      "df['day_release'] = df['release_date'].apply(lambda x: x.day)\n",
      "df['weekday_release'] = df['release_date'].apply(lambda x: x.weekday())\n",
      "```\n",
      "\n",
      "## Data Exploration\n",
      "\n",
      "We now have many features and can proceed to a further data exploration. Let's start by analyzing the features we just created in relation to the release date:\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist(df['weekday_release'], bins=14)\n",
      "plt.title(\"Weekday release\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_8.png)\n",
      "\n",
      "More songs seem to be released on Fridays! That's an interesting insight.\n",
      "\n",
      "Regarding the release month :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist(df['month_release'], bins=24)\n",
      "plt.title(\"Month release\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_9.png)\n",
      "\n",
      "January seems to be a popular choice, although we should probably be careful. Some missing data might be filled by default to January 1st. During the months of July and August, however, there are few songs being released. On the following graph, we see that most songs are available on most markets:\n",
      "\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist(df['available_markets'], bins=50)\n",
      "plt.title(\"Number of markets\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_10.png)\n",
      "\n",
      "This seems intuitive since a song targeting a large audience has more chances to succeed. \n",
      "\n",
      "A strong feature will probably be the popularity of the artist :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist(df[df['Hit']==1]['popularity'], bins=50, density=True, alpha=0.5, label=\"Hit\")\n",
      "plt.hist(df[df['Hit']==0]['popularity'], bins=50, density=True, alpha=0.5, label=\"Not Hit\")\n",
      "plt.title(\"Artist Popularity\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_11.png)\n",
      "\n",
      "In both cases, the popularity of the artist as defined by Spotify's API is really high. We however notice that the distribution of the popularity of the artist of hit songs is slightly shifted to the right. On the other hand, several songs that did not become a hit were released by artists with a popularity of less than 70. A really high popularity might be a key feature.\n",
      "\n",
      "Finally, let's explore the effect of duration on the hit songs:\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist(df[df['Hit']==1]['duration_ms'], bins=50, density=True, alpha=0.5, label=\"Hit\")\n",
      "plt.hist(df[df['Hit']==0]['duration_ms'], bins=50, density=True, alpha=0.5, label=\"Not Hit\")\n",
      "plt.title(\"Duration in ms.\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_12.png)\n",
      "\n",
      "The distribution looks quite similar in both cases. We do not expect the duration of the song to be an important feature.\n",
      "\n",
      "## Same model, better data\n",
      "\n",
      "We can now build a second classifier. However, we still have quite a limited number of data points and an unbalanced dataset. Can oversampling help?\n",
      "\n",
      "We will use the Synthetic Minority Over-Sampling Technique (SMOTE). SMOTE is implemented in the package `imblearn` for Python.\n",
      "\n",
      "```python\n",
      "from imblearn.over_sampling import SMOTE\n",
      "\n",
      "X = df.drop([\"Artist_Feat\", \"Artist\", \"Artist_Feat_Num\", \"Title\", \"Hit\", \"lookup\", \"release_date\", \"genres\"], axis=1)\n",
      "y = df[\"Hit\"]\n",
      "\n",
      "sm = SMOTE(random_state=42)\n",
      "X_res, y_res = sm.fit_resample(X, y)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_res,y_res, test_size=0.2, random_state=42) \n",
      "```\n",
      "\n",
      "Let's apply the same decision tree we used before :\n",
      "\n",
      "```python\n",
      "dt = DecisionTreeClassifier(max_depth=100)\n",
      "dt.fit(X_train, y_train)\n",
      "y_pred = dt.predict(X_test)\n",
      "f1_score(y_pred, y_test)\n",
      "```\n",
      "\n",
      "The F1-score reaches 83.4%. Since the data is not unbalanced anymore, we can compute the accuracy:\n",
      "\n",
      "```python\n",
      "accuracy_score(y_pred, y_test)\n",
      "```\n",
      "\n",
      "It reaches 84%.\n",
      "\n",
      "## Better model, better data\n",
      "\n",
      "The decision tree is a good choice for a first model. However, more complex models might improve overall performance. Let's try this with a Random Forest Classifier :\n",
      "\n",
      "```python\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "rf=RandomForestClassifier(n_estimators=100)\n",
      "rf.fit(X_train, y_train)\n",
      "y_pred = rf.predict(X_test)\n",
      "\n",
      "f1_score(y_pred, y_test)\n",
      "```\n",
      "\n",
      "The F1-score reaches 93.3% and the accuracy is 94.5%. Plotting the confusion matrix helps us understand the errors our classifier made :\n",
      "\n",
      "```python\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import seaborn as sns\n",
      "\n",
      "cm = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "sns.heatmap(cm, annot=True)\n",
      "plt.title(\"Confusion Matrix\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_13.png)\n",
      "\n",
      "The most common mistake is to have classified a song as a hit whereas it was not (14 cases). \n",
      "\n",
      "We can now try to understand the output of the classifier by looking at the feature importance :\n",
      "\n",
      "```python\n",
      "importances = rf.feature_importances_\n",
      "indices = np.argsort(importances)\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.title('Feature Importances')\n",
      "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
      "plt.yticks(range(len(indices)), [X.columns[i] for i in indices])\n",
      "plt.xlabel('Relative Importance')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_14.png)\n",
      "\n",
      "The most important feature is whether there is a featuring artist or not. Then, the most important features are related to the release date, and the popularity of the artist. After that, we find all features related to the song itself.\n",
      "\n",
      "This analysis highlights something major. Essentially, a song is a hit if it has a good featured artist, is released at the right moment, and the artists who release it are popular. All of this seems logical, but it's also verified empirically by our model!\n",
      "\n",
      "Next steps: could we further improve the model by adding the lyrics? We will explore this in the second part of the article.\n",
      "---\n",
      "title: Build a Language Recognition app\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "In this project, we will build a language recognition app using Markov Chains and likelihood decoding algorithm. If you have not seen my previous articles on this topic, I invite you to check them :)\n",
      "\n",
      "# Language Recognition \n",
      "\n",
      "We aim to build a small web app able to recognize the language of an input text. We will :\n",
      "- Build a transition probabilities matrix from Wikipedia's articles\n",
      "- Find the most \"likely\" language by multiplying the transition probabilities for a given sequence\n",
      "- Identify the highest result to return the language\n",
      "\n",
      "## Build the transition probability matrix\n",
      "\n",
      "Start by importing the following packages :\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Parsing\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "```\n",
      "\n",
      "Then, we'll pick one of the longest articles from Wikipedia: South African Labor Law, and learn transition probabilities from it. Transition probabilities are simply the probabilities, from a given letter (say `s`) to move to another letter (say `o`).\n",
      "\n",
      "```python\n",
      "url = 'https://en.wikipedia.org/wiki/South_African_labour_law'\n",
      "res = requests.get(url)\n",
      "html_page = res.content\n",
      "```\n",
      "\n",
      "We can then parse the content :\n",
      "\n",
      "```python\n",
      "soup = BeautifulSoup(html_page, 'html.parser')\n",
      "text = soup.find_all(text=True)\n",
      "```\n",
      "\n",
      "We will use the following code to clean the content as much as possible :\n",
      "\n",
      "```python\n",
      "output = ''\n",
      "\n",
      "blacklist = [\n",
      "    '[document]',\n",
      "    'noscript',\n",
      "    'header',\n",
      "    'html',\n",
      "    'meta',\n",
      "    'head', \n",
      "    'input',\n",
      "    'script',\n",
      "    '\\n'\n",
      "]\n",
      "\n",
      "for t in text:\n",
      "    if t.parent.name not in blacklist:\n",
      "        output += '{} '.format(t)\n",
      "```\n",
      "\n",
      "The raw text contains many HTML tags, and elements specific to Wikipedia. We need to clean the text a bit :\n",
      "\n",
      "```python\n",
      "def preprocess_text(text) :\n",
      "\n",
      "    text = text.replace('\\n', '').replace('[ edit ]', '').replace(\"\\'\", \"'\")\n",
      "    text = ''.join(c.lower() for c in text if not c.isdigit())\n",
      "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
      "\n",
      "    return text\n",
      "```\n",
      "\n",
      "And apply the function to the clean text :\n",
      "\n",
      "```\n",
      "text = preprocess_text(output)\n",
      "```\n",
      "\n",
      "Then, define two dictionnaries we'll need later on :\n",
      "\n",
      "```\n",
      "dic={1 : ' ', \n",
      "    2 : 'a', \n",
      "    3 : 'b', \n",
      "    4: 'c', \n",
      "    5 : 'd', \n",
      "    6 : 'e', \n",
      "    7: 'f', \n",
      "    8 : 'g', \n",
      "    9 : 'h', \n",
      "    10: 'i', \n",
      "    11: 'j', \n",
      "    12 : 'k', \n",
      "    13 : 'l', \n",
      "    14: 'm', \n",
      "    15 : 'n', \n",
      "    16 : 'o', \n",
      "    17: 'p', \n",
      "    18 : 'q', \n",
      "    19 : 'r' , \n",
      "    20: 's', \n",
      "    21 : 't', \n",
      "    22 : 'u', \n",
      "    23: 'v', \n",
      "    24 : 'w', \n",
      "    25 : 'x' , \n",
      "    26: 'y', \n",
      "    27 : 'z'\n",
      "}\n",
      "\n",
      "dic_2 = {' ' : 0, \n",
      "    'a' : 1, \n",
      "    'b' : 2, \n",
      "    'c' : 3, \n",
      "    'd' : 4, \n",
      "    'e' : 5, \n",
      "    'f' : 6, \n",
      "    'g' : 7, \n",
      "    'h' : 8, \n",
      "    'i' : 9, \n",
      "    'j' : 10, \n",
      "    'k' : 11, \n",
      "    'l' : 12, \n",
      "    'm' : 13, \n",
      "    'n' : 14, \n",
      "    'o' : 15, \n",
      "    'p' : 16, \n",
      "    'q' : 17, \n",
      "    'r' : 18, \n",
      "    's' : 19, \n",
      "    't' : 20, \n",
      "    'u' : 21, \n",
      "    'v' : 22, \n",
      "    'w' : 23, \n",
      "    'x' : 24, \n",
      "    'y' : 25, \n",
      "    'z' : 26\n",
      "}\n",
      "```\n",
      "\n",
      "Alright, we now need to go through the whole text, and compute the number of time we went from one letter to another. I have kept the implementation really simple for explainability purposes. There are several ways to improve this part :\n",
      "\n",
      "```python\n",
      "a = np.zeros(27)\n",
      "b = np.zeros(27)\n",
      "c = np.zeros(27)\n",
      "d = np.zeros(27)\n",
      "e = np.zeros(27)\n",
      "f = np.zeros(27)\n",
      "g = np.zeros(27)\n",
      "h = np.zeros(27)\n",
      "i = np.zeros(27)\n",
      "j = np.zeros(27)\n",
      "k = np.zeros(27)\n",
      "l = np.zeros(27)\n",
      "m = np.zeros(27)\n",
      "n = np.zeros(27)\n",
      "o = np.zeros(27)\n",
      "p = np.zeros(27)\n",
      "q = np.zeros(27)\n",
      "r = np.zeros(27)\n",
      "s = np.zeros(27)\n",
      "t = np.zeros(27)\n",
      "u = np.zeros(27)\n",
      "v = np.zeros(27)\n",
      "w = np.zeros(27)\n",
      "x = np.zeros(27)\n",
      "y = np.zeros(27)\n",
      "z = np.zeros(27)\n",
      "space = np.zeros(27)\n",
      "\n",
      "prev = ' '\n",
      "\n",
      "for char in text:\n",
      "    if prev == ' ':\n",
      "        space[dic_2[char]] += 1\n",
      "    elif prev == 'a' : \n",
      "        a[dic_2[char]] += 1\n",
      "    elif prev == 'b':\n",
      "        b[dic_2[char]] += 1\n",
      "    elif prev == 'c':\n",
      "        c[dic_2[char]] += 1\n",
      "    elif prev == 'd':\n",
      "        d[dic_2[char]] += 1\n",
      "    elif prev == 'e':\n",
      "        e[dic_2[char]] += 1\n",
      "    elif prev == 'f':\n",
      "        f[dic_2[char]] += 1\n",
      "    elif prev == 'g':\n",
      "        g[dic_2[char]] += 1\n",
      "    elif prev == 'h':\n",
      "        h[dic_2[char]] += 1\n",
      "    elif prev == 'i':\n",
      "        i[dic_2[char]] += 1\n",
      "    elif prev == 'j':\n",
      "        j[dic_2[char]] += 1\n",
      "    elif prev == 'k':\n",
      "        k[dic_2[char]] += 1\n",
      "    elif prev == 'l':\n",
      "        l[dic_2[char]] += 1\n",
      "    elif prev == 'm':\n",
      "        m[dic_2[char]] += 1\n",
      "    elif prev == 'n':\n",
      "        n[dic_2[char]] += 1\n",
      "    elif prev == 'o':\n",
      "        o[dic_2[char]] += 1\n",
      "    elif prev == 'p':\n",
      "        p[dic_2[char]] += 1\n",
      "    elif prev == 'q':\n",
      "        q[dic_2[char]] += 1\n",
      "    elif prev == 'r':\n",
      "        r[dic_2[char]] += 1\n",
      "    elif prev == 's':\n",
      "        s[dic_2[char]] += 1\n",
      "    elif prev == 't':\n",
      "        t[dic_2[char]] += 1\n",
      "    elif prev == 'u':\n",
      "        u[dic_2[char]] += 1\n",
      "    elif prev == 'v':\n",
      "        v[dic_2[char]] += 1\n",
      "    elif prev == 'w':\n",
      "        w[dic_2[char]] += 1\n",
      "    elif prev == 'x':\n",
      "        x[dic_2[char]] += 1\n",
      "    elif prev == 'y':\n",
      "        y[dic_2[char]] += 1\n",
      "    elif prev == 'z':\n",
      "        z[dic_2[char]] += 1\n",
      "\n",
      "    prev = char\n",
      "```\n",
      "\n",
      "At that point, we have raw number which we need to transform into probabilities :\n",
      "\n",
      "```python\n",
      "a = a / np.sum(a)\n",
      "b = b / np.sum(b)\n",
      "c = c / np.sum(c)\n",
      "d = d / np.sum(d)\n",
      "e = e / np.sum(e)\n",
      "f = f / np.sum(f)\n",
      "g = g / np.sum(g)\n",
      "h = h / np.sum(h)\n",
      "i = i / np.sum(i)\n",
      "j = j / np.sum(j)\n",
      "k = k / np.sum(k)\n",
      "l = l / np.sum(l)\n",
      "m = m / np.sum(m)\n",
      "n = n / np.sum(n)\n",
      "o = o / np.sum(o)\n",
      "p = p / np.sum(p)\n",
      "q = q / np.sum(q)\n",
      "r = r / np.sum(r)\n",
      "s = s / np.sum(s)\n",
      "t = t / np.sum(t)\n",
      "u = u / np.sum(u)\n",
      "v = v / np.sum(v)\n",
      "w = w / np.sum(w)\n",
      "x = x / np.sum(x)\n",
      "y = y / np.sum(y)\n",
      "z = z / np.sum(z)\n",
      "space = space / np.sum(space)\n",
      "```\n",
      "\n",
      "To retrieve the final matrix, we can declare :\n",
      "\n",
      "```python\n",
      "trans_eng = np.matrix([space, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z])\n",
      "```\n",
      "\n",
      "I have summarized this into a function. Here is the whole code needed :\n",
      "\n",
      "```python\n",
      "global dic\n",
      "global dic_2\n",
      "\n",
      "dic={1 : ' ', \n",
      "    2 : 'a', \n",
      "    3 : 'b', \n",
      "    4: 'c', \n",
      "    5 : 'd', \n",
      "    6 : 'e', \n",
      "    7: 'f', \n",
      "    8 : 'g', \n",
      "    9 : 'h', \n",
      "    10: 'i', \n",
      "    11: 'j', \n",
      "    12 : 'k', \n",
      "    13 : 'l', \n",
      "    14: 'm', \n",
      "    15 : 'n', \n",
      "    16 : 'o', \n",
      "    17: 'p', \n",
      "    18 : 'q', \n",
      "    19 : 'r' , \n",
      "    20: 's', \n",
      "    21 : 't', \n",
      "    22 : 'u', \n",
      "    23: 'v', \n",
      "    24 : 'w', \n",
      "    25 : 'x' , \n",
      "    26: 'y', \n",
      "    27 : 'z'\n",
      "}\n",
      "\n",
      "dic_2 = {' ' : 0, \n",
      "    'a' : 1, \n",
      "    'b' : 2, \n",
      "    'c' : 3, \n",
      "    'd' : 4, \n",
      "    'e' : 5, \n",
      "    'f' : 6, \n",
      "    'g' : 7, \n",
      "    'h' : 8, \n",
      "    'i' : 9, \n",
      "    'j' : 10, \n",
      "    'k' : 11, \n",
      "    'l' : 12, \n",
      "    'm' : 13, \n",
      "    'n' : 14, \n",
      "    'o' : 15, \n",
      "    'p' : 16, \n",
      "    'q' : 17, \n",
      "    'r' : 18, \n",
      "    's' : 19, \n",
      "    't' : 20, \n",
      "    'u' : 21, \n",
      "    'v' : 22, \n",
      "    'w' : 23, \n",
      "    'x' : 24, \n",
      "    'y' : 25, \n",
      "    'z' : 26\n",
      "    }\n",
      "\n",
      "def preprocess_text(text) :\n",
      "\n",
      "    text = text.replace('\\n', '').replace('[ edit ]', '').replace(\"\\'\", \"'\")\n",
      "    text = ''.join(c.lower() for c in text if not c.isdigit())\n",
      "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
      "\n",
      "    return text\n",
      "\n",
      "def compute_transition(url_input):\n",
      "\n",
      "    url = url_input\n",
      "    res = requests.get(url)\n",
      "    html_page = res.content\n",
      "\n",
      "    soup = BeautifulSoup(html_page, 'html.parser')\n",
      "    text = soup.find_all(text=True)\n",
      "\n",
      "    output = ''\n",
      "    blacklist = [\n",
      "        '[document]',\n",
      "        'noscript',\n",
      "        'header',\n",
      "        'html',\n",
      "        'meta',\n",
      "        'head', \n",
      "        'input',\n",
      "        'script',\n",
      "        '\\n'\n",
      "    ]\n",
      "\n",
      "    for t in text:\n",
      "        if t.parent.name not in blacklist:\n",
      "        output += '{} '.format(t)\n",
      "\n",
      "    text = preprocess_text(output)\n",
      "\n",
      "    a = np.zeros(27)\n",
      "    b = np.zeros(27)\n",
      "    c = np.zeros(27)\n",
      "    d = np.zeros(27)\n",
      "    e = np.zeros(27)\n",
      "    f = np.zeros(27)\n",
      "    g = np.zeros(27)\n",
      "    h = np.zeros(27)\n",
      "    i = np.zeros(27)\n",
      "    j = np.zeros(27)\n",
      "    k = np.zeros(27)\n",
      "    l = np.zeros(27)\n",
      "    m = np.zeros(27)\n",
      "    n = np.zeros(27)\n",
      "    o = np.zeros(27)\n",
      "    p = np.zeros(27)\n",
      "    q = np.zeros(27)\n",
      "    r = np.zeros(27)\n",
      "    s = np.zeros(27)\n",
      "    t = np.zeros(27)\n",
      "    u = np.zeros(27)\n",
      "    v = np.zeros(27)\n",
      "    w = np.zeros(27)\n",
      "    x = np.zeros(27)\n",
      "    y = np.zeros(27)\n",
      "    z = np.zeros(27)\n",
      "    space = np.zeros(27)\n",
      "\n",
      "    prev = ' '\n",
      "\n",
      "    for char in text:\n",
      "        if prev == ' ':\n",
      "            space[dic_2[char]] += 1\n",
      "        elif prev == 'a' : \n",
      "            a[dic_2[char]] += 1\n",
      "        elif prev == 'b':\n",
      "            b[dic_2[char]] += 1\n",
      "        elif prev == 'c':\n",
      "            c[dic_2[char]] += 1\n",
      "        elif prev == 'd':\n",
      "            d[dic_2[char]] += 1\n",
      "        elif prev == 'e':\n",
      "            e[dic_2[char]] += 1\n",
      "        elif prev == 'f':\n",
      "            f[dic_2[char]] += 1\n",
      "        elif prev == 'g':\n",
      "            g[dic_2[char]] += 1\n",
      "        elif prev == 'h':\n",
      "            h[dic_2[char]] += 1\n",
      "        elif prev == 'i':\n",
      "            i[dic_2[char]] += 1\n",
      "        elif prev == 'j':\n",
      "            j[dic_2[char]] += 1\n",
      "        elif prev == 'k':\n",
      "            k[dic_2[char]] += 1\n",
      "        elif prev == 'l':\n",
      "            l[dic_2[char]] += 1\n",
      "        elif prev == 'm':\n",
      "            m[dic_2[char]] += 1\n",
      "        elif prev == 'n':\n",
      "            n[dic_2[char]] += 1\n",
      "        elif prev == 'o':\n",
      "            o[dic_2[char]] += 1\n",
      "        elif prev == 'p':\n",
      "            p[dic_2[char]] += 1\n",
      "        elif prev == 'q':\n",
      "            q[dic_2[char]] += 1\n",
      "        elif prev == 'r':\n",
      "            r[dic_2[char]] += 1\n",
      "        elif prev == 's':\n",
      "            s[dic_2[char]] += 1\n",
      "        elif prev == 't':\n",
      "            t[dic_2[char]] += 1\n",
      "        elif prev == 'u':\n",
      "            u[dic_2[char]] += 1\n",
      "        elif prev == 'v':\n",
      "            v[dic_2[char]] += 1\n",
      "        elif prev == 'w':\n",
      "            w[dic_2[char]] += 1\n",
      "        elif prev == 'x':\n",
      "            x[dic_2[char]] += 1\n",
      "        elif prev == 'y':\n",
      "            y[dic_2[char]] += 1\n",
      "        elif prev == 'z':\n",
      "            z[dic_2[char]] += 1\n",
      "\n",
      "        prev = char\n",
      "\n",
      "    a = a / np.sum(a)\n",
      "    b = b / np.sum(b)\n",
      "    c = c / np.sum(c)\n",
      "    d = d / np.sum(d)\n",
      "    e = e / np.sum(e)\n",
      "    f = f / np.sum(f)\n",
      "    g = g / np.sum(g)\n",
      "    h = h / np.sum(h)\n",
      "    i = i / np.sum(i)\n",
      "    j = j / np.sum(j)\n",
      "    k = k / np.sum(k)\n",
      "    l = l / np.sum(l)\n",
      "    m = m / np.sum(m)\n",
      "    n = n / np.sum(n)\n",
      "    o = o / np.sum(o)\n",
      "    p = p / np.sum(p)\n",
      "    q = q / np.sum(q)\n",
      "    r = r / np.sum(r)\n",
      "    s = s / np.sum(s)\n",
      "    t = t / np.sum(t)\n",
      "    u = u / np.sum(u)\n",
      "    v = v / np.sum(v)\n",
      "    w = w / np.sum(w)\n",
      "    x = x / np.sum(x)\n",
      "    y = y / np.sum(y)\n",
      "    z = z / np.sum(z)\n",
      "    space = space / np.sum(space)\n",
      "\n",
      "    return np.matrix([space, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z])\n",
      "```\n",
      "\n",
      "We can then pick long articles in English, French and Italian for example :\n",
      "\n",
      "```python\n",
      "trans_eng = compute_transition('https://en.wikipedia.org/wiki/South_African_labour_law')\n",
      "trans_fr = compute_transition('https://fr.wikipedia.org/wiki/Histoire_du_m%C3%A9tier_de_plombier')\n",
      "trans_it = compute_transition('https://it.wikipedia.org/wiki/Storia_d%27Italia')\n",
      "```\n",
      "\n",
      "The transition matrices are now computed! Let's move to the language recognition part.\n",
      "\n",
      "## Identify the language\n",
      "\n",
      "We will now try to identify the language based on the transition likelihood. All we need to do is, for each language, roll back identify the transition probability from one letter to another, and return the most likely language.\n",
      "\n",
      "```python\n",
      "def rec_language(dic, dic_2, bi_eng, bi_fr, bi_it, seq) :\n",
      "\n",
      "    seq = preprocess_text(seq)\n",
      "\n",
      "    key_0 = 0\n",
      "    trans_eng = 1\n",
      "    trans_fra = 1\n",
      "    trans_it = 1\n",
      "\n",
      "    for letter in seq :\n",
      "    \n",
      "    # If unknown character missed by pre-processing\n",
      "        try :\n",
      "            key_1 = dic_2[letter]\n",
      "\n",
      "            trans_eng = trans_eng * bi_eng[key_0, key_1]\n",
      "            trans_fra = trans_fra * bi_fr[key_0, key_1]\n",
      "            trans_it = trans_it * bi_it[key_0, key_1]\n",
      "\n",
      "            key_0 = dic_2[letter]\n",
      "        except :\n",
      "            continue\n",
      "\n",
      "    if trans_eng > trans_fra and trans_eng > trans_it :\n",
      "        print(\"It's English !\")\n",
      "    elif trans_fra > trans_eng and trans_fra > trans_it :\n",
      "        print(\"It's French !\") \n",
      "    else :\n",
      "        print(\"It's Italian !\")\n",
      "```\n",
      "\n",
      "We can now try it in some sentences! First, a french sentence :\n",
      "\n",
      "```python\n",
      "rec_language(dic, dic_2, trans_eng, trans_fr, trans_it, \"Quel beau temps aujourd'hui !\")\n",
      "```\n",
      "\n",
      "Returns : \n",
      "```\n",
      "It's French !\n",
      "```\n",
      "\n",
      "Then, in English :\n",
      "\n",
      "```python\n",
      "rec_language(dic, dic_2, trans_eng, trans_fr, trans_it, 'What a nice weather today !')\n",
      "```\n",
      "\n",
      "Returns :\n",
      "\n",
      "```\n",
      "It's English !\n",
      "```\n",
      "\n",
      "And in italian : \n",
      "\n",
      "```python\n",
      "rec_language(dic, dic_2, trans_eng, trans_fr, trans_it, 'Che bello tempo fa oggi !')\n",
      "```\n",
      "\n",
      "Returns :\n",
      "\n",
      "```\n",
      "It's Italian !\n",
      "```\n",
      "\n",
      "## Potential improvements \n",
      "\n",
      "We fetched the transition probabilities from single articles in Wikipedia. To develop a more robust solution, we should consider a large input corpus.\n",
      "\n",
      "The text pre-processing is not perfect, and we should add some more features to it.\n",
      "\n",
      "Finally, we tested only 3 languages, but we could generalize the solution we have developed to other languages.\n",
      " \n",
      "# Standalone App with Voilà\n",
      " \n",
      " You might have heard of Voilà that lets you run your Jupyter Notebooks as standalone apps. Let's try this out!\n",
      " \n",
      " Start by installing Voilà :\n",
      " \n",
      " ```\n",
      " pip install voila\n",
      " ```\n",
      " \n",
      " Then, in the notebook, create an interactive widget :\n",
      " \n",
      " ```python\n",
      " from ipywidgets import widgets\n",
      " from ipywidgets import interact, interactive, fixed, interact_manual\n",
      " \n",
      " def reco_interactive(x):\n",
      "    return rec_language(dic, dic_2, trans_eng, trans_fr, trans_it, x)\n",
      " ```\n",
      " \n",
      " And run the interactive cell :\n",
      " \n",
      " ```python\n",
      " interact(reco_interactive, x='Hi there!');\n",
      " ```\n",
      " \n",
      " You should see something like this :\n",
      " \n",
      " ![image](https://maelfabien.github.io/assets/images/voila.jpg)\n",
      " \n",
      " Then, to create an app from it, simply run from your terminal, in the notebook's folder :\n",
      " \n",
      " ```\n",
      " voila notebook.ipynb\n",
      " ```\n",
      " \n",
      " You'll have access to a webpage where the interactive widget works as a standalone app!\n",
      " \n",
      "  ![image](https://maelfabien.github.io/assets/images/voila_2.jpg)\n",
      " \n",
      "> **Conclusion** : Although text embedding and deep learning seem to be everywhere nowadays, simple approaches like likelihood Decoding algorithm and Markov Chains can bring value if we're looking for a light, fast and explainable solution (think about including this in a smartphone's software for example). I hope this was useful, and don't hesitate to comment if you have any question.\n",
      "\n",
      "---\n",
      "title: Implementing YoloV3 for object detection\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "Yolo is one of the greatest algorithm for real-time object detection. In its large version, it can detect thousands of object types in a quick and efficient manner. I this article, I won't cover the technical details of YoloV3, but I'll jump straight to the implementation. We will learn to build a simple web application with Streamlit that detects the objects present in an image. This implementation is a simplified version of Streamlit's yolo demo.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# Download the weights and config\n",
      "\n",
      "Create a separate folder for your web application, and download the weights via your command line:\n",
      "\n",
      "```bash\n",
      "wget https://pjreddie.com/media/files/yolov3.weight\n",
      "```\n",
      "\n",
      "Then, download the configuration file:\n",
      "\n",
      "```bash\n",
      "wget https://pjreddie.com/media/files/yolov3.cfg\n",
      "```\n",
      "\n",
      "Finally, download the classes predicted by the YoloV3 algorithm in a text file:\n",
      "\n",
      "```bash\n",
      "wget https://pjreddie.com/media/files/classes.txt\n",
      "```\n",
      "\n",
      "# App layout\n",
      "\n",
      "```python\n",
      "# Add a title and sidebar\n",
      "st.title(\"Object Detection\")\n",
      "st.sidebar.markdown(\"# Model\")\n",
      "confidence_threshold = st.sidebar.slider(\"Confidence threshold\", 0.0, 1.0, 0.5, 0.01)\n",
      "```\n",
      "\n",
      "There are now 3 main steps:\n",
      "- a function to download the image from the selected file\n",
      "- a function to apply the object detection on the image and plot the boxes\n",
      "- a selector on the sidemenu to pick the input image\n",
      "\n",
      "The first function is quick to implement :\n",
      "\n",
      "```python\n",
      "@st.cache(show_spinner=False)\n",
      "def read_img(img):\n",
      "    image = cv2.imread(img, cv2.IMREAD_COLOR)\n",
      "    image = image[:, :, [2, 1, 0]] # BGR -> RGB\n",
      "    return image\n",
      "```\n",
      "\n",
      "Then, build the function to identify the bounding boxes. The code is commented in order to understandd the key steps.\n",
      "\n",
      "```python\n",
      "def yolo_v3(image, confidence_threshold=0.5, overlap_threshold=0.3):\n",
      "\n",
      "\t# Load model architecture\n",
      "    net = cv2.dnn.readNetFromDarknet(\"yolov3.cfg\", \"yolov3.weights\")\n",
      "    output_layer_names = net.getLayerNames()\n",
      "    output_layer_names = [output_layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
      "\n",
      "    # Set input and get output\n",
      "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
      "    net.setInput(blob)\n",
      "    layer_outputs = net.forward(output_layer_names)\n",
      "\n",
      "    boxes, confidences, class_IDs = [], [], []\n",
      "    H, W = image.shape[:2]\n",
      "\n",
      "    # For each detected object, compute the box, find the score, ignore if below\n",
      "    for output in layer_outputs:\n",
      "        for detection in output:\n",
      "            scores = detection[5:]\n",
      "            classID = np.argmax(scores)\n",
      "            confidence = scores[classID]\n",
      "            if confidence > confidence_threshold:\n",
      "                box = detection[0:4] * np.array([W, H, W, H])\n",
      "                centerX, centerY, width, height = box.astype(\"int\")\n",
      "                x, y = int(centerX - (width / 2)), int(centerY - (height / 2))\n",
      "                boxes.append([x, y, int(width), int(height)])\n",
      "                confidences.append(float(confidence))\n",
      "                class_IDs.append(classID)\n",
      "\n",
      "    # Write the name of detected objects above image\n",
      "    f = open(\"classes.txt\", \"r\")\n",
      "    f = f.readlines()\n",
      "    f = [line.rstrip('\\n') for line in list(f)]\n",
      "    try:\n",
      "    \tst.subheader(\"Detected objects: \" + ', '.join(list(set([f[obj] for obj in class_IDs]))))\n",
      "    except IndexError:\n",
      "    \tst.write(\"Nothing detected\")\n",
      "\n",
      "    # Apply non-max suppression to identify best bounding box\n",
      "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, overlap_threshold)\n",
      "    xmin, xmax, ymin, ymax, labels = [], [], [], [], []\n",
      "    \n",
      "    if len(indices) > 0:\n",
      "        for i in indices.flatten():\n",
      "            x, y, w, h = boxes[i][0], boxes[i][1], boxes[i][2], boxes[i][3]\n",
      "            xmin.append(x)\n",
      "            ymin.append(y)\n",
      "            xmax.append(x+w)\n",
      "            ymax.append(y+h)\n",
      "    boxes = pd.DataFrame({\"xmin\": xmin, \"ymin\": ymin, \"xmax\": xmax, \"ymax\": ymax})\n",
      "\n",
      "    # Add a layer on top on a detected object \n",
      "\tLABEL_COLORS = [0, 255, 0]\n",
      "\timage_with_boxes = image.astype(np.float64)\n",
      "\tfor _, (xmin, ymin, xmax, ymax) in boxes.iterrows():\n",
      "\t\timage_with_boxes[int(ymin):int(ymax),int(xmin):int(xmax),:] += LABEL_COLORS\n",
      "\t\timage_with_boxes[int(ymin):int(ymax),int(xmin):int(xmax),:] /= 2\n",
      "\n",
      "\t# Display the final image\n",
      "\tst.image(image_with_boxes.astype(np.uint8), use_column_width=True)\n",
      "```\n",
      "\n",
      "Finally, let the user choose from several inout images such as car images, people, animals or a meeting, and run your pipeline on top of it.\n",
      "\n",
      "```python\n",
      "img_type = st.sidebar.selectbox(\"Select image type?\", ['Cars', 'People', 'Animals', \"Meeting\"])\n",
      "\n",
      "if img_type == 'People':\n",
      "    image_url = \"images/people.jpg\"\n",
      "elif img_type == 'Cars':\n",
      "    image_url = \"images/cars.jpg\"\n",
      "elif img_type == 'Animals':\n",
      "    image_url = \"images/animal.jpg\"\n",
      "elif img_type == 'Meeting':\n",
      "    image_url = \"images/meeting.jpg\"\n",
      "\n",
      "image = read_img(image_url)\n",
      "\n",
      "# Get the boxes for the objects detected by YOLO by running the YOLO model.\n",
      "yolo_v3(image, confidence_threshold)\n",
      "```\n",
      "\n",
      "Finally, to run your Streamlit app, supposing that you called your app \"app.py\", simply run :\n",
      "\n",
      "```bash\n",
      "streamlit run app.py\n",
      "```\n",
      "\n",
      "And you should be able to see this:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/screen_home.png)\n",
      "\n",
      "> **Conclusion** : This project is adapted from Streamlit's demo of Yolo. I tried to make the overall steps easier to understand, and executable on your own images.\n",
      "---\n",
      "title: Data Augmentation in Natural Language Processing\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Natural Language Processing\"\n",
      "---\n",
      "\n",
      "Data Augmentation is a key element in Computer Vision. We create new images and add noise in input data by rotating, zooming or flipping images. It's really helpful when we have a limited amount of data available. But can we achieve something similar with text? We'll introduce \"Easy Data Augmentation (EDA)\", a state-of-the-art paper that is both easy to understand and highly effective.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# When should we use Data Augmentation?\n",
      "\n",
      "This article relies on two sources:\n",
      "- the [original EDA paper](https://arxiv.org/abs/1901.11196)\n",
      "- the [GitHub implementation](https://github.com/jasonwei20/eda_nlp)\n",
      "\n",
      "Data Augmentation techniques in NLP show substantial improvements on datasets with less than 500 observations, as illustrated by the original paper.  \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/aug.png)\n",
      "\n",
      "Classification accuracy can increase by as much as 3% if we create 16 augmented sentences per input sentence.\n",
      "\n",
      "In this article, we'll go through the different data augmentation techniques and how to implement them by hand. The code is mostly from the EDA library, but extracting it and breaking it down is a good way to get used to those techniques. I'll also introduce the EDA package which wraps all this code into a single library.\n",
      "\n",
      "# Data Augmentation Techniques \n",
      "\n",
      "The simple data augmentation techniques are the following:\n",
      "- SR: synonym replacement\n",
      "- RD: random deletion\n",
      "- RS: random swap\n",
      "- RI: random insertion\n",
      "\n",
      "## Synonym replacement (SR)\n",
      "\n",
      "Synonym replacement is a technique in which we replace a word by one of its synonyms. We use WordNet, a large linguistic database, to identify relevant synonyms.\n",
      "\n",
      "```python\n",
      "from nltk.corpus import wordnet \n",
      "\n",
      "def get_synonyms(word):\n",
      "    \"\"\"\n",
      "    Get synonyms of a word\n",
      "    \"\"\"\n",
      "    synonyms = set()\n",
      "    \n",
      "    for syn in wordnet.synsets(word): \n",
      "        for l in syn.lemmas(): \n",
      "            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
      "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
      "            synonyms.add(synonym) \n",
      "    \n",
      "    if word in synonyms:\n",
      "        synonyms.remove(word)\n",
      "    \n",
      "    return list(synonyms)\n",
      "```\n",
      "\n",
      "This first function identifies the synonyms of a given word and pre-processes them. The synonyms are then randomly replaced in the original sentence.\n",
      "\n",
      "```python\n",
      "def synonym_replacement(words, n):\n",
      "    \n",
      "    words = words.split()\n",
      "    \n",
      "    new_words = words.copy()\n",
      "    random_word_list = list(set([word for word in words if word not in stop_words]))\n",
      "    random.shuffle(random_word_list)\n",
      "    num_replaced = 0\n",
      "    \n",
      "    for random_word in random_word_list:\n",
      "        synonyms = get_synonyms(random_word)\n",
      "        \n",
      "        if len(synonyms) >= 1:\n",
      "            synonym = random.choice(list(synonyms))\n",
      "            new_words = [synonym if word == random_word else word for word in new_words]\n",
      "            num_replaced += 1\n",
      "        \n",
      "        if num_replaced >= n: #only replace up to n words\n",
      "            break\n",
      "\n",
      "    sentence = ' '.join(new_words)\n",
      "\n",
      "    return sentence\n",
      "\n",
      "```\n",
      "\n",
      "We randomly select `n` words, and replace them by their synonyms. This function can then be used in an `apply` function on a data frame for example.\n",
      "\n",
      "To create a larger diversity of sentences, one could try to replace 1 word, then 2, then 3, and so on... \n",
      "\n",
      "## Random Deletion (RD)\n",
      "\n",
      "In Random Deletion, we randomly delete a word if a uniformly generated number between 0 and 1 is smaller than a pre-defined threshold. This allows for a random deletion of some words of the sentence.\n",
      "\n",
      "```python\n",
      "def random_deletion(words, p):\n",
      "\n",
      "    words = words.split()\n",
      "    \n",
      "    #obviously, if there's only one word, don't delete it\n",
      "    if len(words) == 1:\n",
      "        return words\n",
      "\n",
      "    #randomly delete words with probability p\n",
      "    new_words = []\n",
      "    for word in words:\n",
      "        r = random.uniform(0, 1)\n",
      "        if r > p:\n",
      "            new_words.append(word)\n",
      "\n",
      "    #if you end up deleting all words, just return a random word\n",
      "    if len(new_words) == 0:\n",
      "        rand_int = random.randint(0, len(words)-1)\n",
      "        return [words[rand_int]]\n",
      "\n",
      "    sentence = ' '.join(new_words)\n",
      "    \n",
      "    return sentence\n",
      "```\n",
      "\n",
      "## Random Swap (RS)\n",
      "\n",
      "In Random Swap, we randomly swap the order of two words in a sentence.\n",
      "\n",
      "```python\n",
      "def swap_word(new_words):\n",
      "    \n",
      "    random_idx_1 = random.randint(0, len(new_words)-1)\n",
      "    random_idx_2 = random_idx_1\n",
      "    counter = 0\n",
      "    \n",
      "    while random_idx_2 == random_idx_1:\n",
      "        random_idx_2 = random.randint(0, len(new_words)-1)\n",
      "        counter += 1\n",
      "        \n",
      "        if counter > 3:\n",
      "            return new_words\n",
      "    \n",
      "    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n",
      "    return new_words\n",
      "\n",
      "def random_swap(words, n):\n",
      "    \n",
      "    words = words.split()\n",
      "    new_words = words.copy()\n",
      "    \n",
      "    for _ in range(n):\n",
      "        new_words = swap_word(new_words)\n",
      "        \n",
      "    sentence = ' '.join(new_words)\n",
      "    \n",
      "    return sentence\n",
      "```\n",
      "\n",
      "## Random Insertion (RI)\n",
      "\n",
      "Finally, in Random Insertion, we randomly insert synonyms of a word at a random position.\n",
      "\n",
      "```python\n",
      "def random_insertion(words, n):\n",
      "    \n",
      "    words = words.split()\n",
      "    new_words = words.copy()\n",
      "    \n",
      "    for _ in range(n):\n",
      "        add_word(new_words)\n",
      "        \n",
      "    sentence = ' '.join(new_words)\n",
      "    return sentence\n",
      "\n",
      "def add_word(new_words):\n",
      "    \n",
      "    synonyms = []\n",
      "    counter = 0\n",
      "    \n",
      "    while len(synonyms) < 1:\n",
      "        random_word = new_words[random.randint(0, len(new_words)-1)]\n",
      "        synonyms = get_synonyms(random_word)\n",
      "        counter += 1\n",
      "        if counter >= 10:\n",
      "            return\n",
      "        \n",
      "    random_synonym = synonyms[0]\n",
      "    random_idx = random.randint(0, len(new_words)-1)\n",
      "    new_words.insert(random_idx, random_synonym)\n",
      "```\n",
      "\n",
      "# EDA Library\n",
      "\n",
      "EDA Library is available on GitHub. It allows you to perform data augmentation automatically using a simple command line:\n",
      "\n",
      "```python\n",
      "python code/augment.py --input=<insert input filename>\n",
      "```\n",
      "\n",
      "Otherwise, I would personally recommend implementing your own functions and see what works best for your case and control the multiplication factor between the initial number of sentences and the final one.\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "In the article, we covered the main Data Augmentation techniques in NLP. This is an active field of research, and the papers about this topic are quite recent, so it might evolve quite a lot!\n",
      "\n",
      "---\n",
      "title: Probabilistic Linear Discriminant Analysis (PLDA)\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Speech Processing\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "Probabilistic Linear Discriminant Analysis (PLDA)[http://people.irisa.fr/Guillaume.Gravier/ADM/articles2018/Probabilistic_Linear_Discriminant_Analysis.pdf] brought a major contribution to the field of both computer vision and speech processing, especially for the task of speaker identification and verification.\n",
      "\n",
      "PLDA relies on Linear Discriminant Analysis (LDA), which is a linear dimensionality reduction method. While PCA identifies the linear subspace in\n",
      "which most of the data’s energy is concentrated, LDA identifies the subspace in which the data between different classes is most spread out, relative to the spread within each class. Hence, LDA can be used for classification. \n",
      "\n",
      "I wrote a quick illustrated article on [LDA](https://maelfabien.github.io/machinelearning/LDA/) if you want to check it.\n",
      "\n",
      "Among the limitations of LDA, one can cite:\n",
      "- the fact that LDA cannot predict unseen classes (as we do in speaker verification and face recognition)\n",
      "- the fact that LDA typically relies on a PCA step, and we don't know which dominant projection directions identified by LDA is contributing to the object or voice recognition task\n",
      "\n",
      "\n",
      "TO CONTINUE\n",
      "\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/bs_1.png)\n",
      "\n",
      "\n",
      "---\n",
      "title: Install and run Elasticsearch + Kibana locally\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Elastic Search, Logstash, Kibana\"\n",
      "---\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/els.jpg)\n",
      "\n",
      "Previously, we covered the Elastic Cloud. This is a fully managed solution that may not fit all the needs. How can you run Elasticsearch locally? How can you launch Kibana? And how do you use Dev Tools? We'll answer those questions here.\n",
      "\n",
      "# I. Install Elasticsearch\n",
      "\n",
      "Download Elasticsearch through <span style=\"color:blue\">[this link](https://www.elastic.co/downloads/elasticsearch)</span>. \n",
      "\n",
      "Download the version that matches your OS. I'm running on macOS, so the article might contain some commands specific to macOS.\n",
      "\n",
      "*Step 1* : Download and unzip Elasticsearch\n",
      "\n",
      "Find the file in your downloads, move it to the folder in which you want to store it. On a mac, double-click on the file to unzip it.\n",
      "\n",
      "*Step 2 *: In your terminal, go to the folder in question\n",
      "\n",
      "```bash\n",
      "cd elasticsearch-6.6.1/\n",
      "```\n",
      "\n",
      "(Depending on your version, this link might change)\n",
      "\n",
      "*Step 3 *: Fire up the engines! \n",
      "\n",
      "We'll now start the elastic search server.\n",
      "\n",
      "```bash\n",
      "bin/elasticsearch\n",
      "```\n",
      "The `bin` directory will also be used to start Elasticsearch SQL CLI to interact with Elasticsearch using SQL statements for example. \n",
      "\n",
      "Another important directory is the `config` one in which we can, for example, modify the default port for Elasticsearch (9200) in the `elasticsearch.yaml` file.\n",
      "\n",
      "Elasticsearch is now up and running. Let's download Kibana now!\n",
      "\n",
      "# II. Install Kibana\n",
      "\n",
      "Kibana can be downloaded from the following <span style=\"color:blue\">[link](https://www.elastic.co/downloads/kibana)</span>.\n",
      "\n",
      "Download the file that matches your OS.\n",
      "\n",
      "*Step 1* : Download and unzip Kibana\n",
      "\n",
      "Find the file in your downloads, move it to the folder in which you want to store it. On a mac, double-click on the file to unzip it.\n",
      "\n",
      "*Step 2 *: Fire up Kibana\n",
      "\n",
      "In your terminal, go to the folder that contains Kibana, and run this command. \n",
      "\n",
      "```bash\n",
      "bin/kibana\n",
      "```\n",
      "\n",
      "If you have not modified the default port of Kibana, everything should now work. You should see a log similar to this one :\n",
      "```\n",
      "log   [17:49:10.844] [info][listening] Server running at http://localhost:5601\n",
      "```\n",
      "\n",
      "*Step 3 *: Open your browser\n",
      "\n",
      "Now, head to the following link in your browser : <span style=\"color:blue\">[http://localhost:5601](http://localhost:5601)</span>.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/el_1.jpg)\n",
      "\n",
      "We now have both Kibana and Elasticsearch running!\n",
      "---\n",
      "title: AutoML with h2o\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Parameters and Model Optimization\"\n",
      "---\n",
      "\n",
      "The interest in AutoML is rising over time. This graph shows the trends in Google for the AutoML search term.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/auto1.jpg)\n",
      "\n",
      "AutoML algorithms are reaching really good rankings in data science competitions (see [this article](https://towardsdatascience.com/achieving-a-top-5-position-in-an-ml-competition-with-automl-89a5a6fb8060))\n",
      "\n",
      "But *what is* AutoML ? How does it work? And mainly, how can you implement an AutoML in Python?\n",
      "\n",
      "# What is AutoML?\n",
      "\n",
      "AutoML is a framework whose role is to optimize the machine learning workflow, which includes automatic training and tuning of many models within a user-specified time-limit.\n",
      "\n",
      "The idea is to fasten the work of the Data Scientist when it comes to model selection and parameter tuning. On the other hand, the user simply inputs the training data, eventually some validation data, and a time limit.\n",
      "\n",
      "AutoML will automatically try several models, choose the best performing models, tune the parameters of the *leader* models, try to stack them...\n",
      "\n",
      "AutoML outputs a leaderboard of algorithms, and you can select the best performing algorithm given several criteria that are measured (MSE, RMSE, log loss, Auc...).\n",
      "\n",
      "# Why and when should you use AutoML?\n",
      "\n",
      "Building models and tuning the hyperparameters is a long process for any data scientist. The search space for the optimal parameters is enormous, and this is only for 1 chosen model.\n",
      "\n",
      "AutoML can be highly parallelized, so bear in mind that a couple of GPUs will help.\n",
      "\n",
      "AutoML can be used to :\n",
      "- Assess the feature importance\n",
      "- Try a lot of models and parameters as a first guess\n",
      "\n",
      "Once a model and a set of parameters have been identified, you have 2 options :\n",
      "- either the model is good enough and satisfies your criteria\n",
      "- or you can use the selected set of model + parameters as a starting point for a GridSearch or Bayesian HyperOpt\n",
      "\n",
      "# How does AutoML work?\n",
      "\n",
      "AutoML **does not** use a GIANT double for-loop to test every model and every parameter. It's much smarter than that. It uses Reinforcement Learning.\n",
      "\n",
      "A controller neural net can propose a “child” model architecture, which can then be trained and evaluated for quality on a particular task. That feedback is then used to inform the controller how to improve its proposals for the next round. \n",
      "\n",
      "Eventually, the controller learns to assign a high probability to areas of architecture space that achieve better accuracy on a held-out validation dataset, and low probability to areas of architecture space that score poorly.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/auto2.jpg)\n",
      "\n",
      "To make the controller a little more complex, it uses anchor points, and set-selection attention to form skip connections. \n",
      "\n",
      "At that point, you might think that AutoML frameworks are extremely long to run. In AutoML, each gradient update to the controller parameters θ corresponds to training one child network to convergence. \n",
      "\n",
      "You're right, training a single child network can take hours. For this reason, according to Google's Blog, AutoML uses distributed training and asynchronous parameter updates to speed up the learning process of the controller. It uses a parameter-server scheme where we have a parameter server of S shards, that store the shared parameters for K controller replicas. Each controller replica samples m different child architectures that are trained in parallel. The controller then collects gradients according to the results of that minibatch of m architectures at convergence and sends them to the parameter server to update the weights across all controller replicas.\n",
      "\n",
      "# Example in Python\n",
      "\n",
      "Several companies are currently AutoML pipelines. Among them, Google and h2o. In this example, we'll use h2o's solution. I suggest you run this in Google Colab using GPU's, but you can also run it locally.\n",
      "\n",
      "Start by importing the necessary packages :\n",
      "\n",
      "```python\n",
      "!pip install requests\n",
      "!pip install tabulate\n",
      "!pip install \"colorama>=0.3.8\"\n",
      "!pip install future\n",
      "!pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o\n",
      "```\n",
      "\n",
      "\n",
      "## The Data\n",
      "\n",
      "We'll use the Credit Card Fraud detection, a famous Kaggle dataset that can be found [here](https://www.kaggle.com/mlg-ulb/creditcardfraud).\n",
      "\n",
      "The dataset contains transactions made by credit cards in September 2013 by European cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
      "\n",
      "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, the original features are not provided. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
      "\n",
      "```python\n",
      "### General\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "```\n",
      "\n",
      "```python\n",
      "df = pd.read_csv('creditcard.csv')\n",
      "df.head()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/auto3.jpg)\n",
      "\n",
      "If you explore the data, you'll notice that only 0.17% of the transactions are fraudulent. We'll use the F1-Score metric, a harmonic mean between the precision and the recall.\n",
      "\n",
      "To understand the nature of the fraudulant transactions, simply plot the following graph :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.scatter(df[df.Class == 0].Time, df[df.Class == 0].Amount, c='green', alpha=0.4, label=\"Not Fraud\")\n",
      "plt.scatter(df[df.Class == 1].Time, df[df.Class == 1].Amount, c='red', label=\"Fraud\")\n",
      "plt.title(\"Amount of the transaction over time\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/auto4.jpg)\n",
      "\n",
      "Fraudulent transactions have a limited amount. We can guess that these transactions must remain \"unseen\" and not attracting too much attention.\n",
      "\n",
      "## h2o AutoML\n",
      "\n",
      "Now, let's import h2o AutoML :\n",
      "\n",
      "```python\n",
      "### h2o AutoML\n",
      "import h2o\n",
      "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
      "from h2o.automl import H2OAutoML\n",
      "```\n",
      "\n",
      "The, initialize the h2o session :\n",
      "\n",
      "```python\n",
      "# Initialize\n",
      "h2o.init()\n",
      "```\n",
      "\n",
      "If you're running this locally, you should see something like this :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/auto5.jpg)\n",
      "\n",
      "If you follow the local link to the instance, you can access the h2o Flow :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/auto6.jpg)\n",
      "\n",
      "I'll further explore Flow in another article, but Flow aims to do the same thing with a visual interface. In h2o, you need to import the dataset as an h2o object, and use built-in functions to split the data frame :\n",
      "\n",
      "```python\n",
      "# Load the data\n",
      "df = h2o.import_file(\"/Users/maelfabien/Desktop/LocalDB/CreditCard/creditcard.csv\")\n",
      "\n",
      "d = df.split_frame(ratios = [0.8], seed = 1234)\n",
      "df_train = d[0] # using 80% for training\n",
      "df_test = d[1] #rest 20% for testing\n",
      "```\n",
      "\n",
      "We then define a list of the columns we'll use as predictors :\n",
      "\n",
      "```\n",
      "# Predictor columns\n",
      "predictors = list(df.columns) \n",
      "predictors.remove('Time')\n",
      "predictors.remove('Class')\n",
      "```\n",
      "\n",
      "As you might have guessed, we're facing a binary classification problem here. The default case is regression in AutoML. To \"cast\" a column type to integer, use this :\n",
      "\n",
      "```\n",
      "# Cast binary\n",
      "df_train['Class'] = df_train['Class'].asfactor()\n",
      "```\n",
      "\n",
      "We are now ready to define the model and train it. We specify the maximal number of models to test, and the overall maximal runtime in seconds.\n",
      "\n",
      "```\n",
      "aml = H2OAutoML(max_models = 50, seed = 1, max_runtime_secs=21000)\n",
      "aml.train(x = predictors, y = 'Class', training_frame = df_train, validation_frame = df_test)\n",
      "```\n",
      "\n",
      "By default, the maximal runtime is 1 hour. Your model will be training for 21'000 seconds now (I left it to train overnight). Now, let's display all the models that have been tested and their performance :\n",
      "\n",
      "```python\n",
      "print(aml.leaderboard)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/auto7.jpg)\n",
      "\n",
      "The leaderboard is established using Cross Validation, which more or less guarantees that the top performing models are indeed consistently performing well. \n",
      "\n",
      "To display only the best model, use `print(aml.leader)`.\n",
      "\n",
      "We can now make a prediction using the leader model, simply using:\n",
      "\n",
      "```python\n",
      "aml.leader.predict(new_data)\n",
      "```\n",
      "\n",
      "We can then save the best model :\n",
      "\n",
      "```python\n",
      "h2o.save_model(aml.leader, path = \"./model_credit_card\")\n",
      "```\n",
      "\n",
      "Once your work is over, shut down the session : \n",
      "\n",
      "```python\n",
      "h2o.shutdown()\n",
      "```\n",
      "\n",
      "In this simple example, h2o outperformed the tuning I manually did.\n",
      "\n",
      "> **Conclusion** : I hope this article on AutoML was interesting. It's a really hot topic, and I do expect large improvements to be made over the next years in this field. \n",
      "\n",
      "Sources :\n",
      "- [How AutoML works](https://medium.com/@gangele397/how-does-automl-works-b0f9e45fbb24)\n",
      "- [Google AI Blog](https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html?m=1)\n",
      "- [H2o package exercises](http://eric.univ-lyon2.fr/~ricco/tanagra/fichiers/fr_Tanagra_Package_H2O_Python.pdf)\n",
      "---\n",
      "title: Introduction to Google Cloud Platform - Week 1 Module 1\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Road to Google Cloud Platform Certification\"\n",
      "---\n",
      "\n",
      "The following series of articles is based on the Data Engineering with Google Cloud Platform specialization on Coursera. This is a 1 months program, that required about 16h of work per week.\n",
      "\n",
      "# Introduction to GCP\n",
      "\n",
      "## History and context\n",
      "\n",
      "How were computing resources managed over time?\n",
      "- 1980s: Server on-premises. You own everything, and you manage it.\n",
      "- 2000s: Data Centers. Rent the space, but pay and manage the hardware. No direct physical access to the computers.\n",
      "- Now: First Generation Cloud with Virtualized Data Centers. You rent hardware and space, still controlling and configuring virtual machines. Pay only for what you provision.\n",
      "- Next: Managed Services. Completely elastic storage, processing, ML. Pay for what you use.\n",
      "\n",
      "Google has built one of the most powerful infrastructures on the planet. On every 5 CPUs being produced worldwide, Google buys one. Google has over 100 points of presence that use private fiber, on all continents. The idea of having so many edge locations is to allow users to access the resources without having to go across the globe, but simply get a cached version of that resource from the edge location. \n",
      "\n",
      "The main software released by Google in Data Engineering for data processing are :\n",
      "- Google File System, Colossus\n",
      "- MapReduce, Dremel, Flume\n",
      "- BigTable\n",
      "- Tensorflow\n",
      "- ...\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_1.jpg)\n",
      "\n",
      "## What is GCP?\n",
      "\n",
      "Most of the time, companies want to move from their local environment to Google Cloud Platform. Therefore, to move to the Cloud, the steps are :\n",
      "- change where you compute\n",
      "- improve scalability and reliability, scaling up\n",
      "- change how you compute, transforming your business\n",
      "\n",
      "Overall, GCP allows to :\n",
      "- spend less on ops and administration\n",
      "- incorporate real-time data into apps and architectures\n",
      "- apply machine learning broadly and easily \n",
      "- become a truly data-driven company\n",
      "\n",
      "There are four fundamental aspects of Google's core infrastructure and a top layer of products and services that you will interact with most often. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_3.jpg)\n",
      "\n",
      "# Compute Power for Analytic and ML Workloads\n",
      "\n",
      "CPUs on the Cloud are provided by a Compute Engine of Virtual Machines. Compute Engine allows high-level access to all underlying Virtual Machines. There is no need to SSH into every single machine for example. It also allows us to increase the number of CPUs on demand for example.\n",
      "\n",
      "If you select preemptible machines, you get up to 80% of discount on your machine charge, but you agree to give it up if someone comes along that's willing to pay full price for those machines. For examples, if the jobs you are doing are distributed among other workers, you don't care if the machine is re-assigned to someone else.\n",
      "\n",
      "8 Google products serve more than 1 billion users. For a simple Google Photo stabilization video service, we talk about a billion data points (several Megapixels for each image of a video). Over 1.2 billion photos are uploaded every day on Google Photo (13 PetaBytes). On YouTube, it's more than 400 hours every minute (1 PetaByte).\n",
      "\n",
      "A phone's hardware is not sophisticated enough to train an ML model of that size. Google pre-trains models in its data centers and deploys trained models on the devices. Google open sources pre-trained models as building blocks :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_4.jpg)\n",
      "\n",
      "Jeff Dean estimates that if everyone used Voice Search for 3 minutes, Google would need to double its infrastructure. Moore's Law is not catching up the rate, and we reach physical limits. One of the ways to overcome this issue is to develop specialized hardware for a given task. This is what Google is doing with Tensor Processing Units (TPUs), that are specialized for Machine Learning and Deep Learning with more memory and faster processing. For example, eBay uses TPUs for its infrastructure and allows them to speed their processes by a factor of 10.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_5.jpg)\n",
      "\n",
      "Google optimized the data center cooling energy by 40% and improved power usage effectiveness (PUE) by 15% recently. \n",
      "\n",
      "# Compute Power for Analytic and ML Workloads\n",
      "\n",
      "So far, we used storage to host one CSV file, an image, and an HTML file. But how should we store all the data generated? In the Storage Bucket too!\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_22.jpg)\n",
      "\n",
      "The data needs to be separated from the Compute Engine. There are 2 ways to create buckets :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_23.jpg)\n",
      "\n",
      "For big-data analytics, we usually choose regional storage or multi-regional storage for geo-redundancy. Nearline and Coldline are respectively used for monthly and yearly access to data. The resource access hierarchy is the following in GCP :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_24.jpg)\n",
      "\n",
      "An organization has several folders, each of which contains projects (with unique names), and each project has services attached to it. Organization and folders are however not mandatory, but useful to define rules that apply to a whole organization. \n",
      "\n",
      "To move a file to a bucket, the gs_util can be used the following way :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_25.jpg)\n",
      "\n",
      "# Build on Google's Global Network\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_26.jpg)\n",
      "\n",
      "Google has laid thousands of miles of fiber optic cable that crosses oceans with repeaters to amplify optical signals. Google's data centers around the world are interconnected by this private Google network, which by some publicly available estimates, carries as much as 40 percent of the world's internet traffic every day. \n",
      "\n",
      "Thanks to the speed of the data center networks at Google, they are now able to split computation and storage and no longer have both on the same machine. The speed reached 1 Petabyte/s of total bisection bandwidth. This basically means that you can perform computation on data located elsewhere.\n",
      "\n",
      "You need to serve out the results of your analytics and predictions, perhaps to users who are all around the world. This is where Edge points of presence come in. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_27.jpg)\n",
      "\n",
      "The network, Google's Network, interconnects with the public Internet at more than 90 internet exchanges and more than 100 points of presence worldwide. When an Internet user sends traffic to a Google resource, Google responds to the user's request from an Edge network location that will provide the lowest delay or latency. Google's Edge caching network places content close to end-users to minimize latency. Your applications in GCP, like your machine learning models, can take advantage of this Edge network too.\n",
      "\n",
      "# Security: On-premise vs Cloud-native\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_28.jpg)\n",
      "\n",
      "Thanks to its scale, Google can manage a lot of security layers that would be almost impossible to manage (at that level) for an on-premise service. The only part left to secure for the user is the data access policy, but Google provides tools such as IAM to define policies. Communications to Google are encrypted in transit and offer multiple layers of security. Stored data is automatically encrypted at rest and distributed for availability and reliability.\n",
      "\n",
      "In Big Query, data are key encrypted, and keys are themselves key-encrypted.\n",
      "\n",
      "# Google Cloud Big Data Tools\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_3.jpg)\n",
      "\n",
      "## Evolution of Google Cloud Big Data Tools\n",
      "Google invented new data processing methods as it grew :\n",
      "- Google File System to handle a large amount of data\n",
      "- MapReduce to distribute computations over clusters\n",
      "- Recording and retrieving millions of rows with BigTable\n",
      "- Dremel to automate MapReduce\n",
      "- ...\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_30.jpg)\n",
      "\n",
      "## Which service to choose?\n",
      "\n",
      "GCP offers a range of services in terms of computation, among which :\n",
      "- Compute Engine to manage services instances on your own\n",
      "- Google Kubernetes Engine (GKE) allows running containerized applications on clusters of machines.\n",
      "- App Engine is a way to run code in the Cloud without having to worry about the infrastructure\n",
      "- Cloud Functions is a serverless application of Functions as a service and executes your code in response to events (e.g a new file)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_31.jpg)\n",
      "\n",
      "Similarly, most applications need a database system. There are several options :\n",
      "- From a compute engine's VM, set up a fully custom database\n",
      "- Or use managed services such as Bigtable, Cloud Storage, Cloud SQL, Cloud Spanner or CloudDatastore\n",
      "\n",
      "Here is a list of all Big Data and ML products in GCP :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_32.jpg)\n",
      "\n",
      "# What can you do with GCP?\n",
      "\n",
      "Here is a series of use cases presented in the certification :\n",
      "- AutoML Vision to automatically recognize common elements of furnishings and architectures in real estate\n",
      "- Company email routing based on NLP services using email classification\n",
      "- Sorting bad potatoes on a food chain\n",
      "- ...\n",
      "\n",
      "Example: Architecture overview of delivery service :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_33.jpg)\n",
      "\n",
      "\n",
      "> **Conclusion **: This is the end of the Introduction To Google Cloud Platform module, part of week 1 of the GCP Specialization on Coursera.\n",
      "\n",
      "---\n",
      "title: Destabilizing Netorks\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Criminal Networks\"\n",
      "---\n",
      "\n",
      "In this article, I will summarize and discuss the paper: [\"Destabilizing Netorks\"](http://www.casos.cs.cmu.edu/publications/protected/2000-2004/2000-2002/carley_2001_destabilizingnetworks.pdf) by Kathleen M. Carley, Ju-Sung Lee and David Krackhardt.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# Background\n",
      "\n",
      "Classic network analysis tools are able to identify:\n",
      "- individuals whose removal would alter the network significantly, usuually due to a high centrality\n",
      "- individuals likely to act\n",
      "- individuals able to propagate information rapidly\n",
      "- individuals with more power\n",
      "- individuals providing redundancy in the network\n",
      "\n",
      "Tools are also able to identify patterns:\n",
      "- basic structure\n",
      "- central tendency\n",
      "- coherency of the network\n",
      "- significantly different sub-networks\n",
      "- ...\n",
      "\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/var.png)\n",
      "\n",
      "# Discussion\n",
      "\n",
      "---\n",
      "title: Autograde, a grading tool for teachers\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "Many teachers are working from home during COVID-19 crisis, receiving and grading works online. My mother is in this situation. To help her, I built AutoGrade, an application that makes grading faster, by highlighting words of interest in the text of a student.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/autograde.png)\n",
      "\n",
      "The app can be found [here](https://autograde.onrender.com/).\n",
      "\n",
      "Users of the app simply type words to look for on the left sidebar. Then, they paste the text to correct. The app will automatically highlight words sharing a similar root (hence, \"stats\" and \"statistically\" are recognized as the same word). It also highlights in another color words that are close but don't share the same root. Reading is therefore faster.\n",
      "\n",
      "It's a simple tool, built with Streamlit #Python, and #NLTK. Nothing hard, but if it can help other teachers in their awesome work during these COVID-times, I'd be really happy. If ever you are interested in the tool, want specific features (e.g detect words with similar meaning), another language support (French is ready), just contact me.\n",
      "\n",
      "The Github repository can also be found here:\n",
      "\n",
      "<div class=\"github-card\" data-github=\"maelfabien/AutoGrade\" data-width=\"100%\" data-height=\"\" data-theme=\"default\"></div>\n",
      "<script src=\"//cdn.jsdelivr.net/github-cards/latest/widget.js\"></script>\n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "title: Explore a dataset on Google BigQuery\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "Storing and querying massive datasets can be time-consuming and expensive without the right hardware and infrastructure. Google BigQuery is an enterprise data warehouse that solves this problem by enabling super-fast SQL queries using the processing power of Google's infrastructure.\n",
      "\n",
      "The aim of this lab is to explore public data using Big Query, create queries and upload our own data.\n",
      "\n",
      "# Public Data\n",
      "\n",
      "From the GCP console, start by clicking on Big Query in the side menu : \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_34.jpg)\n",
      "\n",
      "Then, select \"Explore public dataset\" :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_35.jpg)\n",
      "\n",
      "Type \"USA Names\" in the search bar, and select the following dataset :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_36.jpg)\n",
      "\n",
      "Click on \"View Dataset\" :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_37.jpg)\n",
      "\n",
      "The dataset will now appear in your side menu :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_38.jpg)\n",
      "\n",
      "We can use the query editor to write SQL queries in Big Query :\n",
      "\n",
      "```\n",
      "SELECT name, gender,\n",
      "SUM(number) AS total\n",
      "FROM `bigquery-public-data.usa_names.usa_1910_2013`\n",
      "GROUP BY name, gender\n",
      "ORDER BY total DESC\n",
      "LIMIT 10\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_39.jpg)\n",
      "\n",
      "The result table is displayed in the following way :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_40.jpg)\n",
      "\n",
      "# Your own data\n",
      "\n",
      "Alright, we can build the same approach using our own datasets. Download the baby names dataset from the following link : [http://www.ssa.gov/OACT/babynames/names.zip](http://www.ssa.gov/OACT/babynames/names.zip). Put the files on your desktop (or wherever you want). Open a file and observe the structure. \n",
      "\n",
      "We will now create a dataset in BigQuery. In the resources tab, click on your project's name. Then, click on \"Create Dataset\" in the central page.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_41.jpg)\n",
      "\n",
      "Give your dataset a name, and a region.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_42.jpg)\n",
      "\n",
      "Then, click on \"Create Table\" :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_43.jpg)\n",
      "\n",
      "Give your table a name, select the file (for example the year 2014) and the file format (CSV).\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_44.jpg)\n",
      "\n",
      "The upload should appear in your Job History on the side menu. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_45.jpg)\n",
      "\n",
      "Once ready, click on the table's name from the resources menu, and preview the columns.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_46.jpg)\n",
      "\n",
      "We can now create a query to retrieve the 5 most famous males names :\n",
      "\n",
      "```\n",
      "SELECT name, count\n",
      "FROM `babynames.names_2014`\n",
      "WHERE gender = 'M'\n",
      "ORDER BY count DESC LIMIT 5\n",
      "```\n",
      "\n",
      "If you click on run, you should see the following result!\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_47.jpg)\n",
      "s\n",
      "\n",
      "---\n",
      "title: Sound Visualization\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Signal Processing\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "In this article, we'll explore visualization techniques for signal which allow us to derive some additional insights from the data.\n",
      "\n",
      "# Spectrogram \n",
      "\n",
      "Spectrograms offer a powerful representation of the data. It plots over the time, for a given range of frequencies, the power (dB) of a signal. This allows us to spot periodic patterns over time, and regions of activity.\n",
      "\n",
      "Spectrograms are used in state-of-the-art sound classification algorithms to turn signals into images and apply CNNs on top on those images.\n",
      "\n",
      "There are several types of spectrograms to plot.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/spectro.png)\n",
      "\n",
      "## Linear-frequency power spectrogram\n",
      "\n",
      "A linear-frequency power spectrogram represents the time on the x-axis, the frequency in Hz on a linear scale on the y-axis, and the power in dB.\n",
      "\n",
      "```python\n",
      "import librosa\n",
      "\n",
      "y, sr = librosa.load(filename)\n",
      "D = librosa.amplitude_to_db(librosa.stft(y), ref=np.max)\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "librosa.display.specshow(D, y_axis='linear')\n",
      "plt.colorbar(format='%+2.0f dB')\n",
      "plt.title('Linear-frequency power spectrogram')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/spectro_1.png)\n",
      "\n",
      "## Log-frequency power spectrogram\n",
      "\n",
      "This spectrogram presents the same information except for a logarithmic scale on the y-axis for the frequencies. Sometimes, as in our case, it's a better scale if most of the information is located on lower frequencies and some noise are at high frequencies.\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "librosa.display.specshow(D, y_axis='log')\n",
      "plt.colorbar(format='%+2.0f dB')\n",
      "plt.title('Log-frequency power spectrogram')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/spectro_2.png)\n",
      "\n",
      "## Constant-Q power spectrogram\n",
      "\n",
      "\n",
      "Unlike the Fourier transform, but similar to the mel scale, the constant-Q transform uses a logarithmically spaced frequency axis.\n",
      "\n",
      "```python\n",
      "CQT = librosa.amplitude_to_db(librosa.cqt(y, sr=sr), ref=np.max)\n",
      "plt.figure(figsize=(12,8))\n",
      "librosa.display.specshow(CQT, x_axis='time', y_axis='cqt_hz')\n",
      "plt.colorbar(format='%+2.0f dB')\n",
      "plt.title('Constant-Q power spectrogram (Hz)')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/spectro_4.png)\n",
      "\n",
      "# Chromagram\n",
      "\n",
      "Chromagram display the intensity of each pitch $$ C, C♯, D, D♯, E , F, F♯, G, G♯, A, A♯, B $$ for each time interval. One main property of chroma features is that they capture harmonic and melodic characteristics of music, while being robust to changes in timbre and instrumentation.\n",
      "\n",
      "```python\n",
      "C = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
      "plt.figure(figsize=(12,8))\n",
      "librosa.display.specshow(C, x_axis='time', y_axis='chroma')\n",
      "plt.colorbar()\n",
      "plt.title('Chromagram')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/spectro_3.png)\n",
      "\n",
      "# Tempogram\n",
      "\n",
      "The tempo, measured in Beats Per Minute (BPM) measures the rate of the musical beat. The tempogram is a feature matrix which indicates the prevalence of certain tempi at each moment in time.\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "Tgram = librosa.feature.tempogram(y=y, sr=sr)\n",
      "librosa.display.specshow(Tgram, x_axis='time', y_axis='tempo')\n",
      "plt.colorbar()\n",
      "plt.title('Tempogram')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/spectro_5.png)\n",
      "\n",
      "# Spectrum\n",
      "\n",
      "The spectrum of a discrete signal is computed using the fast Fourier transform (FFT) and displays the mangitude (or the energy) at each frequence within a signal.\n",
      "\n",
      "```python\n",
      "import scipy\n",
      "\n",
      "X = scipy.fft(y)\n",
      "f = np.linspace(0, sr, len(X))\n",
      "plt.figure(figsize=(12, 8))\n",
      "plt.plot(f, X) \n",
      "plt.xlabel('Frequency (Hz)')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/spectro_6.png)\n",
      "\n",
      "# Power Spectral Density\n",
      "\n",
      "The power spectrum of a signal describes the distribution of power into frequency components composing that signal.\n",
      "\n",
      "```python\n",
      "freqs, psd = signal.welch(y)\n",
      "\n",
      "plt.figure(figsize=(12, 8))\n",
      "plt.semilogx(freqs, psd)\n",
      "plt.title('Power spectral density')\n",
      "plt.xlabel('Frequency')\n",
      "plt.ylabel('Power')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/spectro_7.png)\n",
      "\n",
      "> **Conclusion** : I hope that you enjoyed this article. These type of plots are nowadays used as images to classify sounds by CNNs.\n",
      "---\n",
      "title: Improved Few-Shot Text classification\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Natural Language Processing\"\n",
      "---\n",
      "\n",
      "In the [previous article](https://maelfabien.github.io/machinelearning/NLP_5/), we replicated the paper \"Few-Shot Text Classification with Pre-Trained Word Embeddings and a Human in the Loop\" by Katherine Bailey and Sunny Chopra Acquia. This article addresses the problem of few-shot text classification using distance metrics and pre-trainened embeddings. We saw that a K-NN classifier could outperform the cosine similarity classifier if the number of classes increases. \n",
      "\n",
      "We saw that the number of samples could have a large impact on the classification accuracy (up to 30% for the same class), and therefore, gaining new samples is essential.\n",
      "\n",
      "In this article, I am going to present two extensions to the previous approach :\n",
      "- a better classifier\n",
      "- data augmentation\n",
      "\n",
      "Data augmentation is really popular in Computer Vision when we don't have enough samples. The techniques are simple (crop, zoom, rotate...) but really efficient. I will write an article on this topic soon.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# Recall\n",
      "\n",
      "## Cosine Similarity\n",
      "\n",
      "Recall that the fact that there is a \"Human in the loop\" simply refers to the fact that we have a potentially large corpus of unlabeled data and require the user to label a few examples of each class.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_fs_1.png)\n",
      "\n",
      "Then, using a pre-trained Word Embedding model (Word2Vec, Glove..), we compute the average embedding of each email / short text in the training examples :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_fs_2.png)\n",
      "\n",
      "At this point, we compute the avereage embedding for each class :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_fs_3.png)\n",
      "\n",
      "This average embedding per class can be seen as a centroid in a high dimensional space. From that point, when a new observation comes in, we simply have to check how far it is from both centroids, and take the closest. The distance metric used in the paper is the cosine distance :\n",
      "\n",
      "$$ similarity = cos(\\theta) = \\frac {  A \\dot B } { \\mid \\mid A  \\mid \\mid  \\mid \\mid B  \\mid \\mid } $$\n",
      "\n",
      "Here is the process when a new sentence to classify comes in :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_fs_4.png)\n",
      "\n",
      "## K-NN\n",
      "\n",
      "We also explored a K-NN classifier on the pre-trained embeddings. Let's suppose that the embeding dimension is only 2 (or that we apply a PCA with 2 components) to represent this problem graphically. The classification task with the KNN is the following :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_fs_6.png)\n",
      "\n",
      "Compared to the previous article, you just need to update your imports :\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from random import seed\n",
      "from random import sample\n",
      "import random\n",
      "from random import shuffle\n",
      "import re\n",
      "\n",
      "seed(42)\n",
      "np.random.seed(42)\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import gensim.downloader as api\n",
      "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.metrics import accuracy_score\n",
      "from scipy import spatial\n",
      "\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from xgboost import XGBClassifier\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.corpus import wordnet \n",
      "```\n",
      "\n",
      "And import pre-trained Word2Vec :\n",
      "\n",
      "```python\n",
      "model2 = api.load('word2vec-google-news-300')\n",
      "```\n",
      "\n",
      "All the other steps (import the file and clean it) are similar, so I won't replicate them here.\n",
      "\n",
      "# Data Augmentation\n",
      "\n",
      "The implementation of the data augmentation is inspired by [Easy Data Augmentation (EDA) package](https://github.com/jasonwei20/eda_nlp/blob/master/code/eda.py), introduced after [this paper](https://arxiv.org/pdf/1901.11196.pdf).\n",
      "\n",
      "Data augmentation in NLP will be further explored in a dedicated article, but I'll make a quick summary of the techniques :\n",
      "- replace words : randomly replace words by a synonym\n",
      "- random deletion : deletes word of a sentence with a random probability\n",
      "- random swap : randomly swap words within a sentence\n",
      "- random insertion : randomly insert a synonym in the sentence\n",
      "\n",
      "All these techniques simply reply on the idea that we need to create a bit of \"noise\" in the input data and therefore artificially increase the number of samples we consider.  And as we previously saw, this might have a large impact on the final accuracy.\n",
      "\n",
      "A major question we should now answer is : how many augmented sentences should we create with 1 input sentence? Luckily, the authors of the paper \"EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks\" provide a summary of the performance gain :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_fs_9.png)\n",
      "\n",
      "The axis $$ n_{aug} $$ presents the number of augmented sentences created from a single sentence. Although the authors have not implemented EDA for few-shot learning, with really few training examples (as we have), it seems like 10 to 16 new sentences per input sentence is a good ratio. This should allow us to move from a 5-shot learning to a 50 or even more.\n",
      "\n",
      "Let's now implement those data augmentation techniques use the EDA package as a basis: \n",
      "\n",
      "## Replace words\n",
      "\n",
      "We first need to define a function that finds the synonyms of a word. This relies on `synsets`, a lexical dictionary that groups synonyms.\n",
      "\n",
      "```python\n",
      "def get_synonyms(word):\n",
      "    \n",
      "    synonyms = set()\n",
      "    \n",
      "    for syn in wordnet.synsets(word): \n",
      "        for l in syn.lemmas(): \n",
      "            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
      "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
      "            synonyms.add(synonym) \n",
      "    \n",
      "    if word in synonyms:\n",
      "        synonyms.remove(word)\n",
      "    \n",
      "    return list(synonyms)\n",
      "```\n",
      "\n",
      "Then, create a function to randomly replace a word by one of its synonyms:\n",
      "\n",
      "```python\n",
      "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', \n",
      "'ours', 'ourselves', 'you', 'your', 'yours', \n",
      "'yourself', 'yourselves', 'he', 'him', 'his', \n",
      "'himself', 'she', 'her', 'hers', 'herself', \n",
      "'it', 'its', 'itself', 'they', 'them', 'their', \n",
      "'theirs', 'themselves', 'what', 'which', 'who', \n",
      "'whom', 'this', 'that', 'these', 'those', 'am', \n",
      "'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
      "'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
      "'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
      "'because', 'as', 'until', 'while', 'of', 'at', \n",
      "'by', 'for', 'with', 'about', 'against', 'between',\n",
      "'into', 'through', 'during', 'before', 'after', \n",
      "'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
      "'out', 'on', 'off', 'over', 'under', 'again', \n",
      "'further', 'then', 'once', 'here', 'there', 'when', \n",
      "'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
      "'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
      "'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', \n",
      "'very', 's', 't', 'can', 'will', 'just', 'don', \n",
      "'should', 'now', '']\n",
      "\n",
      "def synonym_replacement(words, n):\n",
      "    \n",
      "    words = words.split()\n",
      "    \n",
      "    new_words = words.copy()\n",
      "    random_word_list = list(set([word for word in words if word not in stop_words]))\n",
      "    random.shuffle(random_word_list)\n",
      "    num_replaced = 0\n",
      "    \n",
      "    for random_word in random_word_list:\n",
      "        synonyms = get_synonyms(random_word)\n",
      "        \n",
      "        if len(synonyms) >= 1:\n",
      "            synonym = random.choice(list(synonyms))\n",
      "            new_words = [synonym if word == random_word else word for word in new_words]\n",
      "            num_replaced += 1\n",
      "        \n",
      "        if num_replaced >= n: #only replace up to n words\n",
      "            break\n",
      "\n",
      "    sentence = ' '.join(new_words)\n",
      "\n",
      "    return sentence\n",
      "```\n",
      "\n",
      "Finally, apply this function recursively to the existing dataset (append new rows below) :\n",
      "\n",
      "```python\n",
      "def iterative_replace(df):\n",
      "    \n",
      "    df = df.reset_index().drop(['index'], axis=1)\n",
      "    index_row = df.index\n",
      "    df_2 = pd.DataFrame()\n",
      "    \n",
      "    for row in index_row:\n",
      "        for k in range(1,6):\n",
      "            df_2 = df_2.append({'Text':synonym_replacement(df.loc[row]['Text'], k), 'Label':df.loc[row]['Label']}, ignore_index=True)\n",
      "    return df_2\n",
      " ```\n",
      "\n",
      "## Delete words\n",
      "\n",
      "Let us now build a function to randomly delete words if the value drawn from a uniform distribution is smaller than a threshold :\n",
      "\n",
      "```python\n",
      "def random_deletion(words, p):\n",
      "\n",
      "    words = words.split()\n",
      "    \n",
      "    #obviously, if there's only one word, don't delete it\n",
      "    if len(words) == 1:\n",
      "        return words\n",
      "\n",
      "    #randomly delete words with probability p\n",
      "    new_words = []\n",
      "    for word in words:\n",
      "        r = random.uniform(0, 1)\n",
      "        if r > p:\n",
      "            new_words.append(word)\n",
      "\n",
      "    #if you end up deleting all words, just return a random word\n",
      "    if len(new_words) == 0:\n",
      "        rand_int = random.randint(0, len(words)-1)\n",
      "        return [words[rand_int]]\n",
      "\n",
      "    sentence = ' '.join(new_words)\n",
      "    \n",
      "    return sentence\n",
      "```\n",
      "\n",
      "And apply it iteratively :\n",
      "\n",
      "```python\n",
      "def iterative_delete(df):\n",
      "    \n",
      "    df = df.reset_index().drop(['index'], axis=1)\n",
      "    index_row = df.index\n",
      "    df_2 = pd.DataFrame()\n",
      "    \n",
      "    for row in index_row:\n",
      "        df_2 = df_2.append({'Text':random_deletion(df.loc[row]['Text'], 0.25), 'Label':df.loc[row]['Label']}, ignore_index=True)\n",
      "    return df_2\n",
      "```\n",
      "\n",
      "## Random swap\n",
      "\n",
      "For the random swap, we randomly swap two words in the sentence:\n",
      "\n",
      "```python\n",
      "def random_swap(words, n):\n",
      "    \n",
      "    words = words.split()\n",
      "    new_words = words.copy()\n",
      "    \n",
      "    for _ in range(n):\n",
      "        new_words = swap_word(new_words)\n",
      "        \n",
      "    sentence = ' '.join(new_words)\n",
      "    \n",
      "    return sentence\n",
      "\n",
      "def swap_word(new_words):\n",
      "    \n",
      "    random_idx_1 = random.randint(0, len(new_words)-1)\n",
      "    random_idx_2 = random_idx_1\n",
      "    counter = 0\n",
      "    \n",
      "    while random_idx_2 == random_idx_1:\n",
      "        random_idx_2 = random.randint(0, len(new_words)-1)\n",
      "        counter += 1\n",
      "        \n",
      "        if counter > 3:\n",
      "            return new_words\n",
      "    \n",
      "    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n",
      "    return new_words\n",
      "```\n",
      "\n",
      "And apply it recursively:\n",
      "\n",
      "```python\n",
      "def iterative_swap(df):\n",
      "    \n",
      "    df = df.reset_index().drop(['index'], axis=1)\n",
      "    index_row = df.index\n",
      "    df_2 = pd.DataFrame()\n",
      "    for row in index_row:\n",
      "        df_2 = df_2.append({'Text':random_swap(df.loc[row]['Text'], 2), 'Label':df.loc[row]['Label']}, ignore_index=True)\n",
      "    return df_2\n",
      "```\n",
      "\n",
      "## Random insertion\n",
      "\n",
      "Finally, regarding the random insertion of synonyms :\n",
      "\n",
      "```python\n",
      "def random_insertion(words, n):\n",
      "    \n",
      "    words = words.split()\n",
      "    new_words = words.copy()\n",
      "    \n",
      "    for _ in range(n):\n",
      "        add_word(new_words)\n",
      "        \n",
      "    sentence = ' '.join(new_words)\n",
      "    return sentence\n",
      "\n",
      "def add_word(new_words):\n",
      "    \n",
      "    synonyms = []\n",
      "    counter = 0\n",
      "    \n",
      "    while len(synonyms) < 1:\n",
      "        random_word = new_words[random.randint(0, len(new_words)-1)]\n",
      "        synonyms = get_synonyms(random_word)\n",
      "        counter += 1\n",
      "        if counter >= 10:\n",
      "            return\n",
      "        \n",
      "    random_synonym = synonyms[0]\n",
      "    random_idx = random.randint(0, len(new_words)-1)\n",
      "    new_words.insert(random_idx, random_synonym)\n",
      "```\n",
      "\n",
      "And apply it recursively:\n",
      "\n",
      "```python\n",
      "def iterative_insert(df):\n",
      "    \n",
      "    df = df.reset_index().drop(['index'], axis=1)\n",
      "    index_row = df.index\n",
      "    df_2 = pd.DataFrame()\n",
      "    \n",
      "    for row in index_row:\n",
      "        df_2 = df_2.append({'Text':random_insertion(df.loc[row]['Text'], 2), 'Label':df.loc[row]['Label']}, ignore_index=True)\n",
      "        \n",
      "    return df_2\n",
      "```\n",
      "\n",
      "## Data Augmentation\n",
      "\n",
      "Now that the functions are defined, we apply them :\n",
      "\n",
      "```python\n",
      "df_replace = iterative_replace(train)\n",
      "df_delete = iterative_delete(train)\n",
      "df_swap = iterative_swap(train)\n",
      "df_insert = iterative_insert(train)\n",
      "\n",
      "train = pd.concat([train, df_replace, df_delete, df_swap, df_insert], axis=0).reset_index().drop(['index'], axis=1)\n",
      "```\n",
      "\n",
      "The `train` dataframe used to have shape `(8, 2)`. It now has a shape `(72, 2)`. The data augmentation ratio is 9 here.\n",
      "\n",
      "# Model Performance \n",
      "\n",
      "Note: The functions `return_score` and `return_score_knn` are the same as in the first article.\n",
      "\n",
      "It is now time to evaluate the performance of the models we previously built (Cosine-based and K-NN).\n",
      "\n",
      "## Cosine Similarity\n",
      "\n",
      "We can build the prediction :\n",
      "\n",
      "```python\n",
      "all_accuracy_aug = {2:[],3:[],4:[],5:[]}\n",
      "for num_samples in range(1,50):\n",
      "    for num_cl in range(2, 6):\n",
      "        all_accuracy_aug[num_cl].append(return_score(num_samples,num_cl))\n",
      "```\n",
      "\n",
      "We can plot the accuracy over the number of classes and training samples:\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(all_accuracy_aug[2], label=\"2 classes\")\n",
      "plt.plot(all_accuracy_aug[3], label=\"3 classes\")\n",
      "plt.plot(all_accuracy_aug[4], label=\"4 classes\")\n",
      "plt.plot(all_accuracy_aug[5], label=\"5 classes\")\n",
      "plt.title(\"Accuracy depending on the number of samples and classes\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_fs_10.png)\n",
      "\n",
      "The accuracy does not seem to improve significantly. We can now do the same for the K-NN approach.\n",
      "\n",
      "## K-NN approach\n",
      "\n",
      "First, compute the accuracy for each number of sample and number of classes:\n",
      "```python\n",
      "all_accuracy_knn_aug = {2:[],3:[],4:[],5:[]}\n",
      "for num_samples in range(1,50):\n",
      "    for num_cl in range(2, 6):\n",
      "        all_accuracy_knn_aug[num_cl].append(return_score_knn(num_samples,num_cl))\n",
      "```\n",
      "\n",
      "Then, plot the accuracy:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_fs_11.png)\n",
      "\n",
      "The accuracy remains quite high, but seems to suffer from a large volatility depending on the number of samples we take. A performance summary is presented here:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_fs_12.png)\n",
      "\n",
      "Since the volatility seems to be one of the main issues here, could more powerful models help leverage the data augmentation?\n",
      "\n",
      "# Better Model\n",
      "\n",
      "In terms of model, I chose to explore both XGBoost and a Random Forest Classifier. The Random Forest seemed to outperform the XGBoost with a lower training time, and I will present the approach for this model only (although it can be easily replicated for all models).\n",
      "\n",
      "Start by re-defining a `return_score` function in which you change the classifier to Random Forest :\n",
      "\n",
      "```python\n",
      "def return_score_rf(sample_size, num_classes):\n",
      "    \n",
      "    train, test = gen_sample(sample_size, num_classes)\n",
      "    \n",
      "    X_train = train['Text']\n",
      "    y_train = train['Label'].values\n",
      "    X_test = test['Text']\n",
      "    y_test = test['Label'].values\n",
      "\n",
      "    X_train_mean = X_train.apply(lambda x : transform_sentence(x, model2))\n",
      "    X_test_mean = X_test.apply(lambda x : transform_sentence(x, model2))\n",
      "\n",
      "    X_train_mean = pd.DataFrame(X_train_mean)['Text'].apply(pd.Series)\n",
      "    X_test_mean = pd.DataFrame(X_test_mean)['Text'].apply(pd.Series)\n",
      "        \n",
      "    clf = RandomForestClassifier(n_estimators=150)\n",
      "    clf.fit(X_train_mean, y_train)\n",
      "    \n",
      "    y_pred = clf.predict(X_test_mean)\n",
      "    \n",
      "    return accuracy_score(y_pred, y_test)\n",
      "```\n",
      "\n",
      "Then, compute the accuracy on the test set for all class and train set samples:\n",
      "\n",
      "```python\n",
      "all_accuracy_rf_aug = {2:[],3:[],4:[],5:[]}\n",
      "for num_samples in range(1,50):\n",
      "    for num_cl in range(2, 6):\n",
      "        all_accuracy_rf_aug[num_cl].append(return_score_rf(num_samples,num_cl))\n",
      "```\n",
      "\n",
      "Finally, plot the results :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(all_accuracy_rf_aug[2], label=\"2 classes\")\n",
      "plt.plot(all_accuracy_rf_aug[3], label=\"3 classes\")\n",
      "plt.plot(all_accuracy_rf_aug[4], label=\"4 classes\")\n",
      "plt.plot(all_accuracy_rf_aug[5], label=\"5 classes\")\n",
      "plt.title(\"Accuracy depending on the number of samples and classes\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_fs_13.png)\n",
      "\n",
      "The Random Forest seems to be largely outperforming the K-NN Classifier here:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_fs_14.png)\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "Overall, throughout this 2-parts article, we saw different approaches that rely on trained models. The cosine similarity approach is still a good choice for binary classification with really few examples (less than 3). However, with only 8 training samples per class, we are able to outperform the 5-class performance of the cosine model by more than 40% thanks to the Random Forest Classifier. The data augmentation allows us to apply models that are a bit more complex by multiplying the number of training samples by a factor of 9 here.\n",
      "\n",
      "The final accuracy is quite impressive, close to 80% for 3, 4 or 5 classes, and more than 95% for 2 classes when we have only 8 training samples per class.\n",
      "\n",
      "The pre-trained model (Word2Vec) was moreover trained on a general corpus, whereas our classification task was focused on Stackoverflow questions. We could also leverage the fact that some supervised learning problem would allow us to pre-train a model on a corpus that is closer to our task here.\n",
      "\n",
      "I hope you found this article interesting. Please leave a comment if you have any question, or if you implement similar solutions and would like to discuss your results. \n",
      "\n",
      "---\n",
      "title: Data structures\n",
      "layout: post\n",
      "tags: [da_tuto]\n",
      "subtitle : \"Data Analysis Basics\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "Sometimes, we want to manipulate a list of integers, or a list of strings, or even a list of variables. These lists are called data structures, or collections. There are several collection types which can help :\n",
      "\n",
      "# Data structures\n",
      "\n",
      "## List\n",
      "\n",
      "A list contains a list of items of any kind, any size. It is mutable and ordered. Think about it as a way to save the grades of several students at a test for example.\n",
      "\n",
      "```python \n",
      "grades = [13, 15, 11, 9, 20]\n",
      "```\n",
      "\n",
      "A list can hold any kind of item, for example:\n",
      "\n",
      "```python \n",
      "a = 3\n",
      "b = \"Hello\"\n",
      "var_a = [a, b]\n",
      "```\n",
      "\n",
      "A list can also contain other lists:\n",
      "\n",
      "```python \n",
      "var_a = [[2,0], [1,1]]\n",
      "```\n",
      "\n",
      "When you create a collection, you might want to retrieve the value of a certain element, e.g the grade of a certain student. This is called indexing. To index a list, simply put into brackets the index of the item:\n",
      "\n",
      "```python\n",
      "grades = [13, 15, 11, 9, 20]\n",
      "grades[0]\n",
      "```\n",
      "\n",
      "```\n",
      "13\n",
      "```\n",
      "\n",
      "In Python, the indexing starts at 0, which means that to retrieve the first value, you actually retrieve the one at position 0. We can then retrieve the second element using index 1.\n",
      "\n",
      "```python\n",
      "grades[1]\n",
      "```\n",
      "\n",
      "```\n",
      "15\n",
      "```\n",
      "\n",
      "To retrieve values between indexes 1 (included) and 3 (excluded), you can simply type:\n",
      "\n",
      "```python\n",
      "grades[1:3]\n",
      "```\n",
      "\n",
      "```\n",
      "[15, 11]\n",
      "```\n",
      "\n",
      "The output is a sub-list that contains only the values we are interested in. You can retrieve the elements from the 2nd position (included) until the end using:\n",
      "\n",
      "```python\n",
      "grades[1:]\n",
      "```\n",
      "\n",
      "```\n",
      "[15, 11, 9, 20]\n",
      "```\n",
      "\n",
      "All you need to do is to not specify an end index. To take the elements from the beginning until the 3rd element (excluded):\n",
      "\n",
      "```python\n",
      "grades[:2]\n",
      "```\n",
      "\n",
      "```\n",
      "[13, 15]\n",
      "```\n",
      "\n",
      "You can also count backwards to start from the end. Simply use a minus, and start at minus 1 as the last element, and move backwards.\n",
      "\n",
      "```python\n",
      "grades[-1]\n",
      "```\n",
      "\n",
      "```\n",
      "20\n",
      "```\n",
      "\n",
      "```python\n",
      "grades[-2]\n",
      "```\n",
      "\n",
      "```\n",
      "9\n",
      "```\n",
      "\n",
      "You can also use the indexing until the end this way:\n",
      "\n",
      "```python\n",
      "grades[-2:]\n",
      "```\n",
      "\n",
      "```\n",
      "[9, 20]\n",
      "```\n",
      "\n",
      "You can replace an element of a list by indexing:\n",
      "\n",
      "```python\n",
      "grades[0] = 14\n",
      "```\n",
      "\n",
      "```\n",
      "[13, 15, 11, 9, 20]\n",
      "```\n",
      "\n",
      "To add a new element, we say that we append a new element, and use the corresponding keyword:\n",
      "\n",
      "```python\n",
      "grades.append(15)\n",
      "```\n",
      "\n",
      "If you want to specify the position of the element to insert, you can also use the word `insert` :\n",
      "\n",
      "```python\n",
      "grades.insert(2, 11)\n",
      "```\n",
      "\n",
      "You don't need to do `list = list.append...`, this operation modifies the original list by itself.\n",
      "\n",
      "A list has a length, and you can use the keyword `len` in Python to count the number of elements.\n",
      "\n",
      "```python\n",
      "len(grades)\n",
      "```\n",
      "\n",
      "```\n",
      "5\n",
      "```\n",
      "\n",
      "To get the maximum of a list, simply use the `max` keyword:\n",
      "\n",
      "```python\n",
      "max(grades)\n",
      "```\n",
      "\n",
      "```\n",
      "20\n",
      "```\n",
      "\n",
      "Same goes for the minimum with `min`. You can also compute the sum with the keyword `sum`. There are few keywords to remember in Python for data analytics, but don't worry, we've already covered a good part of them.\n",
      "\n",
      "To compute the average of a list, you can divide the sum by the number of components:\n",
      "\n",
      "```python\n",
      "sum(grades) / len(grades)\n",
      "```\n",
      "\n",
      "```\n",
      "13.6\n",
      "```\n",
      "\n",
      "## Tuple\n",
      "\n",
      "A tuple is a list that cannot be transformed (immutable).\n",
      "\n",
      "```python \n",
      "var_a = (1,3,4,5)\n",
      "```\n",
      "\n",
      "You cannot add element to a tuple or modify it without creating a second one or transforming it into a list.\n",
      "\n",
      "## Set\n",
      "\n",
      "A set is an unordered list made of unique elements only. If you add several duplicates, the set will only keep each value once.\n",
      "\n",
      "```python\n",
      "var_a = set([1, 2, 3, 4])\n",
      "```\n",
      "\n",
      "## Dictionary\n",
      "\n",
      "A dictionnary has a key and a value for each item. You can use it to store the id of a customer and the value attached to that id for example.\n",
      "\n",
      "```python \n",
      "dict_a = {'id_1':13, 'id_2': 15}\n",
      "```\n",
      "\n",
      "A dictionary can also contain new dictionaries as values. For example:\n",
      "\n",
      "```python \n",
      "dict_b = {'id_1':{'address':'London', 'paid':180}, 'id_2':{'address':'Paris', 'paid':220}}\n",
      "```\n",
      "\n",
      "You can retrieve the value in `dict_a` for the key `id_1` by simply using square brackets:\n",
      "\n",
      "```python\n",
      "dict_a['id_1']\n",
      "```\n",
      "\n",
      "```\n",
      "13\n",
      "```\n",
      "\n",
      "You can add a new element to the dictionary by referencing a new key:\n",
      "\n",
      "```python\n",
      "dict_a['id_3'] = 12\n",
      "```\n",
      "\n",
      "This is it for the data structures, you'll manipulate them a lot in the next sections.\n",
      "\n",
      "> If you found the article useful or see ways in which it could be improved, please leave a comment :)\n",
      "---\n",
      "title: Lab - Create a Cloud DataProc Cluster\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Road to Google Cloud Platform Certification\"\n",
      "---\n",
      "# Create a Cloud DataProc Cluster\n",
      "\n",
      "To create a DataProc cluster, from the GCP Console, go to DataProc :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_158.jpg)\n",
      "\n",
      "If needed, click on Enable API. Then, click on Create Cluster :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_159.jpg)\n",
      "\n",
      "Select the name of the cluster, and the zone. The zone corresponds to where you want the computations to be done. A usual rule is to set the storage and the computation in the same region. It's usually faster and less expensive this way.\n",
      "\n",
      "Then, regarding the Master, you'll have to choose between :\n",
      "- Single node for experimentation\n",
      "- Standard with 1 Master only\n",
      "- Highly Available with 3 Masters (for long jobs)\n",
      "\n",
      "HDFS storage is available for storage, but choosing Cloud Storage is often easier.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_160.jpg)\n",
      "\n",
      "The default replication factor in HDFS on DataProc is 2. You can also install custom softwares on the master and the workers.\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "google4551492_student@training-vm:~$ cd ~\n",
      "google4551492_student@training-vm:~$ nano myenv\n",
      "google4551492_student@training-vm:~$ source myenv\n",
      "google4551492_student@training-vm:~$ echo $PROJECT_ID\n",
      "qwiklabs-gcp-a877680de40fd80b\n",
      "google4551492_student@training-vm:~$ echo $MYREGION $MYZONE\n",
      "us-central1 us-central1-a\n",
      "google4551492_student@training-vm:~$ echo $BUCKET\n",
      "qwiklabs-gcp-a877680de40fd80b\n",
      "google4551492_student@training-vm:~$ echo $BROWSER_IP\n",
      "195.132.52.140\n",
      "google4551492_student@training-vm:~$ \n",
      "```\n",
      "\n",
      "\n",
      "---\n",
      "title: Introduction to Continuous Signal Processing\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Signal Processing\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "I am relying on the excellent series on Youtube by Iman: Signal Processing 101 for this series of articles. This series focuses on continuous signal processing. I am also working on a digital/discrete signal processing series.\n",
      "\n",
      "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tPVduVtOJac\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
      "\n",
      "<br> \n",
      "\n",
      "# 1. What is signal processing?\n",
      "\n",
      "> A signal is a time-varying physical process. \n",
      "\n",
      "Signals can be :\n",
      "- voice\n",
      "- videos or images\n",
      "- temperature records\n",
      "- stock prices\n",
      "- health records\n",
      "- ...\n",
      "\n",
      "The act of processing a signal using a system is called signal processing. A signal contains information. A system processes this information.\n",
      "\n",
      "Signal processing is implied when you:\n",
      "- make a phone call\n",
      "- use a voice assistance\n",
      "- listen to the radio\n",
      "- edit a picture\n",
      "- ...\n",
      "\n",
      "# 2. Signal representation\n",
      "\n",
      "Signal can be :\n",
      "- 1-dimensional : On a voice record for example, each point can be represented on a value vs. time plot. If you know the time, you can retrieve the value.\n",
      "- 2-dimensional: An image is 2 dimensional, since you need both x and y to characterize the value of a pixel on an image\n",
      "- 3-dimensional: A video is made of a sequence of images. You need x, y and the index of the image `t` to know the value of a pixel.\n",
      "\n",
      "This is called signal representation. We will however focus on 1-dimensional signals.\n",
      "\n",
      "# 3. Discrete vs. Continuous\n",
      "\n",
      "Signal processing is divided in 2 categories:\n",
      "- **continuous/analog** signal processing : a signal continuous in time taking continuous range of amplitude values, defined for all times.\n",
      "- **discrete/digital** signal processing : a discrete signal for which we only know values of the signal at discrete points in time.\n",
      "\n",
      "A 1-dimensional **continous** signal could for example be:\n",
      "\n",
      "$$ x(t) = sin(2 \\pi f_o t) $$\n",
      "\n",
      "Where:\n",
      "- `t` represents the time\n",
      "- $$ f_o $$ represents the frequency, in Hz (number of cycles per second)\n",
      "- $$ 2 \\pi f_o t $$ is an angle measured in radians\n",
      "\n",
      "A **discrete-time** signal quantizes the time and the signal amplitude. We might have a discrete signal taking the following form:\n",
      "\n",
      "```\n",
      "(0, 0)\n",
      "(1, 0.31)\n",
      "(2, 0.59)\n",
      "(3, 0.81)\n",
      "...\n",
      "```\n",
      "\n",
      "Where the first value is the index, and the second is the value. The discrete signal can be represented as :\n",
      "\n",
      "$$ x(n) = sin(2 \\pi f_o n t_s) $$\n",
      "\n",
      "Where:\n",
      "- n is the index (0, 1, 2, 3...)\n",
      "- $$ t_s $$ is the time sample, i.e. the time between 2 indexs\n",
      "\n",
      "Continuous and discrete signals can be represented as such:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/audio_5.png)\n",
      "\n",
      "# 4. Signal transformation\n",
      "\n",
      "## 4.a. Continuous\n",
      "\n",
      "We can apply transformations to a continuous signal $$ x(t) $$. For example:\n",
      "- $$ x(2t) $$ : doubles the speed of a signal. We talk about signal compressing in time direction.\n",
      "- $$ x(t/2) $$ : reduces by 2 the speed of a signal. We talk about signal expansion in time direction.\n",
      "- $$ x(-t) $$ : time reversal transformation.\n",
      "- $$ x(t+2) $$ : time shifting transformation to the left. We play the signal two units sooner.\n",
      "- $$ x(t-2) $$ : time shifting transformation to the right. We play the signal two units later.\n",
      "\n",
      "Transformations applied in the parenthesis usually affect the time direction. However, transformations applied outside the parenthesis affect the value axis:\n",
      "- $$ 2x(t) $$ : magnify the amplitude. We multiply the amplitude by two. If the value of 2 is smaller than 1, we say that we compress the signal in value direction.\n",
      "- $$ x(t) + 2 $$ : shift the whole signal up by 1 unit.\n",
      "\n",
      "To summarize, we can apply the following transformations:\n",
      "- time shifting\n",
      "- time scaling\n",
      "- amplitude shifting\n",
      "- amplitude scaling\n",
      "\n",
      "## 4.b. Discrete\n",
      "\n",
      "A Discrete System is any software operating on a discrete-time signal sequence, and producing a discrete output sequence using a transformation. A simple Discrete System can be defined by a *difference equation*:\n",
      "\n",
      "$$ y(n) = 2 x(n) - 1 $$\n",
      "\n",
      "The same concepts as for continuous signals regarding the transformations apply.\n",
      "\n",
      "> For what comes next, we will focus exclusively on continuous signals.\n",
      "\n",
      "# 5. Continuous Signal properties\n",
      "\n",
      "## Even signals\n",
      "\n",
      "> If $$ x(t) = x(-t) $$ this means that the signal is symmetric around the x-axis. \n",
      "\n",
      "We talk about even signals.\n",
      "\n",
      "## Odd signals\n",
      "\n",
      "> If $$ x(t) = -x(-t) $$, we reflect the signal with respect to the origin and the signal is odd.\n",
      "\n",
      "To test if a function is even or odd or neither, just replace t by -t in the expression and check for the equality.\n",
      "\n",
      "## Periodic\n",
      "\n",
      "> A signal is periodic if the same pattern repeats for ever.\n",
      "\n",
      "Some common periodic functions are $$ sin(a t) $$, $$ cos(a t) $$, $$ e^{j a t} $$.\n",
      "\n",
      "The fundamental period for these signals is $$ \\tau = \\frac{2 \\pi}{a} $$. A more complex signal could be :\n",
      "\n",
      "$$ x(t) = sin(\\pi t + \\pi / 4) $$\n",
      "\n",
      "In this case, the period is : $$ \\tau = 2 \\pi / \\pi = 2 $$.\n",
      "\n",
      "We can combine two periodic signals $$ x_1 $$ of period $$ T_1 $$ and $$ x_2 $$ of period $$ T_2 $$. The output signal is periodic if $$ \\frac{T_1}{T_2} $$ is rational. \n",
      "\n",
      "The period of this third signal is the least common multiple between $$ T_1 $$ and $$ T_2 $$.\n",
      "\n",
      "For example, if:\n",
      "- the first signal is : $$ x_1(t) = cos(6 \\pi t) $$. The period is $$ \\frac{2 \\pi}{6 \\pi} = \\frac{1}{3} $$.\n",
      "- the second signal is : $$ x_2(t) = sin(30 \\pi t) $$. The period is $$ \\frac{2 \\pi}{30 \\pi} = \\frac{1}{15} $$.\n",
      "- the combined signal can be written as : $$ x(t) = cos(6 \\pi t) + sin(30 \\pi t) $$\n",
      "\n",
      "The ratio between $$ T_1 $$ and $$ T_2 $$ is 5 which is rational. The combined signal is  Therefore, $$ T_3 $$ is the least common multiple between $$ T_1 $$ and $$ T_2 $$ which is $$ \\frac{1}{3} $$.\n",
      "\n",
      "# 6. Continuous Elementary Signals\n",
      "\n",
      "There are some elementary signals which are basic building blocks of many other signals:\n",
      "- the unit step\n",
      "- the unit impulse\n",
      "\n",
      "## Unit Step\n",
      "\n",
      "The unit step is a function which is 1 when t is greater or equal than 0, and 0 otherwise.\n",
      "\n",
      "$$\n",
      "u(t) = \\left\\{\n",
      "\t    \\begin{array}{ll}\n",
      "\t        1 & \\mbox{if } t ≥ 0 \\\\\n",
      "\t        0 & \\mbox{otherwise.}\n",
      "\t    \\end{array}\n",
      "\t\\right.\n",
      "$$\n",
      "\n",
      "For example, a function that takes values 1 between 0 and 1, and 0 everywhere else can be seen as the unit step minus a shifted version of the unit step by 1 unit:\n",
      "\n",
      "$$ f(t) = u(t) - u(t-1) $$\n",
      "\n",
      "## Unit Impulse\n",
      "\n",
      "This function is 0 everyhere except at the origin, where the value is 1. \n",
      "\n",
      "$$\n",
      "\\delta(t) = \\left\\{\n",
      "\t    \\begin{array}{ll}\n",
      "\t        0 & \\mbox{if } t ≠ 0 \\\\\n",
      "\t        0 & \\mbox{if } t = 0\n",
      "\t    \\end{array}\n",
      "\t\\right.\n",
      "$$\n",
      "\n",
      "### Equivalence property\n",
      "\n",
      "The *equivalence property* states that:\n",
      "\n",
      "$$ x(t) * \\delta(t - t_0) = x(t_0) * \\delta(t - t_0) $$\n",
      "\n",
      "This is because the only point at which $$ x(t) $$ is not 0 when multiplied by the delta term is at $$ t_0 $$, since $$ delta(t-t_0) $$ is a shifted unit impulse by $$ t_0 $$.\n",
      "\n",
      "This can be used to reduce the expression of this signal for example:\n",
      "\n",
      "$$ sin(t) \\delta(t - \\frac{\\pi}{6}) = sin(\\frac{\\pi}{6}) \\delta(t - \\frac{\\pi}{6}) = \\frac{1}{2} \\delta(t - \\frac{\\pi}{6}) $$\n",
      "\n",
      "### Sifting property\n",
      "\n",
      "The *sifting property* similartly states that:\n",
      "\n",
      "$$ \\int_{- \\infty}^\\infty x(t) \\delta(t-t_0) dt= x(t_0) $$\n",
      "\n",
      "This can be used to reduce the expression of this signal for example:\n",
      "\n",
      "$$ \\int_{- \\infty}^\\infty cos(2t) \\delta(t-1) dt = cos(2 * 1) = cos(2) $$\n",
      "\n",
      "Note that there is a strong link between the unit impulse and the unit step functions. Indeed:\n",
      "\n",
      "$$ \n",
      "\n",
      "\\int_{- \\infty}^t \\delta(\\tau) d \\tau = \\left\\{\n",
      "\t    \\begin{array}{ll}\n",
      "\t        0 & \\mbox{if } t < 0 \\\\\n",
      "\t        1 & \\mbox{if } t ≥ 0\n",
      "\t    \\end{array}\n",
      "\t\\right. $$ = u(t) \n",
      "$$\n",
      "\n",
      "# 7. Continuous System properties\n",
      "\n",
      "Recall that a system is any software operating on a  signal sequence, and producing an output sequence using a transformation.\n",
      "\n",
      "For example:\n",
      "\n",
      "$$ y(t) = x(-2 t) $$\n",
      "\n",
      "## Memoryless\n",
      "\n",
      "A system is said to be *memoryless* if the output at a time t depends only on the input at that same time, and not on information from the past.\n",
      "\n",
      "The system mentioned above is obviously not memoryless since the output depends on previous or future values of the input. \n",
      "\n",
      "## Causality\n",
      "\n",
      "A system is said to be *causal* if the output only depends on current or previous values of the input, not future values.\n",
      "\n",
      "A song for example might be a non-causal system since the signal has already been totally recorded.\n",
      "\n",
      "## Stability\n",
      "\n",
      "A system is said to be BIBO (bounded-input bounded-output) *stable* if for any bounded input, we get a bounded output. The signal should be within a finite range.\n",
      "\n",
      "For example, $$ y(t) = \\frac{x(t)}{t} $$ is not bounded. \n",
      "\n",
      "## Invertibility\n",
      "\n",
      "A system is said to be *invertible* if each element of the input corresponds to a unique element of the output, as a one-to-one.\n",
      "\n",
      "An example of a non-invertible system is the following:\n",
      "\n",
      "$$ y(t) = sin(x(t)) $$\n",
      "\n",
      "Since when $$ x(t) = 0 $$, then $$ y(t) = sin(0) = 0 $$. But we can find a second input that leads to the same output with $$ x(t) = \\pi $$ and therefore $$ y(t) = sin(\\pi) = 0 $$.\n",
      "\n",
      "## Time-invariant\n",
      "\n",
      "A system is said to be *time-invariant* if it does not change over time. A system is time-invariant if a time-shift in the input results in the same time-shift in the output: $$ y(t) = y(t - T) $$.\n",
      "\n",
      "The following system is not time-invariant for example:\n",
      "\n",
      "$$ y(t) = x(2t) $$\n",
      "$$ y(t - T) = x(2(t-T)) = x(2t - 2T) ≠ x(2t - T) $$\n",
      "\n",
      "On the other hand, $$ y(t) = sin(x(t)) $$ is time-invariant.\n",
      "\n",
      "## Linear \n",
      "\n",
      "A system is said to be linear if the linear combination of inputs leads to a linear combination of the outputs: $$ a x_1(t) + b x_2(t) = a y_1(t) + b y_2(t) $$.\n",
      "\n",
      "> A system is said to be *LTI* (or Linear & Time-Invariant) if it is both Linear and Time-invariant.\n",
      "\n",
      "## Impulse Response\n",
      "\n",
      "The impulse response of a dynamic system is the output when presented with a brief input signal called an impulse. \n",
      "\n",
      "If a system is LTI, and you make an impulse signal $$ \\delta(t) $$ go through a black-box system, then the impulse response, aka the output, is $$ h(t) $$.\n",
      "\n",
      "To find the impulse response, one simply needs to replace the input by $$ \\delta(t) $$. For example:\n",
      "\n",
      "$$ y(t) = x(t-2) $$\n",
      "\n",
      "Leads to the impulse response:\n",
      "\n",
      "$$ h(t) = \\delta(t-2) $$\n",
      "\n",
      "## Convolution\n",
      "\n",
      "Take an LTI system. Based on these properties, you know that:\n",
      "\n",
      "$$ \\delta(t) \\rightarrow h(t) $$\n",
      "\n",
      "$$ \\delta(t - \\tau) \\rightarrow h(t - \\tau) $$\n",
      "\n",
      "$$ x(\\tau) \\delta(t - \\tau) \\rightarrow x(\\tau) h(t - \\tau) $$\n",
      "\n",
      "$$ \\int_{- \\infty}^\\infty x(\\tau) d \\tau \\delta(t - \\tau) \\rightarrow \\int_{- \\infty}^\\infty x(\\tau) h(t - \\tau) d \\tau $$\n",
      "\n",
      "What this highlights when we apply a series of linear transformations here is the following. By sifting property, the left term is equal to $$ x(t) $$.\n",
      "\n",
      "$$ x(t) \\rightarrow y(t) = \\int_{- \\infty}^\\infty x(\\tau) h(t - \\tau) d \\tau $$\n",
      "\n",
      "This transformation is called the convolution, and is denoted as such:\n",
      "\n",
      "$$ y(t) = x(t) * h(t) $$\n",
      "\n",
      "This mainly means that for an LTI system, when the impulse response is given, you can find the output of any input if you just find the result of the integral.\n",
      "---\n",
      "title: Multilingual and Low-Resource Speech Recognition\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Speech Processing\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "In the world, more than 6'000 languages exist, 3'000 of them currently being endangered. In Europe, 24 official languages are spoken. Google Cloud Speech API covers 60 languages and 50 accents/dialects, and Siri covers 20 languages and 20 accents/dialects.\n",
      "\n",
      "Many of the low-resourced languages have:\n",
      "- limited web presence, hence lack resources on text corpora and pronunciation\n",
      "- lack of linguistic expertise\n",
      "\n",
      "# Speech Recognition of under-resources languages\n",
      "\n",
      "There are several steps to manage and under-resources language:\n",
      "- train multilingual acoustic and language models\n",
      "- transfer knowledge between languages\n",
      "- construct pronunciation lexica\n",
      "- deal with language specific characteristics\n",
      "\n",
      "## Multi / cross-lingual acoustic models\n",
      "\n",
      "We can use NN hidden layers to learn a multilingual representation, shared between languages, and add an output layer for monolingual language specific.\n",
      "\n",
      "There are several ways to do this:\n",
      "- use a **hat-swap architecture**, which is a network with an output layer for each language but shared hidden layers\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_64.png)\n",
      "\n",
      "- use a **multilingual phone set** and build a common hybrid HMM-DNN system with a separate output layer per language, a parallel version of the hat-swap\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_65.png)\n",
      "\n",
      "- use a **multilingual bottleneck**, a bottleneck hidden layer (trained as multilingual) as features of a HMM-GMM or a HMM-DNN system\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_66.png)\n",
      "\n",
      "## Graphemes\n",
      "\n",
      "A way to overcome lacks of phone-based pronunciations is to use graphemes (letters) rather than phones. The only problem is that there is not always a direct link between graphemes and sounds, for example in English.\n",
      "\n",
      "## Morphology\n",
      "\n",
      "Many languages are harder than English, have larger vocabulary sizes, and:\n",
      "- are compound (e.g German): words can be decomposed into constituent parts\n",
      "- are inflected languages (e.g Arabic or Slavic)\n",
      "- are inflected and compound (e.g Finnish)\n",
      "\n",
      "We can therefore model at the morph level in order to reduce the Out-Of-Vocabulary rate (OOV).\n",
      "\n",
      "Segmenting speech and text into morphs requires a lot of linguistic work. However, some automatic approaches try to cluster words (wuch as Morfessor) based on the frequency of sub-strings of letters in a word list. It may however require larger context.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_67.png)\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "If you want to improve this article or have a question, feel free to leave a comment below :)\n",
      "\n",
      "References:\n",
      "- [ASR 14, University of Edimburgh](http://www.inf.ed.ac.uk/teaching/courses/asr/2019-20/asr14-multiling.pdf)\n",
      "\n",
      "---\n",
      "title: Key Resources\n",
      "layout: post\n",
      "tags: [statistics]\n",
      "subtitle : \"Books, papers and talks\"\n",
      "---\n",
      "\n",
      "This article is a simple list of useful resources I have found and used to make this series of articles. \n",
      "I will try to keep them ranked by subject. This list will be frequently updated. \n",
      "\n",
      "## Elements of Statistics\n",
      "\n",
      "- \"Econometrics\", Bruce E. Hansen\n",
      "\n",
      "## Elements of Machine Learning\n",
      "\n",
      "- \"The Elements of Statistical Learning\", Hastie, Tibshirani, Friedman\n",
      "- \"An Introduction to Statistical Learning\", James, Witten, Hastie, Tibshirani\n",
      "\n",
      "---\n",
      "title: Tableau-like in Python with Altair\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Data Viz\"\n",
      "---\n",
      "\n",
      "If you ever used Tableau, you know how easy and user-friendly it is for the end-user. Altair is a great Python library that allows you to program dashboard and other great stuff done in Tableau. Altair uses a so-called declarative approach in which we state what we want instead of stating how to get it.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "{% highlight matlab %}\n",
      "{% endhighlight %}\n",
      "\n",
      "# Before we start\n",
      "\n",
      "We'll be using Jupyter Lab on this one. First of all, install Jupyter Lab, Altaire and Vega :\n",
      "\n",
      "```pip install jupyterlab altair vega vega_datasets```\n",
      "\n",
      "Then, launch Jupyter Lab :\n",
      "\n",
      "```jupyter lab```\n",
      "\n",
      "We will use data from the French population that contains for each city :\n",
      "- geolocation\n",
      "- population\n",
      "- population density\n",
      "\n",
      "You can download the dataset right [here](https://maelfabien.github.io/assets/files/france.csv).\n",
      "\n",
      "Just put the file in your working directory. We are now ready to go!\n",
      "\n",
      "# The basics of Altair\n",
      "\n",
      "Start by importing the Altair library and the file :\n",
      "\n",
      "```python\n",
      "import altair as alt \n",
      "france = 'france.csv'\n",
      "```\n",
      "\n",
      "If you want to further understand the structure of the data, read it using pandas :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france.jpg)\n",
      "\n",
      "## Basic principles\n",
      "\n",
      "In Altair, to draw a `Chart`, you need to `encode` the variables to a certain type :\n",
      "- Q: Stands for quantitative\n",
      "- N: Nominal / Categorical data\n",
      "- O: Ordinal data\n",
      "- T: Temporal data\n",
      "\n",
      "Let's plot the map of France where the X-axis corresponds to the `x` column, and the Y-axis corresponds to the `y` column.\n",
      "\n",
      "```python\n",
      "alt.Chart(france).mark_point().encode(\n",
      "    x='x:Q',\n",
      "    y='y:Q')\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france2.jpg)\n",
      "\n",
      "- `alt.Chart` is used to declare the chart\n",
      "- `france` is the path to our file\n",
      "- `mark_point` is the type of marker we're using. We'll later see `mark_circle` or `mark_square`\n",
      "- `encode` is used to encode the variables \n",
      "- `x='x:Q'` means that we encode the column x to being quantitative. Same for the y column.\n",
      "\n",
      "There is a major issue with the graph above, the origin of the plot is (0,0). We need to change that. We will also delete the axis values :\n",
      "\n",
      "```\n",
      "alt.Chart(france).mark_point().encode(\n",
      "    x=alt.X('x:Q', axis=None),\n",
      "    y=alt.Y('y:Q', axis=None, scale=alt.Scale(zero=False)))\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france3.jpg)\n",
      "\n",
      "Notice how we now declare the axis as objects in Altair using `alt.X` and `alt.Y`. We also disable the zero scale.\n",
      "\n",
      "The points above are a bit too big. Let's reduce their size in the `mark_point`. We will also assign the graph to a variable, and call that variable. We are also maxing the graph a bit bigger :\n",
      "\n",
      "```python\n",
      "map = alt.Chart(france, width=600, height=600).mark_point(size=1).encode(\n",
      "    x=alt.X('x:Q', axis=None),\n",
      "    y=alt.Y('y:Q', axis=None, scale=alt.Scale(zero=False))\n",
      ")\n",
      "\n",
      "map\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france4.jpg)\n",
      "\n",
      "## Size\n",
      "\n",
      "Let's now set the size of the marks accordingly to the population of the city.\n",
      "\n",
      "```python\n",
      "map = alt.Chart(france, width=600, height=600).mark_point(size=1).encode(\n",
      "    x=alt.X('x:Q', axis=None),\n",
      "    y=alt.Y('y:Q', axis=None, scale=alt.Scale(zero=False)), \n",
      "    size='population:Q',\n",
      ")\n",
      "\n",
      "map\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france7.jpg)\n",
      "\n",
      "Notice that we can also declare the size as an object itself using :\n",
      "\n",
      "```python\n",
      "size = alt.Size('population:Q')\n",
      "```\n",
      "\n",
      "## Color\n",
      "\n",
      "Remember that we do have the information of the population density per city. We can decide to set the color depending on the population's density using `alt.Color` :\n",
      "\n",
      "```python\n",
      "map = alt.Chart(france, width=600, height=600).mark_point(size=0.5).encode(\n",
      "    x=alt.X('x:Q', axis=None),\n",
      "    y=alt.Y('y:Q', axis=None, scale=alt.Scale(zero=False)),\n",
      "    size=alt.Size('population:Q'),\n",
      "    color=alt.Color('density:Q')\n",
      ")\n",
      "\n",
      "map\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france8.jpg)\n",
      "\n",
      "It cool, but we have an issue here: the population's density is extremely large in Paris and the Ile-de-France region, and the rest of France is too clear. Let's now define a threshold (say a density of 2000) above which the color doesn't change. We will also use a color template.\n",
      "\n",
      "```python\n",
      "map = alt.Chart(france, width=600, height=600).mark_point(size=0.5).encode(\n",
      "    x=alt.X('x:Q', axis=None),\n",
      "    y=alt.Y('y:Q', axis=None, scale=alt.Scale(zero=False)),\n",
      "    size=alt.Size('population:Q'),\n",
      "    color=alt.Color('density:Q', scale=alt.Scale(scheme='viridis', domain=[0,2000]))\n",
      "\n",
      ")\n",
      "\n",
      "map\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france9.jpg)\n",
      "\n",
      "That looks much better ! We can see the main cities of France appear in yellow on the map.\n",
      "\n",
      "## Marks\n",
      "\n",
      "So far, we've only used `mark_point` as a marker. We can specify the marker's shape according to the following rules :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france5.jpg)\n",
      "\n",
      "For example :\n",
      "\n",
      "```python\n",
      "map = alt.Chart(france, width=600, height=600).mark_square(size=1000).encode(\n",
      "    x=alt.X('x:Q', axis=None),\n",
      "    y=alt.Y('y:Q', axis=None, scale=alt.Scale(zero=False)),\n",
      "    size=alt.Size('population:Q'),\n",
      "    color=alt.Color('density:Q', scale=alt.Scale(scheme='viridis'))\n",
      "\n",
      ")\n",
      "\n",
      "map\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france10.jpg)\n",
      "\n",
      "## Histograms\n",
      "\n",
      "To create histograms, there are three modifications to make :\n",
      "- use `mark_bar` marker\n",
      "- specify a bin, i.e the number of categories to create on the X-axis\n",
      "- display the sum of the category (or the mean, count, std... for example) on the Y-axis\n",
      "\n",
      "```python\n",
      "population = alt.Chart(france, width=600, height=300).mark_bar().encode(\n",
      "    x=alt.X('population:Q', bin=alt.Bin(maxbins=60)),\n",
      "    y='sum(population):Q')\n",
      "\n",
      "population\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france12.jpg)\n",
      "\n",
      "Try to do the same for density on your side, and call it `density`.\n",
      "\n",
      "## Heatmaps\n",
      "\n",
      "If you add a bin on the Y-axis too, you obtain a heatmap! We will now create a heatmap of France, that aggregates information within the region (class X and class Y), and displays the count of the number of records within this region :\n",
      "\n",
      "```python\n",
      "heat_map = alt.Chart(france, width=600, height=600).mark_square(size=50).encode(\n",
      "    x=alt.X('x:Q', axis=None, bin=alt.Bin(maxbins=90)),\n",
      "    y=alt.Y('y:Q', axis=None, scale=alt.Scale(zero=False), bin=alt.Bin(maxbins=90)),\n",
      "    color=alt.Color('count(population):Q', scale=alt.Scale(scheme='viridis')))\n",
      "\n",
      "heat_map\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france13.jpg)\n",
      "\n",
      "## Dashboards\n",
      "\n",
      "One of the nice features in Altair is to be able to build, as in Tableau, nice looking Dashboards. For example, here's how to display the two histograms and the map of France we just built :\n",
      "\n",
      "```python\n",
      "population.properties(height=100) & density.properties(height=100) & map\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france14.jpg)\n",
      "\n",
      "# Interactive charts\n",
      "\n",
      "Altair is great for developing interactive charts. \n",
      "\n",
      "## Tooltips\n",
      "\n",
      "The first step toward developing interactive charts is to integrate tooltips. Tooltips display a certain text message on overlay. For example :\n",
      "\n",
      "```python\n",
      "map = alt.Chart(france, width=600, height=600).mark_point(size=0.5).encode(\n",
      "    x=alt.X('x:Q', axis=None),\n",
      "    y=alt.Y('y:Q', axis=None, scale=alt.Scale(zero=False)),\n",
      "    size=alt.Size('population:Q'),\n",
      "    tooltip=['place:N', 'population:Q', 'density:Q'],\n",
      "    color=alt.Color('density:Q', scale=alt.Scale(scheme='viridis', domain=[0,2000]))\n",
      "\n",
      ")\n",
      "\n",
      "map\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france11.jpg)\n",
      "\n",
      "## Selection intervals\n",
      "\n",
      "In some cases, you might want to select a specific region of the map. This is done by adding a selection interval on the map :\n",
      "\n",
      "```python\n",
      "brush = alt.selection_interval()\n",
      "map.add_selection(brush)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france15.jpg)\n",
      "\n",
      "As you might guess, this is quite pointless so far. We need to add more features, such as setting to gray the non-selected part using the `alt.value` argument in the color.\n",
      "\n",
      "```python\n",
      "map = alt.Chart(france, width=600, height=600).mark_point(size=1).encode(\n",
      "    x=alt.X('x:Q', axis=None),\n",
      "    y=alt.Y('y:Q', axis=None, scale=alt.Scale(zero=False)),\n",
      "    size='population:Q',\n",
      "    tooltip=['place:N', 'population:Q', 'density:Q'],\n",
      "    color=alt.condition(brush, 'density:Q', alt.value('lightgrey'), scale=alt.Scale(scheme='viridis', domain=[0,2000]))\n",
      ").add_selection(brush)\n",
      "\n",
      "map\n",
      "```\n",
      "\n",
      "## Transform filters\n",
      "\n",
      "At that point, this is a cool feature, but it remains useless. What we might want to do is on the Dashboard, when a region is selected on the map, update it on the histograms (to get for example the population's histogram in this region).\n",
      "\n",
      "This can be achieved using `transform_filter`.\n",
      "\n",
      "```python\n",
      "population = population = alt.Chart(france, width=800, height=100).mark_bar().encode(\n",
      "    x=alt.X('population:Q', bin=alt.Bin(maxbins=60)),\n",
      "    y='sum(population):Q').transform_filter(brush)\n",
      "\n",
      "population & map\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france17.jpg)\n",
      "\n",
      "As you can see, the transformation filter refers to the brush filter. This is interesting. The last improvement we'll explore is to update the view in both ways.  If we select a region of the histogram, it will update the map.\n",
      "\n",
      "Start off by defining the two selection intervals :\n",
      "\n",
      "```python\n",
      "brush = alt.selection_interval()\n",
      "pop_selection = alt.selection_interval(encodings=['x'])\n",
      "```\n",
      "\n",
      "Then, define the map and add a selection according to population, and define the population histogram, add the population selection and add the transformation.\n",
      "\n",
      "```\n",
      "map = alt.Chart(france, width=600, height=600).mark_point(size=1).encode(\n",
      "    x=alt.X('x:Q', axis=None),\n",
      "    y=alt.Y('y:Q', axis=None, scale=alt.Scale(zero=False)),\n",
      "    size='population:Q',\n",
      "    color=alt.condition(pop_selection, 'density:Q', alt.value('lightgrey'), scale=alt.Scale(scheme='viridis', domain=[0,2000]))\n",
      ").add_selection(brush)\n",
      "\n",
      "population = alt.Chart(france, width=600, height=100).mark_bar().encode(\n",
      "    x=alt.X('population:Q', bin=alt.Bin(maxbins=50)),\n",
      "    y='sum(population):Q'\n",
      ").add_selection(pop_selection).transform_filter(brush)\n",
      "\n",
      "population & map\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france18.jpg)\n",
      "\n",
      "> **Conclusion** : That's it ! I hope this introduction to Altair was interesting. I love to use this on my Data Viz projects. Feel free to leave a comment if you have one.\n",
      "---\n",
      "title: Easy Question Answering with AllenNLP\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Natural Language Processing\"\n",
      "---\n",
      "\n",
      "AllenNLP is an Apache 2.0 NLP research library, built on PyTorch, for developing state-of-the-art deep learning models on a wide variety of linguistic tasks. AllenNLP is built and maintained by the Allen Institute for Artificial Intelligence, in close collaboration with researchers at the University of Washington and elsewhere.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# What is Question Answering?\n",
      "\n",
      "Question Answering (QA), or Machine Comprehension (MC) aims to answer a query about a given context by modeling the interactions between both context and queries. Typical approaches in QA rely on attention mechanismes, in order to focus on a small part of the text and summarize it with a fixed-size vector.\n",
      "\n",
      "AllenNLP implements a pre-trained Bi-Directional Attention Flow (BIDAF). This network is a multi-stage hierarchical process that represents the context at different levels of granularity and uses bidirectional attention flow mechanism to obtain a query-aware context representation without early summarization. This approach was published in 2017 by the Allen Institute for Artificial Intelligence in [this paper](https://arxiv.org/pdf/1611.01603.pdf).\n",
      "\n",
      "According to the original paper, the steps of the BIDAF are the following:\n",
      "- Character Embedding Layer maps each word to a vector space using character-level CNNs.\n",
      "- Word Embedding Layer maps each word to a vector space using a pre-trained word embedding model.\n",
      "- Contextual Embedding Layer utilizes contextual cues from surrounding words to refine\n",
      "the embedding of the words. These first three layers are applied to both the query and context.\n",
      "- Attention Flow Layer couples the query and context vectors and produces a set of queryaware feature vectors for each word in the context.\n",
      "- Modeling Layer employs a Recurrent Neural Network to scan the context.\n",
      "- Output Layer provides an answer to the query.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/bidaf.png)\n",
      "\n",
      "# Using AllenNLP's library\n",
      "\n",
      "To install AllenNLP, simply run : \n",
      "\n",
      " ```bash\n",
      " pip install allennlp\n",
      " ```\n",
      "\n",
      "If you have SpaCy installed and encounter some issues with your current SpaCy version, I encourage you to switch to version 2.1.8 of SpaCy and downgrade the `en_core_news` package to version 2.1.0. This worked out for me. We'll deploy this pre-trained QA algorithm on a Streamlit web application. To do so, import the following packages:\n",
      "\n",
      "```python\n",
      "from allennlp import pretrained\n",
      "import streamlit as st\n",
      "import seaborn as sns\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "```\n",
      "\n",
      "Then, start to build the application:\n",
      "\n",
      "```python\n",
      "st.title(\"Question Answering\")\n",
      "\n",
      "# Avoids loading the model each time in streamlit\n",
      "# Loads the model\n",
      "model = st.cache(\n",
      "       pretrained.bidirectional_attention_flow_seo_2017,\n",
      "       allow_output_mutation=True\n",
      ")()\n",
      "```\n",
      "\n",
      "Once the pre-trained model has been loaded, we can run it on the Wikipedia article of Netflix:\n",
      "\n",
      "```python\n",
      "passage = st.text_area(\"Article\", \"\"\"Netflix, Inc. is an American media-services provider and production company headquartered in Los Gatos, California, founded in 1997 by Reed Hastings and Marc Randolph in Scotts Valley, California. The company's primary business is its subscription-based streaming service which offers online streaming of a library of films and television programs, including those produced in-house. As of April 2019, Netflix had over 148 million paid subscriptions worldwide, including 60 million in the United States, and over 154 million subscriptions total including free trials. It is available worldwide except in mainland China (due to local restrictions), Syria, North Korea, and Crimea (due to US sanctions). The company also has offices in the Netherlands, Brazil, India, Japan, and South Korea. Netflix is a member of the Motion Picture Association (MPA). Netflix's initial business model included DVD sales and rental by mail, but Hastings abandoned the sales about a year after the company's founding to focus on the initial DVD rental business. Netflix expanded its business in 2010 with the introduction of streaming media while retaining the DVD and Blu-ray rental business. The company expanded internationally in 2010 with streaming available in Canada, followed by Latin America and the Caribbean. Netflix entered the content-production industry in 2012, debuting its first series Lilyhammer. Since 2012, Netflix has taken more of an active role as producer and distributor for both film and television series, and to that end, it offers a variety of \"Netflix Original\" content through its online library. By January 2016, Netflix services operated in more than 190 countries. Netflix released an estimated 126 original series and films in 2016, more than any other network or cable channel. Their efforts to produce new content, secure the rights for additional content, and diversify through 190 countries have resulted in the company racking up billions in debt: $21.9 billion as of September 2017, up from $16.8 billion from the previous year. $6.5 billion of this is long-term debt, while the remaining is in long-term obligations. In October 2018, Netflix announced it would raise another $2 billion in debt to help fund new content.\"\"\")\n",
      "```\n",
      "\n",
      "We must then define a question to ask:\n",
      "\n",
      "```python\n",
      "question = st.text_input(\"Question\", \"Where are the headquarters of Netflix?\")\n",
      "```\n",
      "\n",
      "We compute the result easily through this function:\n",
      "\n",
      "```python\n",
      "result = model.predict(question, passage)\n",
      "```\n",
      "\n",
      "From the result, we want the \"best_span\", \"question_tokens\", and \"passage_tokens\" which contain respectively the position of the answer, a tokenized version of the question and a tokenized version of the article/passage.\n",
      "\n",
      "```python\n",
      "start, end = result[\"best_span\"]\n",
      "question_tokens = result[\"question_tokens\"]\n",
      "passage_tokens = result[\"passage_tokens\"]\n",
      "```\n",
      "\n",
      "In order to display the result, we will only pick the 10 words before and the 10 words after the answer. We also display in bold the exact words which contain the answer:\n",
      "\n",
      "```python\n",
      "mds = [f\"**{token}**\" if start <= i <= end else token if start - 10 <= i <= end + 10 else \"\" for i, token in enumerate(passage_tokens)]\n",
      "st.markdown(\" \".join(mds))\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/qa_1.png)\n",
      "\n",
      "At that moment, the web application works well but the explainability remains limited. In order to imporve that, we can plot the attention layers. The X-axis represents the question, and the Y-axis represents the input text. The darker the column, the most important the attention is in this area. We notice that words such as \"When\" or \"Where\" play a big word in the attention layers, since they expect a date or a place in return.\n",
      "\n",
      "```python\n",
      "attention = result[\"passage_question_attention\"]\n",
      "\n",
      "plt.figure(figsize=(12,12))\n",
      "sns.heatmap(attention, cmap=\"YlGnBu\")\n",
      "plt.autoscale(enable=True, axis='x')\n",
      "plt.xticks(np.arange(len(question_tokens)), labels=question_tokens)\n",
      "st.pyplot()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/qa_2.png)\n",
      "\n",
      "> **Conclusion** : I hope that this introduction was useful. I simply wanted to demonstrate how easy it can be to create a small QA web service.\n",
      "---\n",
      "title: Voice Computing in Python\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Signal Processing\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "These notes are a summary of \"An Introduction to Voice Computing in Python\" by Jim Schwoebel, crossed with some personal notes and external resources. This books covers the key concepts of Voice Computing, recording, playing, storing and converting audio, extracting features, creating ML models on top, generating data...\n",
      "\n",
      "# I. History\n",
      "\n",
      "> A **voice computer** is any computerized system that can process voice inputs.\n",
      "\n",
      "In 1876, Alexander Graham Bell invented an acoustic telegraph to transmit audio frequencies through electrical wires. Thomas Edison invented the phonograph in 1877, first machine to record a sound and play it back.\n",
      "\n",
      "One of the first speech recognition software was written in 1952 by Bells Labs and could only recognize digits. In 1985, IBM releases a software using Hidden Markov Models to recognize over 1000 words.\n",
      "\n",
      "In 1970, Sony invented the Digital Sound Encoding (DSE) to record an analog signal through a condenser microphone and convert it into a ditial representation (series of numbers). \n",
      "\n",
      "We talk about Analog-to-digital converters (A/D) and Digital-to-analog (D/A). This  led to a range of compressed formats, including WAV, invented in 1993. In 2001, lossless speech codecs (.FLAC) were invented, being much smaller and high quality.\n",
      "\n",
      "Today, several tools such as Python, Tensorflow, Keras, Librosa, Kaldi, and speech-to-text APIs make voice computing easier.\n",
      "\n",
      "# II. Fundamentals\n",
      "\n",
      "A *sound wave* is a vibration which causes a local pressure deviation from the ambient (average or equilibrium) atmospheric pressure, called the sound pressure.\n",
      "\n",
      "A microphone is a transducer that converts this sound wave into an electrical signal. The electrical signal is measured in Amperes. The ampere is the electric current corresponding to the flow of 1/(1.602 176 634 × 10-19) elementary charges per second. It can also be expressed as Coulombs per seconds.\n",
      "\n",
      "$$ 1A = 1 C/s $$\n",
      "\n",
      "We record sound through a microphone. It uses a diaphragm\n",
      "to produce changes in distance over capacitor plates to produce electrical current.\n",
      "\n",
      "The most common types of microphones are:\n",
      "- condenser: Uses two capacitor plates and a diaphragm to produce electric current.\n",
      "- dynamic: Uses induction coils attached to a diaphragm to produce sound via electromagnetic induction.\n",
      "\n",
      "In most computers, a sound card receives an analog signal, converts it into a digital format using A/D converter and plays it back using D/A converters on speakers. The digital format file is generated through an audio codec software.\n",
      "\n",
      "An audio transcoding is the conversion of one file format to another. The most common audio coding formats are:\n",
      "- MP3: high compression, signal loss\n",
      "- WAV: larger file sizes\n",
      "- AAC: proprietary format, better than MP3\n",
      "- FLAC: small size, little loss, but must be converted before analysis\n",
      "- OPUS: human voice range files, losses can be problematic\n",
      "\n",
      "1 minute of .WAV file is 3.5MB. Once converted to FLAC, it only takes 1.5MB. This is really useful for storage.\n",
      "\n",
      "Audio channels is the number of audio inputs and outputs of a recorded audio signal:\n",
      "- mono: 1 channel, 1 microphone, 1 output on speakers\n",
      "- stereo: 2 channels, 2 microphones (left and right), 2 speakers\n",
      "\n",
      "We can convert stereo into mono by averaging the amplitude of both channels into a sigle signal channel. We can also convert mono into stereo by duplicating the information on the other channel.\n",
      "\n",
      "Speakers work by making a magnet move to produce pressure waves. A loudspeaker is a speaker made of several sub-speakers, called drivers, each in charge of different parts of the audio spectrum:\n",
      "- tweeters: 2'000 - 20'000 Hz\n",
      "- mid-range: 250 - 2'000 Hz\n",
      "- woofers: 40 - 500 Hz\n",
      "\n",
      "# III. Basic audio processing in Python\n",
      "\n",
      "## III.1. Read Files\n",
      "\n",
      "There are several key libraries in Python to read and manipulate audio files each having specific advantages:\n",
      "- Pydub: Simple features to manipulate file\n",
      "\n",
      "```python\n",
      "from pydub import AudioSegment\n",
      "data = AudioSegment.from_wav(\"test.wav\")\n",
      "```\n",
      "\n",
      "- SoX: Remove silence, remove noise, add chorus... Can be done in command line or using Pysox.\n",
      "- Wave: adding filters\n",
      "\n",
      "```python\n",
      "import wave\n",
      "data=wave.open('test.wav', mode='rb')\n",
      "```\n",
      "\n",
      "- LibROSA: audio engineering and featurization\n",
      "\n",
      "```python\n",
      "import librosa\n",
      "y, sr = librosa.load('test.wav')\n",
      "```\n",
      "\n",
      "- SciPy: connecting to Matlab, Fortran made easy\n",
      "\n",
      "```python\n",
      "from scipy.io import wavfile\n",
      "fs, data = wavfile.read('test.wav')\n",
      "```\n",
      "\n",
      "## III.2. Manipulate files\n",
      "\n",
      "We can either use SoX or Pydub for audio file manipulation. Here are a series of commands using SoX:\n",
      "\n",
      "- Combine files: take in one.wav and two.wav to make three.wav\n",
      "```bash\n",
      "sox one.wav two.wav three.wav\n",
      "```\n",
      "\n",
      "- Trim: take first second of one.wav and output to output.wav\n",
      "```bash\n",
      "sox one.wav output.wav trim 0 1\n",
      "```\n",
      "\n",
      "- Increase volume: make volume 2x in one.wav and output to volup.wav\n",
      "```bash\n",
      "sox -v 2.0 one.wav volup.wav\n",
      "```\n",
      "\n",
      "- Reverse: reverse one.wav and output to reverse.wav\n",
      "```bash\n",
      "sox one.wav reverse.wav reverse\n",
      "```\n",
      "\n",
      "- Change sampling rate: change sample rate of one.wav to 16000 Hz\n",
      "```bash\n",
      "sox one.wav -r 16000 sr.wav\n",
      "```\n",
      "\n",
      "- Changing audio quality: change audio file to 16 bit quality \n",
      "```bash\n",
      "sox -b 16 one.wav 16bit.wav\n",
      "```\n",
      "\n",
      "- Convert mono file to stereo: convert mono file to stereo by cloning channels\n",
      "```bash\n",
      "sox one.wav -c 2 stereo.wav\n",
      "```\n",
      "\n",
      "- Convert stereo file to mono: make stereo file mono by averaging out the channels\n",
      "```bash\n",
      "sox stereo.wav -c 1 mono.wav\n",
      "```\n",
      "\n",
      "- Change audio file speed: double speed of file\n",
      "```bash\n",
      "sox one.wav 2x.wav speed 2.0\n",
      "```\n",
      "\n",
      "Here are some commands to manipulate audio files using Pydub:\n",
      "\n",
      "- Load audio file:\n",
      "```python\n",
      "from pydub import AudioSegment\n",
      "song = AudioSegment.from_wav(\"song.wav\")\n",
      "```\n",
      "\n",
      "- Slice audio:\n",
      "```python\n",
      "# pydub does things in milliseconds\n",
      "ten_seconds = 10 * 1000 \n",
      "first_10_seconds = song[:ten_seconds] \n",
      "last_5_seconds = song[-5000:]\n",
      "```\n",
      "\n",
      "- Make beginning louder and end quieter:\n",
      "```python\n",
      "# boost volume by 6dB\n",
      "beginning = first_10_seconds + 6 \n",
      "# reduce volume by 3dB\n",
      "end = last_5_seconds - 3\n",
      "```\n",
      "\n",
      "- Crossfade:\n",
      "```python\n",
      "# 1.5 second crossfade\n",
      "with_style = beginning.append(end, crossfade=1500)\n",
      "```\n",
      "\n",
      "- Repeat:\n",
      "```python\n",
      "# repeat the clip twice\n",
      "do_it_over = with_style * 2\n",
      "```\n",
      "\n",
      "- Fading in and out:\n",
      "```python\n",
      "# 2 sec fade in, 3 sec fade out\n",
      "awesome = do_it_over.fade_in(2000).fade_out(3000)\n",
      "```\n",
      "\n",
      "- Output results as .MP3:\n",
      "```python\n",
      "# export .mp3\n",
      "awesome.export(\"mashup.mp3\", format=\"mp3\")\n",
      "```\n",
      "\n",
      "## III.3. Playing audio\n",
      "\n",
      "There are two ways to play audio:\n",
      "- *synchronous*: playing the audio file is the only code executed at the moment\n",
      "- *asynchronous*: playing the audio file as a background and keep executing code\n",
      "\n",
      "For *synchronous* playing, we can use Pygame.\n",
      "\n",
      "```python\n",
      "import pygame\n",
      "pygame.mixer.init() \n",
      "pygame.mixer.music.load('test.wav') \n",
      "pygame.mixer.music.play()\n",
      "```\n",
      "\n",
      "For asynchronous playing, one can use Sounddevice:\n",
      "\n",
      "```python\n",
      "import sounddevice as sd\n",
      "import soundfile as sf\n",
      "import time\n",
      "\n",
      "data, fs = sf.read('test.wav') \n",
      "sd.play(data, fs)\n",
      "\n",
      "print(\"Execute this\")\n",
      "time.sleep(5)\n",
      "\n",
      "sd.stop()\n",
      "```\n",
      "\n",
      "## III.4. Recording audio\n",
      "\n",
      "You can check the configuration of your microphone using Sounddevice. This will display the information of the input and the output.\n",
      "\n",
      "```python\n",
      "import sounddevice as sd\n",
      "mics=sd.query_devices() \n",
      "for i in range(len(mics)): \n",
      "\tprint(mics[i])\n",
      "```\n",
      "\n",
      "To actually record a sound, we can use the sd.rec function provided by sounddevice.\n",
      "\n",
      "```python\n",
      "duration = 10\n",
      "fs = 10000\n",
      "channels = 1\n",
      "myrecording = sd.rec(int(duration * fs), samplerate=fs, channels=channels) \n",
      "sd.wait()\n",
      "sf.write('sync_record.wav', myrecording, fs)\n",
      "```\n",
      "\n",
      "## III.5. Transcoding\n",
      "\n",
      "To transcode from one format to another, one can use FFmpeg. It has a wrapper called ffmpy in Python. To run a conversion from one format to another, simply specify the input file name, and the expected output extension.\n",
      "\n",
      "```python\n",
      "filename='test.mp3'\n",
      "ff = ffmpy.FFmpeg(inputs={filename:None}, outputs={filename[0:-4]+'.wav': None})\n",
      "ff.run()\n",
      "```\n",
      "\n",
      "We can also use ffmpeg directly in command lines.\n",
      "\n",
      "- Convert audio from one file format to another:\n",
      "```bash\n",
      "ffmpeg -i input.mp3 output.ogg\n",
      "```\n",
      "\n",
      "- Extract audio from a video:\n",
      "```bash\n",
      "ffmpeg -i video.mp4 -vn -ab 256 audio.mp3\n",
      "```\n",
      "\n",
      "- Merge audio and video files:\n",
      "```bash\n",
      "ffmpeg -i video.mp4 -i audio.mp3 -c:v copy -c:a aac -strict experimental output.mp4\n",
      "```\n",
      "\n",
      "- Add a cover image to audio file:\n",
      "```bash\n",
      "ffmpeg -loop 1 -i image.jpg -i audio.mp3 -c:v libx264 -c:a aac -strict experimental -b:a 192k -shortest output.mp4\n",
      "```\n",
      "\n",
      "- Crop an audio file (second 90 to second 120):\n",
      "```bash\n",
      "ffmpeg -ss 00:01:30 -t 30 -acodec copy -i inputfile.mp3 outputfile.mp3\n",
      "```\n",
      "\n",
      "## III.6. Speech-to-text (Transcribing)\n",
      "\n",
      "There are three alternatives for this approach:\n",
      "- use open-source pre-trained models such as PocketSphinx\n",
      "- use proprietary models such as Google Speech-to-text\n",
      "- train your own model with Kaldi or from scratch\n",
      "\n",
      "### PocketSphinx\n",
      "\n",
      "```python\n",
      "import speech_recognition as sr_audio \n",
      "\n",
      "r = sr_audio.Recognizer()\n",
      "audio = r.record(sr_audio.AudioFile(filename)) \n",
      "text=r.recognize_sphinx(audio) \n",
      "print('transcript: '+text) \n",
      "```\n",
      "\n",
      "### Google API\n",
      "\n",
      "You need to have your credentials set in your bash profile.\n",
      "\n",
      "```python\n",
      "text=r.recognize_google_cloud(audio)\n",
      "print('transcript: '+text) \n",
      "```\n",
      "\n",
      "## III.7. Text-to-speech\n",
      "\n",
      "Text-to-speech systems are the other side of voice assistants. Once you understood and processed the query, you need to pronounce the answer, give the temperature...\n",
      "\n",
      "The most popular open-source library to do that is pyttsx3.\n",
      "\n",
      "```python\n",
      "import pyttsx3\n",
      "\n",
      "engine = pyttsx3.init() \n",
      "engine.say(\"Test 1 2 3\") \n",
      "engine.runAndWait()\n",
      "```\n",
      "\n",
      "You can also go for the Google Cloud API solution, free for up to 1 million characters per month.\n",
      "\n",
      "```python\n",
      "from google.cloud import texttospeech\n",
      "\n",
      "client = texttospeech.TextToSpeechClient()\n",
      "input_text = texttospeech.types.SynthesisInput(text=text)\n",
      "\n",
      "# Names of voices can be retrieved with client.list_voices(). \n",
      "voice = texttospeech.types.VoiceSelectionParams(language_code='en-US', ssml_gender=texttospeech.enums.SsmlVoiceGender.FEMALE, name='en-US-Wavenet-A')\n",
      "\n",
      "audio_config = texttospeech.types.AudioConfig(audio_encoding=texttospeech.enums.AudioEncoding.MP3)\n",
      "response = client.synthesize_speech(\"Test 1 2 3\", voice, audio_config)\n",
      "\n",
      "# The response's audio_content is binary.\n",
      "with open(filename, 'wb') as out: \n",
      "    out.write(response.audio_content)\n",
      "    print('Audio content written to file %s'%(filename))\n",
      "```\n",
      "\n",
      "# IV. Sound Collection\n",
      "\n",
      "## IV.1. Mistakes to avoid\n",
      "\n",
      "When collecting sound, major mistakes are:\n",
      "- bad microphone\n",
      "- too much distance\n",
      "- noisy environment\n",
      "- bad sample rate\n",
      "- bad number of channels\n",
      "- bad transcoding methodology\n",
      "\n",
      "To make it comparable, all these variables should be kept constant over the recording.\n",
      "\n",
      "Mono channels have a typical sampling rate of 16'000 Hz, whereas stereo channels have a sampling rate of 44'100 Hz. Accordin to the Nyquist rate, a sound should be sampled at twice the maximum frequency. The human ear hears sounds up to 20'000 Hz, therefore 44'100 > 20'000 * 2 satisfies this requirement.\n",
      "\n",
      "*Beamforming* is a signal processing technique that allows for directional reception of audio signals to enhance audio quality. Acoustic echo suppression (AES) and acoustic echo cancellation (AEC) algorithms improve voice quality by suppressing acoustic echo (e.g. sounds played back through loudspeaker) and line echo (e.g. electrical impulses in wires and impedance mismatches). Together, these techniques greatly enhance audio files to achieve a higher signal-to-noise (SNR) ratio.\n",
      "\n",
      "The collection can be passive (no response expected from the program, in the background), or active. Here is an example of a voice-first application, falling under the active-synchronous (AS) mode category, that launches a wheather website if the person says \"weather\" within the first 2 seconds after launching the code:\n",
      "\n",
      "```python\n",
      "import sounddevice as sd\n",
      "import soundfile as sf\n",
      "import speech_recognition as sr_audio \n",
      "import os, pyttsx3, pygame, time\n",
      "\n",
      "def sync_record(filename, duration, fs, channels):\n",
      "    print('recording')\n",
      "    myrecording = sd.rec(int(duration * fs), samplerate=fs, channels=channels) \n",
      "    sd.wait()\n",
      "    sf.write(filename, myrecording, fs)\n",
      "    print('done recording')\n",
      "    \n",
      "def sync_playback(filename):\n",
      "    # takes in a file and plays it back \n",
      "    pygame.mixer.init() \n",
      "    pygame.mixer.music.load(filename) \n",
      "    pygame.mixer.music.play()\n",
      "    \n",
      "def speak_text(text): \n",
      "    engine=pyttsx3.init() \n",
      "    engine.setProperty('voice', 'com.apple.speech.synthesis.voice.Alex')\n",
      "    engine.say(text) \n",
      "    engine.runAndWait()\n",
      "    \n",
      "def transcribe_audio_sphinx(filename):\n",
      "# transcribe the audio (note this is only done if a voice sample) r=sr_audio.Recognizer()\n",
      "    with sr_audio.AudioFile(filename) as source:\n",
      "        audio = r.record(source)\n",
      "        \n",
      "    text=r.recognize_sphinx(audio) \n",
      "    print('transcript: '+text)\n",
      "    return text\n",
      "\n",
      "def fetch_weather():\n",
      "    os.system('open https://www.yahoo.com/news/weather')\n",
      "    \n",
      "speak_text('What do you want to search?')\n",
      "sync_record('response.wav',2,16000,1) \n",
      "transcript=transcribe_audio_sphinx('response.wav')\n",
      "\n",
      "if transcript.lower().find('weather') >= 0:\n",
      "    fetch_weather()\n",
      "```\n",
      "\n",
      "## IV.2. Cleaning audio files\n",
      "\n",
      "### IV.2.a. Noise reduction\n",
      "\n",
      "There are many ways to remove the noise from a given audio recording. All it requires is a small sample where there is only a background noise, and then automatically delete this noise from the rest of the sample.\n",
      "\n",
      "The steps of the algorithm are usually the following:\n",
      "- An FFT is calculated over the noise audio clip\n",
      "Statistics are calculated over FFT of the the noise (in frequency)\n",
      "- A threshold is calculated based upon the statistics of the noise (and the desired sensitivity of the algorithm)\n",
      "- An FFT is calculated over the signal\n",
      "- A mask is determined by comparing the signal FFT to the threshold\n",
      "- The mask is smoothed with a filter over frequency and time\n",
      "- The mask is appled to the FFT of the signal, and is inverted\n",
      "\n",
      "We can use the noisereduce package in Python:\n",
      "\n",
      "```python\n",
      "import noisereduce as nr\n",
      "# load data\n",
      "rate, data = wavfile.read(\"mywav.wav\")\n",
      "# select section of data that is noise\n",
      "noisy_part = data[10000:15000]\n",
      "# perform noise reduction\n",
      "reduced_noise = nr.reduce_noise(audio_clip=data, noise_clip=noisy_part, verbose=True)\n",
      "```\n",
      "\n",
      "### IV.2.b. Change volume\n",
      "\n",
      "Changing volume is another way to clean the audio file.\n",
      "\n",
      "```python\n",
      "from pydub import AudioSegment\n",
      "song = AudioSegment.from_wav(\"song.wav\")\n",
      "# boost volume by 5 dB\n",
      "more_volume = song + 5\n",
      "```\n",
      "\n",
      "### IV.2.c. Change sampling rate\n",
      "\n",
      "By changing the sampling rate, one can modify the quality of the audio. This can be done using SoX:\n",
      "\n",
      "```bash\n",
      "sox 'test.wav' -r 48000k 'new_samle_rate.wav'\n",
      "```\n",
      "\n",
      "## IV.3. Speaker Diarization\n",
      "\n",
      "Separating multiple speakers in a conversation is called Speaker Diarization. This is usually done using Fisher Linear Semi-Discriminant Analysis.\n",
      "---\n",
      "title: Deploy a Streamlit WebApp with Docker\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "Deploying your model in an interactive web application as a container can be challenging. Well, at least it used to. In this project, I will show you how to deploy a Named Entity Recognition web application using Spacy, Streamlit, and Docker in a few lines of code.\n",
      "\n",
      "# The application\n",
      "\n",
      "First, create a folder called `docker-spacy` (arbitrary). The application is just 11 lines of code thanks to the framework offered by [Streamlit](https://streamlit.io/). Streamlit is a web application framework designed for data science. It is extremely light and has a pre-built layout.\n",
      "\n",
      "```python\n",
      "import spacy\n",
      "import streamlit as st\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "def return_NER(value):\n",
      "    doc = nlp(value)\n",
      "    return [(X.text, X.label_) for X in doc.ents]\n",
      "\n",
      "# Add title on the page\n",
      "st.title(\"Spacy - Named Entity Recognition\")\n",
      "\n",
      "# Ask user for input text\n",
      "input_sent = st.text_input(\"Input Sentence\", \"Your input sentence goes here\")\n",
      "\n",
      "# Display named entities\n",
      "for res in return_NER(input_sent):\n",
      "    st.write(res[0], \"-->\", res[1])\n",
      "```\n",
      "\n",
      "Save this file as `app.py`. We can test the application by running:\n",
      "\n",
      "```bash \n",
      "streamlit run app.py\n",
      "```\n",
      "\n",
      "The application is now visible in your browser at: `http://localhost:8501`\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/dock_0.png)\n",
      "\n",
      "And there we are. We have a simple web app doing Named Entity Recognition in Spacy in 11 lines of code! The application is extremely simple, and unlike Flask, you don't have to manage the HTML, the CSS, the GET/POST methods or anything.\n",
      "\n",
      "# Requirements\n",
      "\n",
      "The next step is to build a file called `requirements.txt`. This file will contain all dependencies. Inside, you should have:\n",
      "\n",
      "```\n",
      "streamlit\n",
      "spacy\n",
      "https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
      "```\n",
      "\n",
      "The last link is an alternative to the traditional way to install pre-trained models for NER in Python (`python -m spacy install en_core_web_sm`). The advantage of using the direct download link is to have everything in the same requirements file, which is easier for the Dockerfile.\n",
      "\n",
      "# Dockerfile\n",
      "\n",
      "The dockerfile will contain all the instructions to build the image. It is relatively easy, and only takes 7 lines:\n",
      "\n",
      "```\n",
      "FROM python:3.7\n",
      "EXPOSE 8501\n",
      "WORKDIR /app\n",
      "COPY requirements.txt ./requirements.txt\n",
      "RUN pip3 install -r requirements.txt\n",
      "COPY . .\n",
      "CMD streamlit run app.py\n",
      "```\n",
      "\n",
      "We first base our image on a pre-existing image. We then expose it on port 8501. We set the working directory to an app sub-folder. We copy the requirements and install them. We then copy the whole content of the app and run the command to launch the application!\n",
      "\n",
      "We now only need to build the container:\n",
      "```\n",
      "docker build -f Dockerfile -t app:latest .\n",
      "```\n",
      "\n",
      "And run it (make sure to stop your local Streamlit before!):\n",
      "```\n",
      "docker run -p 8501:8501 app:latest\n",
      "```\n",
      "\n",
      "Your container is now available on `http://localhost:8501/` !\n",
      "\n",
      "This is how you can build a simple yet state-of-the-art NER web application using Docker, Spacy, and Streamlit. You can now use AWS, GCP or Azure to deploy the container in a few minutes :)\n",
      "\n",
      "NB: Awesome work in the comments by Zeph Grunschlag to deploy it on AWS Fargate in 1 command line, the repo is [here](https://github.com/tzaffi/streamlit-cdk-fargate).\n",
      "---\n",
      "title: Lab - Classify Images with Pre-Built ML models using Cloud Vision and AutoML\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Road to Google Cloud Platform Certification\"\n",
      "---\n",
      "\n",
      "The aim of this lab is to :\n",
      "- Setup the API key for ML Vision API\n",
      "- Invoke the pre-trained ML Vision API to classify images\n",
      "- Review label predictions from Vision API\n",
      "- Train and evaluate custom AutoML Vision image classification model\n",
      "- Predict with AutoML on a new image\n",
      "\n",
      "We will compare the 2 approaches (Cloud Vision and AutoML) for the task of cloud classification.\n",
      "\n",
      "# Cloud Vision API\n",
      "\n",
      "In the API and Services menu, select \"Library\", look for Vision, and make sure Cloud Vision API is enabled.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_133.jpg)\n",
      "\n",
      "We will later need a pair of credentials for the API key. To get it, in the API & Services tab, click on \"Credentials\".\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_120.jpg)\n",
      "\n",
      "Then, click on \"Create credentials\" :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_121.jpg)\n",
      "\n",
      "Then, on \"Create API key\" :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_134.jpg)\n",
      "\n",
      "Copy the key since we'll need it later.\n",
      "\n",
      "We then use Cloud Storage to store data files. Create a bucket, and give it the project's name. Then, click on import data, and load the following image :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/cirrus.jpg)\n",
      "\n",
      "Call the image `cirrus.png`. Your bucket should now look like this :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_135.jpg)\n",
      "\n",
      "We now need to make the file publicly available \n",
      "\n",
      "```\n",
      "google4495658_student@cloudshell:~ (qwiklabs-gcp-85d215d05620f509)$ gsutil acl ch -u AllUsers:R gs://qwiklabs-gcp-85d215d05620f509/*\n",
      "```\n",
      "\n",
      "We now need to create a Vision API request in a json file. We'll use Nano. In the Shell :\n",
      "\n",
      "```\n",
      "google4495658_student@cloudshell:~ (qwiklabs-gcp-85d215d05620f509)$ nano request.json\n",
      "```\n",
      "\n",
      "It will open a nano editor. Paste the following code, and make sure to change the name of your bucket :\n",
      "\n",
      "```\n",
      "{\n",
      "    \"requests\": [\n",
      "        {\n",
      "            \"image\": {\n",
      "                \"source\": {\n",
      "                    \"gcsImageUri\": \"gs://qwiklabs-gcp-85d215d05620f509/cirrus.png\"\n",
      "                }\n",
      "            },\n",
      "            \"features\": [\n",
      "                {\n",
      "                    \"type\": \"LABEL_DETECTION\",\n",
      "                    \"maxResults\": 10\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "To exit the editor, type CTRL + X. If you look at the files present, it should look like this :\n",
      "\n",
      "```\n",
      "google4495658_student@cloudshell:~ (qwiklabs-gcp-85d215d05620f509)$ ls\n",
      "README-cloudshell.txt  request.json\n",
      "```\n",
      "\n",
      "We call the vision API with curl :\n",
      "\n",
      "```\n",
      "curl -s -X POST -H \"Content-Type: application/json\" --data-binary @request.json  https://vision.googleapis.com/v1/images:annotate?key=<YOUR_KEY>\n",
      "```\n",
      "\n",
      "Replace <YOUR_KEY> by the API key you copied earlier. Your response should have the following format :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_136.jpg)\n",
      "\n",
      "The output classes are `sky`, `blue`, `clouds`... It's true. But what if we want a model that identifies the exact types of clouds? Like cirrus, cumulonimbus or cumulus? We can fine-tune a model with AutoML Vision.\n",
      "\n",
      "# AutoML Vision\n",
      "\n",
      "We first need to set AutoML vision. Go to : [https://cloud.google.com/automl/ui/vision](https://cloud.google.com/automl/ui/vision). Allow the log-in.\n",
      "\n",
      "Specify the name of your project :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_137.jpg)\n",
      "\n",
      "Then, on the setup page, click on \"Go to billing\", and \"Go to linked billing account\" :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_138.jpg)\n",
      "\n",
      "Then, click on \"Setup now\". It will run for a few minutes. At some point, you'll be redirected to AutoML Vision page :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_139.jpg)\n",
      "\n",
      "On GCP Storage, you should notice a second storage bucket with the extension `vcm`.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_140.jpg)\n",
      "\n",
      "Set this bucket as an environment variable.\n",
      "\n",
      "```\n",
      "export BUCKET=qwiklabs-gcp-85d215d05620f509-vcm\n",
      "```\n",
      "\n",
      "Using the gsutil command line utility for Cloud Storage, we can copy the training images into our bucket :\n",
      "\n",
      "```\n",
      "gsutil -m cp -r gs://automl-codelab-clouds/* gs://qwiklabs-gcp-85d215d05620f509-vcm\n",
      "```\n",
      "\n",
      "We now have 3 folders :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_141.jpg)\n",
      "\n",
      "Now that your training data in Cloud Storage, you need a way for AutoML Vision to find them. To do this you'll create a CSV file where each row contains a URL to a training image and the associated label for that image. The CSV file has already been created before the lab.\n",
      "\n",
      "We first needd to copy the file :\n",
      "\n",
      "```\n",
      "gsutil cp gs://automl-codelab-metadata/data.csv .\n",
      "```\n",
      "\n",
      "Then run the following command to update the CSV with the files in our project:\n",
      "\n",
      "```\n",
      "sed -i -e \"s/placeholder/qwiklabs-gcp-85d215d05620f509-vcm/g\" ./data.csv\n",
      "```\n",
      "\n",
      "And upload this file to our Cloud Storage bucket:\n",
      "\n",
      "```\n",
      "gsutil cp ./data.csv gs://qwiklabs-gcp-85d215d05620f509-vcm\n",
      "```\n",
      "\n",
      "The bucket now contains an additional file : \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_142.jpg)\n",
      "\n",
      "Back on AutoML Vision UI, click on New Dataset, and add the link to the dataset we just created :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_143.jpg)\n",
      "\n",
      "Leave the other parameters to their default value, and click on Create Dataset. The images will then load into AutoML :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_144.jpg)\n",
      "\n",
      "After a few minutes, you'll be able to inspect the images :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_145.jpg)\n",
      "\n",
      "And edit the label if needed :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_146.jpg)\n",
      "\n",
      "Once you're ready, click on the \"Train\" tab, and on \"Start Training\" :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_147.jpg)\n",
      "\n",
      "You'll have access to 1 hour of free training at first :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_151.jpg)\n",
      "\n",
      "After a few minutes, once the training is over, click on \"Evaluate\". You'll have access to the evaluation report :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_148.jpg)\n",
      "\n",
      "You can now make predictions using the Predict Tab :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_149.jpg)\n",
      "\n",
      "And upload new images on which you'd like to make the prediction :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_150.jpg)\n",
      "\n",
      "---\n",
      "title: Launch and access an AWS EC2 Cluster\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"GDelt Project\"\n",
      "---\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ec2_head.jpg)\n",
      "\n",
      "Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers. This tutorial will help you to get started with AWS EC2.\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "## Configure the instance\n",
      "\n",
      "a. From AWS Console, click on “EC2” and \"Launch instance\" :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/EC2_launch_instances.jpg)\n",
      "\n",
      "b. Then, select the Amazon Machine Image (AMI) which defines your boot OS. For this tutorial, I chose ``` Ubuntu Server 18.04 LTS (HVM), SSD Volume Type ```  image :\n",
      "![image](https://maelfabien.github.io/assets/images/EC2_launch_Ubuntu.jpg)\n",
      "\n",
      "c. Next step, select the type of instance you would like to launch. Make sure to select an instance that matches your needs. t2-micro are perfect for test phases. When deploying services in production, you might want to focus on m5a.xlarge for example. Take a look at the following link for the detailed hourly cost of each machine : <span style=\"color:blue\">[https://aws.amazon.com/fr/ec2/pricing/reserved-instances/pricing/](https://aws.amazon.com/fr/ec2/pricing/reserved-instances/pricing/)</span> \n",
      "\n",
      "Depending on the complexity of your project and the resiliency requirements, you might want to add several instances to your cluster. Here, for example, we'll add 8 instances.\n",
      "![image](https://maelfabien.github.io/assets/images/EC2_number.jpg)\n",
      "\n",
      "d. At the next step, you'll be invited to add storage. Some Big Data projects imply several To. of data. For the sake of simplicity, we'll use the standard storage of 8Gb. The storage is typically paid by Gb stored monthly.\n",
      "\n",
      "e. If needed, add tags. Else, just move on to the next step.\n",
      "\n",
      "f. The security group allows your instances to communicate with other instances within the same security group. The communication between the slaves and the master is essential to transfer data typically.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/EC2_security_group.jpg)\n",
      "\n",
      "This specific configuration allows SSH from anywhere. You might want to change this setting when working on real data.\n",
      "\n",
      "We're almost done. Just review your previous step, and click \"Launch\".\n",
      "\n",
      "## Create a key pair\n",
      "\n",
      "a. “Amazon uses public-key cryptography to encrypt and decrypt login information. Public–key cryptography uses a public key to encrypt a piece of data, such as a password, then the recipient uses the private key to decrypt the data. The public and private keys are known as a key pair”. At this step, simply create a key pair and make sure to save it!\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/EC2_key_pair.jpg)\n",
      "\n",
      "If you are working on Windows or Linux, I think the extension would be something like ``` .txt ```. Make sure to change it to ``` .pem ``` to identify your file as a key pair.\n",
      "\n",
      "b. All your instances are now being initialized. For clarity, change the name of your instances accordingly to their role.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/EC2_change_name.jpg)\n",
      "\n",
      "## SSH Connection\n",
      "\n",
      "a. Protect your key pair from accidental overwriting \n",
      "\n",
      "Open your terminal. Once you are in the folder that contains your key pair, copy-paste this code :\n",
      "\n",
      "``` bash \n",
      "chmod 400 Cluster_test_Key_Pair.pem\n",
      "```\n",
      "\n",
      "b. Try a connexion to one of your instances\n",
      "Copy the public DNS of Master 1 from your AWS Console : \n",
      "![image](https://maelfabien.github.io/assets/images/EC2_copy_DNS.jpg)\n",
      "\n",
      "Then, execute the following commande bellow :\n",
      "``` bash\n",
      "ssh -i \"<path to your keyPair directory>/Cluster_test_Key_Pair.pem\" ubuntu@<copy the public DNS> \n",
      "```\n",
      "\n",
      "You should now be connected to your newly created EC2 cluster!\n",
      "\n",
      "> In the next article, we'll focus on installing Apache-Cassandra on an AWS EC2 Cluster\n",
      "---\n",
      "title: Basic Time Series Forecasting\n",
      "layout: post\n",
      "tags: [statistics]\n",
      "subtitle : \"Time Series\"\n",
      "---\n",
      "\n",
      "In this article, we'll introduce the key concepts of time series forecasting. We will be using data from monthly anti-diabetic sales index on the Australian market between 1991 and 2008.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "The data I'm using can be downloaded from [https://maelfabien.github.io/assets/files/file.csv](https://maelfabien.github.io/assets/files/file.csv).\n",
      "\n",
      "Start by importing the following packages :\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn import preprocessing\n",
      "```\n",
      "\n",
      "Then, load the data :\n",
      "```python\n",
      "df = pd.read_csv('file.csv', parse_dates=['date'], index_col='date')\n",
      "df.head()\n",
      "```\n",
      "\n",
      "| value | date |\n",
      "| 1991-07-01 |  3.526591 | \n",
      "| 1991-08-01 | 3.180891 | \n",
      "| 1991-09-01 | 3.252221 | \n",
      "| 1991-10-01 |  3.611003 | \n",
      "| 1991-11-01 |  3.565869 | \n",
      "\n",
      "\n",
      "# I. The process behind forecasting\n",
      "\n",
      "The key steps behind time series forecasting are the following :\n",
      "\n",
      "![images](https://maelfabien.github.io/assets/images/ts2_2.jpg)\n",
      "\n",
      "- Step 1: Make the Time Series Stationary (we'll cover that in this article)\n",
      "- Step 2: Split the Time Series into a train and a test to fit future models and compare model performance. If we are in prediction, we take the whole data as train and apply no test.\n",
      "- Step 3: Rolling window forecasting. We fit the chosen model on all data available and forecast the next value.\n",
      "- Step 4: We attach the prediction of the previous step to the observations, and re-fit the model on all available data to make a prediction\n",
      "- Step 5: Once we finished our rolling window and have an array of predictions, we just need to apply the inverse transformation that we applied for the stationarity transformations.\n",
      "- Step 6: We can assess the performance of a model by applying simple metrics such as MSE.\n",
      "\n",
      "# II. Is the series stationary?\n",
      "\n",
      "The first question you should ask is: Is the series stationary? There are several ways to check this :\n",
      "- by looking at the plots\n",
      "- by running an ADFuller test\n",
      "\n",
      "We can first plot the time series  :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(df['value'], label=\"value\")\n",
      "plt.title(\"Monthly anti-diabetic sales index on the Australian market between 1991 and 2008\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![images](https://maelfabien.github.io/assets/images/ts2_0.jpg)\n",
      "\n",
      "As we mentioned in the article \"Key Concepts in Time Series\", it is really important that your series is stationary before applying any model on it. The time series does not look stationary at that point. But is it really the case? We can run an ADFuller test to confirm this statement. \n",
      "\n",
      "```python\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "\n",
      "result = adfuller(df['value'].ffill(0))\n",
      "print('ADF Statistic: %f' % result[0])\n",
      "print('p-value: %f' % result[1])\n",
      "print('Critical Values:')\n",
      "for key, value in result[4].items():\n",
      "    print('\\t%s: %.3f' % (key, value))\n",
      "```\n",
      "\n",
      "The output is :\n",
      "\n",
      "```python\n",
      "ADF Statistic: 3.145186\n",
      "p-value: 1.000000\n",
      "Critical Values:\n",
      "    1%: -3.466\n",
      "    5%: -2.877\n",
      "    10%: -2.575\n",
      "```\n",
      "\n",
      "> The more negative the AD-Fuller, the more likely we are to reject the null hypothesis and the more likely we are to have a stationary dataset.\n",
      "\n",
      "As part of the output, we get a look-up table to help determine the ADF statistic. We can see that our statistic value of 3.14 is way higher than the value of -2.5 at 10%.\n",
      "\n",
      "We cannot reject the null hypothesis, meaning that the process has a unit root and the time series is not stationary according to ADF test.\n",
      "\n",
      "Let's start our journey towards a stationary time series :)\n",
      "\n",
      "# III.  Remove Heteroskedasticity\n",
      "\n",
      "The first thing you'll notice when looking at the time series above it the fact that the series has an increasing variance. It might be a real issue since time series are not good at predicting increasing variance over time.\n",
      "\n",
      "To make the time series stationary, we need to apply transformations to it. Usual transformations to remove heteroskedasticity (or increasing variance over time) include :\n",
      "- log\n",
      "- square root\n",
      "- ...\n",
      "\n",
      "To make sure that the log transformation would make sense, just plot it :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(np.log(df['value']))\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![images](https://maelfabien.github.io/assets/images/ts2_1.jpg)\n",
      "\n",
      "Applying a log transform is definitely a good idea here (but it's by far not always the case). Moreover, the log transform comes along with some advantages in terms of interpretation :\n",
      "- Trend measured in natural-log units ≈ percentage growth\n",
      "- Errors measured in natural-log units ≈ percentage errors\n",
      "\n",
      "# IV.  Remove the trend \n",
      "\n",
      "The next step is to remove the trend from the series. Do you see any kind of trend that could fit? \n",
      "\n",
      "Well, the linear trend is pretty much what we're looking for! Other popular trends including exponential or logarithmic trends for example.\n",
      "\n",
      "```python\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "X = np.array(range(len(np.log(df['value']))))\n",
      "y = np.log(df['value']).ffill(axis=0)\n",
      "\n",
      "# Linear Trend\n",
      "reg = LinearRegression().fit(X.reshape(-1,1), y)\n",
      "pred_lin = reg.predict(X.reshape(-1,1))  \n",
      "\n",
      "# Logarithmic trend\n",
      "a_1,b_1 = np.polyfit(np.log(X+1), y, 1)\n",
      "pred_log = a_1 * np.log(X+1) + b_1\n",
      "\n",
      "# Exponential trend\n",
      "a_2,b_2 = np.polyfit(X+1, np.log(y), 1)\n",
      "pred_exp = np.exp(b_2) + np.exp( (X+1) * a_2)\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(np.log(df['value']), label=\"Index\")\n",
      "plt.plot(df['value'].index, pred_lin, label=\"linear trend\")\n",
      "plt.plot(df['value'].index, pred_log, label=\"log trend\")\n",
      "plt.plot(df['value'].index, pred_exp, label=\"exp trend\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![images](https://maelfabien.github.io/assets/images/ts2_3.jpg)\n",
      "\n",
      "The linear trend is the best fit in this case. Our new series now becomes :\n",
      "\n",
      "`log(Initial series) - linear trend`\n",
      "\n",
      "To rebuild the final series, you would need :\n",
      "\n",
      "`exp(value) + linear trend at that index`\n",
      "\n",
      "Let's define our new series :\n",
      "\n",
      "```python\n",
      "series = np.log(df['value'])-pred_lin\n",
      "```\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(series)\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![images](https://maelfabien.github.io/assets/images/ts2_4.jpg)\n",
      "\n",
      "# V.  Remove Seasonality \n",
      "\n",
      "Seasonality in time series denotes a recurrent pattern over time. When a series is seasonal, it means that value at a given point in the past is really close to the value we observe today. \n",
      "\n",
      "In the graph above, it seems to be the case. There seems to be a yearly pattern in the way this time series evolves.\n",
      "\n",
      "The seasonality might take the form of a large peak, a sinusoidal curve...\n",
      "\n",
      "There are mainly two ways to model seasonality in time series :\n",
      "- identify patterns that look sinusoidal for example, and fit the right parameters\n",
      "- the easiest option to remove the trend is to compute the first difference. For example, if there is a yearly seasonality, we can take $ y_t $ - $ y_{t-12} $ (since the measures are made monthly)\n",
      "\n",
      "The most common way is the difference transformation. We subtract the value 12 months ago to the value of today.\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(series - series.shift(12))\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "This way, all we have left to forecast is how different we will be from the same point in time 12 months ago. It makes our whole computations easier.\n",
      "\n",
      "![images](https://maelfabien.github.io/assets/images/ts2_5.jpg)\n",
      "\n",
      "We have removed most of the trend here, and remain with a stationary series. To make sure that our series is stationary, we can look at the plot: There seems to be no recurrent pattern in the data, constant variance and mean, no trend... And we can compute the ADFuller test again!\n",
      "\n",
      "```python\n",
      "series_stationary = series - series.shift(12)\n",
      "result = adfuller(series_stationary.dropna())\n",
      "print('ADF Statistic: %f' % result[0])\n",
      "print('p-value: %f' % result[1])\n",
      "print('Critical Values:')\n",
      "for key, value in result[4].items():\n",
      "    print('\\t%s: %.3f' % (key, value))\n",
      "```\n",
      "\n",
      "And print the results :\n",
      "\n",
      "```python\n",
      "ADF Statistic: -5.214559\n",
      "p-value: 0.000008\n",
      "Critical Values:\n",
      "    1%: -3.467\n",
      "    5%: -2.878\n",
      "    10%: -2.575\n",
      "```\n",
      "\n",
      "The p-value is close to 0, and the ADF Statistic is below the 1% critical value. We reject the null hypothesis that the series has a unit root and is not stationary. The series is therefore stationary!\n",
      "\n",
      "# VI.  Model the time series\n",
      "\n",
      "In this article, I won't cover the details and the different models of time series forecasting. I'll cover that in future articles. I will pick a SARIMA model and try to make predictions.\n",
      "\n",
      "We first built the train and test sets :\n",
      "\n",
      "```python\n",
      "# ARIMA\n",
      "from statsmodels.tsa.arima_model import ARIMA\n",
      "size = int(len(series_stationary.dropna()) * 0.75)\n",
      "train, test = series_stationary.dropna()[0:size], series_stationary.dropna()[size:len(series_stationary.dropna())]\n",
      "test = test.reset_index()['value']\n",
      "history = [x for x in train]\n",
      "predictions = []\n",
      "```\n",
      "\n",
      "Then, to assess the performance of the model, we will make our model predict the next value using only real training value. In other words, we apply the rolling window, and at each step, we give the model once more observation to predict from instead of using the last predicted value. This is a common way to better assess the performance of a model.\n",
      "\n",
      "``` python\n",
      "for t in range(len(test)):\n",
      "    model = ARIMA(history, order=(1,1,1))\n",
      "    model_fit = model.fit(disp=0)\n",
      "    output = model_fit.forecast()\n",
      "    yhat = output[0]\n",
      "    predictions.append(yhat)\n",
      "    obs = test[t]\n",
      "    history.append(obs)\n",
      "```\n",
      "\n",
      "We can plot our prediction in front of the test :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(predictions, label=\"Prediction\")\n",
      "plt.plot(test, label=\"Series\")\n",
      "plt.title(\"ARIMA prediction\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![images](https://maelfabien.github.io/assets/images/ts2_6.jpg)\n",
      "\n",
      "To assess the performance of the model, simply use :\n",
      "\n",
      "```python\n",
      "from sklearn.metrics import mean_squared_error\n",
      "mean_squared_error(predictions, test)\n",
      "```\n",
      "\n",
      "# VII.  Build Predictions\n",
      "\n",
      "Alright, we now have all the elements to recompose the time series ! To make a prediction, we start by taking the value from 1 year ago :\n",
      "\n",
      "```python\n",
      "start_idx = 204\n",
      "end_idx = 250\n",
      "series = series_stationary.reset_index()['value']\n",
      "\n",
      "for i in range(start_idx, end_idx) :\n",
      "    series[i] =  series[i-12]\n",
      "```\n",
      "\n",
      "Then, add a linear trend effect :\n",
      "\n",
      "```python\n",
      "X_full = np.array(range(0, end_idx))\n",
      "pred_lin = reg.predict(X_full.reshape(-1,1))   \n",
      "```\n",
      "\n",
      "Then, add the prediction of the time series (I've chosen a SARIMA model here) :\n",
      "\n",
      "```\n",
      "history = [x for x in series[:start_idx]]\n",
      "predictions = list()\n",
      "\n",
      "for t in range(end_idx-start_idx):\n",
      "    model = SARIMAX(history, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
      "    model_fit = model.fit(disp=False)\n",
      "    output = model_fit.forecast()\n",
      "    yhat = output[0]\n",
      "    predictions.append(yhat)\n",
      "    history.append(yhat)\n",
      "```\n",
      "\n",
      "Then, we can plot the prediction :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(df.reset_index().value, label=\"Series\")\n",
      "plt.plot(np.exp(series[204:]+pred_lin[204:]) + np.array(predictions), label=\"Recompose + SARIMA\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![images](https://maelfabien.github.io/assets/images/ts2_7.jpg)\n",
      "\n",
      "You might now wonder what would have happened if we applied the model on non-stationary data. Well, let's try it out!\n",
      "\n",
      "```python\n",
      "# SARIMA example\n",
      "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
      "\n",
      "size = int(len(df['value'].dropna()) * 0.75)\n",
      "train, test = df['value'].dropna()[0:size], df['value'].dropna()[size:len(df['value'].dropna())]\n",
      "test = test.reset_index()['value']\n",
      "history = [x for x in train]\n",
      "predictions = list()\n",
      "\n",
      "for t in range(len(test)):\n",
      "    model = SARIMAX(history, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
      "    model_fit = model.fit(disp=False)\n",
      "    output = model_fit.forecast()\n",
      "    yhat = output[0]\n",
      "    predictions.append(yhat)\n",
      "    obs = test[t]\n",
      "    history.append(yhat)\n",
      "    \n",
      "history = [x for x in train]\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(np.concatenate([history, predictions]), label='Prediction')\n",
      "plt.plot(np.concatenate([history, test]), label='Test')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![images](https://maelfabien.github.io/assets/images/ts2_8.jpg)\n",
      "\n",
      "I hope this quick example convinced you of the impact of making your time series stationary.\n",
      "\n",
      "> **Conclusion**: I hope you found this article useful. Don't hesitate to drop a comment if you have a question.\n",
      "\n",
      "---\n",
      "title: Create an Auto-Encoder\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "Autoencoder is a type a neural network widely used for unsupervised dimension reduction. So, how does it work? What can it be used for? And how do we implement it in Python?\n",
      "\n",
      "The origins of autoencoders have been discussed, but one of the most likely origins of the autoencoder is a paper written in 1987 by Ballard, \"Modular Learning in Neural Networks\" which can be found [here](https://www.aaai.org/Papers/AAAI/1987/AAAI87-050.pdf).\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "## What is an autoencoder?\n",
      "\n",
      "An autoencoder is a special type of neural network architecture that can be used efficiently reduce the dimension of the input. It is widely used for images datasets for example.\n",
      "\n",
      "Let's consider an input image. The input will be sent into several hidden layers of a neural network. Those layers are used to compress the image into a smaller dimension, by reducing the dimensions of the layers as we move on. At some point, the input image will be encoded into a short code. \n",
      "\n",
      "On the other hand, we build new layers that will learn to decode the short code, to rebuild the initial image. We are now teaching a network to take an input image, reduce its dimension (encoding), and rebuild it on the other side (decoding).\n",
      "\n",
      "The network will learn by itself to gather the most important information in the short code.\n",
      "\n",
      "Therefore, all we need to do is to keep the encoding part of the model, and we have a great way to reduce the input dimension in an unsupervised way!\n",
      "\n",
      "We can summarize the network architecture as follows :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/autoencoder_structure.jpg)\n",
      "\n",
      "With an image dataset, the layers that are usually used are the following :\n",
      "- convolution layers\n",
      "- activation layers\n",
      "- max-pooling layers\n",
      "- upsampling layers\n",
      "\n",
      "Otherwise, with numerical problems, dense layers are simple to use. \n",
      "\n",
      "\"If linear activations are used, or only a single sigmoid hidden layer, then the optimal solution to an autoencoder is strongly related to principal component analysis (PCA). The weights of an autoencoder with a single hidden layer of size `p` (where `p` is less than the size of the input) span the same vector subspace as the one spanned by the first `p` principal components, and the output of the autoencoder is an orthogonal projection onto this subspace. The autoencoder weights are not equal to the principal components and are generally not orthogonal, yet the principal components may be recovered from them using the singular value decomposition\" (Wikipedia)\n",
      "\n",
      "## Variations \n",
      "\n",
      "Autoencoder can also be used for :\n",
      "\n",
      "1. Denoising autoencoder\n",
      "Take a partially corrupted input image, and teach the network to output the de-noised image.\n",
      "\n",
      "2. Sparse autoencoder\n",
      "In a Sparse autoencoder, there are more hidden units than inputs themselves, but only a small number of the hidden units are allowed to be active at the same time. This makes the training easier.\n",
      "\n",
      "3. Concrete autoencoder\n",
      "A concrete autoencoder is an autoencoder designed to handle discrete features. In the latent space representation, the features used are only user-specifier.\n",
      "\n",
      "4. Contractive autoencoder\n",
      "Contractive autoencoder adds a regularization in the objective function so that the model is robust to slight variations of input values.\n",
      "\n",
      "5. Variational autoencoder (VAE)\n",
      "Variational autoencoders (VAEs) don't learn to morph the data in and out of a compressed representation of itself. Instead, they learn the parameters of the probability distribution that the data came from. These types of autoencoders have much in common with latent factor analysis.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vae.jpg)\n",
      "\n",
      "## Create an autoencoder in Python\n",
      "\n",
      "For this example, we'll use the MNIST dataset. Start by importing the following packages :\n",
      "\n",
      "```python\n",
      "### General Imports ###\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "### Autoencoder ###\n",
      "import tensorflow as tf\n",
      "import tensorflow.keras\n",
      "\n",
      "from tensorflow.keras import models, layers\n",
      "from tensorflow.keras.models import Model, model_from_json\n",
      "\n",
      "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, UpSampling2D, Input\n",
      "\n",
      "from tensorflow.keras.datasets import mnist\n",
      "```\n",
      "\n",
      "Then, load and reshape the data :\n",
      "\n",
      "```python\n",
      "(X_train, _), (X_test, _) = mnist.load_data()\n",
      "shape_x = 28\n",
      "shape_y = 28\n",
      "\n",
      "X_train = X_train.astype('float32') / 255.\n",
      "X_test = X_test.astype('float32') / 255.\n",
      "\n",
      "X_train = X_train.reshape(-1,shape_x,shape_y,1)\n",
      "X_test = X_test.reshape(-1,shape_x,shape_y,1)\n",
      "```\n",
      "\n",
      "Now, let's build the model!\n",
      "\n",
      "```\n",
      "input_img = Input(shape=(shape_x, shape_y, 1))\n",
      "\n",
      "# Ecoding\n",
      "x = Conv2D(16, (3, 3), padding='same', activation='relu')(input_img)\n",
      "x = MaxPooling2D(pool_size=(2,2), padding='same')(x)\n",
      "x = Conv2D(1,(3, 3), padding='same', activation='relu')(x)\n",
      "encoded = MaxPooling2D(pool_size=(2,2), padding='same')(x)\n",
      "\n",
      "# Decoding\n",
      "x = Conv2D(1,(3, 3), padding='same', activation='relu')(encoded)\n",
      "x = UpSampling2D((2, 2))(x)\n",
      "x = Conv2D(16,(3, 3), padding='same', activation='relu')(x)\n",
      "x = UpSampling2D((2, 2))(x)\n",
      "x = Conv2D(1,(3, 3), padding='same')(x)\n",
      "\n",
      "decoded = Activation('linear')(x)\n",
      "```\n",
      "\n",
      "I chose to use linear activation since we're talking about pixels values. You might also use sigmoid as the final activation function. You can visualize what is going on using the `model.summary()` function as follows :\n",
      "\n",
      "```python\n",
      "autoencoder = Model(input_img, decoded)\n",
      "autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')\n",
      "autoencoder.summary()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/auto.jpg)\n",
      "\n",
      "Using the hidden layers, we send the input image into a much lowe dimension :\n",
      "\n",
      "` 7*7*1 = 49 `\n",
      "\n",
      "Instead of :\n",
      "\n",
      "` 28*28*1 = 784 `\n",
      "\n",
      "Now, let's train the model! We don't need any `y_train` here, both the input and the output will be the train images.\n",
      "\n",
      "```python\n",
      "autoencoder.fit(X_train, X_train, nb_epoch = 15, batch_size = 64, validation_split = 0.1)\n",
      "```\n",
      "\n",
      "Save the weights of the autoencoder :\n",
      "\n",
      "```python\n",
      "# Save autoencoder weight\n",
      "json_string = autoencoder.to_json()\n",
      "autoencoder.save_weights('autoencoder.h5')\n",
      "open('autoencoder.h5', 'w').write(json_string)\n",
      "```\n",
      "\n",
      "We can build an encoding model using the first part of the model :\n",
      "\n",
      "```python\n",
      "encoder = Model(inputs = input_img, outputs = encoded)\n",
      "```\n",
      "\n",
      "We can get the encoded input with :\n",
      "\n",
      "```python\n",
      "X_train_enc = encoder.predict(X_train)\n",
      "```\n",
      "\n",
      "## Visualize the output\n",
      "\n",
      "We can simply visualize the output using the `predict` function of the autoencoding model :\n",
      "\n",
      "```python\n",
      "encoded_imgs = encoder.predict(X_test)\n",
      "decoded_imgs = autoencoder.predict(X_test)\n",
      "```\n",
      "\n",
      "To display the images, we can simply plot the entry image and the decoded image :\n",
      "```\n",
      "n = 10  \n",
      "plt.figure(figsize=(20, 4))\n",
      "\n",
      "for i in range(n):\n",
      "    # display original\n",
      "    ax = plt.subplot(3, n, i + 1)\n",
      "    plt.imshow(x_test[i].reshape(28, 28))\n",
      "    plt.gray()\n",
      "    ax.get_xaxis().set_visible(False)\n",
      "    ax.get_yaxis().set_visible(False)\n",
      "\n",
      "    # Encoded images\n",
      "    ax = plt.subplot(3, n, i + 1 + n)\n",
      "    plt.imshow(encoded_imgs[i].reshape(7, 7))\n",
      "    plt.gray()\n",
      "    ax.get_xaxis().set_visible(False)\n",
      "    ax.get_yaxis().set_visible(False)\n",
      "    \n",
      "    # display reconstruction\n",
      "    ax = plt.subplot(3, n, i + 1 + 2*n)\n",
      "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
      "    plt.gray()\n",
      "    ax.get_xaxis().set_visible(False)\n",
      "    ax.get_yaxis().set_visible(False)\n",
      "\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/enc_dec.jpg)\n",
      "\n",
      "The first row is the input image. The middle row is the encoded image. The output row is the decoded image.\n",
      "\n",
      "Our model remains quite simple, and we should add some epochs to reduce the noise of the reconstituted image. \n",
      "\n",
      "## Dense version\n",
      "\n",
      "We have just made a deep convolutional autoencoder. Another version one could think of is to treat the input images as flat images and build the autoencoder using Dense layers.\n",
      "\n",
      "```python\n",
      "input_img = Input(shape=(`shape_x * shape_y,))\n",
      "encoded = Dense(128, activation='relu')(input_img)\n",
      "encoded = Dense(64, activation='relu')(encoded)\n",
      "encoded = Dense(32, activation='relu')(encoded)\n",
      "\n",
      "decoded = Dense(64, activation='relu')(encoded)\n",
      "decoded = Dense(128, activation='relu')(decoded)\n",
      "decoded = Dense(shape_x * shape_y, activation='sigmoid')(decoded)\n",
      "```\n",
      "\n",
      "The Github repository of this article can be found [here](https://github.com/maelfabien/Machine_Learning_Tutorials).\n",
      "\n",
      "> **Conclusion **: I hope this quick introduction to autoencoder was clear. Don't hesitate to drop a comment if you have any question.\n",
      "\n",
      "---\n",
      "title: Graph Embedding\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Graph Analysis and Graph Learning\"\n",
      "---\n",
      "\n",
      "In the previous article, we saw ways to learn in graphs, i.e. make node labeling and edge prediction. One of the limitations of graphs remains the absence of vector features. Just like in NLP, we face structured data. But just like in NLP, we can learn an embedding of the graph! There are several levels of embedding in a graph :\n",
      "- Embedding graph components (nodes, edges, features…) ([Node2Vec](https://snap.stanford.edu/node2vec/))\n",
      "- Embedding sub-parts of a graph or a whole graph ([Graph2Vec](https://arxiv.org/abs/1707.05005))\n",
      "\n",
      "After learning an embedding, it can be used as features for several tasks :\n",
      "- classification\n",
      "- recommender systems\n",
      "- ...\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "For what comes next, open a Jupyter Notebook and import the following packages :\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import random\n",
      "import networkx as nx\n",
      "from IPython.display import Image\n",
      "import matplotlib.pyplot as plt\n",
      "```\n",
      "\n",
      "If you have not already installed the `networkx` package, simply run :\n",
      "\n",
      "```bash\n",
      "pip install networkx\n",
      "```\n",
      "\n",
      "The following articles will be using the latest version  `2.x` of  `networkx`. NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\n",
      "\n",
      "To illustrate the different concepts we'll cover and how it applies to graphs we'll take the Karate Club example. This graph is present in the `networkx` package. It represents the relations of members of a Karate Club. However, due to a disagreement of the founders of the club, the club has recently been split in two. We'll try to illustrate this event with graphs. \n",
      "\n",
      "First, load and plot the graph :\n",
      "\n",
      "```python\n",
      "n=34\n",
      "m = 78\n",
      "G_karate = nx.karate_club_graph()\n",
      "\n",
      "pos = nx.spring_layout(G_karate)\n",
      "nx.draw(G_karate, cmap = plt.get_cmap('rainbow'), with_labels=True, pos=pos)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/karate.jpg)\n",
      "\n",
      "# Embedding process\n",
      "\n",
      "If you have some time, check out the [full article](https://towardsdatascience.com/node2vec-embeddings-for-graph-data-32a866340fef) on the embedding process by the author of the `node2vec` library.\n",
      "\n",
      "The embeddings are learned in the same way as word2vec’s skip-gram embeddings are learned, using a skip-gram model. The question is, how can we generate the input corpus for Node2Vec? The data are much more complex, i.e. (un)directed, (un)weighted, (a)cyclic... \n",
      "\n",
      "To generate the corpus, we use **random walks** sampling strategy :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/g_embed.jpg)\n",
      "\n",
      "We can specify the number of walks to run and the length of the walks. \n",
      "\n",
      "# Node Embedding\n",
      "\n",
      "We will focus on the embedding of graph components. Several approaches are possible to embed a node or an edge. For example, [DeepWalk](http://www.perozzi.net/projects/deepwalk/) uses short random walks to learn representations for edges in graphs. We will focus on Node2Vec, a paper that was published by Aditya Grover and Jure Leskovec from Stanford University in 2016.\n",
      "\n",
      "The main difference between DeepWalk and Node2Vec is that in Node2Vec, when developing a random walk, there is a certain probability to go back to the previous node.\n",
      "\n",
      "According to the authors: \"node2vec is an algorithmic framework for representational learning on graphs. Given any graph, it can learn continuous feature representations for the nodes, which can then be used for various downstream machine learning tasks.\"\n",
      "How does Node2Vec work? The model learns low-dimensional representations for nodes by optimizing a neighborhood preserving objective, using random walks. \n",
      "\n",
      "The code of Node2Vec is available on [GitHub](https://github.com/eliorc/node2vec).\n",
      "\n",
      "To install the package, simply run:  `pip install node2vec`\n",
      "\n",
      "Then, in your notebook, we'll embed the Karate graph :\n",
      "\n",
      "```python\n",
      "from node2vec import Node2Vec\n",
      "```\n",
      "\n",
      "Then, pre-compute the probabilities and generate walks :\n",
      "\n",
      "```python\n",
      "node2vec = Node2Vec(G_karate, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
      "```\n",
      "\n",
      "We can then embed the nodes :\n",
      "\n",
      "```python\n",
      "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
      "```\n",
      "\n",
      "To get the vector of a node, say node '2', use get_vector :\n",
      "\n",
      "```python\n",
      "model.wv.get_vector('2')\n",
      "```\n",
      "\n",
      "The outcome has the following form :\n",
      "\n",
      "```python\n",
      "array([-0.03066591,  0.52942747, -0.14170371...\n",
      "```\n",
      "\n",
      "It has a length of 64 since we defined the dimension as 64 above. What can we do with this embedding? One of the first options is for example to identify the most similar node!\n",
      "\n",
      "```python\n",
      "model.wv.most_similar('2')\n",
      "```\n",
      "\n",
      "It returns a list of the most similar nodes and the corresponding probabilities :\n",
      "\n",
      "```\n",
      "[('3', 0.6494477391242981),\n",
      "('13', 0.6262941360473633),\n",
      "('7', 0.6137452721595764),\n",
      "...\n",
      "```\n",
      "\n",
      "If the nodes have labels, we can train an algorithm based on the embedding and attach a label. This could be a way to node labeling.\n",
      "\n",
      "# Edge Embedding\n",
      "\n",
      "Edges can also be embedded, and the embedding can be further used for classification.\n",
      "\n",
      "```python\n",
      "from node2vec.edges import HadamardEmbedder\n",
      "edges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n",
      "```\n",
      "\n",
      "Then, retrieve the vectors by specifying the name of the 2 linked nodes :\n",
      "```python\n",
      "edges_embs[('1', '2')]\n",
      "```\n",
      "\n",
      "Which heads :\n",
      "\n",
      "```python\n",
      "array([-8.1781112e-03, -1.8037426e-01,  4.9451444e-02...\n",
      "```\n",
      "\n",
      "Again, we can retrieve the most similar edge, which can be used for missing edges prediction for example :\n",
      "\n",
      "```python\n",
      "edges_kv = edges_embs.as_keyed_vectors()\n",
      "edges_kv.most_similar(str(('1', '2')))\n",
      "```\n",
      "\n",
      "This heads :\n",
      "\n",
      "```python\n",
      "[(\"('2', '21')\", 0.8726599216461182),\n",
      "(\"('2', '7')\", 0.856759786605835),\n",
      "(\"('2', '3')\", 0.8566413521766663),\n",
      "...\n",
      "```\n",
      "\n",
      "# Graph Embedding\n",
      "\n",
      "There are also ways to embed a graph or a sub-graph directly. Graph embedding techniques take graphs and embed them in a lower-dimensional continuous latent space before passing that representation through a machine learning model.\n",
      "\n",
      "An approach has been developed in the [Graph2Vec](https://arxiv.org/abs/1707.05005) paper and is useful to represent graphs or sub-graphs as vectors, thus allowing graph classification or graph similarity measures for example. I won't dive deeper into this technique, but feel free to check the Github of this project :\n",
      "\n",
      "[https://github.com/benedekrozemberczki/graph2vec](https://github.com/benedekrozemberczki/graph2vec)\n",
      "\n",
      "\n",
      "It is based on the same idea than doc2vec skip-gram network.\n",
      "\n",
      "To run the embedding, it's as easy as :\n",
      "```python\n",
      "python src/graph2vec.py --input-path data_folder/ --output-path output.csv\n",
      "```\n",
      "\n",
      "> **Conclusion** : I hope that this article on graph embedding was helpful. Don't hesitate to drop a comment if you have any question.\n",
      "\n",
      "Resources :\n",
      "- [Graph Embeding Summary](https://towardsdatascience.com/graph-embeddings-the-summary-cc6075aba007)\n",
      "- [graph2vec](https://github.com/benedekrozemberczki/graph2vec)\n",
      "- [noded2vec](https://github.com/eliorc/node2vec)\n",
      "\n",
      "---\n",
      "title: Introduction to Cloud Dataproc - Week 1 Module 1\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Road to Google Cloud Platform Certification\"\n",
      "---\n",
      "\n",
      "There are many sources of data :\n",
      "- data that you analyze\n",
      "- **data you collect but don't analyze**\n",
      "- data you could collect but don't \n",
      "- data from 3rd parties\n",
      "\n",
      "The main reason behind collecting data but not analyzing it is the fact that those data are often unstructured. It could include :\n",
      "- free form text\n",
      "- images\n",
      "- calls from call center\n",
      "- ...\n",
      "\n",
      "We call unstructured data all the data that are not suited for the specific type of job we want to run on it. GIS data might be structured for Google Maps, but it's unstructured data for a relational database for example. Unstructured data might have a schema, a partial schema or no schema at all.\n",
      "\n",
      "About 90% of enterprise data is unstructured. This is the way ML models for unstructured data are deployed on GCP :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_152.jpg)\n",
      "\n",
      "Those models, that are really data-intensive, and might include Cloud Vision or Speech-To-Text are trained by Google, and trained models are then uploaded to Cloud ML Engine using REST API.\n",
      "\n",
      "# Why Cloud DataProc ?\n",
      "\n",
      "When working with BigData, an efficient Hadoop-based architecture can be built on Cloud DataProc. Cloud DataProc is useful for Tera Bytes or Peta Bytes data levels. But how much is 1 PetaByte of data ?\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_153.jpg)\n",
      "\n",
      "Some businesses will never need such capacities, and running Spark jobs would be more than enough. On the other hand, here's also what on PetaByte is :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_154.jpg)\n",
      "\n",
      "How do you scale to such large amount of data ?\n",
      "- Vertical Scaling : more efficient single machines\n",
      "- Horizontal Scaling : more machines running together\n",
      "\n",
      "MapReduce arised when Google tried to index every single webpage on the Internet back in 2004.  It relies on 3 main steps :\n",
      "- Map\n",
      "- Shuffle \n",
      "- Reduce\n",
      "\n",
      "Hadoop is the Apache solution of MapReduce. However, Hadoop has a major limitation, since the way design the job needs to be tuned for every job we must run. Apache Spark is a powerful and flexible way to process large datasets. Spark uses declarative programming : you tell the program what you want, and it figures out a way to make it happen. Spark can be used in Python, Java... It also comes with a ML library, called SparkML, and Spark SQL offers a SQL implementation on top of Spark.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_155.jpg)\n",
      "\n",
      "A typical Spark and Hadoop deployment involves :\n",
      "- setting up the hardware and the Operating System software (OSS)\n",
      "- optimizing the OSS\n",
      "- debug the OSS\n",
      "- process data\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_156.jpg)\n",
      "\n",
      "When deploying a Hadoop cluster on-prem, lots of time is spent on administration and operational issues (monitoring, scaling, reliability...). One also needs to scale the cluster according to the utilization we make of it. \n",
      "\n",
      "DataProc is a manages service to run Hadoop on GCP. The different options to run Hadoop Clusters are the following :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_157.jpg)\n",
      "\n",
      "A typical Hadoop Dataproc deployment requires just 90 seconds before the cluster is up S running ! Cloud DataProc also supports Hadoop, Pig, Hive and Spark, and has high-level APIs for job submissions. It also offers connectors to BigTable, BigQuery and Cloud Storage.\n",
      "\n",
      "# Architecture and machine types \n",
      "\n",
      "DataProc uses Google Cloud machines and :\n",
      "- Google networking for high performance and encryption\n",
      "- Cloud Identity And Access Management for extended security\n",
      "- Cloud Datalab for jupyter-like notebooks\n",
      "\n",
      "The Cloud Dataproc architecture looks like this :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_161.jpg)\n",
      "\n",
      "In terms on storage, one can store the processed data :\n",
      "- on Cluster with HDFS\n",
      "- off-cluster persistant storage with Cloud Storage\n",
      "- in Big Query for interactive analysis\n",
      "- in Cloud BigTable for structured storage\n",
      "\n",
      "Dataproc is also integrated with Stackdriver logging to simplify monitoring. We can customize the persistent worker nodes to match the needs of our job.\n",
      "\n",
      "There are several machine types available. In the schema below, each CPU logo represents a virtual CPU, and each memory icon is 3.75Gb :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_162.jpg)\n",
      "\n",
      "For scaling purposes, we should match the machine type with our workload. We can create custom machines using command line :\n",
      "\n",
      "\n",
      "```\n",
      "gcloud dataproc clusters create test-cluster /\n",
      "    --worker-machine-type custom-6-30720\n",
      "    --master-machine-type custom-6-23040\n",
      "```\n",
      "\n",
      "This command line creates a cluster where the workers have 6 CPUs for an overall 30 Gb (30*1024 = 30720) storage. The master has 6 CPUs and 22.5 Gb.\n",
      "\n",
      "To reduce the cost of an infrastructure, one can use preemptible VMs : these are VMs that are not used by Google and are available at a discount. However, if Google needs it as a persistent VM for another client, they can take it back with minimal warning. These VMs are around 80% cheaper than standard instances. Since the node can be lost at any time, you can lose processing progress. These VMs can therefore not be used for storage. It shoul be used when you have non-critical processing an huge clusters. Overall, one should keep in mind that having more than 50% of pre-emptible VMs for a job can overall increase costs due to the failing nodes. Preemptible VMs should be a way to complete the cluster to speed up the job and reduce costs. Dataproc manages the joins and leaves of preemptible instances, there is nothing to manage.\n",
      "\n",
      "---\n",
      "title: Face Detection\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "In this tutorial, we'll see how to create and launch a face detection algorithm in Python using OpenCV. We'll also add some features to detect eyes and mouth on multiple faces at the same time. This article will go through the most basic implementations of face detection including Cascade Classifiers, HOG windows and Deep Learning.\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "We'll cover face detection using :\n",
      "- Haar Cascade Classifiers using OpenCV\n",
      "- Histogram of Oriented Gradients using Dlib\n",
      "- Convolutional Neural Networks using Dlib\n",
      "\n",
      "# Introduction\n",
      "\n",
      "We'll be using OpenCV, an open source library for computer vision, written in C/C++, that has interfaces in C++, Python and Java. It supports Windows, Linux, MacOS, iOS and Android. Some of our work will also require using Dlib, a modern C++ toolkit containing machine learning algorithms and tools for creating complex software.\n",
      "\n",
      "## Requirements\n",
      "\n",
      "The first step is to install OpenCV. Run the following command line in your terminal :\n",
      "\n",
      "```python\n",
      "pip install opencv-python\n",
      "```\n",
      "Depending on your version, the file will be installed here :\n",
      "```python\n",
      "/usr/local/lib/python3.7/site-packages/cv2\n",
      "```\n",
      "If you have not yet installed Dlib, run the following command :\n",
      "\n",
      "```python\n",
      "pip install dlib\n",
      "```\n",
      "\n",
      "If you encounter some issues with Dlib, check [this article](https://www.pyimagesearch.com/2018/01/22/install-dlib-easy-complete-guide/).\n",
      "\n",
      "## Imports and models path\n",
      "\n",
      "We'll create a new Jupyter notebook / python file and start off with :\n",
      "```python\n",
      "import cv2\n",
      "import matplotlib.pyplot as plt\n",
      "import dlib\n",
      "from imutils import face_utils\n",
      "\n",
      "font = cv2.FONT_HERSHEY_SIMPLEX\n",
      "```\n",
      "\n",
      "# I. Cascade Classifiers\n",
      "\n",
      "We'll explore Cascade Classifiers at first. \n",
      "\n",
      "## 1. Theory\n",
      "\n",
      "Cascade classifier, or namely cascade of boosted classifiers working with haar-like features, is a special case of ensemble learning, called boosting. It typically relies on [Adaboost](https://maelfabien.github.io/machinelearning/adaboost) classifiers (and other models such as Real Adaboost, Gentle Adaboost or Logitboost).\n",
      "\n",
      "Cascade classifiers are trained on a few hundred sample images of image that contain the object we want to detect, and other images that do not contain those images. \n",
      "\n",
      "How can we detect if a face is there or not ? There is an algorithm, called Viola–Jones object detection framework, that includes all the steps required for live face detection :\n",
      "- Haar Feature Selection,  features derived from Haar wavelets\n",
      "- Create integral image\n",
      "- Adaboost Training\n",
      "- Cascading Classifiers\n",
      "\n",
      "The original [paper](https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf) was published in 2001.\n",
      "\n",
      "### a. Haar Feature Selection\n",
      "\n",
      "There are some common features that we find on most common human faces :\n",
      "- a dark eye region compared to upper-cheeks\n",
      "- a bright nose bridge region compared to the eyes\n",
      "- some specific location of eyes, mouth, nose...\n",
      "\n",
      "The characteristics are called Haar Features. The feature extraction process will look like this :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/haar.jpg)\n",
      "\n",
      "In this example, the first feature measures the difference in intensity between the region of the eyes and a region across the upper cheeks. The feature value is simply computed by summing the pixels in the black area and subtracting the pixels in the white area. \n",
      "\n",
      "$$ Rectangle Feature = \\sum (pixels_{black area}) - \\sum (pixels_{white area}) $$ \n",
      "\n",
      "Then, we apply this rectangle as a convolutional kernel, over our whole image. In order to be exhaustive, we should apply all possible dimensions and positions of each kernel. A simple 24*24 images would typically result in over 160'000 features, each made of a sum/subtraction of pixels values. It would computationally be impossible for live face detection. So, how do we speed up this process ?\n",
      "- once the good region has been identified by a rectangle, it is useless to run the window over a completely different region of the image. This can be achieved by Adaboost.\n",
      "- compute the rectangle features using the integral image principle, which is way faster. We'll cover this in the next section.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/haar_selection.jpg)\n",
      "\n",
      "There are several types of rectangles that can be applied for Haar Features extraction. According to the original paper :\n",
      "- the two-rectangle feature is the difference between the sum of the pixels within two rectangular regions, used mainly for detecting edges (a,b)\n",
      "- the three-rectangle feature computes the sum within two outside rectangles subtracted from the sum in a center rectangle, used mainly for detecting lines (c,d)\n",
      "- the four-rectangle feature computes the difference between diagonal pairs of rectangle (e)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/haar_rectangles.jpg)\n",
      "\n",
      "Now that the features have been selected, we apply them on the set of training images using Adaboost classification, that combines a set of weak classifiers to create an accurate ensemble model. With 200 features (instead of 160'000 initially), an accuracy of 95% is acheived. The authors of the paper have selected 6'000 features. \n",
      "\n",
      "### b. The integral image\n",
      "\n",
      "Computing the rectangle features in a convolutional kernel style can be long, very long. For this reason, the authors, Viola and Jones, proposed an intermediate representation for the image : the integral image. The role of the integral image is to allow any rectangular sum to be computed simply, using only four values. We'll see how it works !\n",
      "\n",
      "Suppose we want to determine the rectangle features at a given pixel with coordinates $$ (x,y) $$. Then, the integral image of the pixel in the sum of the pixels above and to the left of the given pixel. \n",
      "\n",
      "$$ ii(x,y) = \\sum_{x'≤x, y'≤y} i(x', y') $$\n",
      "\n",
      "where $$ ii(x,y) $$ is the integral image and $$ i(x,y) $$ is the original image.\n",
      "\n",
      "When you compute the whole integral image, there is a form a recurrence which requires only one pass over the original image. Indeed, we can define the following pair of recurrences :\n",
      "\n",
      "$$ s(x,y) = s(x,y-1) + i(x,y) $$\n",
      "\n",
      "$$ ii(x,y) = ii (x-1,y) + s(x,y) $$\n",
      "\n",
      "where $$ s(x,y) $$ is the cumulative row sum and and $$ s(x-1) = 0, ii(-1,y) = 0 $$. \n",
      "\n",
      "How can that be useful ? Well, consider a region D for which we would like to estimate the sum of the pixels. We have defined 3 other regions : A, B and C. \n",
      "- The value of the integral image at point 1 is the sum of the pixels in rectangle A\n",
      "\u0001- The value at point 2 is \u0001\u0005\u0004A + B\n",
      "- The value at point 3 is \u0001\u0005\u0004A + C\n",
      "- The value at point 4 is \u0001\u0005\u0004A + B + C + D.\n",
      "\n",
      "Therefore, the sum of pixels in region D can simply be computed as : $$ 4 + 1 - (2+3) $$.\n",
      "\n",
      "And over a single pass, we have computed the value inside a rectangle using only 4 array references.\f",
      "\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/haar_region.jpg)\n",
      "\n",
      "One should simply be aware that rectangles are quite simple features in practice, but sufficient for face detection. Steerable filters tend to be more flexible when it comes to  complex problems. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/steerable.jpg)\n",
      "\n",
      "### c. Learning the classification function with Adaboost\n",
      "\n",
      "Given a set of labeled training images (positive or negative), Adaboost is used to :\n",
      "- select a small set of features\n",
      "- and train the classifier\n",
      "\n",
      "Since most features among the 160'000 are supposed to be quite irrelevant, the weak learning algorithm around which we build a boosting model is designed to select the single rectangle feature which splits best negative and positive examples. \n",
      "\n",
      "### d. Cascading Classifier\n",
      "\n",
      "Although the process described above is quite efficient, a major issue remains. In an image, most of the image is a non-face region. Giving equal importance to each region of the image makes no sense, since we should mainly focus on the regions that are most likely to contain a picture. Viola and Jones achieved an increased detection rate while reducing computation time using Cascading Classifiers.\n",
      "\n",
      "The key idea is to reject sub-windows that do not contain faces while identifying regions that do. Since the task is to identify properly the face, we want to minimize the false negative rate, i.e the sub-windows that contain a face and have not been identified as such.\n",
      "\n",
      "A series of classifiers are applied to every sub-window. These classifiers are simple decision trees :\n",
      "- if the first classifier is positive, we move on to the second\n",
      "- if the second classifier is positive, we move on to the third\n",
      "- ...\n",
      "\n",
      "Any negative result at some point leads to a rejection of the sub-window as potentially containing a face. The initial classifier eliminates most negative examples at a low computational cost, and the following classifiers eliminate additional negative examples but require more computational effort. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/cascade.jpg)\n",
      "\n",
      "The classifiers are trained using Adaboost and adjusting the threshold to minimize the false rate. When training such model, the variables are the following :\n",
      "- the number of classifier stages\n",
      "- the number of features in each stage\n",
      "- the threshold of each stage\n",
      "\n",
      "Luckily in OpenCV, this whole model is already pre-trained for face detection.\n",
      "\n",
      "If you'd like to know more on Boosting techniques, I invite you to check my article on <a href=\"https://maelfabien.github.io/myblog/ml/06-adaboost/\">AdaBoost and Boosting</a>. \n",
      "\n",
      "## 2. Imports\n",
      "\n",
      "The next step simply is to locate the pre-trained weights. We will be using default pre-trained models to detect face, eyes and mouth. Depending on your version of Python, the files should be located somewhere over here :\n",
      "\n",
      "``` \n",
      "/usr/local/lib/python3.7/site-packages/cv2/data \n",
      "```\n",
      "\n",
      "Once identified, we'll declare Cascade classifiers this way :\n",
      "\n",
      "```python\n",
      "cascPath = \"/usr/local/lib/python3.7/site-packages/cv2/data/haarcascade_frontalface_default.xml\"\n",
      "eyePath = \"/usr/local/lib/python3.7/site-packages/cv2/data/haarcascade_eye.xml\"\n",
      "smilePath = \"/usr/local/lib/python3.7/site-packages/cv2/data/haarcascade_smile.xml\"\n",
      "\n",
      "faceCascade = cv2.CascadeClassifier(cascPath)\n",
      "eyeCascade = cv2.CascadeClassifier(eyePath)\n",
      "smileCascade = cv2.CascadeClassifier(smilePath)\n",
      "```\n",
      "\n",
      "## 3. Detect face on an image\n",
      "\n",
      "Before implementing the real time face detection algorithm, let's try a simple version on an image. We can start by loading a test image :\n",
      "\n",
      "```python\n",
      "# Load the image\n",
      "gray = cv2.imread('face_detect_test.jpeg', 0)\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.imshow(gray, cmap='gray')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/test_face.jpg)\n",
      "\n",
      "Then, we detect the face and we add a rectangle around it :\n",
      "\n",
      "```python\n",
      "# Detect faces\n",
      "faces = faceCascade.detectMultiScale(\n",
      "gray,\n",
      "scaleFactor=1.1,\n",
      "minNeighbors=5,\n",
      "flags=cv2.CASCADE_SCALE_IMAGE\n",
      ")\n",
      "\n",
      "# For each face\n",
      "for (x, y, w, h) in faces: \n",
      "    # Draw rectangle around the face\n",
      "    cv2.rectangle(gray, (x, y), (x+w, y+h), (255, 255, 255), 3)\n",
      "```\n",
      "Here is a list of the most common parameters of the `detectMultiScale` function :\n",
      "- scaleFactor : Parameter specifying how much the image size is reduced at each image scale.\n",
      "- minNeighbors : Parameter specifying how many neighbors each candidate rectangle should have to retain it.\n",
      "- minSize : Minimum possible object size. Objects smaller than that are ignored.\n",
      "- maxSize : Maximum possible object size. Objects larger than that are ignored.\n",
      "\n",
      "\n",
      "\n",
      "Finally, display the result :\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.imshow(gray, cmap='gray')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/test_face_output.jpg)\n",
      "\n",
      "Face detection works well on our test image. Let's move on to real time now !\n",
      "\n",
      "## 4. Real time face detection\n",
      "\n",
      "Let's move on to the Python implementation of the live facial detection. The first step is to launch the camera, and capture the video. Then, we'll transform the image to a gray scale image. This is used to reduce the dimension of the input image. Indeed, instead of 3 points per pixel describing Red, Green, Blue, we apply a simple linear transformation :\n",
      "\n",
      "$$ Y_{gray} = 0.2126R +0.7152G +0.0722B $$\n",
      "\n",
      "This is implemented by default in OpenCV.\n",
      "\n",
      "```python\n",
      "video_capture = cv2.VideoCapture(0)\n",
      "\n",
      "while True:\n",
      "    # Capture frame-by-frame\n",
      "    ret, frame = video_capture.read()\n",
      "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
      "```\n",
      "\n",
      "Now, we'll use the ```faceCascade``` variable define above, which contains a pre-trained algorithm, and apply it to the gray scale image.\n",
      "\n",
      "```python\n",
      "    faces = faceCascade.detectMultiScale(\n",
      "    gray,\n",
      "    scaleFactor=1.1,\n",
      "    minNeighbors=5,\n",
      "    minSize=(30, 30),\n",
      "    flags=cv2.CASCADE_SCALE_IMAGE\n",
      "    )\n",
      "```\n",
      "\n",
      "For each face detected, we'll draw a rectangle around the face :\n",
      "```python\n",
      "    for (x, y, w, h) in faces:\n",
      "        if w > 250 :\n",
      "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 3)\n",
      "            roi_gray = gray[y:y+h, x:x+w]\n",
      "            roi_color = frame[y:y+h, x:x+w]\n",
      "```\n",
      "\n",
      "For each mouth detected, draw a rectangle around it :\n",
      "```python\n",
      "    smile = smileCascade.detectMultiScale(\n",
      "        roi_gray,\n",
      "        scaleFactor= 1.16,\n",
      "        minNeighbors=35,\n",
      "        minSize=(25, 25),\n",
      "        flags=cv2.CASCADE_SCALE_IMAGE\n",
      "    )\n",
      "    for (sx, sy, sw, sh) in smile:\n",
      "        cv2.rectangle(roi_color, (sh, sy), (sx+sw, sy+sh), (255, 0, 0), 2)\n",
      "        cv2.putText(frame,'Smile',(x + sx,y + sy), 1, 1, (0, 255, 0), 1)\n",
      "```\n",
      "\n",
      "For each eye detected, draw a rectangle around it :\n",
      "```python\n",
      "    eyes = eyeCascade.detectMultiScale(roi_gray)\n",
      "    for (ex,ey,ew,eh) in eyes:\n",
      "        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
      "        cv2.putText(frame,'Eye',(x + ex,y + ey), 1, 1, (0, 255, 0), 1)\n",
      "```\n",
      "\n",
      "Then, count the total number of faces, and display the overall image :\n",
      "```python\n",
      "    cv2.putText(frame,'Number of Faces : ' + str(len(faces)),(40, 40), font, 1,(255,0,0),2)      \n",
      "    # Display the resulting frame\n",
      "    cv2.imshow('Video', frame)\n",
      "```\n",
      "\n",
      "And implement an exit option when we want to stop the camera by pressing ```q``` :\n",
      "```python\n",
      "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
      "        break\n",
      "```\n",
      "\n",
      "Finally, when everything is done, release the capture and destroy all windows. There are some troubles killing windows on Mac which might require killing Python from the Activity Manager later on.\n",
      "```python\n",
      "video_capture.release()\n",
      "cv2.destroyAllWindows()\n",
      "```\n",
      "\n",
      "## 5. Wrapping it up\n",
      "\n",
      "```python\n",
      "import cv2\n",
      "\n",
      "cascPath = \"/usr/local/lib/python3.7/site-packages/cv2/data/haarcascade_frontalface_default.xml\"\n",
      "eyePath = \"/usr/local/lib/python3.7/site-packages/cv2/data/haarcascade_eye.xml\"\n",
      "smilePath = \"/usr/local/lib/python3.7/site-packages/cv2/data/haarcascade_smile.xml\"\n",
      "\n",
      "faceCascade = cv2.CascadeClassifier(cascPath)\n",
      "eyeCascade = cv2.CascadeClassifier(eyePath)\n",
      "smileCascade = cv2.CascadeClassifier(smilePath)\n",
      "\n",
      "font = cv2.FONT_HERSHEY_SIMPLEX\n",
      "video_capture = cv2.VideoCapture(0)\n",
      "\n",
      "while True:\n",
      "    # Capture frame-by-frame\n",
      "    ret, frame = video_capture.read()\n",
      "\n",
      "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
      "\n",
      "    faces = faceCascade.detectMultiScale(\n",
      "        gray,\n",
      "        scaleFactor=1.1,\n",
      "        minNeighbors=5,\n",
      "        minSize=(200, 200),\n",
      "        flags=cv2.CASCADE_SCALE_IMAGE\n",
      "    )\n",
      "\n",
      "    # Draw a rectangle around the faces\n",
      "    for (x, y, w, h) in faces:\n",
      "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 3)\n",
      "            roi_gray = gray[y:y+h, x:x+w]\n",
      "            roi_color = frame[y:y+h, x:x+w]\n",
      "            cv2.putText(frame,'Face',(x, y), font, 2,(255,0,0),5)\n",
      "\n",
      "    smile = smileCascade.detectMultiScale(\n",
      "        roi_gray,\n",
      "        scaleFactor= 1.16,\n",
      "        minNeighbors=35,\n",
      "        minSize=(25, 25),\n",
      "        flags=cv2.CASCADE_SCALE_IMAGE\n",
      "    )\n",
      "\n",
      "    for (sx, sy, sw, sh) in smile:\n",
      "        cv2.rectangle(roi_color, (sh, sy), (sx+sw, sy+sh), (255, 0, 0), 2)\n",
      "        cv2.putText(frame,'Smile',(x + sx,y + sy), 1, 1, (0, 255, 0), 1)\n",
      "\n",
      "    eyes = eyeCascade.detectMultiScale(roi_gray)\n",
      "    for (ex,ey,ew,eh) in eyes:\n",
      "        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
      "        cv2.putText(frame,'Eye',(x + ex,y + ey), 1, 1, (0, 255, 0), 1)\n",
      "\n",
      "    cv2.putText(frame,'Number of Faces : ' + str(len(faces)),(40, 40), font, 1,(255,0,0),2)      \n",
      "    # Display the resulting frame\n",
      "    cv2.imshow('Video', frame)\n",
      "\n",
      "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
      "        break\n",
      "\n",
      "# When everything is done, release the capture\n",
      "video_capture.release()\n",
      "cv2.destroyAllWindows()\n",
      "```\n",
      "\n",
      "## 6. Results\n",
      "\n",
      "I've made a quick [YouTube](https://www.youtube.com/watch?v=bOflpJ2J7nQ) illustration of the face detection algorithm.\n",
      " \n",
      "# II. Histogram of Oriented Gradients (HOG) in Dlib\n",
      "\n",
      "The second most popular implement for face detection is offered by Dlib and uses a concept called Histogram of Oriented Gradients (HOG). This is an implementation of the original [paper by Dalal and Triggs](https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf).\n",
      "\n",
      "## 1. Theory\n",
      "\n",
      "The idea behind HOG is to extract features into a vector, and feed it into a classification algorithm like a Support Vector Machine for example that will assess whether a face (or any object you train it to recognize actually) is present in a region or not.\n",
      "\n",
      "The features extracted are the distribution (histograms) of directions of gradients (oriented gradients) of the image. Gradients are typically large around edges and corners and allow us to detect those regions.\n",
      "\n",
      "In the original paper, the process was implemented for human body detection, and the detection chain was the following :\n",
      "![image](https://maelfabien.github.io/assets/images/dlib_chain.jpg)\n",
      "\n",
      "### a. Preprocessing\n",
      "\n",
      "First of all, the input images must but of the same size (crop and rescale images). The patches we'll apply require an aspect ratio of 1:2, so the dimensions of the input images might be `64x128` or `100x200` for example.\n",
      "\n",
      "### b. Compute the gradient images\n",
      "\n",
      "The first step is to compute the horizontal and vertical gradients of the image, by applying the following kernels :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gradient-kernels.jpg)\n",
      "\n",
      "The gradient of an image typically removes non-essential information. \n",
      "\n",
      "The gradient of the image we were considering above can be found this way in Python :\n",
      "```python\n",
      "gray = cv2.imread('images/face_detect_test.jpeg', 0)\n",
      "\n",
      "im = np.float32(gray) / 255.0\n",
      "\n",
      "# Calculate gradient \n",
      "gx = cv2.Sobel(im, cv2.CV_32F, 1, 0, ksize=1)\n",
      "gy = cv2.Sobel(im, cv2.CV_32F, 0, 1, ksize=1)\n",
      "mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n",
      "```\n",
      "And plot the picture :\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.imshow(mag)\n",
      "plt.show()\n",
      "```\n",
      "![image](https://maelfabien.github.io/assets/images/grad.jpg)\n",
      "\n",
      "We have not pre-processed the image before though.\n",
      "\n",
      "### c. Compute the HOG\n",
      "\n",
      "The image is then divided into 8x8 cells to offer a compact representation and make our HOG more robust to noise. Then, we compute a HOG for each of those cells. \n",
      "\n",
      "To estimate the direction of a gradient inside a region, we simply build a histogram among the 64 values of the gradient directions (8x8) and their magnitude (another 64 values) inside each region. The categories of the histogram correspond to angles of the gradient, from 0 to 180°. Ther are 9 categories overall : 0°, 20°, 40°... 160°. \n",
      "\n",
      "The code above gave us 2 information :\n",
      "- direction of the gradient\n",
      "- and magnitude of the gradient\n",
      "\n",
      "When we build the HOG, there are 3 subcases :\n",
      "- the angle is smaller than 160° and not halfway between 2 classes. In such case, the angle will be added in the right category of the HOG\n",
      "- the angle is smaller than 160° and exactly between 2 classes. In such case, we consider an equal contribution to the 2 nearest classes and split the magnitude in 2\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hog_1.jpg)\n",
      "\n",
      "- the angle is larger than 160°. In such case, we consider that the pixel contributed proportionally to 160° and to 0°.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hog_2.jpg)\n",
      "\n",
      "The HOG looks like this for each 8x8 cell :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hog.jpg)\n",
      "\n",
      "### d. Block normalization\n",
      "\n",
      "Finally, a 16x16 block can be applied in order to normalize the image and make it invariant to lighting for example. This is simply achieved by dividing each value of the HOG of size 8x8 by the L2-norm of the HOG of the 16x16 block that contains it, which is in fact a simple vector of length `9*4 = 36`. \n",
      "\n",
      "### e. Block normalization\n",
      "\n",
      "Finally, all the 36x1 vectors are concatenated into a large vector. And we are done ! We have our feature vector, on which we can train a soft SVM classifier (C=0.01). \n",
      "\n",
      "## 2. Detect face on an image\n",
      "\n",
      "The implementation is pretty straight forward :\n",
      "\n",
      "```python\n",
      "face_detect = dlib.get_frontal_face_detector()\n",
      "\n",
      "rects = face_detect(gray, 1)\n",
      "\n",
      "for (i, rect) in enumerate(rects):\n",
      "(x, y, w, h) = face_utils.rect_to_bb(rect)\n",
      "    cv2.rectangle(gray, (x, y), (x + w, y + h), (255, 255, 255), 3)\n",
      "    \n",
      "plt.figure(figsize=(12,8))\n",
      "plt.imshow(gray, cmap='gray')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/face_hog.jpg)\n",
      "\n",
      "## 3. Real time face detection\n",
      "\n",
      "As previously, the algorithm is pretty easy to implement. We are also implementing a lighter version by detecting only the face. Dlib makes it really easy to detect facial keypoints too, but it's another topic.\n",
      "\n",
      "```python\n",
      "video_capture = cv2.VideoCapture(0)\n",
      "flag = 0\n",
      "\n",
      "while True:\n",
      "\n",
      "    ret, frame = video_capture.read()\n",
      "\n",
      "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
      "    rects = face_detect(gray, 1)\n",
      "\n",
      "    for (i, rect) in enumerate(rects):\n",
      "\n",
      "        (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
      "\n",
      "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
      "\n",
      "        cv2.imshow('Video', frame)\n",
      "\n",
      "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
      "        break\n",
      "\n",
      "video_capture.release()\n",
      "cv2.destroyAllWindows()\n",
      "```\n",
      "\n",
      "# III. Convolutional Neural Network in Dlib\n",
      "\n",
      "This last method is based on Convolutional Neural Networks (CNN). It also implements a [paper](https://arxiv.org/pdf/1502.00046.pdf) on Max-Margin Object Detection (MMOD) for enhanced results.\n",
      "\n",
      "## 1. A bit of theory\n",
      "\n",
      "Convolutional Neural Network (CNN) are feed-forward neural network that are mostly used for computer vision. They offer an automated image pre-treatment as well as a dense neural network part. CNNs are special types of neural networks for processing datas with grid-like topology. The architecture of the CNN is inspired by the visual cortex of animals.\n",
      "\n",
      "In previous approaches, a great part of the work was to select the filters in order to create the features in order to extract as much information from the image as possible. With the rise of deep learning and greater computation capacities, this work can now be automated. The name of the CNNs comes from the fact that we convolve the initial image input with a set of filters. The parameter to choose remains the number of filters to apply, and the dimension of the filters. The dimension of the filter is called the stride length. Typical values for the stride lie between 2 and 5. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/CNN.jpg)\n",
      "\n",
      "The output of the CNN in this specific case is a binary classification, that takes value 1 if there is a face, 0 otherwise.\n",
      "\n",
      "## 2. Detect face on an image\n",
      "\n",
      "Some elements change in the implementation.\n",
      "\n",
      "The first step is to download the pre-trained model [here](https://github.com/davisking/dlib-models/blob/master/mmod_human_face_detector.dat.bz2). Move the weights to your folder, and define `dnnDaceDetector` :\n",
      "\n",
      "```python\n",
      "dnnFaceDetector = dlib.cnn_face_detection_model_v1(\"mmod_human_face_detector.dat\")\n",
      "```\n",
      "\n",
      "Then, quite similarly to what we have done so far :\n",
      "\n",
      "```python\n",
      "rects = dnnFaceDetector(gray, 1)\n",
      "\n",
      "for (i, rect) in enumerate(rects):\n",
      "\n",
      "    x1 = rect.rect.left()\n",
      "    y1 = rect.rect.top()\n",
      "    x2 = rect.rect.right()\n",
      "    y2 = rect.rect.bottom()\n",
      "\n",
      "    # Rectangle around the face\n",
      "    cv2.rectangle(gray, (x1, y1), (x2, y2), (255, 255, 255), 3)\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.imshow(gray, cmap='gray')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/face_dlib.jpg)\n",
      "\n",
      "## 3. Real time face detection\n",
      "\n",
      "Finally, we'll implement the real time version of the CNN face detection :\n",
      "\n",
      "```python\n",
      "video_capture = cv2.VideoCapture(0)\n",
      "flag = 0\n",
      "\n",
      "while True:\n",
      "    # Capture frame-by-frame\n",
      "    ret, frame = video_capture.read()\n",
      "\n",
      "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
      "    rects = dnnFaceDetector(gray, 1)\n",
      "\n",
      "    for (i, rect) in enumerate(rects):\n",
      "\n",
      "        x1 = rect.rect.left()\n",
      "        y1 = rect.rect.top()\n",
      "        x2 = rect.rect.right()\n",
      "        y2 = rect.rect.bottom()\n",
      "\n",
      "        # Rectangle around the face\n",
      "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
      "\n",
      "    # Display the video output\n",
      "    cv2.imshow('Video', frame)\n",
      "\n",
      "    # Quit video by typing Q\n",
      "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
      "        break\n",
      "\n",
      "video_capture.release()\n",
      "cv2.destroyAllWindows()\n",
      "```\n",
      "\n",
      "# 4. Which one to choose ?\n",
      "\n",
      "Tough question, but we'll just go through 2 metrics that are important :\n",
      "- the computation time\n",
      "- the accuracy\n",
      "\n",
      "In terms of speed, HoG seems to be the fastest algorithm, followed by Haar Cascade classifier and CNNs. \n",
      "\n",
      "However, CNNs in Dlib tend to be the most accurate algorithm. HoG perform pretty well but have some issues identifying small faces. HaarCascade Classifiers perform around as good as HoG overall. \n",
      "\n",
      "I have personally used mainly HoG in my personal projects due to its speed for live face detection. \n",
      "\n",
      "The Github repository of this article can be found [here](https://github.com/maelfabien/Machine_Learning_Tutorials).\n",
      "\n",
      "> **Conclusion** : I hope you enjoyed this quick tutorial on OpenCV for face detection. Don't hesitate to drop a comment if you have any question/remark.\n",
      "\n",
      "Sources :\n",
      "- [HOG](https://www.learnopencv.com/histogram-of-oriented-gradients/)\n",
      "- [DLIB](https://www.pyimagesearch.com/2018/01/22/install-dlib-easy-complete-guide/)\n",
      "- [Viola-Jones Paper](https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf)\n",
      "- [Face Detection 1](https://www.pyimagesearch.com/2018/02/26/face-detection-with-opencv-and-deep-learning/)\n",
      "- [Face Detection 2](https://www.learnopencv.com/face-detection-opencv-dlib-and-deep-learning-c-python/)\n",
      "- [Face Detection 3](https://docs.opencv.org/3.4.3/d7/d8b/tutorial_py_face_detection.html)\n",
      "- [DetectMultiScale](https://docs.opencv.org/2.4/modules/objdetect/doc/cascade_classification.html)\n",
      "- [Viola-Jones](https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework)\n",
      "\n",
      "---\n",
      "title: Recommendation Systems in GCP - Week 1 Module 2\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Road to Google Cloud Platform Certification\"\n",
      "---\n",
      "\n",
      "In this section, we will review product recommendations using Cloud SQL and Apache Spark. \n",
      "\n",
      "# Recommendation Systems\n",
      "\n",
      "## Business applications\n",
      "\n",
      "E-commerce is probably the most common recommendation systems that we encounter. Models learn what we may like based on our preferences. We can give implicit or explicit feedback to the model (click, rating...).\n",
      "\n",
      "Recommendations systems are used for :\n",
      "- Google search results\n",
      "- Facebook feed\n",
      "- Netflix recommendation\n",
      "- Smart Reply in Gmail\n",
      "- Photo grouping in Google Photo\n",
      "- Restaurant recommendation system in Google Maps\n",
      "- ...\n",
      "\n",
      "Recommendation systems must scale to meet demand. \n",
      "\n",
      "## Machine learning\n",
      "\n",
      "Suppose that we will try to build a recommender system for housing rental. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_48.jpg)\n",
      "\n",
      "Suppose that we have ratings provided manually by users as training data. These ratings could come from explicit ratings. Maybe we showed the user the house in the past and they've clicked four stars after seeing the house details, or the ratings have come from implicit ratings. Maybe they've spent a lot of time looking at the website corresponding to this property. \n",
      "\n",
      "We then train an ML model to predict the user's rating of every single house in the catalog and return the 5 houses with the highest grades. We have 2 kinds of training data :\n",
      "- the previous ratings of the specific user\n",
      "- the ratings other people gave to each house\n",
      "\n",
      "One of the main issues is sparsity. Some houses might have a lot of reviews, other not. Some users might have a lot of users with similar profiles, others not. We need to cluster users and items at first.\n",
      "\n",
      "Then, build the predicted rating as: `user-preference * item-quality`.\n",
      "\n",
      "How often and where will we compute the predicted ratings?\n",
      "- Stream : Continuously train the models\n",
      "- Batch : Train the model once a day or once a week. This is the selected approach.\n",
      "\n",
      "We should consider making the computations in a Hadoop Cluster to compute the rating every user would give to every house. Finally, we can store the predictions in Cloud SQL in a transactional way, or in any relational database management system.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_49.jpg)\n",
      "\n",
      "## From On-Premise to GCP\n",
      "\n",
      "Existing on-premise models might require a migration to GCP.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_50.jpg)\n",
      "\n",
      "There are several storage solutions offered on GCP :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_51.jpg)\n",
      "\n",
      "The storage solution to use is an important question :\n",
      "- use Cloud Storage as a global file system\n",
      "- use CloudSQL as an RDBMS for transactional data up to a few Gb\n",
      "- use Datastore as a transactional NoSQL database system\n",
      "- use Bigtable for low-latency NoSQL data such as sensor data\n",
      "- use BigQuery as a SQL Data Warehouse to power analytics\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_52.jpg)\n",
      "\n",
      "We, therefore, use CloudSQL in our example. Cloud SQL has a lot of advantage :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_53.jpg)\n",
      "\n",
      "## Utilizing and tuning on-premise clusters\n",
      "\n",
      "When running jobs on your own clusters, there is a high chance that you either :\n",
      "- underprovision your infrastructure, and your jobs would need more computing power \n",
      "- overprovision, in which case you are just wasting money\n",
      "\n",
      "With GCP, Hadoop Clusters can be seen as fungible resources. You run the clusters when you need to run jobs, and turn them off otherwise. With scheduled deletion, you can automate when the clusters need to be shut down. You can set shutdown triggers based on time, completion of a job...\n",
      "\n",
      "Cloud DataProc AutoScaling provides flexible capacities :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_64.jpg)\n",
      "\n",
      "With AutoScaling, you cannot store the data on the cluster, but we use Cloud Storage or BigTable instead.\n",
      "\n",
      "Moreover, you can incorporate preemptible virtual machines into your cluster architecture. Preemptible VMs are highly affordable, short-lived compute instances that are suitable for batch jobs and fault-tolerant workloads. They last only up to 24 hours and they can be taken away whenever somebody else comes along and offers up new compute needs for them. So if your applications are fault tolerant, and Hadoop applications are, then preemptible instances can reduce your Compute Engine costs significantly.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_65.jpg)\n",
      "\n",
      "Again, since these machines are temporary, you need to store data off-cluster.\n",
      "\n",
      "## Move storage off-cluster with Google Cloud Storage\n",
      "\n",
      "Google's data center network speed enables separation of compute and storage. In other words, we process the data where it is without copying it.\n",
      "\n",
      "We HDFS only on the cluster for working storage, storage during processing, but we will store all actual input and output data on Google Cloud storage.\n",
      "\n",
      "Since the input and output data are off the cluster, the cluster can be created for a single job or type of workload and can be shut down when not in use. DataProc relies on Cloud Storage, BigTable and BigQuery for storage.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_66.jpg)\n",
      "\n",
      "Storage using off-cluster options is usually cheaper since the disks themselves are cheaper, and the computing units can be shut down when not in use.\n",
      "\n",
      "> **Conclusion **: This is the end of the Recommendation System module, part of the week 1 of the GCP Specialization.\n",
      "\n",
      "---\n",
      "title: Big (Open)  Data , the GDELT Project\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"GDelt Project\"\n",
      "---\n",
      "\n",
      "The GDELT Project monitors the world's broadcast, print, and web news from nearly every corner of every country in over 100 languages and identifies the people, locations, organizations, themes, sources, emotions, counts, quotes, images and events driving our global society every second of every day, creating a free open platform for computing on the entire world. With new files uploaded every 15 minutes, GDELT databases contain more than 700 Gb of zipped data for the single year 2018.\n",
      "\n",
      "<span style=\"color:blue\">[https://www.gdeltproject.org/](https://www.gdeltproject.org/)</span>\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/header.jpg)\n",
      "\n",
      "## The project\n",
      "\n",
      "In this series of articles, I am going to present you a recent school project we worked on: Building a resilient architecture for storing a large amount of data from the GDELT project, and building fast responding queries.\n",
      "\n",
      "The GitHub of this project can be found here : <span style=\"color:blue\">[https://github.com/maelfabien/Cassandra-GDELT-Queries](https://github.com/maelfabien/Cassandra-GDELT-Queries)</span>\n",
      "\n",
      "To be able to work with a large amount of data, we have chosen to work with the following architecture :\n",
      "- NoSQL Database: Cassandra\n",
      "- AWS: EMR to transfer the data to Cassandra, and EC2 for the resiliency for the requests\n",
      "- Visualization: A Zeppelin Notebook\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "## The data\n",
      "\n",
      "The following links describe the data in the GDELT table :\n",
      "- Description of the data Mentions and Events : <span style=\"color:blue\">[http://data.gdeltproject.org/documentation/GDELT-Event_Codebook-V2.0.pdf](http://data.gdeltproject.org/documentation/GDELT-Event_Codebook-V2.0.pdf)</span>\n",
      "- Description of the Graph of Events GKG : <span style=\"color:blue\">[http://data.gdeltproject.org/documentation/GDELT-Global_Knowledge_Graph_Codebook-V2.1.pdf](http://data.gdeltproject.org/documentation/GDELT-Global_Knowledge_Graph_Codebook-V2.1.pdf)</span>\n",
      "\n",
      "The tables are the following :\n",
      "![image](https://maelfabien.github.io/assets/images/data.jpg)\n",
      "\n",
      "An event is defined as an action that an actor (Actor1) takes on another actor (Actor2). Mention is an article or any source that talks about an event. The GKG database reflects the events that took place in the world, ordered by theme, type of event and location.\n",
      "\n",
      "The conceptual model of the data is the following : \n",
      "![image](https://maelfabien.github.io/assets/images/concept.jpg)\n",
      "\n",
      "## Architecture\n",
      "\n",
      "The architecture we have chosen is the following : \n",
      "![image](https://maelfabien.github.io/assets/images/archi.jpg)\n",
      "\n",
      "Our architecture is composed of one cluster EMR (1 master and 5 slaves) and one cluster EC2 (8 instances).\n",
      "\n",
      "In our 8 EC2 instances we have :\n",
      "- 2 Masters nodes with apache-Spark-2.3.2 and apache-Zeppelin-0.8.0\n",
      "- 5 Slaves nodes with apache-Spark-2.3.2 and apache-cassandra-3.11.2, including zookeeper installed on 2 of these nodes.\n",
      "\n",
      "The last one is a node created for the resilience of the Master. We Installed zookeeper in it. The architecture will be further described in a future article.\n",
      "\n",
      "The data is stored as a set of zipped CSV Files. The overall size for the year 2018 reaches 700 Gb. To process such a huge amount of data, the approach we chose is the following :\n",
      "- download all the CSV zipped files and load them to an S3 bucket\n",
      "- using Spark-Scala, download all the data in the S3 bucket and load them as data frames in a Zeppelin Notebook\n",
      "- clean the data and split them into several data frames\n",
      "- load the different data frames in Cassandra\n",
      "- make requests in CQL or using spark-cassandra connector directly in Zeppelin\n",
      "\n",
      "> The next articles aim at describing how to build a resilient database architecture that would allow simple and fast queries on a large amount of data.\n",
      "---\n",
      "title: The basis of Machine Learning\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Machine Learning Basics\"\n",
      "---\n",
      "\n",
      "Machine learning (ML) has been a rising trend over the last years. ML includes a set of techniques that go beyond statistics. In this article, we'll cover the most important concepts behind ML.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "## Why machine learning?\n",
      "\n",
      "Machine learning goes beyond statistics. Indeed, we face the following technical challenges :\n",
      "- the volume, complexity, and dimension of data to process\n",
      "- the diversity of the context: supervised, unsupervised...\n",
      "\n",
      "The improvement of the computation power and cloud computing have also allowed machine learning to become a standard that goes beyond statistical inference.\n",
      "\n",
      "## Supervised vs. Unsupervised\n",
      "\n",
      "Suppose we are given a set of data points $$ X_1, X_2, ... X_m $$. These points might represent anything measurable you could think of (e.g humidity in the air, population density or even pixels of an image), and are the variables we will rely on. \n",
      "\n",
      "There are 2 subcases :\n",
      "- either we have labels attached to the variables: $$ y_1, y_2, ... y_m $$. The labels are in some sense the output we want to be able to forecast (e.g the price of a house, a stock price..). This is called supervised learning.\n",
      "- in some cases, we do not have the labels. In those cases, our aim should be to group observations that appear to be similar to clusters. This is called unsupervised learning. \n",
      "- you might also encounter the term of semi-supervised data. This simply means that only some data points are labeled.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/supervised.jpg)\n",
      "\n",
      "## Regression vs. Classification\n",
      "\n",
      "In supervised learning, there are two types of prediction problems you will be solving. Either trying to classify a new observation to the class it belongs to (classification), or trying to predict a continuous outcome (regression). For example :\n",
      "- in fraud detection, you rely on some measured data to predict if a bank payment is fraudulent or not. If it is fraudulent, you should predict 1. Else, predict 0.\n",
      "- in stock market analysis, you will be trying to predict the value of the stock itself, and this is a regression problem\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/classification.jpg)\n",
      "\n",
      "## Train vs. Test\n",
      "\n",
      "Most of the time, to assess the efficiency of a classifier or a regressor, we split the data we have into a train and a test set. The idea is to train the model on the train set, and assess its efficiency on the test set. This allows us to prevent overfitting (i.e developing a model too complex that learns too much from the data we provide it).\n",
      "\n",
      "## Discriminative vs. Generative\n",
      "\n",
      "There are two main categories of models :\n",
      "- the discriminative model (most common) tries to estimate the probability of $$ y $$ given $$ X $$ directly. In classification, we try to estimate the decision boundary between the labels of the data without paying attention to the distribution of the labeled data themselves. Most classifiers are of this type.\n",
      "- the generative model tries to estimate the probability of $$ X $$ given $$ y $$ to later on deduce the probability of $$ y $$ given $$ X $$. We learn the probability distribution of the data. Discriminant analysis and Naïve Bayes are examples of such classifiers.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/discr.jpg)\n",
      "\n",
      "## Error rate and Empirical Risk Minimization\n",
      "\n",
      "We'll consider a classification problem here. The aim of building a model is to be able to predict for a new observation the class it belongs to. If we had to define a metric for this, it would be the accuracy. We want to build a classifier $$ C $$ that minimizes the number of miss-classified examples among our data. The loss of the model, i.e the error measure we can define, is the following :\n",
      "\n",
      "$$ L(C) = E( I (Y≠C(X) ) ) $$\n",
      "\n",
      "An optimal classifier should meet the following criteria :\n",
      "\n",
      "$$ C^* = {argmin}_{C ∈ G} L(C) $$\n",
      "\n",
      "It can quite easily be shown that the solution to this problem is :\n",
      "\n",
      "$$ C^* = 2 * I( P(Y=1 | X = x) > 1/2) - 1 $$\n",
      "\n",
      "In practice, we have access to a limited number of observations. For such reason, we try to minimize the error on the test sample with the empirical risk minimization (ERM) :\n",
      "\n",
      "$$ \\hat{L_n}(C) = \\frac {1} {n} \\sum I (Y_i ≠ C(X_i) ) ) $$\n",
      "\n",
      "Learning works if when $$ n $$ tends to infinity, $$ \\hat{L_n}(C) $$ tends to $$ L(C) $$. The theory of Vapnik-Chervonenkis offers guarantees for the prediction as long as the underlying model is not too complex.\n",
      "\n",
      "## How to assess the performance of a model?\n",
      "\n",
      "Several metrics can be used to assess how well a model is performing. \n",
      "\n",
      "### Classification\n",
      "\n",
      "For a classification problem, we first need to recall the following concepts :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/tp.jpg)\n",
      "\n",
      "Notation :\n",
      "- TP: True positive\n",
      "- TN: True negative\n",
      "- FP: False positive \n",
      "- FN: False negative\n",
      "\n",
      "Then, we can define :\n",
      "- the accuracy, the most widely used metric in classification, useful when there is a certain balance between classes : \n",
      "$$ Accuracy = \\frac {TP + TN} {TP + TN + FP + FN} $$\n",
      "- the precision :\n",
      "$$ Precision = \\frac {TP} {TP + FP} $$\n",
      "- the recall, or precision :\n",
      "$$ Recall = \\frac {TP} {TP + FN} $$\n",
      "- the specificity :\n",
      "$$ Specificity = \\frac {TN} {TN + FP} $$\n",
      "- the F1-Score :\n",
      "$$ F1-Score = 2 * \\frac {Precision * Recall} {Precision + Recall} $$\n",
      "\n",
      "### Regression\n",
      "\n",
      "Let's define $$ y $$ as the true value, and $$ \\hat{y} $$ as the predicted y-value :\n",
      "\n",
      "The most important metrics are :\n",
      "- the Mean Absolute Error (MAE) : \n",
      "$$ MAE = \\frac {1}{n} \\sum \\mid {y - \\hat{y}} \\mid $$\n",
      "- the Mean Square Error (MSE) : \n",
      "$$ MSE = \\frac {1}{n} \\sum (y - \\hat{y})^2 $$\n",
      "- Mean Absolute Percentage Error (MAPE) :\n",
      "$$ MAPE = \\frac {1} {n} \\sum \\mid { \\frac {y - \\hat{y}} {y}} \\mid * 100% $$\n",
      "- Mean Percentage Error (MPE) :\n",
      "$$ MPE = \\frac {1} {n} \\sum { \\frac {y - \\hat{y}} {y}} * 100% $$\n",
      "\n",
      "\n",
      "## Bias - Variance tradeoff\n",
      "\n",
      "A model reaching a high accuracy in the training part, and much lower accuracy. In such a case, we face an overfitting issue. This means that our model is in some sense learning too much from the train set and creating a model that might be too complex.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/overfitting.jpg)\n",
      "\n",
      "There is a tradeoff to make between the complexity of the model and the predictive power. It can be expressed as :\n",
      "$$ E( (y - \\hat{y})^2 ) = {Bias}( \\hat{y})^2 + {Var}( \\hat{y}) + \\sigma^2 $$\n",
      "\n",
      "Where :\n",
      "- $$ {Bias}[ \\hat{y}] = E(\\hat{y}) - y $$\n",
      "- $$ {Var}( \\hat{y}) = E(\\hat{y}^2) - E(\\hat{y})^2 $$\n",
      "- $$ \\sigma $$ is the variance of the noise of the underlying model\n",
      "\n",
      "> **Conclusion** : This brief introduction to machine learning should get you started for the next series of articles on the different algorithms.\n",
      "---\n",
      "title: Sound Feature Extraction\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Signal Processing\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "Sound features can be used to detect speakers, detect the gender, the age, diseases and much more through the voice.\n",
      "\n",
      "To extract features, we must break down the audio file into windows, often between 20 and 100 milliseconds. We then extract these features per window and can run a classification algorithm for example on each window.\n",
      "\n",
      "Start by importing the series:\n",
      "\n",
      "```python\n",
      "x, sr = librosa.load('test.wav')\n",
      "```\n",
      "\n",
      "# 1. Statistical Features\n",
      "\n",
      "A first easy step is to compute the mean, standard deviation, minimum, maximum, median and quartiles of the frequencies of each signal. This can be done using Numpy and it always brings value to our feature extraction. This kind of approach can be used in gender recognition for example, as seen on [Kaggle](https://www.kaggle.com/primaryobjects/voicegender).\n",
      "\n",
      "```python\n",
      "freqs = np.fft.fftfreq(y.size)\n",
      "\n",
      "def describe_freq(freqs):\n",
      "    mean = np.mean(freqs)\n",
      "    std = np.std(freqs) \n",
      "    maxv = np.amax(freqs) \n",
      "    minv = np.amin(freqs) \n",
      "    median = np.median(freqs)\n",
      "    skew = scipy.stats.skew(freqs)\n",
      "    kurt = scipy.stats.kurtosis(freqs)\n",
      "    q1 = np.quantile(freqs, 0.25)\n",
      "    q3 = np.quantile(freqs, 0.75)\n",
      "    mode = scipy.stats.mode(freqs)[0][0]\n",
      "    iqr = scipy.stats.iqr(freqs)\n",
      "    \n",
      "    return [mean, std, maxv, minv, median, skew, kurt, q1, q3, mode, iqr]\n",
      "```\n",
      "\n",
      "This can be applied per time window or to the whole signal depending on its length.\n",
      "\n",
      "# 2. Energy\n",
      "\n",
      "The energy of a signal is the total magnitude of the signal, i.e. how loud the signal is. It is defined as:\n",
      "\n",
      "$$ E(x) = \\sum_n {\\mid x(n) \\mid}^2 $$\n",
      "\n",
      "You can compute it this way:\n",
      "\n",
      "```python\n",
      "def rmse(x):\n",
      "    return np.sum(x**2)\n",
      "```\n",
      "\n",
      "We can then apply this function to each time frame when building our feature extraction sliding window.\n",
      "\n",
      "# 3. Root Mean Square Energy\n",
      "\n",
      "The RMS Energy (RMSE) is simply the square root of the mean squared amplitude over a time window. It is defined by:\n",
      "\n",
      "$$ RMSE(x) = \\sqrt{ \\frac{1}{N} \\sum_n {\\mid x(n) \\mid}^2} $$ \n",
      "\n",
      "\n",
      "```python\n",
      "def rmse(x):\n",
      "    return np.sqrt(np.mean(x**2))\n",
      "```\n",
      "\n",
      "It can also be extracted with Librosa library:\n",
      "\n",
      "```python\n",
      "import librosa\n",
      "\n",
      "rmse = librosa.feature.rmse(y)[0]\n",
      "```\n",
      "\n",
      "# 4. Zero-Crossing Rate\n",
      "\n",
      "The zero crossing rate indicates the number of times that a signal crosses the horizontal axis, i.e. the number of times that the amplitude reaches 0.\n",
      "\n",
      "This can be computed using Librosa:\n",
      "\n",
      "```python\n",
      "zero_crossings = sum(librosa.zero_crossings(x, pad=False))\n",
      "```\n",
      "\n",
      "This will return the total number of times the amplitude crosses the horizontal axis.\n",
      "\n",
      "# 5. Tempo\n",
      "\n",
      "An estimate of the tempo in Beats Per Minute (BPM).\n",
      "\n",
      "```python\n",
      "tempo = librosa.beat.tempo(y)[0]\n",
      "```\n",
      "\n",
      "# 6. Mel Frequency Cepstral Coefficients (MFCC)\n",
      "\n",
      "My understanding of MFCC highly relies on [this excellent article](http://www.practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/). The MFCC are state-of-the-art features for speaker identification, disease detection, speech recognition, and by far the most used among all features present in this article.\n",
      "\n",
      "Start by taking a short window frame (20 to 40 ms) in which we can assume that the audio signal is stationary. We then select a frame step (e.g. rolling window) of around 10 ms.\n",
      "\n",
      "We then compute the power spectrum of each frame through a periodogram, which is inspired by the human cochlea (an organ in the ear) which vibrates at different spots depending on the frequency of the incoming sounds. To do so, start by taking the Discrete Fourrier Transform of the frame:\n",
      "\n",
      "$$ S_i(k) = \\sum_{n=1}^N s_i(n)h(n) e^{-j 2 \\pi k n / N} $$\n",
      "\n",
      "where:\n",
      "- $$ s_i(n) $$ is the framed time signal (i frames)\n",
      "- $$ N $$ is the number of samples in a Hamming Window\n",
      "- $$ h(n) $$ is the Hamming Window\n",
      "- $$ K $$ is the length of the DFT\n",
      "\n",
      "To compute the periodogram estimate of the power spectrum, we apply:\n",
      "\n",
      "$$ P_i(k) = \\frac{1}{N} {\\mid S_i(k) \\mid}^2 $$\n",
      "\n",
      "Since the cochlea is not so good to discriminate between two closely spaced frequencies, especially when they are high, we take clumps of periodogram bins and sum them up. This is done by applying Mel filterbank, filters which tell us exactly how to space our filterbanks and how wide to make them. These filters are typically narrower around 0Hz and wider for higher frequencies.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/melfb.png)\n",
      "\n",
      "The formula to move from frequencies to Mel scale is the following:\n",
      "\n",
      "$$ M(f) = 1125 ln (1 + \\frac{f}{700}) $$\n",
      "\n",
      "The Mel filterbank is a set of 26 triangular filters which we apply to the periodogram power spectral estimate. Each filter is mostly made of 0's but has a non-zero triangle in some region. We multiply the values of the periodogram by the ones of the filters.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/melfb2.png)\n",
      "\n",
      "We then take the logarithm of the all those 26 series of energy of those filterbanks since we do not percieve loudness linearly, but close to logarithmically.\n",
      "\n",
      "We finally apply a Discrete Cosine Transform to the 26 log filterbank energies in order to decorrelate the overlapping filterbanks energies. This gives us 26 coefficients, called the MFCC. Not all of them are useful, and for Automatic Speech Recognition, we typically only use the 12-13 lower values.\n",
      "\n",
      "MFCCs are widely used to classify phonemes. They can easily be extracted using `librosa` library:\n",
      "\n",
      "```python\n",
      "import librosa\n",
      "y, sr = librosa.load(filename)\n",
      "mfcc=librosa.feature.mfcc(y)\n",
      "```\n",
      "\n",
      "It returns a numpy array of size 20 (MFCC extracted) * the number of windows (for the file `test.wav`, 431).\n",
      "\n",
      "```python\n",
      "array([[  22.456623 ,  -19.06088  , -164.62514  , ..., -240.06525  ,\n",
      "        -257.81137  , -260.06912  ],\n",
      "       [  95.18431  ,   89.34006  ,   37.92323  , ...,  171.81778  ,\n",
      "         160.94362  ,   97.23265  ],\n",
      "...\n",
      "```\n",
      "\n",
      "Since these series can get quite long as one new data point is created every 20ms, one can always extract the mean, variance, quartiles, min, max and median as a descriptive statistic at the end of an audio sample, and compare several audio samples on this basis.\n",
      "\n",
      "# 7. Mel Frequency Cepstral Differential Coefficients\n",
      "\n",
      "MFCC lacks information on the evolution of the coefficients between frames. What we can therefore do is to compute the 12 trajectories of the MFC coefficients and append them to the 12 original coefficients. This highly improves results on ASR tasks.\n",
      "\n",
      "$$ d_t = \\frac{\\sum_{n=1}^N n (c_{t+n} - c_{t-n})}{2 \\sum_{n=1}^N n^2} $$\n",
      "\n",
      "Where:\n",
      "- $$ d_t $$ is the delta coefficient\n",
      "- $$ t $$ is the frame considered\n",
      "- $$ c_t $$ is the coefficient at time $$ t $$ (we commonly use N=2)\n",
      "\n",
      "# 8. Polyfeatures\n",
      "\n",
      "The polyfeatures returns the coefficients of fitting an nth-order polynomial to the columns of a spectrogram. This can be easily extracted using Librosa.\n",
      "\n",
      "```python\n",
      "poly_features=librosa.feature.poly_features(y) #order 1 by default\n",
      "\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(poly_features[0], label=\"0\")\n",
      "plt.plot(poly_features[1], label=\"1\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/feat_0.png)\n",
      "\n",
      "One can then apply a mean of the coefficients or get statistics to describe the series, otherwise it tends to be too much values given.\n",
      "\n",
      "# 9. Tempogram\n",
      "\n",
      "The tempo, measured in Beats Per Minute (BPM) measures the rate of the musical beat. The tempogram is a feature matrix which indicates the prevalence of certain tempi at each moment in time. Librosa has a built-in function to extract this information. It is common to focus only on the first N rows (e.g 13) of the matrix.\n",
      "\n",
      "```python\n",
      "hop_length = 512\n",
      "oenv = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)\n",
      "tempogram = librosa.feature.tempogram(onset_envelope=oenv, sr=sr, hop_length=hop_length)\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "for i in range(1,14):\n",
      "    plt.plot(tempogram[i], label=i)\n",
      "plt.legend()\n",
      "plt.title(\"Tempogram\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/feat_1.png)\n",
      "\n",
      "As before, one can take descriptive statistics on the output series.\n",
      "\n",
      "# 10. Spectal Features\n",
      "\n",
      "Spectral features are extracted from the spectrogram. Spectrograms offer a powerful representation of the data. It plots over the time, for a given range of frequencies, the power (dB) of a signal. This allows us to spot periodic patterns over time, and regions of activity.\n",
      "\n",
      "The most common feature to extract is the spectral centroid. Each frame of a magnitude spectrogram is normalized and treated as a distribution over frequency bins, from which the mean (centroid) is extracted per frame.\n",
      "\n",
      "```python\n",
      "spec_centroid = librosa.feature.spectral_centroid(y)[0]\n",
      "```\n",
      "\n",
      "There are other features that contain information:\n",
      "\n",
      "```python\n",
      "spectral_bandwidth=librosa.feature.spectral_bandwidth(y)[0]\n",
      "spectral_contrast=librosa.feature.spectral_contrast(y)[0]\n",
      "spectral_flatness=librosa.feature.spectral_flatness(y)[0]\n",
      "spectral_rolloff=librosa.feature.spectral_rolloff(y)[0]\n",
      "```\n",
      "\n",
      "# 11. Fundamental Frequency\n",
      "\n",
      "One can also extract fundamental frequencies of a voice, which are the lowest frequencies of a periodic voice waveform. This is really useful for classifying gender, since males have lower fundamental frequencies than females in most cases. The voiced speech of a typical adult male will have a fundamental frequency from 85 to 180 Hz, and that of a typical adult female from 165 to 255 Hz.\n",
      "\n",
      "It corresponds to the smallest value of $$ T $$ such that:\n",
      "\n",
      "$$ x(t)=x(t+T){\\text{ for all }}t\\in {\\mathbb  {R}} $$\n",
      "\n",
      "And the fundamental frequency is defined by:\n",
      "\n",
      "$$ f_0 = \\frac{1}{T} $$\n",
      "\n",
      "# 12. Jitter Feature\n",
      "\n",
      "Jitter Feature measures the deviation of periodicity in a periodic signal. It can be computed as:\n",
      "\n",
      "$$ Jitter(T) = \\frac{1}{N-1} \\sum_{i=1}^{N-1} \\mid T_i - T_{i+1} \\mid $$\n",
      "\n",
      "# 13. Meta features\n",
      "\n",
      "In order to add some more features, on can create meta features. These features are the result of a regression or a classification algorithm that is ran halfway through the feature extraction process. We can for example train an algorithm to detect gender based on MFCC features, and for each new sample, predict whether this is a male or a female and add it as a features. \n",
      "\n",
      "Among meta features, the most popular are:\n",
      "- gender\n",
      "- age category\n",
      "- ethnicity / accent\n",
      "- diseases\n",
      "- emotions\n",
      "- audio quality\n",
      "- fatigue level\n",
      "- stress level\n",
      "\n",
      "# 14. Dimension reduction\n",
      "\n",
      "One common issue when dealing with audio features is the number of features created. It can easily exceed 1'000 features on a set of audio samples, and we therefore need to think of dimension reduction techniques. \n",
      "\n",
      "The most common unsupervised approaches are:\n",
      "- Principal Component Analysis (PCA)\n",
      "- K-means clustering\n",
      "\n",
      "And for supervised approaches:\n",
      "- Supervised Dictionary Learning (SDL)\n",
      "- Linear Discriminant Analysis\n",
      "- Variational Autoencoders\n",
      "\n",
      "Most techniques are easy to implement using scikit-learn, but since it's a bit less common, here's how to implement the SDL approach:\n",
      "\n",
      "```python\n",
      "# supervised dictionary learning\n",
      "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
      "dico_X = MiniBatchDictionaryLearning(n_components=50, alpha=1, n_iter=500).fit_transform(X)\n",
      "dico_Y = MiniBatchDictionaryLearning(n_components=50, alpha=1, n_iter=500).fit_transform(Y)\n",
      "```\n",
      "\n",
      "> **Conclusion**: I hope you enjoyed this article. If you think about other features, please let me know in the comments !\n",
      "---\n",
      "title: Install Apache Spark on EC2 instances\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Amazon Web Services\"\n",
      "---\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ec2_spark.jpg)\n",
      "\n",
      "This topic will help you install Apache-Spark on your AWS EC2 cluster.  \n",
      "We'll go through a standard configuration which allows the elected Master to spread its jobs on Worker nodes.  \n",
      "\n",
      "The \"election\" of the primary master is handled by Zookeeper. \n",
      "\n",
      "This tutorial will be divided into 5 sections. \n",
      "1. Install Apache-Spark on your instances\n",
      "2. Configuration of your Master nodes\n",
      "3. Configuration of your Slave nodes\n",
      "4. Add dependencies to connect Spark and Cassandra\n",
      "5. Launch your Master and your Slave nodes\n",
      "\n",
      "The goal of this final tutorial is to configure Apache-Spark on your instances and make them communicate with your Apache-Cassandra Cluster with full resilience.\n",
      "\n",
      "# 1. Install Apache Spark\n",
      "\n",
      "## a. A few words on Spark :\n",
      "\n",
      "Spark can be configured with multiple cluster managers like YARN, Mesos, etc. Along with that, it can be configured in standalone mode. \n",
      "For this tutorial, I choose to deploy Spark in Standalone Mode.\n",
      "\n",
      "> ** Standalone Deploy Mode ** :\n",
      "This is the simplest way to deploy Spark on a private cluster. Both driver and worker nodes run on the same machine.\n",
      "Standalone mode is good to go for developing applications in Spark. Spark processes run in JVM. \n",
      "\n",
      "**Java should be pre-installed on the machines on which we have to run Spark job.** \n",
      "\n",
      "## b. Connect via SSH on every node except the node named Zookeeper :\n",
      "\n",
      "Make sure an SSH connection is established. If you don't remenber how to do that, you can check the last section of \n",
      "<span style=\"color:blue\">[this tutorial](https://maelfabien.github.io/bigdata/05-EC2/)</span>.\n",
      "\n",
      "## c. On Spark's Website :\n",
      "\n",
      "**For the sake of stability, I chose to install version 2.3.2.**  \n",
      "If you want to choose the version 2.4.0, you need to be careful! Some software (like Apache Zeppelin) don't match this version yet (End of 2018).\n",
      "\n",
      "From Apache Spark's website, download the `tgz` file : \n",
      "<span style=\"color:blue\">[https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)</span> \n",
      "\n",
      "## d. Download the `.tar.gz` file : \n",
      "\n",
      "On **each** node, execute the following command :  \n",
      "``` wget http://apache.mediamirrors.org/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7.tgz```\n",
      "\n",
      "You should get something like this :  \n",
      "![image](https://maelfabien.github.io/assets/images/Spark_Wget.jpg)\n",
      "\n",
      "Make sure to repeat this step for every node.\n",
      "\n",
      "## e. Extract the software : \n",
      "\n",
      "On **each** node, extract the software and remove the `.tar.gz` file by executing the command bellow :  \n",
      "```bash\n",
      "$ tar -xv spark-2.3.2-bin-hadoop2.7.tgz \n",
      "$ rm spark-2.3.2-bin-hadoop2.7.tgz\n",
      "```\n",
      "Replace the Spark version by the one you just selected. The terminal of your Master nodes will look like this :  \n",
      "![image](https://maelfabien.github.io/assets/images/Spark_Extract1.jpg) \n",
      "\n",
      "\n",
      "The terminal of your Slave nodes will look like this :  \n",
      "![image](jpghttps://maelfabien.github.io/assets/images/Spark_Extract2.jpg) \n",
      "\n",
      "## f.  ```~/.bashrc``` file\n",
      "\n",
      "You need to define the ``` $SPARK_HOME``` path on **each node**. \n",
      "In order to define the  ``` $SPARK_HOME``` , execute  ```$ vi ~/.bashrc ``` and paste the following code :\n",
      "\n",
      "```bash\n",
      "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
      "export JRE_HOME=$JAVA_HOME/jre\n",
      "export PATH=$PATH:$JAVA_HOME/bin:$JAVA_HOME/jre/bin\n",
      "export SPARK_HOME=/home/ubuntu/spark-2.3.2-bin-hadoop2.7\n",
      "```\n",
      "Make sure to replace the versions by the ones you selected above.\n",
      "\n",
      "\n",
      "To quit and save changes : \n",
      "Press `ESC`, type `:wq` then `ENTER`.\n",
      "\n",
      "Back on your terminal, execute  ``` source ~/.bashrc```\n",
      "\n",
      "Make sure to iterate this step for **every** node.\n",
      "\n",
      "\n",
      "# 2. Configuration of your Master nodes \n",
      "\n",
      "We have 2 files to modify :\n",
      "* `spark-env.sh`\n",
      "* `spark-default.conf`\n",
      "\n",
      "Those two files are in the `conf` directory from `spark-2.3.2-bin-hadoop2.7`.\n",
      "Move in this directory and follow the following steps :\n",
      "\n",
      "## a. Save the original files :\n",
      "\n",
      "First of all, you need to save the original files.  \n",
      "\n",
      "```bash \n",
      "$ cp spark-env.sh.template spark-env.sh \n",
      "$ cp spark-defaults.conf.template spark-defaults.conf \n",
      "```\n",
      "![image](https://maelfabien.github.io/assets/images/Spark_copy.jpg)\n",
      "\n",
      "Let's begin the modification of Spark configuration files. \n",
      "\n",
      "## b. `spark-default.conf` file :\n",
      "\n",
      "Now, we'll add some lines in the `spark-default.conf` file : \n",
      "```bash\n",
      "spark.master                        spark://PRIVATE_DNS_MASTER1:7077,PRIVATE_DNS_MASTER2:7077\n",
      "spark.jars.packages                 datastax:spark-cassandra-connector:2.0.0-s_2.11\n",
      "spark.cassandra.connection.host     <PRIVATE_DNS_Slaves> (separated by ',')\n",
      "```\n",
      "\n",
      "Here's an example of my cluster : \n",
      "```bash\n",
      "spark.master                        spark://ip-172-31-35-3.ec2.internal:7077,ip-172-31-43-237.ec2.internal:7077\n",
      "spark.jars.packages                 datastax:spark-cassandra-connector:2.0.0-s_2.11\n",
      "spark.cassandra.connection.host     ip-172-31-33-255.ec2.internal,ip-172-31-40-97.ec2.internal,ip-172-31-43-212.ec2.internal,ip-172-31-45-7.ec2.internal,ip-172-31-32-5.ec2.internal\n",
      "```\n",
      "\n",
      "Example :  \n",
      "![image](https://maelfabien.github.io/assets/images/Spark_default.jpg)\n",
      "\n",
      "Save and quit.\n",
      "\n",
      "## c. `spark-env.sh` file :\n",
      "\n",
      "Now, we'll add some lines in the `spark-env.sh` file : \n",
      "```bash\n",
      "export SPARK_LOCAL_IP=<PRIVATE_DNS_this_NODE>\n",
      "export SPARK_MASTER_HOST=<PRIVATE_DNS_this_NODE>\n",
      "export SPARK_MASTER_OPTS=\"-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=<PRIVATE_DNS_Node_Zk1>:2181,<PRIVATE_DNS_Node_Zk2>:2182,<PRIVATE_DNS_Node_Zk3>:2183\"\n",
      "```\n",
      "\n",
      "Here's an example of my Master2 node : \n",
      "```bash\n",
      "export SPARK_LOCAL_IP=ip-172-31-43-237.ec2.internal\n",
      "export SPARK_MASTER_HOST=ip-172-31-43-237.ec2.internal\n",
      "export SPARK_MASTER_OPTS=\"-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=ip-172-31-33-255.ec2.internal:2181,ip-172-31-40-97.ec2.internal:2182,ip-172-31-39-129.ec2.internal:2183\"\n",
      "```\n",
      "\n",
      "For example :  \n",
      "![image](https://maelfabien.github.io/assets/images/Spark_env.jpg)\n",
      "\n",
      "# 3. Configuration of your slave nodes \n",
      "\n",
      "We have 2 files to modify :\n",
      "* `spark-env.sh`\n",
      "* `spark-default.conf`\n",
      "\n",
      "These two files are in the `conf` directory from `spark-2.3.2-bin-hadoop2.7`.\n",
      "Move inside this directory and follow the following modification.  \n",
      "\n",
      "## a. Save the original files :\n",
      "\n",
      "Save the original files :\n",
      "\n",
      "```bash \n",
      "$ cp spark-env.sh.template spark-env.sh \n",
      "$ cp spark-defaults.conf.template spark-defaults.conf \n",
      "```\n",
      "![image](https://maelfabien.github.io/assets/images/Spark_copy.jpg)\n",
      "\n",
      "Let's start the modification of Spark configuration files. \n",
      "\n",
      "## b. `spark-default.conf` file :\n",
      "\n",
      "Now open the file with `vi` and add some lines in the `spark-default.conf` file : \n",
      "```bash\n",
      "spark.master                        spark://PRIVATE_DNS_MASTER1:7077,PRIVATE_DNS_MASTER2:7077\n",
      "spark.jars.packages                 datastax:spark-cassandra-connector:2.0.0-s_2.11\n",
      "spark.cassandra.connection.host     <PRIVATE_DNS_Slaves> (separated by ',')\n",
      "```\n",
      "\n",
      "Here`s an example of my cluster : \n",
      "```bash\n",
      "spark.master                        spark://ip-172-31-35-3.ec2.internal:7077,ip-172-31-43-237.ec2.internal:7077\n",
      "spark.jars.packages                 datastax:spark-cassandra-connector:2.0.0-s_2.11\n",
      "spark.cassandra.connection.host     ip-172-31-33-255.ec2.internal,ip-172-31-40-97.ec2.internal,ip-172-31-43-212.ec2.internal,ip-172-31-45-7.ec2.internal,ip-172-31-32-5.ec2.internal\n",
      "```\n",
      "\n",
      "Example :  \n",
      "![image](https://maelfabien.github.io/assets/images/Spark_env.jpg)\n",
      "\n",
      "Save and quit.\n",
      "\n",
      "## c. `spark-env.sh` file :\n",
      "\n",
      "Open the file and add those lines in the `spark-env.sh` file : \n",
      "```bash\n",
      "export SPARK_LOCAL_IP=<PRIVATE_DNS_this_NODE>\n",
      "export SPARK_MASTER_HOST=<PRIVATE_DNS_MASTER1,PRIVATE_DNS_MASTER1>\n",
      "```\n",
      "\n",
      "Here`'s an example of my Slave1 node : \n",
      "```bash\n",
      "export SPARK_LOCAL_IP=ip-172-31-33-255.ec2.internal\n",
      "export SPARK_MASTER_HOST=ip-172-31-35-3.ec2.internal,ip-172-31-43-237.ec2.internal\n",
      "```\n",
      "\n",
      "Example :  \n",
      "![image](https://maelfabien.github.io/assets/images/Spark_env2.jpg)\n",
      "\n",
      "\n",
      "# 4. Add dependencies to connect Spark and Cassandra\n",
      "\n",
      "The configuration of Spark for both Slave and Master nodes is now finished.   \n",
      "Therefore, if you want to use Spark to launch Cassandra jobs, you need to add some dependencies in the `jars` directory from Spark.  \n",
      "\n",
      "This part is quite simple. You just need to download some `.jar` files.  \n",
      "\n",
      "For *each* node, in the `jars` directory., execute :\n",
      "```bash\n",
      "sudo wget http://central.maven.org/maven2/com/twitter/jsr166e/1.1.0/jsr166e-1.1.0.jar;\n",
      "sudo wget http://central.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.11/2.4.0/spark-cassandra-connector_2.11-2.4.0.jar\n",
      "```\n",
      "\n",
      "Those two files are very important to link Spark and Cassandra. \n",
      "\n",
      "# 5. Launch your Master and your Slave nodes\n",
      "\n",
      "For this final section, I'll show you how to launch Spark on all your nodes.  \n",
      "We will also check the Master Resilience handled by Zookeeper.  \n",
      "\n",
      "To make sure that your Spark cluster will be launched correctly, you should :   \n",
      "* Launch Zookeeper on all nodes on which the software is installed. \n",
      "* Launch the two Spark Master \n",
      "* Launch all Spark Worker nodes \n",
      "\n",
      "## a. Launch Zookeeper :\n",
      "\n",
      "To execute Zookeeper, go on the `Zookeeper_X/Bin/` directory and execute this command on each ZooKeeper node : \n",
      "\n",
      "```./zkServer.sh Start ```\n",
      "\n",
      "## b. Launch Spark on your Master nodes : \n",
      "\n",
      "Go back to the Spark directory, and from the  `sbin` directory, start your Master : \n",
      "\n",
      "```bash \n",
      "$ cd /home/ubuntu/spark-2.3.2-bin-hadoop2.7/sbin\n",
      "$ ./start-master.sh\n",
      "```\n",
      "![image](https://maelfabien.github.io/assets/images/Spark_master_launch.jpg)\n",
      "\n",
      "To get a snapshot of the cluster, n your browser, go on `<IPV4_IP_Public_Master>:8080` from the Node’s Public IP.\n",
      "\n",
      "You will see the following page :  \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Spark_master_web.jpg)\n",
      "\n",
      "## c. Launch Spark on your Slave nodes : \n",
      "\n",
      "The command line to launch Spark on all slave nodes is quite different. You have to specify the address of the Master 1 and 2.  \n",
      "```bash \n",
      "$ cd /home/ubuntu/spark-2.3.2-bin-hadoop2.7/sbin\n",
      "$ ./start-slave.sh spark://<PRIVATE_DNS_MASTER1>:7077,<PRIVATE_DNS_MASTER2>:7077\n",
      "```\n",
      "Here's an example :  \n",
      "![image](https://maelfabien.github.io/assets/images/Spark_slave_launch.jpg)\n",
      "\n",
      "Once all slave nodes are running, reload your master browser page. All Worker nodes will be attached to the Master 1 :   \n",
      "![image](https://maelfabien.github.io/assets/images/Spark_master_web2.jpg)\n",
      "\n",
      "## d. Master Resilience : \n",
      "\n",
      "What if you shutdown Master 1? Zookeeper will handle the selection of a new Master!\n",
      "\n",
      "When you'll stop Master 1, the Master 2 will be elected as the new Master and all Worker nodes will be attached to the newly elected master. \n",
      "\n",
      "If you want to visualize what's going on : \n",
      "* Shutdown Master 1\n",
      "* Check the port 8080 of Master 2\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Spark_master1_down.jpg)\n",
      "\n",
      "Check the transfer of Master : \n",
      "\n",
      "Then execute :\n",
      "![image](https://maelfabien.github.io/assets/images/Spark_master2_cat.jpg)\n",
      "\n",
      "You'll see that Zookeeper elected Master 2 as the primary master :  \n",
      "![image](https://maelfabien.github.io/assets/images/Spark_master2_election.jpg)\n",
      "\n",
      "From the Spark UI of Master 2, you'll see that all slave nodes are now attached :  \n",
      "![image](https://maelfabien.github.io/assets/images/Spark_master2_web.jpg)\n",
      "\n",
      "> **Conclusion **: We covered the basics of setting up Apache Spark on an AWS EC2 instance. We ran both the Master and Slave daemons on the same node. Finally, we demonstrated the resilience of our Masters thanks to Zookeeper. \n",
      "---\n",
      "title: Classify Images using Vision API and Cloud AutoML - Week 2 Module 2\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Road to Google Cloud Platform Certification\"\n",
      "---\n",
      "\n",
      "# Google pre-trained models\n",
      "\n",
      "ML for unstructured datasets, like images, drives value for businesses. ML for unstructured data can be found in different business use cases :\n",
      "- image classification\n",
      "- NLP email classification\n",
      "- chatbots\n",
      "- text extraction from an image\n",
      "- ...\n",
      "\n",
      "Google offers several pre-built models for unstructured data :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_109.jpg)\n",
      "\n",
      "The Vision API can :\n",
      "- label an image\n",
      "- detect a face\n",
      "- perform Optical Character Recognition to extract text from the image\n",
      "- detect explicit content \n",
      "- detect landmark (popular monuments for example)\n",
      "- detect logo\n",
      "\n",
      "You can try the Vision API on your own on [https://cloud.google.com/vision/](https://cloud.google.com/vision/). I've tried to upload a picture of a horse :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_110.jpg)\n",
      "\n",
      "It is correctly identified as a horse. The response of the algorithm is a JSON file :\n",
      "\n",
      "```\n",
      "{\n",
      "    \"cropHintsAnnotation\": {\n",
      "        \"cropHints\": [\n",
      "            {\n",
      "            \"boundingPoly\": {\n",
      "                \"vertices\": [\n",
      "                {\n",
      "                    \"x\": 419\n",
      "                },\n",
      "                {\n",
      "                    \"x\": 1499\n",
      "                },\n",
      "                {\n",
      "                    \"x\": 1499,\n",
      "                    \"y\": 1332\n",
      "                },\n",
      "                {\n",
      "                    \"x\": 419,\n",
      "                    \"y\": 1332\n",
      "                }\n",
      "                ]\n",
      "            },\n",
      "    \"confidence\": 0.79999995,\n",
      "    \"importanceFraction\": 0.7\n",
      "    },\n",
      "...\n",
      "```\n",
      "\n",
      "All those tools can be tried on the Web :\n",
      "- Video : [https://cloud.google.com/video-intelligence/](https://cloud.google.com/video-intelligence/)\n",
      "- Translate : [https://cloud.google.com/translate/](https://cloud.google.com/translate/)\n",
      "- Speech to text : [https://cloud.google.com/speech-to-text/](https://cloud.google.com/speech-to-text/)\n",
      "- ...\n",
      "\n",
      "# Create a chatbot with DialogFlow\n",
      "\n",
      "We will use Dialogflow. It is an end to end developer platform for building rich and natural conversations. It now has a community of over 600'000 developers. \n",
      "\n",
      "It has built-in features :\n",
      "- Entity recognition\n",
      "- Sentiment analysis\n",
      "- Content classification\n",
      "- Multi-language support\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_111.jpg)\n",
      "\n",
      "We can train the agent quickly, with only a few examples. Otherwise, we can choose among pre-trained agents\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_112.jpg)\n",
      "\n",
      "Dialogflow offers analytics on how users interact with your chatbot. It can be integrated on most popular platforms in just a click :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_113.jpg)\n",
      "\n",
      "# Fine-tune an image classification with Vision AutoML\n",
      "\n",
      "Suppose that we need to make a specific classification at a certain granularity level: cloud classification for weather forecast. We can use Vision AutoML to fine-tune Vision API to our specific challenge.\n",
      "\n",
      "We first upload the images corresponding to each label :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_114.jpg)\n",
      "\n",
      "Then, train the model :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_115.jpg)\n",
      "\n",
      "And evaluate the model :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_116.jpg)\n",
      "\n",
      "In the last tab, we can make predictions by either uploading images, or generating a new API. Here is a summary of when to use Vision AutoML :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_117.jpg)\n",
      "\n",
      "\n",
      "---\n",
      "title: A guide to Inception Model in Keras\n",
      "layout: post\n",
      "tags: [deeplearning]\n",
      "subtitle : \"Deep Neural Networks\"\n",
      "---\n",
      "\n",
      "Inception is a deep convolutional neural network architecture that was introduced in 2014. It won the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC14). It was mostly developed by Google researchers. Inception's name was given after the eponym movie.\n",
      "\n",
      "The original paper can be found [here](https://arxiv.org/pdf/1409.4842.pdf).\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "Inception architecture can be used in computer vision tasks that imply convolutional filters.\n",
      "\n",
      "## What is an inception module?\n",
      "\n",
      "In Convolutional Neural Networks (CNNs), a large part of the work is to choose the right layer to apply, among the most common options (1x1 filter, 3x3 filter, 5x5 filter or max-pooling). All we need is to find the optimal local construction and to repeat it spatially. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/inception.jpg)\n",
      "\n",
      "As these “Inception modules” are stacked on top of each other, their output correlation statistics are bound to vary: as features of higher abstraction are captured by higher layers, their spatial concentration is expected to decrease suggesting that the ratio of 3×3 and 5×5 convolutions should increase as we move to higher layers.\n",
      "\n",
      "However, the computational cost of such a solution highly increases. For this reason, in the figure `b`, dimension reduction through 1X1 convolutions are used as dimension reduction techniques.\n",
      "\n",
      "## GoogLeNet\n",
      "\n",
      "The most famous Inception-based algorithm is GoogLeNet, which corresponds to the team name of Google's team in ILSVRC14. This was an homage to Yann LeCuns who introduced LeNet 5 network. \n",
      "\n",
      "The architecture they went for was the following :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lenet.jpg)\n",
      "\n",
      "## In Keras \n",
      "\n",
      "Let's import the required packages :\n",
      "\n",
      "```python\n",
      "import keras \n",
      "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
      "from keras.layers.convolutional import Conv2D, MaxPooling2D, SeparableConv2D\n",
      "from keras.regularizers import l2\n",
      "from keras.optimizers import SGD, RMSprop\n",
      "from keras.utils import to_categorical\n",
      "from keras.layers.normalization import BatchNormalization\n",
      "from keras.utils.vis_utils import plot_model\n",
      "from keras.layers import Input, GlobalAveragePooling2D\n",
      "from keras import models\n",
      "from keras.models import Model\n",
      "```\n",
      "\n",
      "\n",
      "Import our data. I'm working on the Facial Emotion Recognition 2013 challenge from Kaggle. The `path` links to my local storage folder :\n",
      "\n",
      "``` python\n",
      "X_train = np.load(path + \"X_train.npy\")\n",
      "X_test = np.load(path + \"X_test.npy\")\n",
      "y_train = np.load(path + \"y_train.npy\")\n",
      "y_test = np.load(path + \"y_test.npy\")\n",
      "````\n",
      "\n",
      "Define the input dimension and the number of classes we want to get in the end :\n",
      "\n",
      "``` python\n",
      "shape_x = 48\n",
      "shape_y = 48\n",
      "nRows,nCols,nDims = X_train.shape[1:]\n",
      "input_shape = (nRows, nCols, nDims)\n",
      "classes = np.unique(y_train)\n",
      "nClasses = len(classes)\n",
      "```\n",
      "\n",
      "Now, let's build our first inception layer!\n",
      "\n",
      "```python\n",
      "input_img = Input(shape=(shape_x, shape_y, 1))\n",
      "\n",
      "### 1st layer\n",
      "layer_1 = Conv2D(10, (1,1), padding='same', activation='relu')(input_img)\n",
      "layer_1 = Conv2D(10, (3,3), padding='same', activation='relu')(layer_1)\n",
      "\n",
      "layer_2 = Conv2D(10, (1,1), padding='same', activation='relu')(input_img)\n",
      "layer_2 = Conv2D(10, (5,5), padding='same', activation='relu')(layer_2)\n",
      "\n",
      "layer_3 = MaxPooling2D((3,3), strides=(1,1), padding='same')(input_img)\n",
      "layer_3 = Conv2D(10, (1,1), padding='same', activation='relu')(layer_3)\n",
      "\n",
      "mid_1 = tensorflow.keras.layers.concatenate([layer_1, layer_2, layer_3], axis = 3)\n",
      "```\n",
      "\n",
      "As you might see, we are implementing figure `b` from the picture above. We can now flatten the output and add some dense layers :\n",
      "\n",
      "```python\n",
      "flat_1 = Flatten()(mid_1)\n",
      "\n",
      "dense_1 = Dense(1200, activation='relu')(flat_1)\n",
      "dense_2 = Dense(600, activation='relu')(dense_1)\n",
      "dense_3 = Dense(150, activation='relu')(dense_2)\n",
      "output = Dense(nClasses, activation='softmax')(dense_3)\n",
      "```\n",
      "\n",
      "This quite simple architecture leads to 83'760'487 trainable parameters! Of course, one can even go deeper by addition layers connected to the `mid_1` layer.\n",
      "\n",
      "We can build the model :\n",
      "\n",
      "```python\n",
      "model = Model([input_img], output)\n",
      "```\n",
      "\n",
      "If you would like to visualize the model architecture, use `plot_model` :\n",
      "\n",
      "```python\n",
      "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/inception_mod.jpg)\n",
      "\n",
      "We are finally ready to compile the model.\n",
      "\n",
      "```python\n",
      "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
      "\n",
      "batch_size = 128\n",
      "epochs = 150\n",
      "```\n",
      "\n",
      "And run it!\n",
      "\n",
      "```python\n",
      "history = model.fit(X_train, y_train, epochs=150, batch_size=64, validation_data=(X_test, y_test))\n",
      "````\n",
      "\n",
      "The Github repository of this article can be found [here](https://github.com/maelfabien/Machine_Learning_Tutorials).\n",
      "\n",
      "> **Conclusion** : Inception models remain expensive to train. Transfer learning brings part of the solution when it comes to adapting such algorithms to your specific task. \n",
      "---\n",
      "title: Binary output prediction and Logistic Regression\n",
      "layout: post\n",
      "tags: [statistics]\n",
      "subtitle : \"Logistic Regression\"\n",
      "---\n",
      "\n",
      "So far, with the linear model, we have seen how to predict continuous variables. What happens when you want to classify with a linear model?\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# Linear Probability Model\n",
      "\n",
      "Suppose that our aim is to do binary classification : $$ y_i = \\{0,1\\} $$. Let's consider the model :\n",
      "\n",
      "$$ y = \\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k + u $$\n",
      "\n",
      "Where $$ E(u \\mid X_1, ... X_k) = 0 $$. How can we perform binary classification with this model? Let's start with a dataset in which you have binary observations and you decide to fit a linear regression on top of it. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/log_1.jpg)\n",
      "\n",
      "Statistically speaking, the model above is incorrect :\n",
      "- we would need to define a threshold under which we classify as 0, and above which we classify 1\n",
      "- what if the values are greater than 1? Or smaller than 0?\n",
      "- ...\n",
      "\n",
      "Linear probability model has however one main advantage: the coefficients remain easily interpretable!\n",
      "\n",
      "$$ \\Delta P(Y=1 \\mid X) = \\beta_j \\Delta X_j $$ \n",
      "\n",
      "In other words, the impact of a coefficient can be measured as a contribution percentage to the final classification. Overall, this model needs to be adjusted/transformed to throw the predicted between values between 0 and 1. This is the main idea of logistic regression!\n",
      "\n",
      "# Logistic Regression\n",
      "\n",
      "We are now interested in $$ P(Y=1 \\mid X) = P(Y=1 \\mid X_1, X_2, .. X_k) = G(\\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k) $$. As you might guess, the way we define $$ G $$ will define the way we make our mapping. \n",
      "- If $$ G $$ is linear, this is obviously the **linear** regression\n",
      "- If $$ G $$ is a sigmoid : $$ G(z) = \\frac {1} {1 + e^{-z}} $$, then the model is a **logistic** regression\n",
      "- If $$ G $$ is a normal transformation $$ G(z) = \\Phi(z) $$, then the model is a **probit** regression\n",
      "\n",
      "In this article, we'll focus on logistic regression.\n",
      "\n",
      "## Sigmoid and Logit transformations\n",
      "\n",
      "The **sigmoid** transformation is used to map values between 0 and 1 :\n",
      "\n",
      "$$ Sig(z) = \\frac {1} {1 + e^{-z}} $$\n",
      "\n",
      "To understand precisely what this does, let's implement it in Python :\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import math\n",
      "\n",
      "def sigmoid(x):\n",
      "    a = []\n",
      "    for item in x:\n",
      "        a.append(1/(1+math.exp(-item)))\n",
      "    return a\n",
      "```\n",
      "\n",
      "Then, plot if for a range of values of $$ X $$ :\n",
      "\n",
      "```python\n",
      "x = np.arange(-3., 3., 0.2)\n",
      "sig = sigmoid(x)\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(x,sig, label='sigmoid')\n",
      "plt.plot(x,x, label='input')\n",
      "plt.title(\"Sigmoid Function\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/log_2.jpg)\n",
      "\n",
      "The inverse transform is called the **logit** transform. It takes values that are in the range 0 to 1 and maps them to a linear form.\n",
      "\n",
      "```python\n",
      "def logit(x):\n",
      "    a = []\n",
      "    for item in x:\n",
      "        a.append(math.log(item/(1-item)))\n",
      "    return a\n",
      "```\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(x,sig, label='sigmoid')\n",
      "plt.plot(x,logit(sig), label='logit tranform')\n",
      "plt.title(\"Sigmoid - Logit Function\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/log_3.jpg)\n",
      "\n",
      "## The logistic regression model\n",
      "\n",
      "### Partial effect\n",
      "\n",
      "In the logistic regression model : \n",
      "\n",
      "$$ P(Y=1) = \\frac {1} {1 + exp^{-(\\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p)}} $$\n",
      "\n",
      "How can we interpret the partial effect of $$ X_1 $$ on $$ Y $$ for example ? Well, the weights in the logistic regression **cannot** be interpreted as for linear regression. We need to use the logit transform :\n",
      "\n",
      "$$ \\log( \\frac {P(y=1)} {1-P(y=1)} ) = \\log ( \\frac {P(y=1)} {P(y=0)} ) $$ \n",
      "\n",
      "$$ = odds = \\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k $$\n",
      "\n",
      "We define the this ratio as the \"odds\". Therefore, to estimate the impact of $$ X_j $$ increasing by 1 unit, we can compute it this way :\n",
      "\n",
      "$$ \\frac {odds_{X_{j+1}}} {odds} = \\frac {exp^{\\beta_0 + \\beta_1 X_1 + ... + \\beta_j (X_j + 1) + ... + \\beta_k X_k}} {exp^{\\beta_0 + \\beta_1 X_1 + ... + \\beta_j X_j + ... + \\beta_k X_k}} $$\n",
      "\n",
      "$$ = exp^{\\beta_j (X_j + 1) - \\beta_j X_j} = exp^{\\beta_j} $$\n",
      "\n",
      "A change in $$ X_j $$ by one unit increases the log odds ratio by the value of the corresponding weight.\n",
      "\n",
      "### Test Hypothesis\n",
      "\n",
      "To test for a single coefficient, we apply, as previously, a Student test :\n",
      "\n",
      "$$ t_{stat} = \\frac {\\beta} {\\sigma(\\beta)} $$\n",
      "\n",
      "For multiple hypotheses, we choose the  Likelihood Ratio tests. The coefficients are now normally distributed, so the sum of several coefficients follows a $$ X^2 $$ (Chi-Squared) distribution.\n",
      "\n",
      "The Likelihood ratio test is implemented in most stats packages in Python, R, and Matlab, and is defined by :\n",
      "\n",
      "$$ LR = 2(L_{ur} - L_r) $$\n",
      "\n",
      "We reject the null hypothesis if $$ LR > Crit_{val} $$.\n",
      "\n",
      "### Important parameters\n",
      "\n",
      "In the Logistic Regression, the single most important parameter is the regularization factor. It is essential to choose properly the type of regularization to apply (usually by Cross-Validation).\n",
      "\n",
      "### Implementation in Python\n",
      "\n",
      "We'll use Scikit-Learn version of the Logistic Regression, for binary classification purposes. We'll be using the Breast Cancer database.\n",
      "\n",
      "```python\n",
      "# Imports\n",
      "from sklearn.datasets import load_breast_cancer\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import f1_score\n",
      "\n",
      "data = load_breast_cancer()\n",
      "```\n",
      "\n",
      "We then split the data into train and test :\n",
      "\n",
      "```python\n",
      "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size = 0.25)\n",
      "```\n",
      "\n",
      "By default, L2-Regularization is implemented. Using L1-Regularization, we achieve :\n",
      " \n",
      " ```\n",
      " lr = LogisticRegression(penalty='l1')\n",
      " lr.fit(X_train, y_train)\n",
      " \n",
      " y_pred = lr.predict(X_test)\n",
      " print(accuracy_score(y_pred, y_test))\n",
      " print(f1_score(y_pred, y_test))\n",
      " ```\n",
      " \n",
      " ```\n",
      " 0.958041958041958\n",
      " 0.9655172413793104\n",
      " ```\n",
      " \n",
      " If we move on to L2-Regularization :\n",
      " \n",
      " ```python\n",
      " lr = LogisticRegression(penalty='l2', solver='lbfgs')\n",
      " lr.fit(X_train, y_train)\n",
      " \n",
      " y_pred = lr.predict(X_test)\n",
      " print(accuracy_score(y_pred, y_test))\n",
      " print(f1_score(y_pred, y_test))\n",
      " ```\n",
      " \n",
      " ```\n",
      " 0.9440559440559441\n",
      " 0.9540229885057472\n",
      " ```\n",
      " \n",
      " We notice the importance of the choice of :\n",
      " - the solver\n",
      " - the regularization\n",
      " \n",
      " We can now illustrate the impact of the tolerance factor C. The larger C is, the less restrictive is the regularization.\n",
      " \n",
      " ```python\n",
      " from sklearn.model_selection import GridSearchCV\n",
      " \n",
      " parameters = {'C':[0.1, 0.5, 1, 2, 5, 10, 100]}\n",
      " lr = LogisticRegression(penalty='l2', max_iter = 5000, solver='lbfgs')\n",
      " \n",
      " clf = GridSearchCV(lr, parameters, cv=5)\n",
      " clf.fit(X_train, y_train)\n",
      " ```\n",
      " \n",
      " We fetch the best parameters using :\n",
      " \n",
      " ```\n",
      " clf.best_params_\n",
      " ```\n",
      " \n",
      " And find :\n",
      " \n",
      " ```\n",
      " {'C': 10}\n",
      " ```\n",
      " \n",
      " Using this classifier, we acheive the following results :\n",
      " \n",
      " ```python\n",
      " y_pred = clf.predict(X_test)\n",
      " print(accuracy_score(y_pred, y_test))\n",
      " print(f1_score(y_pred, y_test))\n",
      " ```\n",
      "\n",
      "```\n",
      "0.958041958041958\n",
      "0.9655172413793104\n",
      "```\n",
      "\n",
      "We get the same results as with the L1-Penalty, for a rather large value of C. This illustrates well the importance of wisely choosing those parameters, since a 2% accuracy or F1-Score different on a Breast Cancer detection algorithm can make a big difference.\n",
      "\n",
      "> I hope you enjoyed this article. Don't hesitate to comment if you have any question.\n",
      "\n",
      "Sources :\n",
      "- [Interpretable Machine Learning book](https://christophm.github.io/interpretable-ml-book/logistic.html)\n",
      "---\n",
      "title: Bayes Classifier\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Machine Learning Basics\"\n",
      "---\n",
      "\n",
      "The Bayes Classifier is a binary classification algorithm. It is often introduced as one of the first algorithms to master in the field of machine learning. Bayes classifier is called Generative since it is trained to learn the distribution of both marginal classes, instead of the decision frontier itself.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "In each article, I try to summarize the key idea of each algorithm in just a few lines. \n",
      "\n",
      "## Key ideas\n",
      "\n",
      "- Generative Model that tries to estimate $$ P (X = x \\mid Y = 1) $$ and $$ P (X = x \\mid Y = -1) $$\n",
      "- Used for binary classification\n",
      "- Relies on knowing $$ \\eta (x) = P (Y = 1 \\mid X = x) $$ and can therefore never be acheived\n",
      "- Similar to knowing the decision frontier itself\n",
      "\n",
      "## Bayes Classifier\n",
      "\n",
      "In binary classification, we'll try to predict labels : $$ Y $$ is either $$ 1 $$ or $$ -1 $$ . The datas we use is a matrix $$ X $$ of observations with $$ d $$ features.\n",
      "\n",
      "Let $$ \\eta (x) = P (Y = 1 \\mid X = x) $$ be the *prior* probability. \n",
      "\n",
      "Our aim is to define a classification function $$ g $$, i.e $$ g(X) \\in (-1,1) $$ such that :\n",
      "$$ g(x) = 1 if \\eta(x) ≥ \\frac {1} {2} $$ and $$ g(x) = -1 if \\eta(x) < \\frac {1} {2} $$.\n",
      "\n",
      "These two constraints can be combined in a single constraint :\n",
      "\n",
      "$$ g(x) = 2 I(\\eta(x) ≥ \\frac {1} {2}) -1 $$\n",
      "\n",
      "$$ g(x) $$ is called the **Bayes Classifier**.\n",
      "\n",
      "The Bayes classifier can never be reached since it would imply knowing the decision frontier before the classification process. Bayes classifier is the optimal classifier!\n",
      "\n",
      "## Theory behind Bayes\n",
      "\n",
      "Bayes classifier uses the so-called Bayes Rule whose fundamental result states that for two given event $$ Y $$ and $$ X $$ :\n",
      "\n",
      "$$ ℙ(Y \\mid X) = \\frac {ℙ(X \\mid Y)P(Y)} {P(X)} $$\n",
      "\n",
      "Bayes classifier is a Generative model in the sense that we estimate the decision frontier using the marginal distributions $$ ℙ(X \\mid Y) $$.\n",
      "\n",
      "Graphically, the distributions of $$ ℙ(X \\mid Y = 1) $$ and  $$ ℙ(X \\mid Y = -1) $$ are represented by the two ellipses, and the decision boundary is the black line.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/bayes.jpg)\n",
      "\n",
      "## Plug-in principle\n",
      "\n",
      "In most of the other classifiers we'll cover, we are going to use the plug-in principle. The idea is to estimate $$ \\eta (x) $$ the desicion boundary by $$ \\hat{\\eta} (x) $$, and then plug-in this estimator into the classifier, such that :\n",
      "\n",
      "$$ g(x) = 2 I(\\hat{\\eta}(x) ≥ \\frac {1} {2}) -1 $$\n",
      "\n",
      "\n",
      "> **Conclusion** : In the next articles, we'll cover the most common classifiers in supervised learning theory. Don't hesitate to drop a comment if you have any question. \n",
      "\n",
      "---\n",
      "title: Full guide to Linear Regression (Part 1)\n",
      "layout: post\n",
      "tags: [statistics]\n",
      "subtitle : \"Linear Model\"\n",
      "---\n",
      "\n",
      "Before starting this series of articles on Machine Learning, I thought it might be a good idea to go through some Statistical recalls. This first article is an introduction to some more detailed articles on statistics. I will be illustrating some concepts using Python codes. \n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# Linear Regression in 1 dimension\n",
      "\n",
      "## Framework\n",
      "\n",
      "The most basic statistical foundation of machine learning models is the following : $$ Y_i =  f(X_i) + {\\epsilon} $$ . This simple equation states the following :\n",
      "- we suppose we have $$ n $$ observations of a dataset, and we pick on $$ i^{th} $$\n",
      "- $$ Y_i $$ is the output of this observation called the target\n",
      "- $$ X_i $$ is called a feature and is an independent variable we observe\n",
      "- $$ f $$ is the real model that states the link between the features and the output\n",
      "- $$ {\\epsilon} $$ is the noise of the model. The data we observe usually do not stand on a straight line, because there are variations of the measure in real life. \n",
      "\n",
      "The framework we consider here is pretty simple. We only have one feature per observation. In real life, we do usually have several features per observation. An example of this might be the following: you work as a data scientist in an insurance company. You would typically have several pieces of information on each customer: name, address, age, family, car, salary...\n",
      "\n",
      "## Statistical motivation\n",
      "\n",
      "Suppose that we have $$ n $$ observations, and for each observation i, we have one feature $$ X_{i} $$. \n",
      "\n",
      "Our role is to identify the link between $$ Yi $$ and $$ X_{i} $$ in order to be able to predict $$ Y_{n+1} $$ . To build our prediction, we need to identify :\n",
      "- a model, which is the link between $$ Y_i $$ and the features, represented here by the function $$ f $$ in :\n",
      "$$ Y_i =  f(X_i) + {\\epsilon} $$ . In real life, this model $$ f $$ is unknown in real life and is estimated by : $$ \\hat{Y}_i =  \\hat{f}(X_i) $$ where the hat describes an estimation. We try to estimate $$ \\hat{f} $$ that corresponds to real life $$ f $$.\n",
      "- we also need to identify parameters, which will help our prediction to get as close as possible to real datas. The parameters are usually set in order to minimize the distance between our model and the datas, i.e they are set so that the derivative of a loss function (for example : $$ \\sum(\\hat{Y_i} - Y_i)^2 ) $$ is equal to 0.\n",
      "\n",
      "The true model and the true parameters in real life are unknown. For this reason, we make a selection among several models (linear or non-linear). The model selection is usually based on an exploratory data analysis. An article will further develop this question.\n",
      "\n",
      "From now on, we will focus on the most basic model called the unidimensional linear regression. This model states that there is a linear link between the features and the dependent variable. \n",
      "\n",
      "The true model we expect is the following :  $$ Y_i = {\\beta}_0 + {\\beta}_1{X}_{i} + {\\epsilon}_i $$\n",
      "- $$ {\\beta}_0 $$ is called the intercept, it is a constant term\n",
      "- $$ {\\beta}_1 $$ is the coefficient associated with $$ X_i $$ . It describes the weight of $$ X_i $$ on the final output.\n",
      "- $$ {\\epsilon}_i $$ is called the residual. It is a white noise term that explains the variability in real life datas. \n",
      "\n",
      "The hypothesis on $$ {\\epsilon} $$ are :\n",
      "- $$ E({\\epsilon}) = 0 $$ , i.e a white noise condition\n",
      "- $$ {\\epsilon}_i ∼ iid  {\\epsilon} $$ for all i = 1,...,n, i.e a homoskedasticity condition\n",
      "\n",
      "However, the true parameters remain unknown as these are the ones we expect to estimate. Therefore, the model we estimate is the following : $$ \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1{X}_{i} $$ . \n",
      "\n",
      "A classic example would be to try to predict the average housing market price in the economy based on the GDP for example. We would expect pretty much a linear relationship between both. You can find the file I will be using [here]( https://maelfabien.github.io/assets/files/housing.txt)\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import sklearn.linear_model as lm\n",
      "import math\n",
      "from scipy import stats\n",
      "\n",
      "df = pd.read_csv('housing.txt', sep=\" \")\n",
      "df\n",
      "```\n",
      "![image](https://maelfabien.github.io/assets/images/Tab1.jpg){:height=\"20%\" width=\"20%\"}\n",
      "\n",
      "```python\n",
      "#Scatter plot\n",
      "plt.figure(figsize=(7,5))\n",
      "plt.scatter(df['gdp'], df['house'])\n",
      "\n",
      "#On ajoute les titres des axes\n",
      "plt.xlabel(\"Gross Domestic Product)\", fontsize = 12)\n",
      "plt.ylabel(\"Housing price\", fontsize = 12)\n",
      "plt.title(\"Relation between GDP and Average housing prices\")\n",
      "\n",
      "plt.show()\n",
      "```\n",
      "![image](https://maelfabien.github.io/assets/images/Graph1.jpg){:height=\"60%\" width=\"60%\"}\n",
      "\n",
      "Our intuition was right. The relationship looks linear between the GDP and the average housing price. How do we move on from here?\n",
      "\n",
      "## Parameter estimate\n",
      "\n",
      "Now that we know the relationship looks linear, the next step is to estimate the coefficients $$ \\hat{\\beta}_0 , \\hat{\\beta}_1 $$ in order to draw a line that fits our datas. In the linear regression, estimating the parameter means identifying the Betas : $$ \\hat{\\beta}_0 , \\hat{\\beta}_1 $$ so that they minimize the distance with the real datas : \n",
      "\n",
      "$$ argmin \\sum(Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1{X}_{i})^2 $$ .\n",
      "\n",
      "> This approach is known as the Ordinary Least Squares (OLS). We minimize the square of the distance between each data point and our estimate. Why the square? Because the derivative is easier to find, and we take both positive and negative deviations into account.\n",
      "\n",
      "The solution of the OLS problem in 1 dimension is the following :\n",
      "$$ \\hat{\\beta}_1 = \\frac{\\sum(X_i – \\bar{X}) (Y_i – \\bar{Y})} {\\sum(X_i – \\bar{X})^2} $$\n",
      "$$ \\hat{\\beta}_0 = \\bar{Y} – \\hat{\\beta}_1 \\bar{X} $$\n",
      "\n",
      "We can compute those results in Python :\n",
      "\n",
      "```python\n",
      "# Slope Beta 1 :\n",
      "x = df['gdp']\n",
      "x_bar = mean(df['gdp'])\n",
      "y = df['house']\n",
      "y_bar = mean(df['house'])\n",
      "\n",
      "beta1 = ((x - x_bar)*(y-y_bar)).sum() / ((x-x_bar)**2).sum()\n",
      "print(\"Beta_1 coefficient estimate : \" + str(round(beta1,4)))\n",
      "```\n",
      "`0.1012`\n",
      "\n",
      "```python\n",
      "beta0 = y_bar - beta1 * x_bar\n",
      "print(\"Beta_0 coefficient estimate : \" + str(round(beta0,4)))\n",
      "```\n",
      "`-1794.0861`\n",
      "\n",
      "We can plot the estimated linear regression to assess whether it looks good or not :\n",
      "```python \n",
      "#Scatter plot\n",
      "plt.figure(figsize=(7,5))\n",
      "plt.scatter(df['gdp'], df['house'])\n",
      "plt.plot(df['gdp'], beta1 * df['gdp'] + beta0, c='r')\n",
      "#On ajoute les titres des axes\n",
      "plt.xlabel(\"Gross Domestic Product)\", fontsize = 12)\n",
      "plt.ylabel(\"Housing price\", fontsize = 12)\n",
      "plt.title(\"Relation between GDP and Average housing prices\")\n",
      "\n",
      "plt.show()\n",
      "```\n",
      "![image](https://maelfabien.github.io/assets/images/Graph2.jpg){:height=\"60%\" width=\"60%\"}\n",
      "\n",
      "It looks great ! There is a pretty efficient way to do this using Scikit Learn built-in functions. \n",
      "```python\n",
      "x_1 = df[['gdp']]\n",
      "skl_linmod = lm.LinearRegression(fit_intercept = True).fit(x_1,y)\n",
      "\n",
      "beta1_sk = skl_linmod.coef_[0]\n",
      "beta0_sk = skl_linmod.intercept_\n",
      "```\n",
      "\n",
      "This will head the same results. Alright, so far we are satisfied by how our estimate looks like. But how do we assess how good our estimate is?\n",
      "\n",
      "## Assess the accuracy of the estimate using R2\n",
      "\n",
      "A common metric used to assess the overall fit of our model is the $$ R^2 $$ metric (pronounced R squared). The $$ R^2 $$ measures the percentage of the variance of the datas we manage to explain with our model :\n",
      "\n",
      "$$ {R^2} = \\frac{\\sum(Y_i – \\hat{Y})^2} {\\sum(Y_i – \\bar{Y})^2} $$\n",
      "\n",
      "```python\n",
      "r_carre = 1 - (((y-(beta0 + beta1 * x))**2).sum())/((y-y_bar)**2).sum()\n",
      "```\n",
      "`0.861`\n",
      "\n",
      "The $$ R^2 $$ takes values between 0 and 1. The closer we are to 1, the more variance we tend to explain. However, adding variables will always make the $$ R^2 $$ increase, even if the variable we added is not related to the output at all. I do invite you to check the adjusted - $$ R^2 $$ if you would like to know more about how to handle this issue.\n",
      "\n",
      "## Are coefficients significant?\n",
      "\n",
      "Alright, we have now fitted our regression line and estimated the goodness of the fit. But all the variables included in the model might not be worth including?\n",
      "What if for example $$ {\\beta}_1 $$ is not significantly different from 0 ?\n",
      "\n",
      "Both parameters $$ {\\beta}_0, {\\beta}_1 $$can be described by three metrics :\n",
      "- an expectation\n",
      "- a bias\n",
      "- a variance\n",
      "\n",
      "The expectation of the parameter corresponds... to its expected value. Nothing really new here : $$ E(\\hat{\\beta}_j) $$\n",
      "\n",
      "The bias corresponds to how far we are from the actual value. It is given by : $$ E(\\hat{\\beta}_j) - {\\beta}_j $$ . The true bias is typically unknown, as we try to estimate $$ {\\beta}_j $$ . If the bias is 0, we say that the estimator is unbiased.\n",
      "\n",
      "The variance defines the stability of our estimator regarding the observations. Indeed, the features might be highly spread, which would mean a pretty big variance. \n",
      "\n",
      "It can be shown that the variance of $$ {\\beta}_1 $$ is given by :\n",
      "$$ \\hat{\\sigma}{_\\hat{\\beta_1}} = \\frac{\\hat{\\sigma}} {\\sqrt{\\sum(X_i – \\bar{X})^2}} $$\n",
      "\n",
      "And the one of  $$ {\\beta}_0 $$ by :\n",
      "$$ \\hat{\\sigma}{_\\hat{\\beta_0}} = \\hat{\\sigma} \\sqrt{\\frac{1} {n} + \\frac{\\sum(X_i)^2} {\\sum(X_i – \\bar{X})^2}} $$\n",
      "\n",
      "Where the estimated variance $$ \\hat{\\sigma} $$ is defined by : $$ \\hat{\\sigma} = \\sqrt\\frac{\\sum(Y_i – \\hat{Y}_i)^2} {n – p-1} $$ . This is an unbiaised estimator of the variance.\n",
      "\n",
      "Now, we do have all the elements required to compute the standard errors of our parameters :\n",
      "```python\n",
      "sigma2 = math.sqrt(np.var(y))\n",
      "sigma_beta1 = math.sqrt(sigma2 / ((x-x_bar)**2).sum())\n",
      "```\n",
      "`0.00264`\n",
      "\n",
      "```python\n",
      "sigma_beta0 = math.sqrt(sigma2 * (1/len(y) + (x_bar**2)/((x-x_bar)**2).sum()))\n",
      "```\n",
      "`54.09148485682019`\n",
      "\n",
      "Graphically, biais and variance can be represented this way :\n",
      "![image](https://maelfabien.github.io/assets/images/bias.jpg){:height=\"50%\" width=\"50%\"}\n",
      "\n",
      "Why are we spending time on those metrics?\n",
      "Those metrics allow us to compute what we call test hypothesis.\n",
      "\n",
      "## Test Hypothesis\n",
      "\n",
      "For each parameter, we want to test whether the parameter in question has a real impact on the output or not, to avoid adding dimensions that bring no significant information. In the linear regression : $$ \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1{X}_{i} $$ , it would mean testing whether the Betas are significantly different from 0 or not. \n",
      "\n",
      "To do so, we proceed to a statistical test. If our aim is the state if the parameter is significantly different from 0, we are doing a test with :\n",
      "$$ H_0 $$ the null hypothesis : $$ {\\beta}_j = 0 $$ \n",
      "and $$ H_1 $$ the alternative hypothesis : $$ {\\beta}_j ≠ 0 $$ . \n",
      "\n",
      "> Some further theory is needed here : Recall the Central Limit Theorem.\n",
      "$$ \\sqrt{n} \\frac{\\bar{Y}_n - {\\mu}} {\\sigma} $$ converges to $$ ∼ {N(0,1)} $$ as n tends to infinity if $$ {\\sigma} $$ is knowm.\n",
      "\n",
      "In case $$ {\\sigma} $$ is unknown, Slutsky's Lemma states that $$ \\sqrt{n} \\frac{\\bar{Y}_n - {\\mu}} {\\hat{\\sigma}} $$ converges to $$ ∼ {N(0,1)} $$ if $$ {\\hat{\\sigma}} $$ converges to $$ {\\sigma} $$ .\n",
      "\n",
      "Most of the time, $$ {\\sigma} $$ is unknown. From this point, it can be shown that :\n",
      "$$ \\hat{T}_j = \\frac{\\hat{\\beta}_j - 0} {\\hat{\\sigma}_j} ∼ {\\tau}_{n-p-1} $$ where $$ {\\tau}_{n-p-1} $$ and $$ n-p-1 $$ is the degrees of freedom (p is the dimension, equal to 1 here).\n",
      "\n",
      "This metric is called the T-Stat, and it allows us to perform a hypothesis test. The 0 in the numerator can be replaced by any value we would like to test actually. \n",
      "\n",
      "How to interpret the T-Stat?\n",
      "\n",
      "The T-Stat should be compared with the Critical Value. The critical value is the quantile of the corresponding Student distribution at a given level of $$ {\\alpha} $$. If a coefficient is significant at a level $$ {\\alpha} $$ , this means that the T-Stat is above or under the quantiles of the Student Distribution.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Graph3.jpg){:height=\"50%\" width=\"50%\"}\n",
      "\n",
      "Another interpretation is that the probability that the coefficient estimate is not in the interval $$ [- {t}_{1-{\\alpha}/2}; + {t}_{1-{\\alpha}/2} ] $$ is smaller than $$ {\\alpha} $$ . This probability is called the p-value and is defined by :\n",
      "$$ p_{value} = Pr( |\\hat{T}_j| > |{t}_{1-{\\alpha}/2}|) $$\n",
      "\n",
      "Thus, in a null test, we do reject $$ H_0 $$ at a level $$ {\\alpha} $$ (typically 5%) if and only if the p-value is smaller than $$ {\\alpha} $$ , i.e the probability to be in the interval around 0 is really small.\n",
      "\n",
      "```python\n",
      "from scipy import stats\n",
      "\n",
      "t_test = beta1 / sigma_beta1\n",
      "print(\"The T-Stat is : \" + str(round(t_test,4)))\n",
      "\n",
      "p = (1 - stats.t.cdf(abs(t_test), len(df)-2)) * 2\n",
      "print(\"The p-value is : \" + str(round(p,10)))\n",
      "```\n",
      "`The T_stat is : 38.3198`\n",
      "`The p-value is : 0.0`\n",
      "\n",
      "In our example, the T-Stat is pretty big, bigger than the corresponding quantile at 5% of the Student distribution, and the p-value is by far smaller than 5%. Therefore, we reject the null hypothesis : $$ H_0 : {\\beta}_1 = 0 $$ and we conclude that the parameter $$ {\\beta}_1 $$ is not null.\n",
      "\n",
      "## Confidence Interval for the parameters\n",
      "\n",
      "Finally, a notion one should get familiar with is the notion of confidence interval. Using the CLT, one can set a confidence interval around an estimate of a parameter.\n",
      "\n",
      "The lower bound and the upper bound are determined by the critical value of the student distribution at a level $$ {\\alpha} $$, and by the standard deviation of the parameter.\n",
      "$$ {\\beta}_1 ± {t}_{1-{\\alpha}/2} * \\hat{\\sigma}_{\\hat{\\beta_1}} $$\n",
      "\n",
      "```python\n",
      "t_crit = stats.t.ppf(0.975,df=len(y) - 2)\n",
      "\n",
      "c0 = beta1 - t_crit * sigma_beta1\n",
      "c1 = beta1 + t_crit * sigma_beta1\n",
      "```\n",
      "`0.0954, 0.1068`\n",
      "\n",
      "The same process can be done for the parameter $$ {\\beta}_0 $$\n",
      "\n",
      "## Type I and Type II risks\n",
      "\n",
      "Rejecting or not a hypothesis comes at a cost: one could misclassify a parameter. \n",
      "\n",
      "The Type I risk is the probability to reject $$ H_0 $$ whereas it is true.\n",
      "The Type II risk is the probability to not reject $$ H_0 $$ whereas it is false.\n",
      "\n",
      "The Level of a test is 1 - Type I risk, and represents the probability to not-reject $$ H_0 $$ when $$ H_0 $$ is true.\n",
      "\n",
      "Wikipedia summarizes this concept pretty well :\n",
      "![image](https://maelfabien.github.io/assets/images/Graph4.jpg){:height=\"50%\" width=\"50%\"}\n",
      "\n",
      "## Confidence Interval for the model\n",
      "\n",
      "We can define two types of confidence intervals for the model. The first one is the estimation error confidence interval. We need to take into account the error around $$ {\\beta_1} $$ and around $$ {\\beta_0} $$ : \n",
      "\n",
      "$$ CI(x) = \\hat\\beta_0 + \\hat\\beta_1 x \\, \\pm \\, t_{1 - \\frac{\\alpha}{2}}^{(n-2)} \\, \\hat\\sigma\\sqrt{\\frac{1}{n} + \\frac{(x - \\overline{x})^2}{\\sum_{i=1}^n(x_i - \\overline{x})^2}} $$\n",
      "\n",
      "Moreover, the prediction risk is different. The prediction risk corresponds to the risk associated with the extension of our model onto a new prediction : $$ Y_{new} = \\hat{Y} + {\\epsilon} ; {\\epsilon} ∼ N(0,{\\sigma}^2) $$ . We have a new source of volatility : $$ {\\epsilon} $$ . And this leads to a modified confidence interval :\n",
      "\n",
      "$$ PI(x) = \\hat\\beta_0 + \\hat\\beta_1 x \\, \\pm \\, t_{1 - \\frac{\\alpha}{2}}^{(n-2)} \\, \\hat\\sigma\\sqrt{1 + \\frac{1}{n} + \\frac{(x - \\overline{x})^2}{\\sum_{i=1}^n(x_i - \\overline{x})^2}} $$\n",
      "\n",
      "These two confidence intervals can be plotted on the previous graph :\n",
      "\n",
      "```python\n",
      "plt.figure()\n",
      "y = df['house']\n",
      "plt.scatter(df['gdp'], df['house'], label = \"Data\", color = \"green\")\n",
      "plt.plot(df['gdp'], beta0 + beta1 * x, label = \"Regression\", color = \"black\")\n",
      "\n",
      "plt.fill_between(df['gdp'], beta0 + beta1 * x - t_crit * np.sqrt(sigma2 * (1 + 1/len(y) + (x - x_bar)**2/((x-x_bar)**2).sum())), beta0 + beta1 * x + t_crit * np.sqrt(sigma2 * (1 + 1/len(y) + (x - x_bar)**2/((x-x_bar)**2).sum())), color = 'blue', alpha = 0.1, label = '90% PI')\n",
      "plt.fill_between(df['gdp'], beta0 + beta1 * x - t_crit * np.sqrt(sigma2 * (1/len(y) + (x - x_bar)**2/((x-x_bar)**2).sum())), beta0 + beta1 * x + t_crit * np.sqrt(sigma2 * (1/len(y) + (x - x_bar)**2/((x-x_bar)**2).sum())), color = 'red', alpha = 0.4, label = '90% CI')\n",
      "\n",
      "plt.legend()\n",
      "plt.xlabel(\"GDP\", fontsize = 12)\n",
      "plt.ylabel(\"Housing price\", fontsize = 12)\n",
      "plt.title(\"Relation between GDP and Average housing prices\")\n",
      "plt.xlim(min(x), max(x))\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Graph5.jpg){:height=\"60%\" width=\"60%\"}\n",
      "\n",
      "# Linear Regression in 2 dimensions\n",
      "\n",
      "So far, we have covered the unidimensional linear regression framework. But as you might expect, this is only a simple version of the linear regression model. Back to our housing price problem. So far, we only included the GPD variable. But as you may know, interest rates are also major leverage on the housing market. \n",
      "\n",
      "## Framework\n",
      "\n",
      "The more general framework can be described as : $$ Y =  X {\\beta}+ {\\epsilon} $$ . Notice that we are now in matrix form. Indeed :\n",
      "- we suppose we have n observations of a dataset and p features per observation\n",
      "- $$ Y $$ is the output of this observation\n",
      "- $$ {\\beta} $$ is the column vector that contains the true parameters\n",
      "- $$ X $$ is now a matrix of dimension (nxp)\n",
      "\n",
      "Otherwise, the conditions on $$ {\\epsilon} $$ remain the same. \n",
      "- $$ E({\\epsilon}) = 0 $$ , i.e a white noise condition\n",
      "- $$ {\\epsilon}_i ∼ iid  {\\epsilon} $$ for all i = 1,...,n, i.e a homoskedasticity condition\n",
      "\n",
      "In the 2 dimension problem introduced above to predict the housing prices, our task is now to estimate the following model :  $$ \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1{X}_{1i} + \\hat{\\beta}_2{X}_{2i} $$\n",
      "\n",
      "The dataset I'm using can be found [here](https://maelfabien.github.io/myblog/files/housing.txt).\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import math\n",
      "\n",
      "df = pd.read_csv('https://maelfabien.github.io/myblog/files/housing.txt', sep=\" \")\n",
      "df\n",
      "```\n",
      "![image](https://maelfabien.github.io/assets/images/Tab2.jpg){:height=\"55%\" width=\"55%\"}\n",
      "\n",
      "```python\n",
      "fig, axs = plt.subplots(2, figsize=(8,8))\n",
      "\n",
      "axs[0].scatter(df['gdp'], df['house'])\n",
      "axs[1].scatter(df['interests'], df['house'], c='r')\n",
      "\n",
      "axs[0].set_xlabel(\"Gross Domestic Product\", fontsize = 12)\n",
      "axs[0].set_ylabel(\"Housing Price\", fontsize = 12)\n",
      "axs[0].set_title(\"Relation between GDP and Average Housing Prices\")\n",
      "\n",
      "axs[1].set_xlabel(\"Interest rates\", fontsize = 12)\n",
      "axs[1].set_ylabel(\"Housing Price\", fontsize = 12)\n",
      "axs[1].set_title(\"Relation between interest rates and Average Housing Prices\")\n",
      "\n",
      "plt.tight_layout()\n",
      "\n",
      "plt.show()\n",
      "```\n",
      "![image](https://maelfabien.github.io/assets/images/Graph6.jpg){:height=\"70%\" width=\"70%\"}\n",
      "\n",
      "The linear relationship with the interest rate seems to hold again! Let's dive into the model. \n",
      "\n",
      "## OLS estimate\n",
      "\n",
      "Recall that to estimate the parameters, we want to minimize the sum of squared residuals. In the higher dimension, the minimization problem is :\n",
      "\n",
      "$$ argmin \\sum(Y_i - X \\hat{\\beta})^2 $$ .\n",
      "\n",
      "In our specific example, $$ argmin \\sum(Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 {X}_{1i} - \\hat{\\beta}_2 {X}_{2i})^2 $$ \n",
      "\n",
      "> The general solution to OLS is : $$ \\hat{\\beta} = {(X^TX)^{-1}X^TY} $$ . Simply remember that this solution can only be derived if the Gram matrix defined by $$ X^TX $$ is invertible. Equivalently, $$ Ker(X) = {0} $$ . This condition guarantees the **uniqueness** of the OLS estimator. \n",
      "\n",
      "```python\n",
      "x = np.hstack([np.ones(shape=(len(y), 1)), df[['gdp', 'interests']]]).astype(float)\n",
      "xt = x.transpose()\n",
      "X_gram = x\n",
      "gram = np.matmul(xt, x)\n",
      "i_gram = inv(gram)\n",
      "```\n",
      "We can use the Gram matrix to compute the estimator $$ \\hat{\\beta} $$ :\n",
      "```python\n",
      "Beta = np.matmul(inv(gram), np.matmul(xt,y))\n",
      "\n",
      "#Display coefficients Beta0, Beta1 and Beta2\n",
      "print(\"Estimator of Beta0 : \" + str(round(Beta[0],4)))\n",
      "print(\"Estimator of Beta1 : \" + str(round(Beta[1],4)))\n",
      "print(\"Estimator of Beta2 : \" + str(round(Beta[2],4)))\n",
      "```\n",
      "`Estimator of Beta0 : -1114.2614`\n",
      "`Estimator of Beta1 : -0.0079`\n",
      "`Estimator of Beta2 : 21252.7946`\n",
      "\n",
      "We will discuss cases in which this condition is not met in the next article. \n",
      "\n",
      "## Orthogonal projection\n",
      "\n",
      "Note that $$ \\hat{Y} = X \\hat{\\beta} = X(X^TX)^{-1}X^TY = H_XY $$ where $$ H_X = X(X^TX)^{-1}X^T $$ .\n",
      "We say that $$ H_X $$ is an orthogonal projector since it meets two conditions :\n",
      "- $$ H_X $$ is symmetrical : $$ {H_X}^T = H_X $$\n",
      "- $$ H_X $$ is idempotent : $$ {H_X}^2 = H_X $$\n",
      "\n",
      "Graphically, this projection states that we project $$ \\hat{Y} $$ onto the hyperplane formed by the columns of X.\n",
      "![image](https://maelfabien.github.io/assets/images/Graph7.jpg){:height=\"70%\" width=\"70%\"}\n",
      "\n",
      "This allows us to redefine the vectors of residuals :\n",
      "$$ {\\epsilon} = Y - \\hat{Y} = (I - H_X)Y $$ where $$ I $$ is the identity matrix.\n",
      "\n",
      "$$ (I - H_X) $$ is an orthogonal projector on the orthogonal of $$ Vect(X) $$ .\n",
      "\n",
      "## Bias and Variance of the parameters\n",
      "\n",
      "Notice that $$ \\hat{\\beta} - {\\beta} = (X^TX)^{-1}X^TY - (X^TX)^{-1}X^TX{\\beta} = (X^TX)^{-1}X^T{\\epsilon} $$\n",
      "\n",
      "The bias of the parameter is defined by : $$ E(\\hat{\\beta}) - {\\beta} = E(\\hat{\\beta} - {\\beta}) = E((X^TX)^{-1}X^T{\\epsilon}) = (X^TX)^{-1}X^TE({\\epsilon}) = 0 $$ .\n",
      "The estimate $$ \\hat{\\beta} $$ of the parameter $$ {\\beta} $$ is **unbiaised**.\n",
      "\n",
      "The variance of the parameter is given by : $$ Cov(\\hat{\\beta}) = (X^TX)^{-1}X^TCov({\\epsilon})X(X^TX)^{-1} = (X^TX)^{-1}{\\sigma}^2 $$ .\n",
      "Therefore, for each $$ {\\beta}_j $$ : $$ \\hat{\\sigma}^2_j = \\hat{\\sigma}^2{(X^tX)^{-1}_{j,j}} $$\n",
      "\n",
      "```python\n",
      "#Standard errors of the coefficients\n",
      "sigma = math.sqrt(((y - Beta[0] - Beta[1] * xt[1] - Beta[2]*xt[2])**2).sum()/(len(y)-3))\n",
      "sigma2 = sigma ** 2\n",
      "\n",
      "se_beta0 = math.sqrt(sigma2 * inv(gram)[0,0])\n",
      "se_beta1 = math.sqrt(sigma2 * inv(gram)[1,1])\n",
      "se_beta2 = math.sqrt(sigma2 * inv(gram)[2,2])\n",
      "\n",
      "print(\"Estimator of the standard error of Beta0 : \" + str(round(se_beta0,3)))\n",
      "print(\"Estimator of the standard error of Beta1 : \" + str(round(se_beta1,3)))\n",
      "print(\"Estimator of the standard error of Beta2 : \" + str(round(se_beta2,3)))\n",
      "```\n",
      "`Estimator of the standard error of Beta0 : 261.185`\n",
      "`Estimator of the standard error of Beta1 : 0.033`\n",
      "`Estimator of the standard error of Beta2 : 6192.519`\n",
      "\n",
      "The significance of each variable is assessed using an hypothesis test : $$ \\hat{T}_j = \\frac{\\hat{\\beta}_j} {\\hat{\\sigma}_j} $$\n",
      "```python\n",
      "#Student t-stats\n",
      "t0 = Beta[0]/se_beta0\n",
      "t1 = Beta[1]/se_beta1\n",
      "t2 = Beta[2]/se_beta2\n",
      "print(\"T-Stat Beta0 : \" + str(round(t0,3)))\n",
      "print(\"T-Stat Beta1 : \" + str(round(t1,3)))\n",
      "print(\"T-Stat Beta2 : \" + str(round(t2,3)))\n",
      "```\n",
      "`T-Stat Beta0 : -4.266`\n",
      "`T-Stat Beta1 : -0.242`\n",
      "`T-Stat Beta2 : 3.432`\n",
      "\n",
      "Something pretty interesting happens here: by adding a new variable to the model, the coefficient of $$ {\\beta}_1 $$ quite logically changed but became non-significant. We do expect $$ {\\beta}_2 $$ to explain the model way better now.\n",
      "\n",
      "\n",
      "The Quadratic Risk is : $$ E[(\\hat{\\beta} - {\\beta})^2] = {\\sigma}^2 \\sum{\\lambda}_k^{-1} $$ where $$ {\\lambda}_k $$ are the eigenvalues of the Gram matrix.\n",
      "\n",
      "The prediction risk is : $$ E[(\\hat{Y} - Y)^2] = {\\sigma}^2 (p+1) $$\n",
      "\n",
      "The Github repository of this article can be found [here](https://github.com/maelfabien/Machine_Learning_Tutorials).\n",
      "---\n",
      "title: Introduction to Natural Language Processing\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Natural Language Processing\"\n",
      "---\n",
      "\n",
      "One of the main challenges, when dealing with text, is to build an efficient preprocessing pipeline.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# What is Natural Language Processing?\n",
      "\n",
      "Natural Language Processing (NLP) is at the crossroads of artificial intelligence, linguistics and machine learning.\n",
      "\n",
      "> Natural Language Processing aims to extract meaning from textual data.\n",
      "\n",
      "Therefore, NLP has many applications, especially in :\n",
      "- translation (DeepL or Google Translate)\n",
      "- document classification\n",
      "- spell-checkers\n",
      "- automatic summary\n",
      "- human-computer interactions\n",
      "- speech recognition\n",
      "- speech synthesis\n",
      "- opinion analysis\n",
      "\n",
      "NLP is generally divided in 2 to 3 main tasks :\n",
      "- Text preprocessing: A step whose role is to standardize the text input according to the usage one wants to make of it\n",
      "- Representing text as vectors, which can generally be done either using BoW/TF-IDF methods (which we'll cover in future articles) or by learning an embedding of the text as a vector (Word2Vec for example)\n",
      "\n",
      "In the next articles, we'll cover these 2 major steps, but we'll also cover some extensions and some cool applications of NLP!\n",
      "---\n",
      "title: Key Concepts of Data Visualization\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Data Viz\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "{% highlight matlab %}\n",
      "{% endhighlight %}\n",
      "\n",
      "In this article, we'll introduce the field of Data Visualization. I'm trying to bring a clear summary of the different courses I have had and the resources I have read. I am listing the resources I am referring to at the end of this article.\n",
      "\n",
      "There are many things you should already be familiar with, but I think it's a great exercise to formalize it. Hopefully, you'll also learn some new stuff!\n",
      "\n",
      "# What is Data Viz?\n",
      "\n",
      "Let's kick this with a definition of Data Visualization :\n",
      "\n",
      "> Visualization is a cognitive process that allows to form a mental image to gain insights, discover, make decisions and explain. Data Visualization is the use of computer-supported, interactive visual representations of data to amplify cognition.\n",
      "\n",
      "## Basis of Data Viz\n",
      "\n",
      "There are 3 main subfields of Data Viz :\n",
      "- Scientific visualization, whose role is to model real-world phenomena \n",
      "- Information visualization, whose role is to map a more abstract concept into 2D or 3D for decision making and analysis purposes\n",
      "- Visual analytics, which is the frontier of Data mining and Machine Learning\n",
      "\n",
      "There are 3 main types of variables :\n",
      "- Qualitative (Nominal or ordinal)\n",
      "- Quantitative\n",
      "- Metadata which is descriptive information about the data\n",
      "\n",
      "There are cognitive benefits of information visualization since it can amplify cognition in several ways :\n",
      "- Increasing memory and processing resources available\n",
      "- Reducing search for information\n",
      "- Enhancing the recognition of patterns\n",
      "- Enabling perceptual inference operations\n",
      "- Using perceptual attention mechanisms for monitoring \n",
      "- Encoding information in a manipulable medium \n",
      "\n",
      "The value of information visualization can be computed using the information of the number of users visualizing the data, the frequency and the time spared at each time.\n",
      "\n",
      "The key attributes to implement or not when developing a visualization tool are :\n",
      "- the scalability to a large number of data\n",
      "- the interactivity to offer multiple views to a user\n",
      "\n",
      "## Why use Data Viz?\n",
      "\n",
      "The volume of data produced yearly is growing exponentially. From 800 Exabytes in 2008 to 900 Zettabytes in 2017 (so large it's even hard to count the number of 0s: 900000000000000000000000 bytes). How do we make sense of such a large amount of data? How can we make this data understandable and exploitable?\n",
      "\n",
      "We use Data Viz to help people rapidly narrow in from a large space and find parts of the data to study more carefully. Visualization and visual contents have been chosen since among the human senses, the vision has the highest band with, with over 100Mb/s theoretically. Ears only allow the processing of 100 bytes/second on the other hand. \n",
      "\n",
      "Why is vision so effective?\n",
      "- *Preattentive processing *: some visual features can be perceived very rapidly and accurately by our low-level visual system (e.g identifying a red dot in the middle or grey dots)\n",
      "- *Gestalt theory *: the visual systems understand an image using proximity, similarity, continuity, symmetry, close and relative size features.\n",
      "\n",
      "We use the viz for better decision making, better anomaly detection... Overall, we want the end-user to be better and more effective at his task. Computers allow simply to map a large volume of data and to update the visualization as often as needed. \n",
      "\n",
      "## When to use Data Viz?\n",
      "\n",
      "There are several conditions for information browsing to be useful :\n",
      "- When there is a good underlying structure so that items close to one another can be inferred to be similar\n",
      "- When users are unfamiliar with a collection’s contents\n",
      "- When users have a limited understanding of how a system is organized and prefer a less cognitively loaded method of exploration\n",
      "- When users have difficulty verbalizing the underlying information need\n",
      "- When information is easier to recognize than describe \n",
      "\n",
      "# Data mapping\n",
      "\n",
      "How do we map data to a *representation *? \n",
      "- We first define a space, usually using axis\n",
      "- Then we define the marks we'll be using, which are the things that occur in the space (points, stars, dots...)\n",
      "- Then the graphical properties of marks (size, position, orientation, color, texture...)\n",
      "\n",
      "Our aim should be to increase the use of space, encode data, and make the graph efficient. There are overall only 5 main categories of graphs :\n",
      "\n",
      "- *Data tables*, like Excel Sheets for example\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france19.jpg)\n",
      "\n",
      "- *Graphs on rails* or planes (visually, we can see a 1D plot as a single rail, a 2D plot as a combination of rails, a Pie chart as a single rail folded...)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france20.jpg)\n",
      "\n",
      "- *Geospatial maps*, which is the mapping of the latitude and longitude on a 2D plane, in which we can add some information (features on a map, size of components...)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france21.jpg)\n",
      "\n",
      "- *Network diagrams*, whose role is to display the relation between items. In such diagrams \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_9.jpg)\n",
      "\n",
      "- *Symbols and conceptual images*, which can be any logo whose aim is to present an information (e.g the PEGI 12 logo on a video game)\n",
      "\n",
      "One of the issues that arise is when we need to represent **hyper-variate** data. How to do this?\n",
      "- Represent the data on a data table. This is a high dimensional mapping on a 2D plane. This is however not very efficient.\n",
      "- Reduce the dimension using PCA or t-SNE for example, but this implies losing the meaning behind the clusters for example\n",
      "- Plot a scatterplot matrix to identify the correlation between individual dimensions, but this does not combine the different dimensions, and we can easily get too many graphs to look at.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france22.jpg)\n",
      "\n",
      "- Encode dimensions in features of the markers: e.g the size of the marker, the shape, the color... This can be efficient, but there is a limit to the number of dimensions we can map, and it quickly gets impossible to read. (Check Chernoff Faces for a fun example of how to map many dimensions on human-looking faces.)\n",
      "- Use parallel coordinates: we encode variable along a horizontal row. Parallel coordinates are the best way to represent high dimensional data. Vertical lines specify different values that a variable can take. Data points are represented as a polyline. Different variable can have values taking quite different ranges. We might need to scale it. We also need to reorder the dimensions to have the most similar next to one another.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/france23.jpg)\n",
      "\n",
      "# Marks and channels\n",
      "\n",
      "These are the building blocks of a design space of visual encodings :\n",
      "- Marks are basic geometric elements that depict items or links (point, line, area)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/viz_1.jpg)\n",
      "\n",
      "- and channels control their appearance, independently of the dimensionality go the geometric primitive. (position, color, shape, tilt (angle), size)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/viz_2.jpg)\n",
      "\n",
      "An area mark has both dimensions of its size constrained intrinsically as part of its shape, so area marks typically are not size coded or shape coded. (e.g the shape of a country). Bar plots can be made wider to encode an additional dimension. Point marks can be size coded and shape coded. \n",
      "\n",
      "Which channel to choose?\n",
      "- The channels that show magnitude information are good for ordered data. (how much is it? How much longer? How much wider ?…)\n",
      "- The channels that show identity information are good with categorical data. (What is it? Where is it? Is it a line ?…)\n",
      "\n",
      "Usually, a mark represents an item. (In a table dataset for example). In a network dataset, a mark can represent an item (node), or a link between items. There are 2 types of link marks :\n",
      "- Connection mark shows a pairwise relationship between 2 items using a line\n",
      "- Containment marks show hierarchical relationships using areas, nested in each other at multiple levels\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/viz_3.jpg)\n",
      "\n",
      "The human visual system does not process 2 different channels in the same way. We decide which channel to use according to 2 principles :\n",
      "- **Expressiveness **: visual encoding should express only the information in the dataset attributes. Ordered data should be shown as ordered, and vice versa\n",
      "- **Effectiveness **: the most important attributes should be encoded with the most effective channels to be most noticeable\n",
      "\n",
      "What channel is effective when it comes to visual encoding?\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/viz_4.jpg)\n",
      "\n",
      "How has this ranking been established? Why are some channels better than others? There are some ways to measure effectiveness :\n",
      "- Accuracy: how close is human perceptual judgment to some objective measurement of the stimulus? Psychophysics study the measurement of general human perception. Our responses to the sensory experience of magnitude are characterizable by power laws, where the exponent depends on the exact sensory modality. The psychophysical power law of Stevens states that $$ S = I^n $$ where $$ S $$ in the received sensation, $$ I $$ the physical intensity and $$ n $$ the power-law exponent.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/viz_5.jpg)\n",
      "\n",
      "- **Discriminability **: if you encode data using a particular visual channel, are the differences between items perceptible to the human as intended?\n",
      "- **Separability **: You cannot treat all visual channels as completely independent from each other, because some have dependencies and interactions with others. You must consider a continuum of potential interactions between channels for each pair, ranging from the orthogonal and independent separable channels to the inextricably combined integral channels. \n",
      "- **Popout **: How well a distinct item stands out from many others immediately. Our low-level visual system does massively parallel processing on these visual channels, without the need for the viewer to consciously directly attention to items one by one. However, when the popout does not occur, it requires a serial search, whose time depends on the number of marks in the graph. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/viz_6.jpg)\n",
      "\n",
      "- **Grouping **: Arises from the use of link marks or identity channels to encode categorical attributes. Containment is the strongest cue for grouping, with connection coming in second, and proximity the third.\n",
      "\n",
      "The human perceptual system is fundamentally based on relative judgments, not absolute ones; this principle is known as *Weber’s Law*. For instance, the amount of length difference we can detect is a percentage of the object’s length. When considering questions such as the accuracy and discriminability of our perceptions, we must distinguish between relative and absolute judgments. For example, when two objects are directly next to each other and aligned, we can make much more precise judgments than when they are not aligned and when they are separated with many other objects between them. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/viz_7.jpg)\n",
      "\n",
      "# Tasks and interactions\n",
      "\n",
      "InfoVis is developed to answer the user's tasks. \n",
      "\n",
      "InfoVis is used at ** high-level** to either : \n",
      "- Produce Information :\n",
      "    - Annotate: Manual task to bring new attributes to the data (e.g cluster label)\n",
      "    - Record: Save or capture visualization elements as persistent artifacts  (screenshots, book-marked elements, locations, parameter settings… to assemble a graphical history).\n",
      "    - Derive: produce new data elements based on existing data elements. Extend the dataset beyond attributes. For example, go beyond City name and add Coordinates using an external table. Or compute trade balance using import and exports. A new derived attribute may be created using arithmetic, logical or statistical operations ranging from simple to complex.\n",
      "- Consume information that has already been generated as data stored\n",
      "    - Present information: To tell a story with data, guide the audience through a series of cognitive operations, Communicate… Whether it is dynamic or static.\n",
      "    - Discover: Use Vis to find new knowledge that was not previously known :\n",
      "        -  Generate a new hypothesis\n",
      "        - Verify or disconfirm the existing hypothesis\n",
      "- Enjoy: Curiosity (Infographic on a blog post for example)\n",
      "\n",
      "At ** middle-level**, Search is used to localize known or unknown data.\n",
      "- Lookup: The user knows what he’s looking for and where it is \n",
      "- Locate: Find a known target at an unknown location. \n",
      "- Browse: Exact identify of a search target might not be known in advance but specified based on characteristics\n",
      "- Explore: Users are not sure of the location.\n",
      "\n",
      "At ** low-level**, the tasks take the following form :\n",
      "- **Retrieve value **: Maybe the simplest task, in which we aim to display a value\n",
      "- **Filter **: Filters should only show the data matching a set of conditions\n",
      "- **Compute derived value **: Compute an aggregate value for a set of data cases\n",
      "- **Find extremum **: Find cases at either extreme of the data set for a given attribute\n",
      "- **Sort **: Rank data cases according to some ordinal metric\n",
      "- **Determine range **: Identify the span of values for a given set of data cases and attributes\n",
      "- **Characterize distribution **: For a given set of cases and attributes, characterize the distribution of the value over the set\n",
      "- **Find anomalies **: Identify anomalies in a set of data cases concerning a given relationship/expectation\n",
      "- **Cluster **: Within a set of cases, identify any clusters of similar attribute values\n",
      "- **Correlate **: For a given set of cases and 2 attributes, determine useful attributes correlations\n",
      "- ...\n",
      "\n",
      "Once a target or set of targets for a search has been found, a low-level user goal is to query these targets to :\n",
      "- Identify: Single target. \n",
      "- Compare: Multiple targets.\n",
      "- Summarize: All possible targets.\n",
      "\n",
      "\n",
      "In InfoVis, the user first understands the information displayed on a graph and then starts to work around and manipulate the data. This part is called the interaction. There are several kinds of interactions :\n",
      "- **Select **: Mark something as interesting\n",
      "- **Explore **: Show something else\n",
      "- **Reconfigure** : Show a different arrangement\n",
      "- **Encode** : Show a different representation\n",
      "- **Abstract/Elaborate** : Show less or more details\n",
      "- **Filter **: Show something conditionally\n",
      "- **Connect** : Show related items\n",
      "\n",
      "To aim for visual continuity and avoid a conversation break, the ideal response time is somewhere around 0.1 second.\n",
      "\n",
      "# Visual Perception\n",
      "\n",
      "Semiotics is the study of symbols and how they convey meaning. **Perceptual processing** is the process to seek to better understand visual perception and visual information. The visual perception, with some underlying hypothesis, is made of 2 major steps :\n",
      "- a parallel extraction of low-level properties of the scene, called **preattentive** process\n",
      "- a sequential goal-directed process\n",
      "\n",
      "## Preattentive process \n",
      "\n",
      "The preattentive process is the early, parallel detection of color, texture, shape, spatial attributes. The main features of the preattentive process are the following :\n",
      "- It is achieved without any kind of focus or attention\n",
      "- It usually lasts less than 200-250ms (The length of eye movements is 200ms)\n",
      "- It is parallelized by the low-vision system\n",
      "\n",
      "The preattentive process follows some basic rules :\n",
      "- A larger set size means a longer processing time\n",
      "- A larger set difference (i.e labels difference) implies a smaller processing time\n",
      "- Categorical processing is easy to find when unique\n",
      "- We, as humans, cannot process both color and shape pre-attentively\n",
      "\n",
      "Certain elements lend themselves to preattentive processing :\n",
      "- Width\n",
      "- Size\n",
      "- Curvature\n",
      "- Number\n",
      "- Intensity\n",
      "- Hue\n",
      "- Closure\n",
      "- ...\n",
      "\n",
      "## Sequential goal-directed processing\n",
      "\n",
      "Sequential goal-directed processing is a serial processing of an object using memory and spatial layout. We usually split an object into subsystems for recognition tasks. This is a slow serial process, implying work and long-term memory.\n",
      "\n",
      "# Famous Tools\n",
      "\n",
      "Along with `matplotlib, pandas, seaborn`..., there are some really good tools that are used for Data Visualization : \n",
      "- Tableau, for drag&drop UX, and great dashboards\n",
      "- Altair, for a programmatic tool similar to Tableau, in Python\n",
      "- D3.js for interactive JS graphs\n",
      "- ...\n",
      "\n",
      "# Recommended resources\n",
      "\n",
      "The key resources I have been working in this field are the following :\n",
      "- Visualization Analysis and Design by Tamara Munzner\n",
      "- [The Value of Information Visualization](https://www.win.tue.nl/~vanwijk/infovis_springer.pdf)\n",
      "\n",
      "\n",
      "You can download the dataset right [here](https://maelfabien.github.io/assets/files/france.csv).\n",
      "\n",
      "> **Conclusion** : That's it ! I hope this introduction to Altair was interesting. I love to use this on my Data Viz projects. Feel free to leave a comment if you have one.\n",
      "---\n",
      "title: Wav2Spk, learning speaker emebddings for Speaker Verification using raw waveforms\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Speech Processing\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "MFCCs have long been the standard hand-crafted features used as inputs for the majority of speech-related tasks. Nowadays, most X-vectors for speaker identification and speaker verification systems still rely on MFCCs, voice activity detection (VAD) and cepstral mean and variance normalization (CMVN). MFCCs are used in speaker recognition, in conjunction with Gaussian mixture models, i-vectors, x-vectors, and more recently ResNet and DenseNet speaker embeddings.\n",
      "\n",
      "Lately, some end-to-end models that directly embed the raw waveform and perform downstream tasks arised. These approaches, although encouraging, only reached limited performances.\n",
      "\n",
      "At Interspeech 2020, a paper by Weiwei Lin and Man-Wai Mak caught my attention. The paper claims to learn speaker emebeddings from raw waveforms using a simple DNN architecture, with a similar approach to [Wav2Vec](https://maelfabien.github.io/machinelearning/wav2vec/).\n",
      "\n",
      "Let's dive into the paper :)\n",
      "\n",
      "# Model Architecture\n",
      "\n",
      "## Feature encoding\n",
      "\n",
      "Why don't we directly input waveforms to an X-vector system? Well, because the frames it processes are 25 to 30 ms long, and the effective receptive field of the X-vector would be too small.\n",
      "\n",
      "One architecture that has often been used in speech is Convolutional Neural Networks. Using CNNs with large strides and kernel sizes as an encoder network has proven to be efficient in Wav2Vec. Here, the authors use 5 convolutional layers with kernel sizes (10, 8, 4, 4, 4) and strides (5, 4, 2, 2, 2). This encodes 30ms of speech and 10ms frame shift.\n",
      "\n",
      "## Temporal gating\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/wav_0.png)\n",
      "\n",
      "\n",
      "# Final word\n",
      "\n",
      "I hope this wav2vec series summary was useful. Feel free to leave a comment \n",
      "\n",
      "All references:\n",
      "- [wav2vec paper](https://arxiv.org/abs/1904.05862)\n",
      "- [vq - wav2vec](https://arxiv.org/abs/1910.05453)\n",
      "- [wav2vec2.0 paper](https://arxiv.org/abs/2006.11477)\n",
      "- [Self-training and Pre-training are Complementary for Speech Recognition](https://arxiv.org/abs/2010.11430)\n",
      "- [wav2vec explained, on YouTube](https://www.youtube.com/watch?v=XkUVOijzAt8)\n",
      "- [wav2vec 2.0, on YouTube](https://www.youtube.com/watch?v=aUSXvoWfy3w)\n",
      "- [Self-supervised training in CV](https://www.fast.ai/2020/01/13/self_supervised/#:~:text=We%20would%20like%20something%20which,than%20requiring%20separate%20external%20labels.)\n",
      "- [More self-supervised learning in CV](https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html)\n",
      "\n",
      "---\n",
      "title: Statistics in Matlab\n",
      "layout: post\n",
      "tags: [statistics]\n",
      "subtitle : \"Matlab\"\n",
      "---\n",
      "\n",
      "In this article, we'll cover the main functions to conduct a statistical analysis. I'm working on a sample data set that describes the salaries of employees depending on several features.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "{% highlight matlab %}\n",
      "{% endhighlight %}\n",
      "\n",
      "# I. Data exploration\n",
      "\n",
      "The file I'm working on comes with a small description `DES`. The description of the data is the following.\n",
      "\n",
      "```matlab\n",
      "wage      educ      exper     tenure    nonwhite  female    married   numdep   \n",
      "smsa      northcen  south     west      construc  ndurman   trcommpu  trade    \n",
      "services  profserv  profocc   clerocc   servocc   lwage     expersq   tenursq  \n",
      "\n",
      "Obs:   526\n",
      "\n",
      "1. wage                     average hourly earnings\n",
      "2. educ                     years of education\n",
      "3. exper                    years potential experience\n",
      "4. tenure                   years with current employer\n",
      "5. nonwhite                 =1 if nonwhite\n",
      "6. female                   =1 if female\n",
      "7. married                  =1 if married\n",
      "8. numdep                   number of dependents\n",
      "9. smsa                     =1 if live in SMSA\n",
      "10. northcen                 =1 if live in north central U.S\n",
      "11. south                    =1 if live in southern region\n",
      "12. west                     =1 if live in western region\n",
      "13. construc                 =1 if work in construc. indus.\n",
      "14. ndurman                  =1 if in nondur. manuf. indus.\n",
      "15. trcommpu                 =1 if in trans, commun, pub ut\n",
      "16. trade                    =1 if in wholesale or retail\n",
      "17. services                 =1 if in services indus.\n",
      "18. profserv                 =1 if in prof. serv. indus.\n",
      "19. profocc                  =1 if in profess. occupation\n",
      "20. clerocc                  =1 if in clerical occupation\n",
      "21. servocc                  =1 if in service occupation\n",
      "22. lwage                    log(wage)\n",
      "23. expersq                  exper^2\n",
      "24. tenursq                  tenure^2\n",
      "```\n",
      "\n",
      "The first step is to load the data set in our path :\n",
      "```matlab\n",
      "wage1=load('WAGE1.raw')\n",
      "[n,k]=size(wage1)\n",
      "```\n",
      "\n",
      "Then, we'll plot a histogram of hourly salaries :\n",
      "```matlab\n",
      "hist(wage1(:,1))\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/wage1.jpg)\n",
      "\n",
      "Now, let's plot the histogram for the number of years of education.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/wage2.jpg)\n",
      "\n",
      "We can take a look at the mean of each feature :\n",
      "\n",
      "```matlab\n",
      "mean(wage1(:,:))'\n",
      "```\n",
      "\n",
      "```\n",
      "ans =\n",
      "\n",
      "5.8961\n",
      "12.5627\n",
      "17.0171\n",
      "5.1046\n",
      "0.1027\n",
      "...\n",
      "```\n",
      "\n",
      "We can also take a look at other descriptive statistics :\n",
      "\n",
      "```matlab\n",
      "std(wage1(:,:))'\n",
      "min(wage1)'\n",
      "max(wage1)'\n",
      "cov(wage1)'\n",
      "corrcoef(wage1(:,:))'\n",
      "```\n",
      "\n",
      "\n",
      "# II. Linear regression\n",
      "\n",
      "## Compute coefficients\n",
      "\n",
      "We now want to assess the effect of columns 2, 3 and 4 (education, experience, years with current employer) on the salary. \n",
      "\n",
      "$$ y_i = \\beta_0 + \\beta_1 * {x_1}_i + \\beta_2 * {x_2}_i  + \\beta_3 * {x_3}_i + u_i $$\n",
      "\n",
      "```matlab\n",
      "y=wage1(:,1)\n",
      "X=[ones(n,1),wage1(:,[2,3,4])]\n",
      "```\n",
      "\n",
      "We can compute the Beta coefficients and the residuals :\n",
      "\n",
      "```matlab\n",
      "beta=inv(X'*X)*X'*y\n",
      "u=y-X*beta\n",
      "```\n",
      "\n",
      "The coefficients are the following :\n",
      "\n",
      "```matlab\n",
      "beta =\n",
      "\n",
      "-2.8727\n",
      "0.5990\n",
      "0.0223\n",
      "0.1693\n",
      "```\n",
      "\n",
      "The first coefficient is constant. Then, the model indicates that an additional year of education should increase the hourly salary by 0.60$. \n",
      "\n",
      "## Residuals\n",
      "\n",
      "### Effect of each feature individually\n",
      "\n",
      "We can also plot a histogram of the residuals. The distribution of the residual appears to be close to a normal one.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/histu.jpg)\n",
      "\n",
      "We can decide to remove outliers above 5 for example :\n",
      "\n",
      "```matlab\n",
      "indices = find(u>2.5);\n",
      "u(indices) = [];\n",
      "hist(u)\n",
      "```\n",
      "\n",
      "## Test hypothesis\n",
      "\n",
      "Are the coefficients significant?\n",
      "\n",
      "To assess the significance of each estimator, we need to estimate their standard deviation.\n",
      "\n",
      "```matlab\n",
      "sig2=u'*u/(n-4)\n",
      "std=sqrt(diag(sig2*inv(X'*X)))\n",
      "```\n",
      "\n",
      "Then, we can easily compute the test statistic :\n",
      "\n",
      "```matlab\n",
      "t = beta./std\n",
      "```\n",
      "\n",
      "```\n",
      "t =\n",
      "\n",
      "-5.6300\n",
      "16.6857\n",
      "2.6471\n",
      "11.1725\n",
      "```\n",
      "\n",
      "What should we compare the test statistics? Well, to the critical value! For example, since we estimate 4 parameters, the 95% critical value is :\n",
      "\n",
      "```\n",
      "C1 = tinv(0.95, n-4)\n",
      "```\n",
      "\n",
      "```\n",
      "C1 =\n",
      "\n",
      "1.6478\n",
      "```\n",
      "\n",
      "The T-stats are all greater than the critical value. For this reason, we reject the hypothesis $$ H_0 $$ that the variables have no impact on the output.\n",
      "\n",
      "### Effect of a single feature\n",
      "\n",
      "If we are interested in the effect of a single feature, we can specify a value for its effect for example.\n",
      "\n",
      "```matlab\n",
      "t_educ = (beta(2)-0.1)/std(2)\n",
      "```\n",
      "\n",
      "### Effect of each feature on `log(wage)`\n",
      "\n",
      "Testing the effect of the features on the `log(wage)` is a way to assess their importance in terms of the percentage change.\n",
      "\n",
      "```matlab\n",
      "y=log(wage1(:,1))\n",
      "beta=inv(X'*X)*X'*y\n",
      "std=sqrt(diag(sig2*inv(X'*X)))\n",
      "t = beta./std\n",
      "```\n",
      "\n",
      "### Test is 2 features have the same effect\n",
      "\n",
      "Now, let's test the following hypothesis: the effect of education is the same as the effect of professional experience. This can be rewritten as : $$ H_0 : \\beta_2 = \\beta_3 $$ . However, estimating the variance of $$  \\beta_2 - \\beta_3 $$ to build the test hypothesis would be challenging. We can apply a simple trick by building a new feature as the sum of the 2 features to test.\n",
      "\n",
      "First of all, append the new feature to the model :\n",
      "\n",
      "```matlab\n",
      "capitaltot = X(:,2) + X(:,3)\n",
      "```\n",
      "\n",
      "This introduces some multicolinearity which we should control by removing $$ X_3 $$ :\n",
      "\n",
      "```matlab\n",
      "X=[X(:,[1,2,4]),capitaltot];\n",
      "y=log(wage1(:,1))\n",
      "```\n",
      "\n",
      "We can run the test : $$ H_0 : \\beta_4 = 0 $$\n",
      "\n",
      "```\n",
      "beta=inv(X'*X)*X'*y\n",
      "std=sqrt(diag(sig2*inv(X'*X)))\n",
      "t = beta(4)/std(4)\n",
      "```\n",
      "\n",
      "This heads :\n",
      "\n",
      "```matlab\n",
      "t =\n",
      "\n",
      "0.4883\n",
      "```\n",
      "\n",
      "The T-test is smaller than the critical value. For this reason, we cannot reject the hypothesis that the effect in the percentage of education is the same as the one of professional experience.\n",
      "\n",
      "### Fisher Test for joint hypothesis\n",
      "\n",
      "Let us now consider the hypothesis : $$ H_0 : \\beta_2 = 0, \\beta_3 = 0 $$ . In such case, we should **not** apply 2 independent tests, since we are interested in the overall hypothesis. \n",
      "\n",
      "We need to define a non-constrained model, a constrained model and test the difference between those two models.\n",
      "\n",
      "```matlab\n",
      "% non-constrained model\n",
      "X0=[ones(n,1),wage1(:,[2,3,4])]\n",
      "y0=log(wage1(:,1))\n",
      "beta0=inv(X0'*X0)*X0'*y0\n",
      "u0=y0-X0*beta0\n",
      "SSR0=u0'*u0\n",
      "```\n",
      "\n",
      "```matlab\n",
      "% model contraint\n",
      "X1=[ones(n,1),wage1(:,[4])]\n",
      "y1=log(wage1(:,1))\n",
      "beta1=inv(X1'*X1)*X1'*y1\n",
      "u1=y1-X1*beta1\n",
      "SSR1=u1'*u1\n",
      "```\n",
      "\n",
      "The Fisher F-Stat is defined as :\n",
      "\n",
      "$$ \\hat{F}_j = \\frac { {SSR}_1 - {SSR}_0 } { {SSR}_0} \\frac {n - K} {2} $$ \n",
      "\n",
      "Where K is the number of variables included in the non-constrained model (4 here).\n",
      "\n",
      "```matlab\n",
      "K = 4\n",
      "F = (SSR1-SSR0)/(SSR0) * (n-K)/2\n",
      "```\n",
      "\n",
      "This heads :\n",
      "\n",
      "```matlab\n",
      "F =\n",
      "\n",
      "80.1478\n",
      "```\n",
      "\n",
      "### Interaction effect\n",
      "\n",
      "Let's define new variables :\n",
      "\n",
      "```matlab\n",
      "X=wage1\n",
      "educ=X(:,2)\n",
      "exper=X(:,3)\n",
      "tenure=X(:,4)\n",
      "female=X(:,6)\n",
      "married=X(:,7)\n",
      "marrmale = (1-female).*married\n",
      "marrfem = female.*married\n",
      "singfem=female.*(1-married)\n",
      "```\n",
      "\n",
      "We create interaction variables :\n",
      "- effect of a married male \n",
      "- effect of a married female \n",
      "- effect of a single female\n",
      "\n",
      "The base case is a single male. We can define test statistics on those variables as we did above.\n",
      "\n",
      "> **Conclusion **: This was a quick introduction to Matlab. I hope you found it useful. Don't hesitate to drop a comment if you have a question.\n",
      "---\n",
      "title: Load and move files to HDFS (2/4)\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Parallel and Distributed Computing\"\n",
      "---\n",
      "In this article, we'll see how to download the input text file for our WordCount job, and put the file into HDFS. \n",
      "\n",
      "## Create a repository on the VM to download the data\n",
      "\n",
      "Using the following command lines, create a repository in the VM, \n",
      "\n",
      "`[raj_ops@sandbox-hdp ~]$ mkdir TP`\n",
      "\n",
      "`[raj_ops@sandbox-hdp ~]$ cd TP`\n",
      "\n",
      "## Download the data and the JAR file\n",
      "\n",
      "Download the .txt file we'll be using for our WordCount from [here](https://norvig.com/big.txt).\n",
      "\n",
      "In the TP repository, you can use the command line directly :\n",
      "\n",
      "`wget https://norvig.com/big.txt`\n",
      "\n",
      "You should have something like this :\n",
      "\n",
      "`[raj_ops@sandbox-hdp TP]$ wget https://norvig.com/big.txt`\n",
      "\n",
      "If everything worked well, by typing `ls`, you should see the file `big.txt`.\n",
      "\n",
      "You will now also add the Jar file, which contains the Java code to execute a MapReduce :\n",
      "\n",
      "`wget https://github.com/maelfabien/maelfabien.github.io/blob/master/assets/files/wc.jar`\n",
      "\n",
      "This code is a pre-compiled version of the code available [here](https://hadoop.apache.org/docs/r2.8.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Example:_WordCount_v1.0).\n",
      "\n",
      "If we try to detail just a little bit the Java code :\n",
      "\n",
      "```java\n",
      "import org.apache.hadoop.conf.Configuration ;\n",
      "import org.apache.hadoop.fs.Path;\n",
      "import org.apache.hadoop.io.IntWritable;\n",
      "import org.apache.hadoop.io.Text;\n",
      "import org.apache.hadoop.mapreduce.Job;\n",
      "import org.apache.hadoop.mapreduce.Mapper;\n",
      "import org.apache.hadoop.mapreduce.Reducer;\n",
      "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; \n",
      "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
      "\n",
      "public class WordCount {\n",
      "    public static class TokenizerMapper\n",
      "    extends Mapper<Object , Text , Text , IntWritable>{\n",
      "\n",
      "    private final static IntWritable one = new IntWritable (1) ;\n",
      "    private Text word = new Text();\n",
      "\n",
      "    public void map( Object key , Text value , Context context ) throws IOException , InterruptedException {\n",
      "        StringTokenizer itr = new StringTokenizer(value.toString()); \n",
      "        while ( i t r . hasMoreTokens () ) {\n",
      "            word.set(itr.nextToken());\n",
      "            context.write(word, one);\n",
      "        } \n",
      "    }\n",
      "}\n",
      "\n",
      "public static class IntSumReducer\n",
      "    extends Reducer<Text , IntWritable , Text , IntWritable> {\n",
      "    private IntWritable result = new IntWritable () ;\n",
      "\n",
      "    public void reduce(Text key , Iterable<IntWritable> values , Context context) \n",
      "    throws IOException , InterruptedException {\n",
      "        int sum = 0;\n",
      "        for (IntWritable val : values) {\n",
      "            sum += val.get();\n",
      "        }\n",
      "        result.set(sum) ;\n",
      "        context.write(key, result);\n",
      "    } \n",
      "}\n",
      "```\n",
      "\n",
      "And in the Main : \n",
      "\n",
      "```java\n",
      "public static void main(String [] args) throws Exception {\n",
      "\n",
      "    /* Provide a configuration of the cluster */\n",
      "    Configuration conf = new Configuration () ;\n",
      "\n",
      "    /* Call the constructor with the configuration object and a name for the job */\n",
      "    Job job = Job.getInstance(conf, ”word count”);\n",
      "\n",
      "    /* Provide an implementation for the Map Class */\n",
      "    job.setMapperClass(TokenizerMapper.class);\n",
      "\n",
      "    /* Provide an implementation for the Combiner Class */\n",
      "    job.setCombinerClass(IntSumReducer.class);\n",
      "\n",
      "    /* Provide an implementation for the Reduce Class */\n",
      "    job.setReducerClass(IntSumReducer.class);\n",
      "\n",
      "    /* Specify the type of the output key/value */\n",
      "    job.setOutputKeyClass(Text.class);\n",
      "    job.setOutputValueClass(IntWritable.class);\n",
      "\n",
      "    /* Give the location of the input/output of the application */\n",
      "    FileInputFormat.addInputPath(job, new Path(args[0]));\n",
      "    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
      "\n",
      "    /* Specify how the input/output will be formatted */\n",
      "    job.setInputFormatClass(TextInputFormat.class);\n",
      "    job.setOutputFormatClass(TextOutputFormat.class);\n",
      "\n",
      "    /* Start the job and wait for its completion! */\n",
      "    job.waitForCompletion(true);\n",
      "}\n",
      "```\n",
      "\n",
      "## Move file to HDFS\n",
      "\n",
      "Hadoop commands launch by default on a working repository based on the name of the user: `/user/<user_name>` \n",
      "\n",
      "- We need to create the repository from our SSH connexion: `/user/raj_ops/TP/input`, and upload our file to HDFS.\n",
      "\n",
      "`hadoop fs -mkdir -p TP/input`\n",
      "\n",
      "- We have downloaded the data under the `big.txt` file. We will upload the file on the folder :\n",
      "\n",
      "`hadoop fs -put big.txt TP/input`\n",
      "\n",
      "- In this command, `big.txt` is in the local repository on the Linux VM whereas the `TP/input` refers to a file in HDFS. We can display the last 5 lines of the file `big.txt` located in HDFS :\n",
      "\n",
      "`hadoop fs -cat TP/input/big.txt | tail -n 5`\n",
      "\n",
      "The book ends on a function written in Python 2, so you should see something like this :\n",
      "\n",
      "`if ord(c) > 127 and c not in s:`\n",
      "\n",
      "`print i, c, ord(c), big[max(0, i-10):min(N, i+10)]`\n",
      "\n",
      "`s.add(c)`\n",
      "\n",
      "`print s`\n",
      "\n",
      "`print [ord(c) for c in s]`\n",
      "\n",
      "## Additional commands\n",
      "\n",
      "To add files, instead of using `hadoop fs -put filename`, we can simply drop them and create folders through the File System offered by Sandbox. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/38.jpg)\n",
      "\n",
      "To delete a file, move to Trash or use `hadoop fs -rm filename`. However, it does not properly speaking delete the file but moves it to the trash. You need to purge the trash frequently :\n",
      "\n",
      "`hadoop fs –expunge`\n",
      "\n",
      "Note that `raj_ops` does not have the rights to purge the trash.\n",
      "\n",
      "> Conclusion: I hope this tutorial was clear and helpful. I'd be happy to answer any question you might have in the comments section.\n",
      "---\n",
      "title: Graph Algorithms\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Graph Analysis and Graph Learning\"\n",
      "---\n",
      "\n",
      "So far, we covered the main kind of graphs, and the most basic characteristics to describe a graph. We'll now cover into more details graph analysis/algorithms and the different ways a graph can be analyzed. \n",
      "\n",
      "To understand the context, here are some use cases for graph algorithms :\n",
      "- real-time fraud detection\n",
      "- real-time recommendations\n",
      "- streamline regulatory compliance\n",
      "- management and monitoring of complex networks\n",
      "- identity and access management\n",
      "- social applications/features\n",
      "- ...\n",
      "\n",
      "3 main categories of graph algorithms are currently supported in most frameworks (`networkx` in Python, or Neo4J for example) :\n",
      "- pathfinding: identify the optimal path, evaluate route availability and quality. This can be used to identify the quickest route or traffic routing for example.\n",
      "- centrality: determine the importance of the nodes in the network. This can be used to identify influencers in social media for example or identify potential attack targets in a network.\n",
      "- community detection: evaluate how a group is clustered or partitioned. This can be used to segment customers and detect fraud for example.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "For what comes next, open a Jupyter Notebook and import the following packages :\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import random\n",
      "import networkx as nx\n",
      "from IPython.display import Image\n",
      "import matplotlib.pyplot as plt\n",
      "```\n",
      "\n",
      "If you have not already installed the `networkx` package, simply run :\n",
      "\n",
      "```bash\n",
      "pip install networkx\n",
      "```\n",
      "\n",
      "The following articles will be using the latest version  `2.x` of  `networkx`. NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\n",
      "\n",
      "To illustrate the different concepts we'll cover and how it applies to graphs we'll take the Karate Club example. This graph is present in the `networkx` package. It represents the relations of members of a Karate Club. However, due to a disagreement of the founders of the club, the club has recently been split in two. We'll try to illustrate this event with graphs. \n",
      "\n",
      "First, load and plot the graph :\n",
      "\n",
      "```python\n",
      "n=34\n",
      "m = 78\n",
      "G_karate = nx.karate_club_graph()\n",
      "\n",
      "pos = nx.spring_layout(G_karate)\n",
      "nx.draw(G_karate, cmap = plt.get_cmap('rainbow'), with_labels=True, pos=pos)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/karate.jpg)\n",
      "\n",
      "# I. Pathfinding and Graph Search Algorithms\n",
      "\n",
      "- Pathfinding algorithms try to find the shortest path between two nodes by minimizing the number of hops. \n",
      "- Search Algorithms does not give the shortest path. Instead, they explore graphs considering neighbors or depths of a graph.\n",
      "\n",
      "## 1. Search Algorithms\n",
      "\n",
      "There are two main graph search algorithms :\n",
      "- Breadth-First Search (BFS) that explore each node's neighbor first, then neighbors or the neighbors...\n",
      "- Depth-First Search (DFS) which try to go down a path as much as possible, and visit new neighbors if possible\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/search.jpg)\n",
      "\n",
      "## 2. Pathfinding algorithms\n",
      "\n",
      "### a. Shortest Path\n",
      "\n",
      "Shortest Path calculates the shortest weighted (if the graph is weighted) path between a pair of nodes. It is used to identify optimal driving directions or degree of separation between two people on a social network for example. \n",
      "\n",
      "In Python :\n",
      "\n",
      "```python\n",
      "# Returns the shortest path between each node\n",
      "nx.shortest_path(G_karate)\n",
      "```\n",
      "\n",
      "Returns a list of the minimal path between each node of the graph :\n",
      "\n",
      "```\n",
      "{0: {0: [0],\n",
      "    1: [0, 1],\n",
      "    2: [0, 2],\n",
      "    ...\n",
      "```\n",
      "\n",
      "### b. Single-Source Shortest Path\n",
      "\n",
      "The Single Source Shortest Path (SSSP) computes the shortest path from a given node to all other nodes in the graph. It is often used for routing protocol for IP networks for example.\n",
      "\n",
      "### c. All Pairs Shortest Path\n",
      "\n",
      "The All Pairs Shortest Path (APSP) algorithm computes the shortest path length between all pairs of nodes. It is quicker than calling the Single Source Shortest Path for every pair of nodes.\n",
      "\n",
      "This algorithm can typically be used to determine traffic load expected on different segments of a transportation grid.\n",
      "\n",
      "```python\n",
      "# Returns shortest path length between each node\n",
      "list(nx.all_pairs_shortest_path_length(G_karate))\n",
      "```\n",
      "\n",
      "Which returns : \n",
      "\n",
      "```\n",
      "[(0,\n",
      "    {0: 0,\n",
      "    1: 1,\n",
      "    2: 1,\n",
      "    3: 1,\n",
      "    4: 1,\n",
      "    ...\n",
      "```\n",
      "\n",
      "### d. Minimum Weight Spanning Tree\n",
      "\n",
      "A minimum spanning tree is a subgraph of the graph (a tree) with the minimum sum of edge weights. A spanning forest is a union of the spanning trees for each connected component of the graph.\n",
      "\n",
      "```python\n",
      "from networkx.algorithms import tree\n",
      "mst = tree.minimum_spanning_edges(G_karate, algorithm='prim', data=False)\n",
      "edgelist = list(mst)\n",
      "sorted(edgelist)\n",
      "```\n",
      "\n",
      "```\n",
      "[(0, 1),\n",
      "(0, 2),\n",
      "(0, 3),\n",
      "(0, 4),\n",
      "(0, 5),\n",
      "(0, 6),\n",
      "...\n",
      "```\n",
      "\n",
      "# II. Community detection\n",
      "\n",
      "Community detection is a process by which we partition the nodes into a set of groups/clusters according to a certain quality criterion. It is typically used to identify social communities, customers behaviors or web pages topics. \n",
      "\n",
      "A *community* is a set of nodes densely connected internally and/or sparsely connected externally. There is however no universal definition that one can give to define communities. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/community.jpg)\n",
      "\n",
      "The process to identify communities is the following :\n",
      "- define a quality criterion\n",
      "- design an algorithm to optimize this criterion\n",
      "\n",
      "We'll try to define this into more details.\n",
      "\n",
      "## Notations\n",
      "\n",
      "- Number of nodes in the graph : $$ n $$ \n",
      "- Nodes in the community : $$ S $$\n",
      "- Number of edges in $$ S $$ : $$ m_s $$\n",
      "- Number of edges in the graph : $$ m $$\n",
      "- Number of nodes in $$ S $$ : $$ n_s $$\n",
      "- Number of edges between $$ S $$ and the rest of the graph : $$ O_s $$\n",
      "\n",
      "We are now ready to define quality criteria.\n",
      "\n",
      "## Quality criteria\n",
      "\n",
      "Quality criteria might be based on : \n",
      "- internal connections \n",
      "- external connections\n",
      "- both\n",
      "\n",
      "*Based on internal connections* :\n",
      "- Internal density of edges : $$ \\frac {m_s} {n_s \\frac {(n_s-1)} {2}} $$\n",
      "- Average internal degree : $$ \\frac {2m_s} {n_s} $$\n",
      "\n",
      "*Based on external connections* :\n",
      "- Expansion : $$ \\frac {O_s} {n_s} $$\n",
      "- Ratio Cut : $$ \\frac {O_s} {n_s ( n- n_s)} $$\n",
      "\n",
      "*Based on both* :\n",
      "- Conductance : $$ \\frac {O_s} {2 m_s + O_s} $$\n",
      "- Normalized cut : $$ \\frac {O_s} {2m_s + O_s} + \\frac {O_s} {2(m-m_s) + O_s} $$\n",
      "- Modularity : $$ \\frac {1} {4} (m_s - E(m_s)) $$\n",
      "\n",
      "A note on modularity :\n",
      "$$ E(m_s) $$ is computed with respect to a random process which preserves the degree of each node. Each degree is split into two parts. Each part is combined with another randomly. \n",
      "\n",
      "Modularity is defined by : $$ \\frac {1} {2m} \\sum_{i,j} (A_{ij} - \\frac {d_i d_j} {2m} ) I_{C_i = C_j} $$\n",
      "\n",
      "However, finding communities maximizing the modularity is an exponentially complex task in the number of groups. It can easily get very costly with a few hundred nodes. For this reason, methods such as the Louvain method have been developed.\n",
      "\n",
      "We'll now cover computationally efficient techniques that scale pretty easily :\n",
      "\n",
      "## Louvain Modularity\n",
      "\n",
      "Before defining the Louvain method, it is important to define the notion of modularity. Modularity is a measure of how well groups have been partitioned into clusters :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/modu.jpg)\n",
      "\n",
      "The pseudo-code of the Louvain method is the following :\n",
      "- Assign a community to each node at first\n",
      "- Alternate the next 2 steps until convergence :\n",
      "    - Optimize local modularity. For each node, create a new community with neighboring node maximizing modularity\n",
      "    - Create a new weighted graph. Communities of the previous step become nodes of the graph\n",
      "    \n",
      "This might seem a bit confusing right now. The only thing we're doing is to group the closest nodes so that we optimize the modularity criteria.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/louvain.jpg)\n",
      "\n",
      "Note that there is no theoretical guarantee of the Louvain method, but it works well in practice.\n",
      "\n",
      "## Strongly Connected Components\n",
      "\n",
      "The Strongly Connected Components (SCC) algorithm finds sets of connected nodes in a directed graph where each node is reachable in both directions from any other node in the same set. It is often used early in a graph analysis process to give us an idea of how our graph is structured, for example, to explore financial statements data when we look at who owns shared in what company.\n",
      "\n",
      "## Weak Connected Components (Union Find)\n",
      "\n",
      "The Weakly Connected Components, or Union Find algorithm finds sets of connected nodes in a directed graph where each node is reachable from any other node in the same set.\n",
      "It only needs a path to exist between pairs of nodes in one direction, whereas SCC needs a path to exist in both directions. As with SCC, Union Find is often used early in analysis to understand a graph’s structure.\n",
      "\n",
      "Union Find is a pre-processing step that is essential before any kind of algorithm, to understand the graph's structure.\n",
      "\n",
      "We can test for connected directed graphs using :\n",
      "\n",
      "```python\n",
      "nx.is_weakly_connected(G)\n",
      "nx.is_strongly_connected(G)\n",
      "```\n",
      "\n",
      "Or for undirected graphs using :\n",
      "\n",
      "```python\n",
      "nx.is_connected(G_karate)\n",
      "```\n",
      "\n",
      "Which returns a boolean. \n",
      "\n",
      "Make sure to check the [Networkx documentation](https://networkx.github.io/documentation/stable/reference/algorithms/component.html) on the Connectivity for implementations.\n",
      "\n",
      "## Hierarchical Clustering\n",
      "\n",
      "In hierarchical clustering, we build a hierarchy of clusters. We represent the clusters under a form a dendrogram :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/dendro.jpg)\n",
      "\n",
      "The idea is to analyze community structures at different scales. We build the dendrogram bottom-up. We start with a cluster at each node and merge the two \"closest\" nodes. \n",
      "\n",
      "But how do we measure close clusters ? We use similarity distances. Let $$ d(i,j) $$ be the length of the shortest path between $$ i $$ and $$ j $$. \n",
      "- Maximum linkage : $$ D( C_1, C_2 ) = min_{i ∈ C_1, j ∈ C_2} d(i,j) $$\n",
      "- Average linkage : $$ D( C_1, C_2 ) = \\frac {1} { \\mid C_1 \\mid \\mid C_2 \\mid } \\sum_{i ∈ C_1, j ∈ C_2} d(i,j) $$\n",
      "- Centroïd linkage : $$ D(C_1, C_2) = d(G_1, G_2) $$ where $$ G_1 $$ and $$ G_2 $$ are the centers of $$ C_1, C_2 $$.\n",
      "\n",
      "The similarity distances can be illustrated as follows :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/linkage.jpg)\n",
      "\n",
      "Back to our Karate example. Then, before applying hierarchical clustering, we need to define the matrix of distances between each node.\n",
      "\n",
      "```python\n",
      "pcc_longueurs=list(nx.all_pairs_shortest_path_length(G_karate))\n",
      "distances=np.zeros((n,n))\n",
      "\n",
      "# distances[i, j] is the length of the shortest path between i and j\n",
      "for i in range(n):\n",
      "    for j in range(n):\n",
      "        distances[i, j] = pcc_longueurs[i][1][j]\n",
      "```\n",
      "\n",
      "Now, we'll use the `AgglomerativeClustering` function of  `sklearn` to identify hierarchical clustering.\n",
      "\n",
      "```python\n",
      "from sklearn.cluster import AgglomerativeClustering\n",
      "\n",
      "clustering = AgglomerativeClustering(n_clusters=2,linkage='average',affinity='precomputed').fit_predict(distances)\n",
      "```\n",
      "\n",
      "And finally, draw the resulting graph with colors depending on the cluster :\n",
      "\n",
      "```python\n",
      "nx.draw(G_karate,  node_color = clustering)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/karate2.jpg)\n",
      "\n",
      "## Clustering Coefficient\n",
      "\n",
      "The clustering coefficient measures how well two nodes tend to cluster together. \n",
      "\n",
      "A **local** clustering coefficient measures how close a node $$ i $$  and its neighbors are to being a complete graph.\n",
      "\n",
      "$$ C_i = \\frac { Triangles_i} {Triples_i} $$\n",
      "\n",
      "The local clustering coefficient is a ratio of the number of triangles centered at node $$ i $$ over the number of triples centered at node $$ i $$. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/clustering_coeff.jpg)\n",
      "\n",
      "A **global** coefficient measures the density of triangles (local clusters) in the graph :\n",
      "\n",
      "$$ CC = \\frac {1} {n} \\sum_i C_i $$\n",
      "\n",
      "In the graph shown above, the clustering coefficient is equal to :\n",
      "\n",
      "$$ CC = \\frac {1} {5} ( 1 + 1 + \\frac {1} {6} + 0 + 0 ) = \\frac {13} {30} $$\n",
      "\n",
      "### Erdos-Rényi\n",
      "\n",
      "For Erdos-Rényi random graphs, $$ E[CC] = E[C_i] = p $$ where $$ p $$ the probability defined in the previous article. \n",
      "\n",
      "### Barabàsi - Albert\n",
      "\n",
      "For Baràbasi-Albert random graphs, the global clustering coefficient follows a power law depending on the number of nodes. The average clustering coefficient of nodes with degree $$ k $$ is $$ C(k) \\propto k^{-1} $$.\n",
      "\n",
      "Nodes with a low degree are connected to other nodes in their community. Nodes with high degrees are linked to nodes in different communities. \n",
      "\n",
      "For a given graph, in `networkx`, the clustering coefficient can be easily computed. First, let's begin with the local clustering coefficients :\n",
      "\n",
      "```python\n",
      "# List of local clustering coefficients\n",
      "list(nx.clustering(G_barabasi).values())\n",
      "```\n",
      "\n",
      "```\n",
      "0.13636363636363635,\n",
      "0.2,\n",
      "0.07602339181286549,\n",
      "0.04843304843304843,\n",
      "0.09,\n",
      "0.055384615384615386,\n",
      "0.07017543859649122,\n",
      "...\n",
      "```\n",
      "And average the results to find the global clustering coefficient of the graph :\n",
      "\n",
      "```python\n",
      "# Global clustering coefficient\n",
      "np.mean(list(nx.clustering(G_barabasi).values()))\n",
      "````\n",
      "```\n",
      "0.0965577637155059\n",
      "```\n",
      "# III. Centrality algorithms\n",
      "\n",
      "It is hard to give a universal measure of centrality. Centrality measures express by how much a node is important when we want to identify important web pages, bottlenecks in transportation networks... \n",
      "\n",
      "A walk is a path which can go through the same node several times. Centrality measures vary with the type of walk considered and the way of counting them. \n",
      "\n",
      "## PageRank Algorithm\n",
      "\n",
      "PageRank estimates a current node’s importance from its linked neighbors and then again from their respective neighbors. Although popularized by Google, it's widely recognized as a way of detecting influential nodes in any network.\n",
      "\n",
      "It is for example used to suggest connections on social networks.\n",
      "\n",
      "PageRank is computed by either iteratively distributing one node's rank (originally based on the degree) over its neighbors or by randomly traversing the graph and counting the frequency of hitting each node during these walks.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/page_rank.jpg)\n",
      "\n",
      "## Degree Centrality\n",
      "\n",
      "Degree Centrality measures the number of incoming and outgoing relationships from a node.\n",
      "\n",
      "$$ C(X_i) = d_i $$, measuring the number of walks of length 1 ending at node $$ i $$\n",
      "\n",
      "Degree Centrality is used to identify the most influential persons on a social network for example.\n",
      "\n",
      "```python\n",
      "c_degree = nx.degree_centrality(G_karate)\n",
      "c_degree = list(c_degree.values())\n",
      "```\n",
      "\n",
      "## Eigenvector Centrality\n",
      "\n",
      "Eigenvector Centrality represents the number of walks of infinite length ending at node $$ i $$. This gives more importance to nodes with well-connected neighbors. \n",
      "\n",
      "$$ C(X_i) = \\frac {1} {\\lambda} \\sum_j A_{ij} C(X_j) $$ were $$ \\lambda $$ the largest eigenvalue of $$ A $$. \n",
      "\n",
      "```python\n",
      "c_eigenvector = nx.eigenvector_centrality(G_karate)\n",
      "c_eigenvector = list(c_eigenvector.values())\n",
      "```\n",
      "\n",
      "## Closeness Centrality\n",
      "\n",
      "Closeness Centrality is a way of detecting nodes that can spread information efficiently through a graph. It is used to research organizational networks where individuals with high closeness centrality are in a favorable position to control and acquire vital information and resources with the organization, for example, networks of fake news accounts or terrorist cells.\n",
      "\n",
      "$$ C(X_i) = \\frac {1} { \\sum_{j ≠ i} d(i,j) } $$, is inversely proportional to the sum of lengths of the shortest paths to other nodes.\n",
      "\n",
      "```python\n",
      "c_closeness = nx.closeness_centrality(G_karate)\n",
      "c_closeness = list(c_closeness.values())\n",
      "```\n",
      "\n",
      "## Betweenness Centrality\n",
      "\n",
      "Betweenness Centrality is a way of detecting the amount of influence a node has over the flow of information in a graph. It is often used to find nodes that serve as a bridge from one part of a graph to another, for example in package delivery process or a telecommunication network.\n",
      "\n",
      "$$ C(X_i) = \\sum_{j≠i, i≠k} \\frac { \\sigma_{jk}(i) } { \\sigma_{jk} } $$ where :\n",
      "- $$ \\sigma_{jk} $$ the number of shortest paths between $$ j $$ and $$ k $$\n",
      "- $$ \\sigma_{jk}(i) $$ the number of shortest paths between $$ j $$ and $$ k $$ going through $$ i $$\n",
      "\n",
      "The betweenness centrality measures the number of times a node acts as a bridge between two nodes. For example :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/centrality.jpg)\n",
      "\n",
      "```python\n",
      "c_betweenness = nx.betweenness_centrality(G_karate)\n",
      "c_betweenness = list(c_betweenness.values())\n",
      "```\n",
      "\n",
      "In Python, the implementation relies on the built-in functions of `networkx` :\n",
      "\n",
      "```python\n",
      "\n",
      "# Plot the centrality of the nodes\n",
      "plt.figure(figsize=(18, 12))\n",
      "f, axarr = plt.subplots(2, 2, num=1)\n",
      "plt.sca(axarr[0,0])\n",
      "nx.draw(G_karate, cmap = plt.get_cmap('inferno'), node_color = c_degree, node_size=300, pos=pos, with_labels=True)\n",
      "axarr[0,0].set_title('Degree Centrality', size=16)\n",
      "\n",
      "plt.sca(axarr[0,1])\n",
      "nx.draw(G_karate, cmap = plt.get_cmap('inferno'), node_color = c_eigenvector, node_size=300, pos=pos, with_labels=True)\n",
      "axarr[0,1].set_title('Eigenvalue Centrality', size=16)\n",
      "\n",
      "plt.sca(axarr[1,0])\n",
      "nx.draw(G_karate, cmap = plt.get_cmap('inferno'), node_color = c_closeness, node_size=300, pos=pos, with_labels=True)\n",
      "axarr[1,0].set_title('Proximity Centrality', size=16)\n",
      "\n",
      "plt.sca(axarr[1,1])\n",
      "nx.draw(G_karate, cmap = plt.get_cmap('inferno'), node_color = c_betweenness, node_size=300, pos=pos, with_labels=True)\n",
      "axarr[1,1].set_title('Betweenness Centrality', size=16)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/karate3.jpg)\n",
      "\n",
      "We observe that the different nodes highlighted by the centrality measures are quite distinct. Betweenness centrality, for example, produces results far from the other methods.\n",
      "\n",
      "> **Conclusion** : I hope that this article was helpful. Don't hesitate to drop a comment if you have any question.\n",
      "\n",
      "Sources : \n",
      "- *A Comprehensive Guide to Graph Algorithms in Neo4j*\n",
      "- Networkx Documentation\n",
      "---\n",
      "title: Key Computer Components\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Parallel and Distributed Computing\"\n",
      "---\n",
      "\n",
      "It is essential to understand what the computer we use daily is made of, at a very high level, to understand the essence of Distributed Computing, Cloud Computing, GPUs, TPUs, and other technologies.\n",
      "\n",
      "# I. Computer components\n",
      "\n",
      "## Motherboard\n",
      "\n",
      "The motherboard is the main circuit board of your computer and is also known as the mainboard or logic board. If you ever open your computer, the biggest piece of silicon you see is the motherboard. Attached to the motherboard, you'll find the CPU, ROM, memory RAM expansion slots, PCI slots, and USB ports. It also includes controllers for devices like the hard drive, DVD drive, keyboard, and mouse. The motherboard is what makes everything on your computer work together.\n",
      "\n",
      "Each motherboard has a collection of chips and controllers known as the chipset. When new motherboards are developed, they often use new chipsets. The good news is that these boards are typically more efficient and faster than their predecessors. The bad news is that older components often do not work with new chipsets.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/comp_1.jpg)\n",
      "\n",
      "## Central Processing Unit\n",
      "\n",
      "The CPU is the primary component of a computer that processes instructions. It runs the operating system and applications, constantly receiving input from the user or active software programs. It processes the data and produces output, which may be stored by an application or displayed on the screen.\n",
      "\n",
      "The CPU contains at least one processor, which is the actual chip inside the CPU that performs calculations. For many years, most CPUs only had one processor, but now it is common for a single CPU to have at least two processors or \"processing cores.\" A CPU with two processing cores is called a dual-core CPU and models with four cores are called quad-core CPUs. High-end CPUs may have six (Hexa-core) or even eight (octo-core) processors. A computer may also have more than one CPU, which each have multiple cores. For example, a server with two Hexa-core CPUs has a total of 12 processors.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/comp_2.jpg)\n",
      "\n",
      "## Graphical Processing Unit\n",
      "\n",
      "A GPU is a processor designed to handle graphics operations. This includes both 2D and 3D calculations, though GPUs primarily excel at rendering 3D graphics.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/comp_3.jpg)\n",
      "\n",
      "***History***\n",
      "Early PCs did not include GPUs, which meant the CPU had to handle all standard calculations and graphics operations. As software demands increased and graphics became more important (especially in video games), a need arose for a separate processor to render graphics. On August 31, 1999, NVIDIA introduced the first commercially available GPU for a desktop computer, called the GeForce 256. It could process 10 million polygons per second, allowing it to offload a significant amount of graphics processing from the CPU.\n",
      "\n",
      "The success of the first graphics processing unit caused both hardware and software developers alike to quickly adopt GPU support. Motherboards were manufactured with faster PCI slots and AGP slots, designed exclusively for graphics cards, became a common option as well. Software APIs like OpenGL and Direct3D were created to help developers make use of GPUs in their programs. Today, dedicated graphics processing is standard – not just in desktop PCs – but also in laptops, smartphones, and video game consoles.\n",
      "\n",
      "***Function***\n",
      "The primary purpose of a GPU is to render 3D graphics, which are comprised of polygons. Since most polygonal transformations involve decimal numbers, GPUs are designed to perform floating-point operations (as opposed to integer calculations). This specialized design enables GPUs to render graphics more efficiently than even the fastest CPUs. Offloading graphics processing to high-powered GPUs is what makes modern gaming possible.\n",
      "\n",
      "While GPUs excel at rendering graphics, the raw power of a GPU can also be used for other purposes. Many operating systems and software programs now support GPU or general-purpose computation on graphics processing units. Technologies like OpenCL and CUDA allow developers to utilize the GPU to assist the CPU in non-graphics computations. This can improve the overall performance of a computer or other electronic device.\n",
      "\n",
      "## Cache Memory\n",
      "\n",
      "Cache memory, or CPU cache, is a type of memory that services the CPU. It is faster than main memory, physically located closer to the processor, and allows the CPU to execute instructions and read and write data at a higher speed. Instructions and data are transferred from the main memory to the cache in blocks to enhance performance. Cache memory is typically static RAM (SRAM) and is identiﬁed by level. Level 1 (L1) cache is built directly into the CPU chip. Level 2 cache (L2) feeds the L1 cache. L2 can be built into the CPU chip, reside on a separate chip, or be a separate bank of chips on the system board. If L2 is built into the CPU, then a level 3 cache (L3) may also be present on the system board.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/comp_4.jpg)\n",
      "\n",
      "## RAM\n",
      "\n",
      "Stands for \"Random Access Memory». RAM is made up of small memory chips that form a memory module. These modules are installed in the RAM slots on the motherboard of your computer.\n",
      "\n",
      "Every time you open a program, it gets loaded from the hard drive into the RAM. This is because reading data from the RAM is much faster than reading data from the hard drive. Running programs from the RAM of the computer allows them to function without any lag time. The more RAM your computer has, the more data can be loaded from the hard drive into the RAM, which can effectively speed up your computer. Adding RAM can be more beneficial to your computer's performance than upgrading the CPU.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/comp_5.jpg)\n",
      "\n",
      "## Hard Disk Drive (HDD)\n",
      "\n",
      "The data is stored on a stack of disks that are mounted inside a solid encasement. These disks spin extremely fast (typically at either 5400 or 7200 RPM) so that data can be accessed immediately from anywhere on the drive. The data is stored on the hard drive magnetically, so it stays on the drive even after the power supply is turned off.\n",
      "\n",
      "The term \"hard drive\" is short for \"hard disk drive.\" The term \"hard disk\" refers to the actual disks inside the drive. However, all three of these terms are usually seen as referring to the same thing -- the place where your data is stored.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/comp_6.jpg)\n",
      "\n",
      "## Solid State Drive (SSD)\n",
      "\n",
      "Stands for \"Solid State Drive.\" An SSD is a type of mass storage device similar to a hard disk drive (HDD). It supports reading and writing data and maintains stored data in a permanent state even without power. Internal SSDs connect to a computer like a hard drive, using standard IDE or SATA connections.\n",
      "\n",
      "While SSDs serve the same function as hard drives, their internal components are much different. Unlike hard drives, SSDs do not have any moving parts (which is why they are called solid-state drives). Instead of storing data on magnetic platters, SSDs store data using flash memory. Since SSDs have no moving parts, they don't have to \"spin up\" while in a sleep state and they don't need to move a drive head to different parts of the drive to access data. Therefore, SSDs can access data faster than HDDs.\n",
      "\n",
      "SSDs have several other advantages over hard drives as well. For example, the read performance of a hard drive declines when data gets fragmented or split up into multiple locations on the disk. The read performance of an SSD does not diminish based on where data is stored on the drive. Therefore defragmenting an SSD is not necessary. Since SSDs do not store data magnetically, they are not susceptible to data loss due to strong magnetic fields close to the drive. Additionally, since SSDs have no moving parts, there is far less chance of a mechanical breakdown. SSDs are also lighter, quieter, and use less power than hard drives. This is why SSDs have become a popular choice for laptop computers.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/comp_7.jpg)\n",
      "\n",
      "> Conclusion: I hope this high-level overview was clear and helpful. I'd be happy to answer any question you might have in the comments section.\n",
      "---\n",
      "title: Manipulating strings\n",
      "layout: post\n",
      "tags: [da_tuto]\n",
      "subtitle : \"Data Analysis Basics\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "Around 85% of the world data is unstructured, and a good part of it is text data. As a data analyst or data scientist, it is a common job to handle text data, and therfore strings.\n",
      "\n",
      "# Character Strings\n",
      "\n",
      "Character strings are sequences of individual letters which all together form a string.\n",
      "\n",
      "```python\n",
      "a = \"My cat is 8\"\n",
      "```\n",
      "\n",
      "Now, we'll dive into some key functions that you can use on strings.\n",
      "\n",
      "You can retrieve the length of a string by simply using the keyword `len` :\n",
      "\n",
      "```python\n",
      "len(a)\n",
      "```\n",
      "\n",
      "```\n",
      "11\n",
      "``\n",
      "\n",
      "You can count the number of occurences of a specific letter by using `.count`:\n",
      "\n",
      "```python\n",
      "a.count(\"M\")\n",
      "```\n",
      "\n",
      "```\n",
      "1\n",
      "```\n",
      "\n",
      "You can apply a lower, upper or title transformation to a string (`upper`, `lower`, `title`).\n",
      "\n",
      "```python\n",
      "a.upper()\n",
      "```\n",
      "\n",
      "```\n",
      "MY CAT IS 8\n",
      "```\n",
      "\n",
      "You can replace a letter or several letters by another letter or group of letters.\n",
      "\n",
      "```python\n",
      "a.replace(\"My\", \"my\")\n",
      "```\n",
      "\n",
      "```\n",
      "my cat is 8\n",
      "```\n",
      "\n",
      "What if I now want to split my string into a list of words, that is to obtain a list of words? You can use the `split` keyword:\n",
      "\n",
      "```python\n",
      "a.split()\n",
      "```\n",
      "\n",
      "```\n",
      "[\"My\", \"cat\", \"is\", \"8\"]\n",
      "```\n",
      "\n",
      "You can specify in the parenthesis the character on which it should split, by default it is the space.\n",
      "\n",
      "You can also index a string to extract a sub-string:\n",
      "\n",
      "```python\n",
      "a[1:6]\n",
      "```\n",
      "\n",
      "```\n",
      "y cat\n",
      "```\n",
      "\n",
      "We start the indexing in 0, and exclude the 6th element. Notice how the space counts as a character too.\n",
      "\n",
      "> If you found the article useful or see ways in which it could be improved, please leave a comment :)\n",
      "---\n",
      "title: Introduction to Spark\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Parallel and Distributed Computing\"\n",
      "---\n",
      "\n",
      "If you haven't seen it, I recommend reading first my tutorial on [Hadoop MapReduce](https://maelfabien.github.io/bgd/#). In this series of articles, we'll talk about Spark. Spark is an Apache Software Foundation project that was developed to speed up the Hadoop Computational software process.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/spark_logo.jpg)\n",
      "\n",
      "# What is Apache Spark?\n",
      "\n",
      "Spark is an open-source large scale data processing and data analysis tool, coded in Scala, a functional programming language, highly typed, less verbal than Java.\n",
      "\n",
      "Spark is available through 4 different APIs :\n",
      "- Scala: Most popular API, in which the language is typed and compiled. Therefore, code errors are checked before launch, which avoids errors after 10 hours of computation for example. Scala's API is often used in production.\n",
      "- Python: Second most popular API. The language is however only checked at runtime. This API is mostly used in POCs and R&D environments.\n",
      "- Java: If the company's infrastructure is built in Java, this might be the best option.\n",
      "- R: This API is rarely used, but there's a community around it.\n",
      "\n",
      "Spark is compatible with most databases and distributed (or not) file systems :\n",
      "- S3 (AWS), \n",
      "- Google Storage\n",
      "- HDFS\n",
      "- Cassandra, \n",
      "- HBase, \n",
      "- Redshift…\n",
      "\n",
      "Spark has an active community of over 1000 contributors, producing around 100 commits/week.\n",
      "\n",
      "## Key concepts\n",
      "\n",
      "> The main feature of Spark is that is stores the working dataset on the cluster's cache memory, to allow faster computing.\n",
      "\n",
      "Spark leverages task parallelization on multiple workers, just like MapReduce. Spark works the same way :\n",
      "- On a single machine for tests and small data samples, local, without any cluster manager\n",
      "- On a cluster for large data volumes\n",
      "\n",
      "Spark is *not a modified version of Hadoop*, but Hadoop is a way to implement Spark. Indeed, Spark can be built with Hadoop components in the following ways :\n",
      "- Spark on top of HDFS\n",
      "- Spark on top of HDFS and Yarn\n",
      "- Spark on top of HDFS and in MapReduce\n",
      "\n",
      "## Key components \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/comp.jpg)\n",
      "\n",
      "- Spark Core, the underlying general execution engine for Spark platform. Other functionalities are built upon Spark Core, which provides in-memory computing and referencing datasets in external storage systems.\n",
      "- Spark SQL: SparkSQL data-frames allow the use of RDD (Resilient Distributed Datasets), providing support for the structured and semi-structured data\n",
      "- Spark Streaming: Spark Streaming allows pseudo-streaming, i.e. ingesting micro-batch (e.g every 20ms) and performing RDD transformations on those micro-batches.\n",
      "- MLib: SparkML is a distributed machine learning framework above Spark because of the distributed memory-based Spark architecture. It includes the whole ML framework: pre-processing, cross-validation, algorithms… in a distributed system.\n",
      "- GraphX: A distributed graph-processing framework on top of Spark, to model user-defined graphs by using Pregel abstraction API.\n",
      "\n",
      "## History \n",
      "\n",
      "- 2003-4: MapReduce is introduced by Google\n",
      "- 2006: Hadoop is developed by Yahoo!\n",
      "- 2009: Spark Research projects start at UC Berkley\n",
      "- 2010 : [Spark Research paper](http://people.csail.mit.edu/matei/papers/2010/hotcloud_spark.pdf)\n",
      "- 2010: Spark is open-sourced\n",
      "- 2012 : [RDD Research paper](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)\n",
      "- 2013: Spark joins the Apache Software Foundation\n",
      "- 2013 : [Spark Streaming paper](https://people.csail.mit.edu/matei/papers/2013/sosp_spark_streaming.pdf)\n",
      "- 2014 : [GraphX paper](https://amplab.cs.berkeley.edu/wp-content/uploads/2014/09/graphx.pdf)\n",
      "- 2015 : [SparkSQL paper](https://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf)\n",
      "- 2016 : [MLib paper](https://arxiv.org/pdf/1505.06807.pdf)\n",
      "- 2017: Deep Learning pipelines in MLib\n",
      "\n",
      "## Why use Spark?\n",
      "\n",
      "Hadoop applications spend more than 90% of their time doing HDFS read-write operations. Using data frames in cache greatly improves the processing time of Spark. Nowadays, Spark is widely used, and the community is way more active than around Hadoop. \n",
      "\n",
      "## Directly Acyclic Graph (DAG)\n",
      "\n",
      "Spark offers a Directed Acyclic Graph service (DAG). The DAG :\n",
      "- Cuts the script in stages: groups of tasks that do not require any data transfer between the nodes of the cluster\n",
      "- Identifies the stages that can be used in parallel\n",
      "- Determines where a task should be executed to minimize data transfer\n",
      "- Optimizes data treatment pipeline\n",
      "- Optimizes intermediate lost data collection\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/dag.jpg)\n",
      "\n",
      "## RDD\n",
      "\n",
      "Resilient Distributed Datasets (RDD) is a fundamental data structure of Spark. It is an immutable, read-only, distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster. RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.\n",
      "\n",
      "There are two ways to create RDDs :\n",
      "- parallelizing an existing collection in your driver program, \n",
      "- or referencing a dataset in an external storage system, such as a shared file system, HDFS, HBase, or any data source offering a Hadoop Input Format.\n",
      "\n",
      "In Spark, there are several data formats :\n",
      "- RDD: one of the version versions of Spark data structure. It takes the form of distributed tuples, e.g `[(1,2,3,4), (10,20,30,40), ...]`. There is no column structure in RDDs.\n",
      "- DataFrame: DataFrames are layered on top of RDDs. It looks like a Pandas DataFrame, with columns, but is indeed an RDD of rows.\n",
      "- DataSet: DataSets are layers on top of DataFrames which allow specifying types for columns.\n",
      "\n",
      "# How does Spark work?\n",
      "\n",
      "There are 2 types of Spark RDD operations :\n",
      "- **Actions** are operations that return values (collect, count, take...). \n",
      "- **Transformations** are operations that can be chained on your working dataset (filter, drop, map, union).\n",
      "\n",
      "> Only actions are evaluated. This is called *lazy* evaluation. You can define several transformations, are they'll all be evaluated when you run the next action. Actions are really expensive, so you should limit the number of actions in your code.\n",
      "\n",
      "## Transformations\n",
      "\n",
      "The main transformations are :\n",
      "- map()\n",
      "- filter()\n",
      "- count()\n",
      "- distinct()\n",
      "- union()\n",
      "- ...\n",
      "\n",
      "There are 2 types of transformations :\n",
      "- Narrow transformation: In Narrow transformation, all the elements that are required to compute the records in single partition live in the single partition of parent RDD. \n",
      "- Wide transformation: In wide transformation, all the elements that are required to compute the records in the single partition may live in many partitions of parent RDD. \n",
      "\n",
      "## Actions\n",
      "\n",
      "Actions, unlike transformations, do not produce new RDDs but values. Actions might be :\n",
      "- count() : the number of elements in RDD\n",
      "- collect() : returns the entire RDDs content\n",
      "-  take(n) : returns n number of elements from RDD\n",
      "- countByValue()\n",
      "- reduce()\n",
      "\n",
      "## Key operations\n",
      "\n",
      "There are some key operations one can do in Spark that are interesting for parallel computation.\n",
      "\n",
      "### Shuffling\n",
      "\n",
      "Since we work on a multi-node system, shuffling makes an efficient redistribution of our data on the working nodes. By shuffling the data across the cluster nodes, we can execute tasks much faster. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/shuffling.jpg)\n",
      "\n",
      "However, this operation requires to write data on disk and is therefore really expensive in terms on time. \n",
      "\n",
      "### Persistence\n",
      "\n",
      "Persistence is the operation that stores your dataset into fast RAM.\n",
      "- In some application, and especially in machine learning, we need to access the same data multiple times.\n",
      "- With .persist() we force Spark to put a data collection into RAM memory.\n",
      "- If you persist your client's dataset, each node will put his part of the dataset into cache memory.\n",
      "\n",
      "### Partitioning\n",
      "\n",
      "The partitioning describes how to split datasets on a cluster. The number of partitions determines the number of tasks that will be created by the scheduler.\n",
      "\n",
      "When you're working with a more complex dataset, you may consider using a partition key, just like SQL. This key will determine which part of the dataset goes on which node.\n",
      "\n",
      "### Broadcasting\n",
      "\n",
      "Broadcasting is a way to force caching a dataset, on every node of the cluster.\n",
      "\n",
      "### Joins\n",
      "\n",
      "Joins in Spark are similar to joins in SQL (INNER, LEFT, RIGHT, FULL).\n",
      "\n",
      "> Conclusion: All those concepts still need illustration, but it's the purpose of the next article. I hope this high-level overview was clear and helpful. I'd be happy to answer any question you might have in the comments section.\n",
      "---\n",
      "title: Character-Level LSTMs for Gender Classification from Name\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Natural Language Processing\"\n",
      "---\n",
      "\n",
      "Identifying a name from a first name is a hard task. Some names could be both masculine and feminine, and we usually have no other indicator than some cultural knowledge that a given name is attached to a male or a female figure. \n",
      "\n",
      "But is it really true? Can't we, by leveraging the letters used in names, identify whether a first name belongs to a male or a female?\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# Previous Work\n",
      "\n",
      "I recently encountered an article called \"Predicting the gender of Indonesian names\" written by Ali Akbar Septiandri. The author, through his article, claims that he was able to predict accurately the gender of indonesian names in more than 90% of the cases. \n",
      "\n",
      "So I started to wonder whether this was specific to Indonesian names or if I could apply it more generally to all names given in France (and not only french names).\n",
      "\n",
      "# Training Data\n",
      "\n",
      "I used a data set that the French government published through its platform `data.gouv.fr`. It is an up-to-date list of the first names in France since 1900, with over 30'000 names, and can be downloaded here: https://www.data.gouv.fr/fr/datasets/ficher-des-prenoms-de-1900-a-2018/. I replaced accents by standard letters since accents nearly double the number of characters to distinguish from and do supposely not bring information on a name's gender. I also added a similar data set with names given in the United States over the same period, of around 90'000 names, which can be downloaded here: https://data.world/howarder/gender-by-name. \n",
      "\n",
      "# My Algorithm\n",
      "\n",
      "Start by importing the relevant packages:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from tensorflow.keras.preprocessing import sequence\n",
      "from tensorflow.keras.models import Sequential\n",
      "from tensorflow.keras.layers import Dense, Embedding, Dense, Activation, Dropout, LSTM, Bidirectional\n",
      "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
      "from sklearn.model_selection import train_test_split\n",
      "from tensorflow.keras.regularizers import l2\n",
      "from tensorflow.keras.utils import plot_model\n",
      "from sklearn.model_selection import train_test_split\n",
      "```\n",
      "\n",
      "## Data pre-processing\n",
      "\n",
      "First, I pre-process the text by removing accents.\n",
      "\n",
      "```python\n",
      "def rmv_acc(string_1):\n",
      "\n",
      "    string_1 = string_1.replace(\"ç\", \"c\")\n",
      "    string_1 = string_1.replace(\"Ç\", \"C\")\n",
      "    string_1 = string_1.replace(\"à\", \"a\")\n",
      "    string_1 = string_1.replace(\"Ä\", \"A\")\n",
      "    string_1 = string_1.replace(\"ä\", \"a\")\n",
      "    string_1 = string_1.replace(\"À\", \"A\")\n",
      "    string_1 = string_1.replace(\"Â\", \"A\")\n",
      "    string_1 = string_1.replace(\"â\", \"a\")\n",
      "    string_1 = string_1.replace(\"é\", \"e\")\n",
      "    string_1 = string_1.replace(\"è\", \"e\")\n",
      "    string_1 = string_1.replace(\"É\", \"E\")\n",
      "    string_1 = string_1.replace(\"È\", \"E\")\n",
      "    string_1 = string_1.replace(\"Ë\", \"E\")\n",
      "    string_1 = string_1.replace(\"ë\", \"e\")\n",
      "    string_1 = string_1.replace(\"Ê\", \"E\")\n",
      "    string_1 = string_1.replace(\"ê\", \"e\")\n",
      "    string_1 = string_1.replace(\"û\", \"u\")\n",
      "    string_1 = string_1.replace(\"Û\", \"U\")\n",
      "    string_1 = string_1.replace(\"ü\", \"u\")\n",
      "    string_1 = string_1.replace(\"Ü\", \"U\")\n",
      "    string_1 = string_1.replace(\"ï\", \"i\")\n",
      "    string_1 = string_1.replace(\"Ï\", \"I\")\n",
      "    string_1 = string_1.replace(\"î\", \"i\")\n",
      "    string_1 = string_1.replace(\"Î\", \"I\")\n",
      "    string_1 = string_1.replace(\"Ô\", \"O\")\n",
      "    string_1 = string_1.replace(\"ô\", \"o\")\n",
      "    string_1 = string_1.replace(\"Ö\", \"O\")\n",
      "    string_1 = string_1.replace(\"ö\", \"o\")\n",
      "    string_1 = string_1.replace(\"Ù\", \"U\")\n",
      "    string_1 = string_1.replace(\"ù\", \"u\")\n",
      "    string_1 = string_1.replace(\"ÿ\", \"y\")\n",
      "    string_1 = string_1.replace(\"æ\", \"ae\")\n",
      "    string_1 = string_1.replace(\"_\", \" \")\n",
      "\n",
      "    return string_1\n",
      "```\n",
      "\n",
      "I also convert both data sources to the same format, and load them in a single dataframe.\n",
      "\n",
      "```python\n",
      "df = pd.read_csv('nat2018.csv', sep=\";\")\n",
      "df = df[['sexe', 'preusuel']].drop_duplicates()\n",
      "df.columns = ['gender', 'name']\n",
      "\n",
      "def sexe(x):\n",
      "    if x == 1:\n",
      "        return \"M\"\n",
      "    else:\n",
      "        return \"F\"\n",
      "    \n",
      "df['gender'] = df['gender'].apply(lambda x: sexe(x)) \n",
      "columnsTitles=[\"name\",\"gender\"]\n",
      "df=df.reindex(columns=columnsTitles)\n",
      "\n",
      "df2 = pd.read_csv('name_gender.csv')[['name', 'gender']]\n",
      "\n",
      "df = pd.concat([df, df2])\n",
      "df['name'] = df['name'].apply(lambda x: str(x).lower())\n",
      "df['name'] = df['name'].apply(lambda x: rmv_acc(x))\n",
      "df = df[[len(e)>1 for e in df.name]]\n",
      "df = df.drop_duplicates()\n",
      "\n",
      "names = df['name'].apply(lambda x: x.lower())\n",
      "gender = df['gender']\n",
      "\n",
      "df.head()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp7_1.png)\n",
      "\n",
      "We can then take a look at the distribution of the length of names in terms of number of letters.\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist([len(a) for a in names], bins=36)\n",
      "plt.title(\"Length of the names\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp7_2.png)\n",
      "\n",
      "For future names to classify, we will set a threshold at 20, meaning that no name should be longer than 20 characters.\n",
      "\n",
      "```python\n",
      "maxlen = 20\n",
      "labels = 2\n",
      "```\n",
      "\n",
      "We can take a look at the class balance:\n",
      "\n",
      "```python\n",
      "print(\"Male : \" + str(sum(gender=='M')))\n",
      "print(\"Female : \" + str(sum(gender=='F')))\n",
      "```\n",
      "\n",
      "```\n",
      "Male : 44580\n",
      "Female : 70147\n",
      "```\n",
      "\n",
      "We have a bit more than 60% of female names.\n",
      "\n",
      "I chose to explore Long Short-Term Memory (LSTMs) algorithms for this task. Such networks are usually applied at a word level, but we will apply it at a \"character-level\". For this reason, we talk about character-level LSTMs.\n",
      "\n",
      "As in LSTMs, we first must define a vocabulary which corresponds to all the unique letters encountered:\n",
      "\n",
      "```python\n",
      "vocab = set(' '.join([str(i) for i in names]))\n",
      "vocab.add('END')\n",
      "len_vocab = len(vocab)\n",
      "``` \n",
      "\n",
      "The vocabulary has a length of 30 here (taking into account special characters and all the alphabet):\n",
      "\n",
      "```python\n",
      "{' ',\n",
      " \"'\",\n",
      " '-',\n",
      " 'END',\n",
      " 'a',\n",
      " 'b',\n",
      " 'c',\n",
      " 'd',\n",
      " 'e',\n",
      " ...}\n",
      " ```\n",
      "\n",
      "We then create a dictionary which maps each letter of vocabulary to a number:\n",
      "\n",
      "```python\n",
      "char_index = dict((c, i) for i, c in enumerate(vocab))\n",
      "char_index\n",
      "```\n",
      "\n",
      "```python\n",
      "{' ': 21,\n",
      " \"'\": 20,\n",
      " '-': 28,\n",
      " 'END': 24,\n",
      " 'a': 0,\n",
      " 'b': 23,\n",
      " 'c': 18,\n",
      " 'd': 13,\n",
      "```\n",
      "\n",
      "We then must create the training dataset:\n",
      "\n",
      "```python\n",
      "X = []\n",
      "y = []\n",
      "\n",
      "# Builds an empty line with a 1 at the index of character\n",
      "def set_flag(i):\n",
      "    tmp = np.zeros(len_vocab);\n",
      "    tmp[i] = 1\n",
      "    return list(tmp)\n",
      "\n",
      "# Truncate names and create the matrix\n",
      "def prepare_X(X):\n",
      "    new_list = []\n",
      "    trunc_train_name = [str(i)[0:maxlen] for i in X]\n",
      "\n",
      "    for i in trunc_train_name:\n",
      "        tmp = [set_flag(char_index[j]) for j in str(i)]\n",
      "        for k in range(0,maxlen - len(str(i))):\n",
      "            tmp.append(set_flag(char_index[\"END\"]))\n",
      "        new_list.append(tmp)\n",
      "\n",
      "    return new_list\n",
      "\n",
      "\n",
      "X = prepare_X(names.values)\n",
      "\n",
      "# Label Encoding of y\n",
      "def prepare_y(y):\n",
      "    new_list = []\n",
      "    for i in y:\n",
      "        if i == 'M':\n",
      "            new_list.append([1,0])\n",
      "        else:\n",
      "            new_list.append([0,1])\n",
      "\n",
      "    return new_list\n",
      "\n",
      "y = prepare_y(gender)\n",
      "```\n",
      "\n",
      "We the  split X and y into `X_train`, `X_test`, `y_train` and `y_test`.\n",
      "\n",
      "```python\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
      "```\n",
      "\n",
      "## Building the model\n",
      "\n",
      "It is now time to create the model and train it. I created an architecture with a first LSTM layer, a dropout to prevent overfitting, a second LSTM layer, a second dropout, a dense layer with 2 neurons corresponding to the classification of the name as either a male or a female. I then add a sigmoid layer which outputs probabilities of belonging to one class or another. An L2 regualization has also been added on the dense layer.\n",
      "\n",
      "I replaced LSTMs by bi-directional LSTMs to overcome the limited performance of the model. Bi-directional LSTMs depend on the whole input sequence since each layer corresponds to both a forward and a backward through time layer. This allows the output units to compute a representation that depends on both the past and the future but is most sensitive to the input values around time t. Replacing LSTMs by bi-directional ones allowed a slight validation accuracy gain.\n",
      "\n",
      "I used Tensorflow as a back-end for a Keras architecture:\n",
      "\n",
      "```python\n",
      "model = Sequential()\n",
      "model.add(Bidirectional(LSTM(512, return_sequences=True), backward_layer=LSTM(512, return_sequences=True, go_backwards=True), input_shape=(maxlen,len_vocab)))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Bidirectional(LSTM(512)))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Dense(2, activity_regularizer=l2(0.002)))\n",
      "model.add(Activation('softmax'))\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
      "```\n",
      "\n",
      "A summary of the model is presented below:\n",
      "\n",
      "```\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_12 (Bidirectio (None, 20, 1024)          2224128   \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 20, 1024)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 1024)              6295552   \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 2)                 2050      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 8,521,730\n",
      "Trainable params: 8,521,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "```\n",
      "\n",
      "We reach 8.5 million parameters. We can also plot the model graphically:\n",
      "\n",
      "```python\n",
      "plot_model(model, to_file='model_2.png', show_shapes=True, expand_nested=True)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/model_2.png)\n",
      "\n",
      "\n",
      "We can define callbacks in Keras to:\n",
      "- apply early stopping if the validation loss does not decrease anymore\n",
      "- save the model which has the minimal validation loss\n",
      "- reduce the learning rate on plateau if the valiadtion accuracy does not increase\n",
      "\n",
      "```python\n",
      "callback = EarlyStopping(monitor='val_loss', patience=5)\n",
      "mc = ModelCheckpoint('best_model_9.h5', monitor='val_loss', mode='min', verbose=1)\n",
      "reduce_lr_acc = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=2, verbose=1, min_delta=1e-4, mode='max')\n",
      "```\n",
      "\n",
      "It is now time to fit the model!\n",
      "\n",
      "```python\n",
      "batch_size = 256\n",
      "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=35, verbose=1, validation_data =(X_test, y_test), callbacks=[callback, mc, reduce_lr_acc])\n",
      "```\n",
      "\n",
      "```\n",
      "Train on 86045 samples, validate on 28682 samples\n",
      "Epoch 1/35\n",
      "86016/86045 [============================>.] - ETA: 0s - loss: 0.4662 - accuracy: 0.7795\n",
      "Epoch 00001: saving model to best_model_9.h5\n",
      "86045/86045 [==============================] - 1402s 16ms/sample - loss: 0.4662 - accuracy: 0.7795 - val_loss: 0.4304 - val_accuracy: 0.8070\n",
      "Epoch 2/35\n",
      "86016/86045 [============================>.] - ETA: 0s - loss: 0.3972 - accuracy: 0.8249\n",
      "Epoch 00002: saving model to best_model_9.h5\n",
      "...\n",
      "```\n",
      "\n",
      "The training takes 2 to 3 hours on CPUs, and reaches the early stopping at around 18 epochs. We can plot the training and validation accuracy curves:\n",
      "\n",
      "```python \n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(history.history['accuracy'])\n",
      "plt.plot(history.history['val_accuracy'])\n",
      "plt.title('Model accuracy')\n",
      "plt.ylabel('Accuracy')\n",
      "plt.xlabel('Epoch')\n",
      "plt.legend(['Train', 'Test'], loc='upper left')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp7_3.png)\n",
      "\n",
      "We reach a validation accuracy of 89.5% but notice a slight overfitting, although drop out, L2-regularization and reduction of learning rate had been applied.\n",
      "\n",
      "## Making predictions\n",
      "\n",
      "Let's make predictions using the trained model now!\n",
      "\n",
      "We first define new names, and process them:\n",
      "\n",
      "```python\n",
      "new_names = [\"mael\", \"jenny\", \"marc\"]\n",
      "X_pred = prepare_X([rmv_acc(e) for e in new_names])\n",
      "```\n",
      "\n",
      "Then, we make the predictions:\n",
      "\n",
      "```python\n",
      "prediction = model.predict(X_pred)\n",
      "prediction\n",
      "```\n",
      "\n",
      "Which returns :\n",
      "\n",
      "```python\n",
      "array([[0.3685085 , 0.6314915 ],\n",
      "       [0.14170532, 0.85829467],\n",
      "       [0.8759549 , 0.12404503]], dtype=float32)\n",
      "```\n",
      "\n",
      "To make it more understandable, we create a function which maps this to labels (M or F), or N (for neutral) if the probability is not high enough:\n",
      "\n",
      "\n",
      "```python\n",
      "def pred(new_names, prediction, dict_answer):\n",
      "    return_results = []\n",
      "    k = 0\n",
      "    for i in prediction:\n",
      "        if max(i) < 0.65:\n",
      "            return_results.append([new_names[k], \"N\"])\n",
      "        else:\n",
      "            return_results.append([new_names[k], dict_answer[np.argmax(i)]])\n",
      "        k += 1\n",
      "    return return_results\n",
      "```\n",
      "\n",
      "If we run this on the predictions above:\n",
      "\n",
      "```python\n",
      "pred(new_names, prediction, dict_answer)\n",
      "```\n",
      "\n",
      "Which returns:\n",
      "\n",
      "```python\n",
      "[['mael', 'N'], ['jenny', 'F'], ['marc', 'M']]\n",
      "```\n",
      "\n",
      "The outcome is good, since Mael in French can be used for both Males and Females, Jenny is a female name and Marc a male name.\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "In conclusion, we covered in this article a method to apply character-level bi-directional LSTMs for gender classification from first names. We are slightly below the accuracy of the authors of the papers \"Predicting the gender of Indonesian names\" but we face a large names database of over 120'000 names given in France and in the US.\n",
      "\n",
      "\n",
      "---\n",
      "title: A full guide to face detection\n",
      "layout: post\n",
      "tags: [tutorials]\n",
      "subtitle : \"Tutorials\"\n",
      "---\n",
      "\n",
      "In this tutorial, we'll see how to create and launch a face detection algorithm in Python using OpenCV. We'll also add some features to detect eyes and mouth on multiple faces at the same time. This article will go through the most basic implementations of face detection including Cascade Classifiers, HOG windows and Deep Learning.\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "We'll cover face detection using :\n",
      "- Haar Cascade Classifiers using OpenCV\n",
      "- Histogram of Oriented Gradients using Dlib\n",
      "- Convolutional Neural Networks using Dlib\n",
      "\n",
      "# Introduction\n",
      "\n",
      "We'll be using OpenCV, an open source library for computer vision, written in C/C++, that has interfaces in C++, Python and Java. It supports Windows, Linux, MacOS, iOS and Android. Some of our work will also require using Dlib, a modern C++ toolkit containing machine learning algorithms and tools for creating complex software.\n",
      "\n",
      "## Requirements\n",
      "\n",
      "The first step is to install OpenCV. Run the following command line in your terminal :\n",
      "\n",
      "```python\n",
      "pip install opencv-python\n",
      "```\n",
      "Depending on your version, the file will be installed here :\n",
      "```python\n",
      "/usr/local/lib/python3.7/site-packages/cv2\n",
      "```\n",
      "If you have not yet installed Dlib, run the following command :\n",
      "\n",
      "```python\n",
      "pip install dlib\n",
      "```\n",
      "\n",
      "If you encounter some issues with Dlib, check [this article](https://www.pyimagesearch.com/2018/01/22/install-dlib-easy-complete-guide/).\n",
      "\n",
      "## Imports and models path\n",
      "\n",
      "We'll create a new Jupyter notebook / python file and start off with :\n",
      "```python\n",
      "import cv2\n",
      "import matplotlib.pyplot as plt\n",
      "import dlib\n",
      "from imutils import face_utils\n",
      "\n",
      "font = cv2.FONT_HERSHEY_SIMPLEX\n",
      "```\n",
      "\n",
      "# I. Cascade Classifiers\n",
      "\n",
      "We'll explore Cascade Classifiers at first. \n",
      "\n",
      "## 1. Theory\n",
      "\n",
      "Cascade classifier, or namely cascade of boosted classifiers working with haar-like features, is a special case of ensemble learning, called boosting. It typically relies on [Adaboost](https://maelfabien.github.io/machinelearning/adaboost) classifiers (and other models such as Real Adaboost, Gentle Adaboost or Logitboost).\n",
      "\n",
      "Cascade classifiers are trained on a few hundred sample images of image that contain the object we want to detect, and other images that do not contain those images. \n",
      "\n",
      "How can we detect if a face is there or not ? There is an algorithm, called Viola–Jones object detection framework, that includes all the steps required for live face detection :\n",
      "- Haar Feature Selection,  features derived from Haar wavelets\n",
      "- Create integral image\n",
      "- Adaboost Training\n",
      "- Cascading Classifiers\n",
      "\n",
      "The original [paper](https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf) was published in 2001.\n",
      "\n",
      "### a. Haar Feature Selection\n",
      "\n",
      "There are some common features that we find on most common human faces :\n",
      "- a dark eye region compared to upper-cheeks\n",
      "- a bright nose bridge region compared to the eyes\n",
      "- some specific location of eyes, mouth, nose...\n",
      "\n",
      "The characteristics are called Haar Features. The feature extraction process will look like this :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/haar.jpg)\n",
      "\n",
      "In this example, the first feature measures the difference in intensity between the region of the eyes and a region across the upper cheeks. The feature value is simply computed by summing the pixels in the black area and subtracting the pixels in the white area. \n",
      "\n",
      "$$ Rectangle Feature = \\sum (pixels_{black area}) - \\sum (pixels_{white area}) $$ \n",
      "\n",
      "Then, we apply this rectangle as a convolutional kernel, over our whole image. In order to be exhaustive, we should apply all possible dimensions and positions of each kernel. A simple 24*24 images would typically result in over 160'000 features, each made of a sum/subtraction of pixels values. It would computationally be impossible for live face detection. So, how do we speed up this process ?\n",
      "- once the good region has been identified by a rectangle, it is useless to run the window over a completely different region of the image. This can be achieved by Adaboost.\n",
      "- compute the rectangle features using the integral image principle, which is way faster. We'll cover this in the next section.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/haar_selection.jpg)\n",
      "\n",
      "There are several types of rectangles that can be applied for Haar Features extraction. According to the original paper :\n",
      "- the two-rectangle feature is the difference between the sum of the pixels within two rectangular regions, used mainly for detecting edges (a,b)\n",
      "- the three-rectangle feature computes the sum within two outside rectangles subtracted from the sum in a center rectangle, used mainly for detecting lines (c,d)\n",
      "- the four-rectangle feature computes the difference between diagonal pairs of rectangle (e)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/haar_rectangles.jpg)\n",
      "\n",
      "Now that the features have been selected, we apply them on the set of training images using Adaboost classification, that combines a set of weak classifiers to create an accurate ensemble model. With 200 features (instead of 160'000 initially), an accuracy of 95% is acheived. The authors of the paper have selected 6'000 features. \n",
      "\n",
      "### b. The integral image\n",
      "\n",
      "Computing the rectangle features in a convolutional kernel style can be long, very long. For this reason, the authors, Viola and Jones, proposed an intermediate representation for the image : the integral image. The role of the integral image is to allow any rectangular sum to be computed simply, using only four values. We'll see how it works !\n",
      "\n",
      "Suppose we want to determine the rectangle features at a given pixel with coordinates $$ (x,y) $$. Then, the integral image of the pixel in the sum of the pixels above and to the left of the given pixel. \n",
      "\n",
      "$$ ii(x,y) = \\sum_{x'≤x, y'≤y} i(x', y') $$\n",
      "\n",
      "where $$ ii(x,y) $$ is the integral image and $$ i(x,y) $$ is the original image.\n",
      "\n",
      "When you compute the whole integral image, there is a form a recurrence which requires only one pass over the original image. Indeed, we can define the following pair of recurrences :\n",
      "\n",
      "$$ s(x,y) = s(x,y-1) + i(x,y) $$\n",
      "\n",
      "$$ ii(x,y) = ii (x-1,y) + s(x,y) $$\n",
      "\n",
      "where $$ s(x,y) $$ is the cumulative row sum and and $$ s(x-1) = 0, ii(-1,y) = 0 $$. \n",
      "\n",
      "How can that be useful ? Well, consider a region D for which we would like to estimate the sum of the pixels. We have defined 3 other regions : A, B and C. \n",
      "- The value of the integral image at point 1 is the sum of the pixels in rectangle A\n",
      "\u0001- The value at point 2 is \u0001\u0005\u0004A + B\n",
      "- The value at point 3 is \u0001\u0005\u0004A + C\n",
      "- The value at point 4 is \u0001\u0005\u0004A + B + C + D.\n",
      "\n",
      "Therefore, the sum of pixels in region D can simply be computed as : $$ 4 + 1 - (2+3) $$.\n",
      "\n",
      "And over a single pass, we have computed the value inside a rectangle using only 4 array references.\f",
      "\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/haar_region.jpg)\n",
      "\n",
      "One should simply be aware that rectangles are quite simple features in practice, but sufficient for face detection. Steerable filters tend to be more flexible when it comes to  complex problems. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/steerable.jpg)\n",
      "\n",
      "### c. Learning the classification function with Adaboost\n",
      "\n",
      "Given a set of labeled training images (positive or negative), Adaboost is used to :\n",
      "- select a small set of features\n",
      "- and train the classifier\n",
      "\n",
      "Since most features among the 160'000 are supposed to be quite irrelevant, the weak learning algorithm around which we build a boosting model is designed to select the single rectangle feature which splits best negative and positive examples. \n",
      "\n",
      "### d. Cascading Classifier\n",
      "\n",
      "Although the process described above is quite efficient, a major issue remains. In an image, most of the image is a non-face region. Giving equal importance to each region of the image makes no sense, since we should mainly focus on the regions that are most likely to contain a picture. Viola and Jones achieved an increased detection rate while reducing computation time using Cascading Classifiers.\n",
      "\n",
      "The key idea is to reject sub-windows that do not contain faces while identifying regions that do. Since the task is to identify properly the face, we want to minimize the false negative rate, i.e the sub-windows that contain a face and have not been identified as such.\n",
      "\n",
      "A series of classifiers are applied to every sub-window. These classifiers are simple decision trees :\n",
      "- if the first classifier is positive, we move on to the second\n",
      "- if the second classifier is positive, we move on to the third\n",
      "- ...\n",
      "\n",
      "Any negative result at some point leads to a rejection of the sub-window as potentially containing a face. The initial classifier eliminates most negative examples at a low computational cost, and the following classifiers eliminate additional negative examples but require more computational effort. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/cascade.jpg)\n",
      "\n",
      "The classifiers are trained using Adaboost and adjusting the threshold to minimize the false rate. When training such model, the variables are the following :\n",
      "- the number of classifier stages\n",
      "- the number of features in each stage\n",
      "- the threshold of each stage\n",
      "\n",
      "Luckily in OpenCV, this whole model is already pre-trained for face detection.\n",
      "\n",
      "If you'd like to know more on Boosting techniques, I invite you to check my article on <a href=\"https://maelfabien.github.io/myblog/ml/06-adaboost/\">AdaBoost and Boosting</a>. \n",
      "\n",
      "## 2. Imports\n",
      "\n",
      "The next step simply is to locate the pre-trained weights. We will be using default pre-trained models to detect face, eyes and mouth. Depending on your version of Python, the files should be located somewhere over here :\n",
      "\n",
      "``` \n",
      "/usr/local/lib/python3.7/site-packages/cv2/data \n",
      "```\n",
      "\n",
      "Once identified, we'll declare Cascade classifiers this way :\n",
      "\n",
      "```python\n",
      "cascPath = \"/usr/local/lib/python3.7/site-packages/cv2/data/haarcascade_frontalface_default.xml\"\n",
      "eyePath = \"/usr/local/lib/python3.7/site-packages/cv2/data/haarcascade_eye.xml\"\n",
      "smilePath = \"/usr/local/lib/python3.7/site-packages/cv2/data/haarcascade_smile.xml\"\n",
      "\n",
      "faceCascade = cv2.CascadeClassifier(cascPath)\n",
      "eyeCascade = cv2.CascadeClassifier(eyePath)\n",
      "smileCascade = cv2.CascadeClassifier(smilePath)\n",
      "```\n",
      "\n",
      "## 3. Detect face on an image\n",
      "\n",
      "Before implementing the real time face detection algorithm, let's try a simple version on an image. We can start by loading a test image :\n",
      "\n",
      "```python\n",
      "# Load the image\n",
      "gray = cv2.imread('face_detect_test.jpeg', 0)\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.imshow(gray, cmap='gray')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/test_face.jpg)\n",
      "\n",
      "Then, we detect the face and we add a rectangle around it :\n",
      "\n",
      "```python\n",
      "# Detect faces\n",
      "faces = faceCascade.detectMultiScale(\n",
      "gray,\n",
      "scaleFactor=1.1,\n",
      "minNeighbors=5,\n",
      "flags=cv2.CASCADE_SCALE_IMAGE\n",
      ")\n",
      "\n",
      "# For each face\n",
      "for (x, y, w, h) in faces: \n",
      "    # Draw rectangle around the face\n",
      "    cv2.rectangle(gray, (x, y), (x+w, y+h), (255, 255, 255), 3)\n",
      "```\n",
      "Here is a list of the most common parameters of the `detectMultiScale` function :\n",
      "- scaleFactor : Parameter specifying how much the image size is reduced at each image scale.\n",
      "- minNeighbors : Parameter specifying how many neighbors each candidate rectangle should have to retain it.\n",
      "- minSize : Minimum possible object size. Objects smaller than that are ignored.\n",
      "- maxSize : Maximum possible object size. Objects larger than that are ignored.\n",
      "\n",
      "\n",
      "\n",
      "Finally, display the result :\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.imshow(gray, cmap='gray')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/test_face_output.jpg)\n",
      "\n",
      "Face detection works well on our test image. Let's move on to real time now !\n",
      "\n",
      "## 4. Real time face detection\n",
      "\n",
      "Let's move on to the Python implementation of the live facial detection. The first step is to launch the camera, and capture the video. Then, we'll transform the image to a gray scale image. This is used to reduce the dimension of the input image. Indeed, instead of 3 points per pixel describing Red, Green, Blue, we apply a simple linear transformation :\n",
      "\n",
      "$$ Y_{gray} = 0.2126R +0.7152G +0.0722B $$\n",
      "\n",
      "This is implemented by default in OpenCV.\n",
      "\n",
      "```python\n",
      "video_capture = cv2.VideoCapture(0)\n",
      "\n",
      "while True:\n",
      "    # Capture frame-by-frame\n",
      "    ret, frame = video_capture.read()\n",
      "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
      "```\n",
      "\n",
      "Now, we'll use the ```faceCascade``` variable define above, which contains a pre-trained algorithm, and apply it to the gray scale image.\n",
      "\n",
      "```python\n",
      "    faces = faceCascade.detectMultiScale(\n",
      "    gray,\n",
      "    scaleFactor=1.1,\n",
      "    minNeighbors=5,\n",
      "    minSize=(30, 30),\n",
      "    flags=cv2.CASCADE_SCALE_IMAGE\n",
      "    )\n",
      "```\n",
      "\n",
      "For each face detected, we'll draw a rectangle around the face :\n",
      "```python\n",
      "    for (x, y, w, h) in faces:\n",
      "        if w > 250 :\n",
      "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 3)\n",
      "            roi_gray = gray[y:y+h, x:x+w]\n",
      "            roi_color = frame[y:y+h, x:x+w]\n",
      "```\n",
      "\n",
      "For each mouth detected, draw a rectangle around it :\n",
      "```python\n",
      "    smile = smileCascade.detectMultiScale(\n",
      "        roi_gray,\n",
      "        scaleFactor= 1.16,\n",
      "        minNeighbors=35,\n",
      "        minSize=(25, 25),\n",
      "        flags=cv2.CASCADE_SCALE_IMAGE\n",
      "    )\n",
      "    for (sx, sy, sw, sh) in smile:\n",
      "        cv2.rectangle(roi_color, (sh, sy), (sx+sw, sy+sh), (255, 0, 0), 2)\n",
      "        cv2.putText(frame,'Smile',(x + sx,y + sy), 1, 1, (0, 255, 0), 1)\n",
      "```\n",
      "\n",
      "For each eye detected, draw a rectangle around it :\n",
      "```python\n",
      "    eyes = eyeCascade.detectMultiScale(roi_gray)\n",
      "    for (ex,ey,ew,eh) in eyes:\n",
      "        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
      "        cv2.putText(frame,'Eye',(x + ex,y + ey), 1, 1, (0, 255, 0), 1)\n",
      "```\n",
      "\n",
      "Then, count the total number of faces, and display the overall image :\n",
      "```python\n",
      "    cv2.putText(frame,'Number of Faces : ' + str(len(faces)),(40, 40), font, 1,(255,0,0),2)      \n",
      "    # Display the resulting frame\n",
      "    cv2.imshow('Video', frame)\n",
      "```\n",
      "\n",
      "And implement an exit option when we want to stop the camera by pressing ```q``` :\n",
      "```python\n",
      "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
      "        break\n",
      "```\n",
      "\n",
      "Finally, when everything is done, release the capture and destroy all windows. There are some troubles killing windows on Mac which might require killing Python from the Activity Manager later on.\n",
      "```python\n",
      "video_capture.release()\n",
      "cv2.destroyAllWindows()\n",
      "```\n",
      "\n",
      "## 5. Wrapping it up\n",
      "\n",
      "```python\n",
      "import cv2\n",
      "\n",
      "cascPath = \"/usr/local/lib/python3.7/site-packages/cv2/data/haarcascade_frontalface_default.xml\"\n",
      "eyePath = \"/usr/local/lib/python3.7/site-packages/cv2/data/haarcascade_eye.xml\"\n",
      "smilePath = \"/usr/local/lib/python3.7/site-packages/cv2/data/haarcascade_smile.xml\"\n",
      "\n",
      "faceCascade = cv2.CascadeClassifier(cascPath)\n",
      "eyeCascade = cv2.CascadeClassifier(eyePath)\n",
      "smileCascade = cv2.CascadeClassifier(smilePath)\n",
      "\n",
      "font = cv2.FONT_HERSHEY_SIMPLEX\n",
      "video_capture = cv2.VideoCapture(0)\n",
      "\n",
      "while True:\n",
      "    # Capture frame-by-frame\n",
      "    ret, frame = video_capture.read()\n",
      "\n",
      "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
      "\n",
      "    faces = faceCascade.detectMultiScale(\n",
      "        gray,\n",
      "        scaleFactor=1.1,\n",
      "        minNeighbors=5,\n",
      "        minSize=(200, 200),\n",
      "        flags=cv2.CASCADE_SCALE_IMAGE\n",
      "    )\n",
      "\n",
      "    # Draw a rectangle around the faces\n",
      "    for (x, y, w, h) in faces:\n",
      "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 3)\n",
      "            roi_gray = gray[y:y+h, x:x+w]\n",
      "            roi_color = frame[y:y+h, x:x+w]\n",
      "            cv2.putText(frame,'Face',(x, y), font, 2,(255,0,0),5)\n",
      "\n",
      "    smile = smileCascade.detectMultiScale(\n",
      "        roi_gray,\n",
      "        scaleFactor= 1.16,\n",
      "        minNeighbors=35,\n",
      "        minSize=(25, 25),\n",
      "        flags=cv2.CASCADE_SCALE_IMAGE\n",
      "    )\n",
      "\n",
      "    for (sx, sy, sw, sh) in smile:\n",
      "        cv2.rectangle(roi_color, (sh, sy), (sx+sw, sy+sh), (255, 0, 0), 2)\n",
      "        cv2.putText(frame,'Smile',(x + sx,y + sy), 1, 1, (0, 255, 0), 1)\n",
      "\n",
      "    eyes = eyeCascade.detectMultiScale(roi_gray)\n",
      "    for (ex,ey,ew,eh) in eyes:\n",
      "        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
      "        cv2.putText(frame,'Eye',(x + ex,y + ey), 1, 1, (0, 255, 0), 1)\n",
      "\n",
      "    cv2.putText(frame,'Number of Faces : ' + str(len(faces)),(40, 40), font, 1,(255,0,0),2)      \n",
      "    # Display the resulting frame\n",
      "    cv2.imshow('Video', frame)\n",
      "\n",
      "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
      "        break\n",
      "\n",
      "# When everything is done, release the capture\n",
      "video_capture.release()\n",
      "cv2.destroyAllWindows()\n",
      "```\n",
      "\n",
      "## 6. Results\n",
      "\n",
      "I've made a quick [YouTube](https://www.youtube.com/watch?v=bOflpJ2J7nQ) illustration of the face detection algorithm.\n",
      " \n",
      "# II. Histogram of Oriented Gradients (HOG) in Dlib\n",
      "\n",
      "The second most popular implement for face detection is offered by Dlib and uses a concept called Histogram of Oriented Gradients (HOG). This is an implementation of the original [paper by Dalal and Triggs](https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf).\n",
      "\n",
      "## 1. Theory\n",
      "\n",
      "The idea behind HOG is to extract features into a vector, and feed it into a classification algorithm like a Support Vector Machine for example that will assess whether a face (or any object you train it to recognize actually) is present in a region or not.\n",
      "\n",
      "The features extracted are the distribution (histograms) of directions of gradients (oriented gradients) of the image. Gradients are typically large around edges and corners and allow us to detect those regions.\n",
      "\n",
      "In the original paper, the process was implemented for human body detection, and the detection chain was the following :\n",
      "![image](https://maelfabien.github.io/assets/images/dlib_chain.jpg)\n",
      "\n",
      "### a. Preprocessing\n",
      "\n",
      "First of all, the input images must but of the same size (crop and rescale images). The patches we'll apply require an aspect ratio of 1:2, so the dimensions of the input images might be `64x128` or `100x200` for example.\n",
      "\n",
      "### b. Compute the gradient images\n",
      "\n",
      "The first step is to compute the horizontal and vertical gradients of the image, by applying the following kernels :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gradient-kernels.jpg)\n",
      "\n",
      "The gradient of an image typically removes non-essential information. \n",
      "\n",
      "The gradient of the image we were considering above can be found this way in Python :\n",
      "```python\n",
      "gray = cv2.imread('images/face_detect_test.jpeg', 0)\n",
      "\n",
      "im = np.float32(gray) / 255.0\n",
      "\n",
      "# Calculate gradient \n",
      "gx = cv2.Sobel(im, cv2.CV_32F, 1, 0, ksize=1)\n",
      "gy = cv2.Sobel(im, cv2.CV_32F, 0, 1, ksize=1)\n",
      "mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n",
      "```\n",
      "And plot the picture :\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.imshow(mag)\n",
      "plt.show()\n",
      "```\n",
      "![image](https://maelfabien.github.io/assets/images/grad.jpg)\n",
      "\n",
      "We have not pre-processed the image before though.\n",
      "\n",
      "### c. Compute the HOG\n",
      "\n",
      "The image is then divided into 8x8 cells to offer a compact representation and make our HOG more robust to noise. Then, we compute a HOG for each of those cells. \n",
      "\n",
      "To estimate the direction of a gradient inside a region, we simply build a histogram among the 64 values of the gradient directions (8x8) and their magnitude (another 64 values) inside each region. The categories of the histogram correspond to angles of the gradient, from 0 to 180°. Ther are 9 categories overall : 0°, 20°, 40°... 160°. \n",
      "\n",
      "The code above gave us 2 information :\n",
      "- direction of the gradient\n",
      "- and magnitude of the gradient\n",
      "\n",
      "When we build the HOG, there are 3 subcases :\n",
      "- the angle is smaller than 160° and not halfway between 2 classes. In such case, the angle will be added in the right category of the HOG\n",
      "- the angle is smaller than 160° and exactly between 2 classes. In such case, we consider an equal contribution to the 2 nearest classes and split the magnitude in 2\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hog_1.jpg)\n",
      "\n",
      "- the angle is larger than 160°. In such case, we consider that the pixel contributed proportionally to 160° and to 0°.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hog_2.jpg)\n",
      "\n",
      "The HOG looks like this for each 8x8 cell :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hog.jpg)\n",
      "\n",
      "### d. Block normalization\n",
      "\n",
      "Finally, a 16x16 block can be applied in order to normalize the image and make it invariant to lighting for example. This is simply achieved by dividing each value of the HOG of size 8x8 by the L2-norm of the HOG of the 16x16 block that contains it, which is in fact a simple vector of length `9*4 = 36`. \n",
      "\n",
      "### e. Block normalization\n",
      "\n",
      "Finally, all the 36x1 vectors are concatenated into a large vector. And we are done ! We have our feature vector, on which we can train a soft SVM classifier (C=0.01). \n",
      "\n",
      "## 2. Detect face on an image\n",
      "\n",
      "The implementation is pretty straight forward :\n",
      "\n",
      "```python\n",
      "face_detect = dlib.get_frontal_face_detector()\n",
      "\n",
      "rects = face_detect(gray, 1)\n",
      "\n",
      "for (i, rect) in enumerate(rects):\n",
      "(x, y, w, h) = face_utils.rect_to_bb(rect)\n",
      "    cv2.rectangle(gray, (x, y), (x + w, y + h), (255, 255, 255), 3)\n",
      "    \n",
      "plt.figure(figsize=(12,8))\n",
      "plt.imshow(gray, cmap='gray')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/face_hog.jpg)\n",
      "\n",
      "## 3. Real time face detection\n",
      "\n",
      "As previously, the algorithm is pretty easy to implement. We are also implementing a lighter version by detecting only the face. Dlib makes it really easy to detect facial keypoints too, but it's another topic.\n",
      "\n",
      "```python\n",
      "video_capture = cv2.VideoCapture(0)\n",
      "flag = 0\n",
      "\n",
      "while True:\n",
      "\n",
      "    ret, frame = video_capture.read()\n",
      "\n",
      "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
      "    rects = face_detect(gray, 1)\n",
      "\n",
      "    for (i, rect) in enumerate(rects):\n",
      "\n",
      "        (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
      "\n",
      "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
      "\n",
      "        cv2.imshow('Video', frame)\n",
      "\n",
      "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
      "        break\n",
      "\n",
      "video_capture.release()\n",
      "cv2.destroyAllWindows()\n",
      "```\n",
      "\n",
      "# III. Convolutional Neural Network in Dlib\n",
      "\n",
      "This last method is based on Convolutional Neural Networks (CNN). It also implements a [paper](https://arxiv.org/pdf/1502.00046.pdf) on Max-Margin Object Detection (MMOD) for enhanced results.\n",
      "\n",
      "## 1. A bit of theory\n",
      "\n",
      "Convolutional Neural Network (CNN) are feed-forward neural network that are mostly used for computer vision. They offer an automated image pre-treatment as well as a dense neural network part. CNNs are special types of neural networks for processing datas with grid-like topology. The architecture of the CNN is inspired by the visual cortex of animals.\n",
      "\n",
      "In previous approaches, a great part of the work was to select the filters in order to create the features in order to extract as much information from the image as possible. With the rise of deep learning and greater computation capacities, this work can now be automated. The name of the CNNs comes from the fact that we convolve the initial image input with a set of filters. The parameter to choose remains the number of filters to apply, and the dimension of the filters. The dimension of the filter is called the stride length. Typical values for the stride lie between 2 and 5. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/CNN.jpg)\n",
      "\n",
      "The output of the CNN in this specific case is a binary classification, that takes value 1 if there is a face, 0 otherwise.\n",
      "\n",
      "## 2. Detect face on an image\n",
      "\n",
      "Some elements change in the implementation.\n",
      "\n",
      "The first step is to download the pre-trained model [here](https://github.com/davisking/dlib-models/blob/master/mmod_human_face_detector.dat.bz2). Move the weights to your folder, and define `dnnDaceDetector` :\n",
      "\n",
      "```python\n",
      "dnnFaceDetector = dlib.cnn_face_detection_model_v1(\"mmod_human_face_detector.dat\")\n",
      "```\n",
      "\n",
      "Then, quite similarly to what we have done so far :\n",
      "\n",
      "```python\n",
      "rects = dnnFaceDetector(gray, 1)\n",
      "\n",
      "for (i, rect) in enumerate(rects):\n",
      "\n",
      "    x1 = rect.rect.left()\n",
      "    y1 = rect.rect.top()\n",
      "    x2 = rect.rect.right()\n",
      "    y2 = rect.rect.bottom()\n",
      "\n",
      "    # Rectangle around the face\n",
      "    cv2.rectangle(gray, (x1, y1), (x2, y2), (255, 255, 255), 3)\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.imshow(gray, cmap='gray')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/face_dlib.jpg)\n",
      "\n",
      "## 3. Real time face detection\n",
      "\n",
      "Finally, we'll implement the real time version of the CNN face detection :\n",
      "\n",
      "```python\n",
      "video_capture = cv2.VideoCapture(0)\n",
      "flag = 0\n",
      "\n",
      "while True:\n",
      "    # Capture frame-by-frame\n",
      "    ret, frame = video_capture.read()\n",
      "\n",
      "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
      "    rects = dnnFaceDetector(gray, 1)\n",
      "\n",
      "    for (i, rect) in enumerate(rects):\n",
      "\n",
      "        x1 = rect.rect.left()\n",
      "        y1 = rect.rect.top()\n",
      "        x2 = rect.rect.right()\n",
      "        y2 = rect.rect.bottom()\n",
      "\n",
      "        # Rectangle around the face\n",
      "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
      "\n",
      "    # Display the video output\n",
      "    cv2.imshow('Video', frame)\n",
      "\n",
      "    # Quit video by typing Q\n",
      "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
      "        break\n",
      "\n",
      "video_capture.release()\n",
      "cv2.destroyAllWindows()\n",
      "```\n",
      "\n",
      "# 4. Which one to choose ?\n",
      "\n",
      "Tough question, but we'll just go through 2 metrics that are important :\n",
      "- the computation time\n",
      "- the accuracy\n",
      "\n",
      "In terms of speed, HoG seems to be the fastest algorithm, followed by Haar Cascade classifier and CNNs. \n",
      "\n",
      "However, CNNs in Dlib tend to be the most accurate algorithm. HoG perform pretty well but have some issues identifying small faces. HaarCascade Classifiers perform around as good as HoG overall. \n",
      "\n",
      "I have personally used mainly HoG in my personal projects due to its speed for live face detection. \n",
      "\n",
      "The Github repository of this article can be found [here](https://github.com/maelfabien/Machine_Learning_Tutorials).\n",
      "\n",
      "> **Conclusion** : I hope you enjoyed this quick tutorial on OpenCV for face detection. Don't hesitate to drop a comment if you have any question/remark.\n",
      "\n",
      "Sources :\n",
      "- [HOG](https://www.learnopencv.com/histogram-of-oriented-gradients/)\n",
      "- [DLIB](https://www.pyimagesearch.com/2018/01/22/install-dlib-easy-complete-guide/)\n",
      "- [Viola-Jones Paper](https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf)\n",
      "- [Face Detection 1](https://www.pyimagesearch.com/2018/02/26/face-detection-with-opencv-and-deep-learning/)\n",
      "- [Face Detection 2](https://www.learnopencv.com/face-detection-opencv-dlib-and-deep-learning-c-python/)\n",
      "- [Face Detection 3](https://docs.opencv.org/3.4.3/d7/d8b/tutorial_py_face_detection.html)\n",
      "- [DetectMultiScale](https://docs.opencv.org/2.4/modules/objdetect/doc/cascade_classification.html)\n",
      "- [Viola-Jones](https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework)\n",
      "\n",
      "---\n",
      "title: Linear Classification\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Advanced Machine Learning\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# Perceptron\n",
      "\n",
      "Let $$ (x_1, y_1), ..., (x_n, y_n) \\in R^d \\times \\{ ±1 \\} $$ be labeled training data. The data is said to be linearly separable if there exists a hyperplane that correctly classifies all the examples :\n",
      "\n",
      "$$ \\forall t \\in n, y_t \\langle w^*, x_t \\rangle > 0 $$\n",
      "\n",
      "In general, finding $$ w^* $$ is impossible, but we search for some $$ \\hat{w} $$ that separates the 2 classes. The objective function to optimize is :\n",
      "\n",
      "$$ f(w) = \\sum_t 1(y_t \\langle w^*, x_t \\rangle ≥ 0) $$\n",
      "\n",
      "This is called a **batch objective** since it relies on a cumulative fit to data. By our assumption : $$ f(w^*) = 0 $$. There are however many solution hyperplanes if we consider scaling of $$ w^* $$. To solve this, we fix  $$ w^* $$ to be the smallest-norm vector that guarantees :\n",
      "\n",
      "$$ \\forall t \\in n, y_t \\langle w^*, x_t \\rangle ≥ 1 $$\n",
      "\n",
      "How can we solve this? Using the Perceptron recursive update. It has been shown that the perceptron converges to a solution in a finite number of steps.\n",
      "\n",
      "The algorithm of the Perceptron is the following :\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/map_d3.jpg)\n",
      "\n",
      "\n",
      "> **Conclusion** : That's it ! I hope this introduction to Online Learning was clear. Don't hesitate to drop a comment if you have any question.\n",
      "---\n",
      "title: Prevent Overfitting of Neural Networks\n",
      "layout: post\n",
      "tags: [deeplearning]\n",
      "subtitle : \"Deep Neural Networks\"\n",
      "---\n",
      "\n",
      "In this short article, we are going to cover the concepts of the main regularization techniques in deep learning, and other techniques to prevent overfitting.\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "## Overfitting\n",
      "\n",
      "Overfitting can be graphically observed when your training accuracy keeps increasing while your validation/test accuracy does not increase anymore. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/overfit.jpg)\n",
      "\n",
      "If we only focus on the training accuracy, we might be tempted to select the model that heads the best accuracy in terms of training accuracy. This is, however, a dangerous approach since the validation accuracy should be our control metric.\n",
      "\n",
      "One of the major challenges of deep learning is avoiding overfitting. Therefore, regularization offers a range of techniques to limit overfitting. They include :\n",
      "- Train-Validation-Test Split\n",
      "- Class Imbalance\n",
      "- Drop-out\n",
      "- Data Augmentation\n",
      "- Early stopping\n",
      "- L1 or L2 Regularization\n",
      "- Learning Rate Reduction on Plateau\n",
      "- Save the best model\n",
      "\n",
      "We'll create a small neural network using Keras Functional API to illustrate this concept.\n",
      "\n",
      "```python\n",
      "import tensorflow.keras\n",
      "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
      "from tensorflow.keras.callbacks import TensorBoard\n",
      "from tensorflow.keras.models import Sequential\n",
      "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D,\n",
      "from tensorflow.keras.regularizers import l2\n",
      "```\n",
      "\n",
      "We'll consider a simple Computer Vision algorithm that could typically suit the MNIST dataset.\n",
      "\n",
      "```python\n",
      "input_img = Input(shape=(28, 28, 1))\n",
      "```\n",
      "\n",
      "## Train-Validation-Test Split\n",
      "\n",
      "The first reflex when you face a sufficient amount of data and are about to apply deep learning techniques would be to create 3 sets :\n",
      "- a train set used to train the model. \n",
      "- a validation set used to select the hyperparameters of the model and control for overfitting\n",
      "- a test set used to test the final accuracy of our model\n",
      "\n",
      "For example, here is a typical split you could be using :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/train_split.jpg)\n",
      "\n",
      "In Keras, once you have built the model, simply specify the validation data this way :\n",
      "\n",
      "```python\n",
      "model = model.fit(X_train, y_train, epochs = epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n",
      "pred = model.predict(X_test)\n",
      "print(accuracy_score(pred, y_test)\n",
      "```\n",
      "\n",
      "There is also a cool feature in Keras to make the split automatically and therefore create a validation set :\n",
      "\n",
      "```python\n",
      "model = model.fit(X_train, y_train, epochs = epochs, batch_size=batch_size, validation_split=0.2)\n",
      "pred = model.predict(X_test)\n",
      "print(accuracy_score(pred, y_test)\n",
      "```\n",
      "\n",
      "Once the right model has been chosen and tuned, you should train in on the whole amount of data available. If you don't have a sufficient amount of data, you can make the model choice based on :\n",
      "- Grid Search or HyperOpt\n",
      "- K-Fold Cross-Validation\n",
      "\n",
      "## Class Imbalance\n",
      "\n",
      "Sometimes, you might face a large class imbalance. This is typically the case for fraud detection, emotion recognition...\n",
      "\n",
      "In such case, if the imbalance is large, as below if data collection is not possible, you should maybe think of helping the network a little bit with manually specified class weights.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/class_imb.jpg)\n",
      "\n",
      "Class weights directly affect the loss function, by modifying the amount of data of each class sent in each batch. Therefore, we have an equivalent amount of data from each class sent in each batch. This, however, requires that the amount of data in the minor class remains sufficiently important so that there is no overfitting on 200 examples being reused all the time for example.\n",
      "\n",
      "Here is how you can implement class weight in Keras :\n",
      "\n",
      "```python\n",
      "class_weight = {\n",
      "    0:1/sum(y_train[:,0]), \n",
      "    1:1/sum(y_train[:,1]), \n",
      "    2:1/sum(y_train[:,2]), \n",
      "    3:1/sum(y_train[:,3]),\n",
      "    4:1/sum(y_train[:,4]),\n",
      "    5:1/sum(y_train[:,5]),\n",
      "    6:1/sum(y_train[:,6])\n",
      "}\n",
      "```\n",
      "\n",
      "And when fitting your model :\n",
      "\n",
      "```\n",
      "```python\n",
      "model = model.fit(X_train, y_train, epochs = epochs, batch_size=batch_size, validation_split=0.2, class_weight = class_weight)\n",
      "```\n",
      "\n",
      "## Drop-out\n",
      "\n",
      "The **drop-out** technique allows us for each neuron, during training, to randomly turn-off a connection with a given probability. This prevents co-adaptation between units. In Keras, the dropout is simply implemented this way :\n",
      "\n",
      "```python\n",
      "x = Conv2D(64, (3, 3), padding='same')(input_img)\n",
      "x = BatchNormalization()(x)\n",
      "x = Activation('relu')(x)\n",
      "x = Dropout(0.25)(x)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/dropout.jpg)\n",
      "\n",
      "## Data Augmentation\n",
      "\n",
      "**Data augmentation** is a popular way in image classification to prevent overfitting. The concept is to simply apply slight transformations on the input images (shift, scale...) to artificially increase the number of images. For example, in Keras :\n",
      "\n",
      "```python\n",
      "datagen = ImageDataGenerator(zoomrange=0.2,# randomly zoom into images\n",
      "    rotationrange=10,# randomly rotate images\n",
      "    widthshiftrange=0.1,# randomly shift images horizontally\n",
      "    heightshiftrange=0.1,# randomly shift images vertically\n",
      "    horizontalflip=True,# randomly flip images\n",
      "    verticalflip=False)# randomly flip images\n",
      "```\n",
      "\n",
      "The transformations would typically look like this :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/datagen.jpg)\n",
      "\n",
      "Then, when fitting your model :\n",
      "\n",
      "```python\n",
      "model.fit_generator(\n",
      "    datagen.flow(X_train, y_train, batch_size=batch_size),\n",
      "    steps_per_epoch=int(np.ceil(X_train.shape[0] / float(batch_size))),\n",
      "    epochs = epochs,\n",
      "    class_weight = class_weight, \n",
      "    validation_data=(X_test, y_test))\n",
      "```\n",
      "\n",
      "## Early stopping\n",
      "\n",
      "Early Stopping is a way to stop the learning process when you notice that a given criterion does not change over a series of epochs. For example, if we want the validation accuracy to increase, and the algorithm to stop if it does not increase for 10 periods, here is how we would implement this in Keras :\n",
      "\n",
      "```\n",
      "earlyStopping = EarlyStopping(monitor='val_acc', patience=10, verbose=0, mode='max')\n",
      "```\n",
      "\n",
      "Then, when fitting the model :\n",
      "\n",
      "```python\n",
      "model.fit_generator(\n",
      "    datagen.flow(X_train, y_train, batch_size=batch_size),\n",
      "    steps_per_epoch=int(np.ceil(X_train.shape[0] / float(batch_size))), \n",
      "    epochs = epochs,\n",
      "    callbacks=[earlyStopping],\n",
      "    class_weight = class_weight, \n",
      "    validation_data=(X_test, y_test))\n",
      "```\n",
      "\n",
      "## Regularization\n",
      "\n",
      "Regularization techniques (L2 to force small parameters, L1 to set small parameters to 0), are easy to implement and can help your network. The L2-regularization penalizes large coefficients and therefore avoids overfitting.\n",
      "\n",
      "For example, on the layer of your network, add :\n",
      "\n",
      "```python\n",
      "x = Dense(nb_classes, activation='softmax', activity_regularizer=l2(0.001))(x)\n",
      "```\n",
      "\n",
      "## Learning Rate Reduction on Plateau\n",
      "\n",
      "This technique is quite interesting and can help your network. Even if you have an \"Adam\" or \"RMSProp\" optimizer, your network might get stuck at some point on a plateau. In such a case, it might be useful to reduce the learning rate and try to get those extra percent of accuracy. Indeed, a learning rate a bit too large might simply mean that you overshoot the minimum and are kind of stuck close to a minimal point.\n",
      "\n",
      "In Keras, here's how to do it :\n",
      "\n",
      "```python\n",
      "reduce_lr_acc = ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='max')\n",
      "```\n",
      "\n",
      "And when you fit the model :\n",
      "\n",
      "```python\n",
      "model.fit_generator(\n",
      "    datagen.flow(X_train, y_train, batch_size=batch_size),\n",
      "    steps_per_epoch=int(np.ceil(X_train.shape[0] / float(batch_size))), \n",
      "    epochs = epochs,\n",
      "    callbacks=[earlyStopping, reduce_lr_acc],\n",
      "    class_weight = class_weight, \n",
      "    validation_data=(X_test, y_test))\n",
      "```\n",
      "\n",
      "Here is for example something that happened to me the day I wrote the article :\n",
      "\n",
      "```\n",
      "Epoch 32/50\n",
      "28709/28709 [==============================] - 89s 3ms/sample - loss: 0.1260 - acc: 0.8092 - val_loss: 3.7332 - val_acc: 0.5082\n",
      "Epoch 33/50\n",
      "28709/28709 [==============================] - 88s 3ms/sample - loss: 0.1175 - acc: 0.8134 - val_loss: 3.7183 - val_acc: 0.5269\n",
      "Epoch 34/50\n",
      "28709/28709 [==============================] - 89s 3ms/sample - loss: 0.1080 - acc: 0.8178 - val_loss: 3.8838 - val_acc: 0.5143\n",
      "Epoch 35/50\n",
      "28704/28709 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.8150\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "28709/28709 [==============================] - 103s 4ms/sample - loss: 0.1120 - acc: 0.8149 - val_loss: 3.7373 - val_acc: 0.5311\n",
      "Epoch 36/50\n",
      "28709/28709 [==============================] - 92s 3ms/sample - loss: 0.0498 - acc: 0.8374 - val_loss: 3.7629 - val_acc: 0.5436\n",
      "Epoch 37/50\n",
      "28709/28709 [==============================] - 91s 3ms/sample - loss: 0.0182 - acc: 0.8505 - val_loss: 4.0153 - val_acc: 0.5478\n",
      "Epoch 38/50\n",
      "28709/28709 [==============================] - 91s 3ms/sample - loss: 0.0119 - acc: 0.8539 - val_loss: 4.1941 - val_acc: 0.5483\n",
      "```\n",
      "\n",
      "Once the learning rate reduced, the validation accuracy greatly improved.\n",
      "\n",
      "## Save the best model\n",
      "\n",
      "At a given epoch, you might encounter a model that reaches a really good accuracy compared to other epochs. But how can you save this model precisely? Using checkpoints!\n",
      "\n",
      "```python\n",
      "mcp_save = ModelCheckpoint('path-to-model/model.hdf5', save_best_only=True, monitor='val_acc', mode='max')\n",
      "```\n",
      "\n",
      "Set the option `save_best_only` to `True` and the checkpoint will only save the weights of the model at the next iteration if the validation accuracy is maximized. You can select any criteria, such as \"Min val_loss\" for example.\n",
      "\n",
      "> *Conclusion *: I hope this quick article gave you some idea on how to prevent overfitting on your next neural network in Keras! Don't hesitate to drop a comment.\n",
      "---\n",
      "title: Earthquake Analysis on GCP\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "In this project, our aim will be to create a VM instance to process real earthquake data and make the analysis publicly available.\n",
      "\n",
      "# Creating a VM on Compute Engine\n",
      "\n",
      "First, go to : [https://cloud.google.com](https://cloud.google.com). Open the console, go to the side menu, and click on: Compute Engine > VM Instances.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_6.jpg)\n",
      "\n",
      "The initialization of Compute Engine might take several minutes. Alright, Compute Engine is now up and running. Click on \"Create\" to create new VM instances :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_7.jpg)\n",
      "\n",
      "We will analyze Earth Quake data for what comes next, so we name the VM accordingly. Set your region (whatever your requirements are). We will stick with the standard machine (1 CPU, 3.75GB memory), but of course, all settings can be modified. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_8.jpg)\n",
      "\n",
      "To be able to write to cloud storage from the VM we will need to allow full access to all Cloud APIs. We will access the VM through SSH, and not HTTP or HTTPS.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_9.jpg)\n",
      "\n",
      "Click on \"Create\", wait a few minutes, and the VM should be up and running :)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_10.jpg)\n",
      "\n",
      "If you click on \"SSH\", a terminal page will launch. Notice that there is absolutely no program installed.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_11.jpg)\n",
      "\n",
      "To install `git` for example, run the following command :\n",
      "\n",
      "```bash\n",
      "sudo apt-get install git\n",
      "```\n",
      "\n",
      "This will allow us to access our code repository through the VM. Google prepared a GitHub repository with the necessary data for the Earthquake demo. To clone the repo, simply run :\n",
      "\n",
      "```bash\n",
      "git clone https://www.github.com/GoogleCloudPlatform/training-data-analyst\n",
      "```\n",
      "\n",
      "Once the data has been loaded, simply go in the folder :\n",
      "\n",
      "```bash\n",
      "cd training-data-analyst/\n",
      "cd courses\n",
      "cd bdml_fundamentals/\n",
      "cd demos\n",
      "cd earthquakevm/\n",
      "```\n",
      "\n",
      "Notice that there is a file called  `ingest.sh`. To view its content, run : \n",
      "\n",
      "```bash\n",
      "less ingest.sh\n",
      "```\n",
      "\n",
      "It removes existing files and makes a `wget` on the file to download. To quit the editor, type `:wq`. The earthquake data comes from USGS. There is also a Python file called `transform.py` that parses and transforms the input data. At that point, there are however many missing libraries in our VM. In this demo, Google packaged all necessary libraries in the `install_missing.sh` script. \n",
      "\n",
      "It contains the following lines (you can check it with `cat install_missing.sh`) :\n",
      "\n",
      "```bash\n",
      "sudo apt-get update\n",
      "sudo apt-get -y -qq --fix-missing install python3-mpltoolkits.basemap python3-numpy python3-matplotlib python3-requests\n",
      "```\n",
      "\n",
      "We can run the command to install missing libraries : \n",
      "\n",
      "```bash\n",
      "./install_missing.sh\n",
      "```\n",
      "\n",
      "Now, download the data by running : \n",
      "\n",
      "```bash\n",
      "./ingest.sh \n",
      "```\n",
      "\n",
      "There is now a file `earthquakes.csv` ! You can check its content by running :\n",
      "\n",
      "```bash\n",
      "head earthquakes.csv\n",
      "```\n",
      "\n",
      "This displays the following lines :\n",
      "\n",
      "```\n",
      "time,latitude,longitude,depth,mag,magType,nst,gap,dmin,rms,net,id,updated,place,type,horizontalError,depthError,mag,Error,magNst,status,locationSource,magSource\n",
      "2019-07-30T19:49:53.860Z,35.8401667,-117.6665,4.07,1.37,ml,24,80,0.06113,0.19,ci,ci38673143,2019-07-30T19:53:42.289,Z,\"24km ESE of Little Lake, CA\",earthquake,0.26,0.61,0.134,19,automatic,ci,ci\n",
      "2019-07-30T19:49:03.730Z,35.9245,-117.7173333,5.78,1.97,ml,29,40,0.05914,0.13,ci,ci38673135,2019-07-30T19:52:54.321, Z,\"17km E of Little Lake, CA\",earthquake,0.17,0.51,0.238,25,automatic,ci,ci\n",
      "...\n",
      "```\n",
      "\n",
      "The file `transform.py` transforms the data into a PNG file. Execute the content of the file :\n",
      "\n",
      "\n",
      "```bash\n",
      "./transform.py\n",
      "```\n",
      "\n",
      "It created a file called `earthquakes.png`:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_12.jpg)\n",
      "\n",
      "How can we get this file and read it on our Cloud Storage bucket? Go back to GCP, and click on Storage in the lateral menu. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_13.jpg)\n",
      "\n",
      "We will create a bucket.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_14.jpg)\n",
      "\n",
      "The name of the bucket has to be globally unique. Set the name, choose the storage location (EU in my case), and leave the default value for the object access control: \"Set object-level and bucket-level permissions\". Finally, click on \"Create\", and the bucket is ready! Back to the terminal window, you can check the content of the bucket with the following command:\n",
      "\n",
      "```bash\n",
      "gsutil ls gs://earthquake_mael\n",
      "```\n",
      "\n",
      "(Where earthquake_mael is the name I gave to my bucket)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_15.jpg)\n",
      "\n",
      "To copy the content of the VM instance to the Bucket, use the copy function in gsutils :\n",
      "\n",
      "```bash\n",
      "gsutil cp earthquakes.* gs://earthquake_mael\n",
      "```\n",
      "\n",
      "The bucket now contains the files we copied!\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_16.jpg)\n",
      "\n",
      "You can click on the `earthquakes.png` file, and this is what it looks like :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/earthquakes.jpg)\n",
      "\n",
      "At that point, we don't need the VM instance anymore. To stop paying, simply pause the VM from Compute Engine (or delete it if you don't need it ever again) :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_17.jpg)\n",
      "\n",
      "Suppose that now, we want to make our image public, as a static web app. How can we do that? Go back to the storage part, select all file you want to make available, and click on \"Permissions\" :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_18.jpg)\n",
      "\n",
      "We then need to create a user and give it rights to view the content stored. Call the user `allUsers`.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_19.jpg)\n",
      "\n",
      "Since the objects are now public, you can click on the public link provided (mine is [here](https://storage.googleapis.com/earthquake_mael/earthquakes.htm)) :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_20.jpg)\n",
      "\n",
      "Anyone with the public link can access the following HTML page that represents earthquakes this week :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_21.jpg)\n",
      "\n",
      "---\n",
      "published: false\n",
      "title: Data for good\n",
      "layout: post\n",
      "tags: [thoughts]\n",
      "subtitle : \"Opportunities, sectors and thoughts\"\n",
      "---\n",
      "\n",
      "In the following, I'll present my own view of data for good, sectors, solutions and initiatives that I find encouraging.\n",
      "\n",
      "> \"Data for good\" is a domain of application of data science that gathers initiatives (for profit and non-profit) which aim to bring value to society or environment. \n",
      "\n",
      "Many data scientists feel the need to contribute to data for good projects, whether it is on their free time or as a full time job. In this article, I will try to summarize by sector, different initiatives I have noticed, and opportunities for data scientists to get involved in data for good projects. \n",
      "\n",
      "[Practitioners](https://towardsdatascience.com/why-data-for-good-lacks-precision-87fb48e341f1) usually qualify a project of \"data for goof\" if the following criteria are met :\n",
      "- the end user is a non-profit or a government\n",
      "- skilled people work on the project\n",
      "- the project creates a social benefit\n",
      "- data tools are provided to the end user, and profit is not the major driver\n",
      "\n",
      "I'll include in the upcoming list some solutions that do not exactly correspond to the criteria defined above, but that bring social benefit.\n",
      "\n",
      "# Sectors and solutions\n",
      "\n",
      "## Healthcare\n",
      "### Elder care\n",
      "### Predict public health risks\n",
      "### Cancer detection\n",
      "https://github.com/drivendataorg/concept-to-clinic\n",
      "\n",
      "\n",
      "## Environment\n",
      "### Identify pollution in oceans\n",
      "### Identify animal migration\n",
      "### Identify illegal deforestation \n",
      "\n",
      "\n",
      "## Energy\n",
      "### Energy consumption optimization\n",
      "### Predictive maintenance on renewable energy systems\n",
      "### Drone images of wind turbines\n",
      "\n",
      "\n",
      "## Agriculture\n",
      "### Crop classification\n",
      "### Crop yield prediction\n",
      "### Water saving solutions\n",
      "\n",
      "\n",
      "## Telecommunications\n",
      "### Fraud detection\n",
      "### Secure mobile money transactions\n",
      "### Hotline volume prediction\n",
      "\n",
      "\n",
      "## Smart City\n",
      "### Traffic prediction\n",
      "### Water leakage tracking\n",
      "\n",
      "\n",
      "## Politics\n",
      "### Algorithm transparency\n",
      "### Explore Open-source public data\n",
      "\n",
      "\n",
      "## Journalism\n",
      "### Automate fact checker\n",
      "### Better visualization\n",
      "\n",
      "\n",
      "## Food\n",
      "### Food facts transparency\n",
      "### Limit food waste\n",
      "\n",
      "\n",
      "## Education\n",
      "### Mentorship\n",
      "### Online resources\n",
      "### Matching with schools students are likely to succeed\n",
      "\n",
      "\n",
      "\n",
      "# Getting involved\n",
      "\n",
      "## Local association and meetups\n",
      "\n",
      "## Becoming a mentor\n",
      "\n",
      "## Becoming an instructor\n",
      "\n",
      "## Online projects\n",
      "\n",
      "Zindi : https://zindi.africa/\n",
      "DrivenData : https://www.drivendata.org/\n",
      "\n",
      "# Evolutions to come\n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "title: Adaptative Boosting (AdaBoost)\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Supervised Learning Algorithms\"\n",
      "---\n",
      "Boosting techniques have recently been rising in Kaggle competitions and other predictive analysis tasks. You may have heard of them under the names of XGBoost or LGBM. In this tutorial, we'll go through Adaboost, one of the first boosting techniques discovered. \n",
      "\n",
      "This article can also be found on <a href=\"https://towardsdatascience.com/boosting-and-adaboost-clearly-explained-856e21152d3e\">Towards Data Science</a>.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "## The limits of Bagging\n",
      "\n",
      "For what comes next, consider a binary classification problem. We are either classifying an observation as 0 or as 1. This is not the purpose of the article, but for the sake of clarity, let's recall the concept of bagging.\n",
      "\n",
      "Bagging is a technique that stands for Bootstrap Aggregating. The essence is to select T bootstrap samples, fit a classifier on each of these samples, and train the models in parallel. Typically, in a Random Forest, decision trees are trained in parallel. The results of all classifiers are then averaged into a bagging classifier :\n",
      "\n",
      "$$ H_T(x) = 1/T \\sum_t {h_t(x)} $$\n",
      "\n",
      "This process can be illustrated in the following way. Let's consider 3 classifiers which produce a classification result and can be either right or wrong. If we plot the results of the 3 classifiers, there are regions in which the classifiers will be wrong. These regions are represented in red. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/bagging_true.jpg)\n",
      "\n",
      "This example works perfectly since when one classifier is wrong, the two others are correct. By voting classifier, you achieve great accuracy! But as you might guess, there are also cases in which Bagging does not work properly, when all classifiers are mistaken in the same region.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/bagging_false.jpg)\n",
      "\n",
      "For this reason, the intuition behind the discovery of Boosting was the following :\n",
      "- instead of training parallel models, one needs to train models sequentially\n",
      "- and each model should focus on where the previous classifier performed poorly\n",
      "\n",
      "## Introduction to Boosting\n",
      "\n",
      "### Concept\n",
      "\n",
      "The intuition described above can be described as such :\n",
      "- Train the model $$ h_1 $$ on the whole set \n",
      "- Train the model $$ h_2 $$ with exaggerated data on the regions in which $$ h_1 $$ performs poorly\n",
      "- Train the model $$ h_3 $$ with exaggerated data on the regions in which $$ h_1 $$ ≠ $$ h_2 $$\n",
      "- ...\n",
      "\n",
      "Instead of training the models in **parallel**, we can train them **sequentially**. This is the essence of Boosting!\n",
      "\n",
      "Boosting trains a series of low performing algorithms, called weak learners, by adjusting the error metric over time. Weak learners are algorithms whose error rate is slightly under 50% as illustrated below :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/weak_classifier.jpg){:height=\"60%\" width=\"60%\"}\n",
      "\n",
      "### Weighted errors\n",
      "\n",
      "How could we implement such classifier? By weighting errors throughout the iterations! This would give more weight to regions in which the previous classifiers performed poorly.\n",
      "\n",
      "Let's consider data points on a 2D plot. Some of them will be well classified, others won't. Usually, the weight attributed to each error when computing the error rate is $$ \\frac {1} {n} $$ where n is the number of data points to classify.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/unweighted_errors.jpg)\n",
      "\n",
      "Now if we apply some weight to the errors :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/weighted_errors.jpg)\n",
      "\n",
      "You might now notice that we give more weight to the data points that are not well classified. Here's an illustration of the weighting process :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/example.jpg)\n",
      "\n",
      "In the end, we want to build a strong classifier that may look like this :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/combined.jpg)\n",
      "\n",
      "### Tree stumps\n",
      "\n",
      "One question you might ask, is how many classifiers should one implement to have it working well? And how is each classifier chosen at each step?\n",
      "\n",
      "The answer lies in the definition of so-called tree stumps! Tree stumps defines a 1-level decision tree. The main idea is that at each step, we want to find the best stump, i.e the best data split, that minimizes the overall error. You can see a stump as a test, in which the assumption is that everything that lies on one side belongs to class 1, and everything that lies on the other side belongs to class 0.\n",
      "\n",
      "There are a lot of combinations possible for a tree stump. Let's see how many combinations we face in our simple example? Let's take a minute to count them.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/stumps.jpg){:height=\"80%\" width=\"80%\"}\n",
      "\n",
      "Well, the answer is... 12 ! It might seem surprising, but it's rather easy to understand.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/stumps_12.jpg){:height=\"80%\" width=\"80%\"}\n",
      "\n",
      "There are 12 possible \"test\" we could make. The \"2\" on the side of each separating line simply represents the fact that all points on one side could be points that belong to class 0, or class 1. Therefore, there are 2 tests embedded in it.\n",
      "\n",
      "At each iteration $$ t $$, we will choose $$ h_t $$ the weak classifier that splits best the data, by reducing the overall error rate the most. Recall that the error rate is a modified error rate version that takes into account what has been introduced before.\n",
      "\n",
      "### Finding the best split\n",
      "\n",
      "As stated above, the best split is found by identifying at each iteration $$ t $$, the best weak classifier $$ h_t $$, generally a decision tree with 1 node and 2 leaves (a stump). Suppose that we are trying a predict whether someone who wants to borrow money will be a good payer or not :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/split.jpg)\n",
      "\n",
      "In this case, the best split at time $$ t $$ is to stump on the Payment history. In this case, the best split at time tt is to stump on the Payment history, since the weighted error resulting from this split is minimal.\n",
      "\n",
      "Simply note that decision tree classifiers like these can in practice be deeper than a simple stump. This will be a hyper-parameter. \n",
      "\n",
      "### Combining classifiers\n",
      "\n",
      "The next logical step is to combine the classifiers into a Sign classifier and depending on which side of the frontier a point will stand, it will be classified as 0 or 1. It can be achieved this way :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/classifier.jpg)\n",
      "\n",
      "Do you see any way to potentially improve the classifier?\n",
      "\n",
      "By adding weights $$ \\alpha^t $$ on each classifier !\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/classifier_full.jpg)\n",
      "\n",
      "\n",
      "### Wrapping it up\n",
      "\n",
      "Let's wrap up in a small pseudo-code what we covered so far.\n",
      "\n",
      "*Step 1* : Let $$ w_{t}(i) = \\frac {1} {N} $$ where $$ N $$ denotes the number of training samples, and let $$ T $$ be the chosen number of iterations.\n",
      "\n",
      "*Step 2* : For $$ t $$ in $$ T $$ :\n",
      "\n",
      "  a. Pick $$ h^t $$ the weak classifier that minimizes $$ \\epsilon_{t} $$ \n",
      "  \n",
      "  $$ \\epsilon_{t} = \\sum _{i=1}^{m} w_{t}(i)[y_{i}\\neq h(x_{i})] $$\n",
      "\n",
      "  b. Compute the weight of the classifier chosen :\n",
      "  \n",
      "  $$ \\alpha_t = \\frac {1} {2} ln \\frac {1-\\epsilon_{t}} {\\epsilon_{t}} $$\n",
      " \n",
      " c. Update the weights of the training examples $$ w_{t+1}^{i} $$ and go back to step a).\n",
      "\n",
      "*Step 3* : $$ H(x) = sign(\\alpha^1 h^1(x) + \\alpha^2 h^2(x) + ... + \\alpha^T h^T(x)) $$\n",
      "\n",
      "If you'd like to understand the intuition behind $ w_i^{t+1} $, here's the formula :\n",
      "$$ w_{t+1}(i) = \\frac { w_{t}(i) } { Z } e ^{- \\alpha^t h^t(x) y(x)} $$\n",
      "\n",
      "The key elements to remember from it are :\n",
      "- $$ Z $$ is a constant whose role is to normalize the weights so that they add up to 1!\n",
      "- $$ \\alpha^t $$ is a weight that we apply to each classifier\n",
      "\n",
      "And we're done! This algorithm is called **AdaBoost**. This is the most important algorithm one needs to understand to fully understand all boosting methods.\n",
      "\n",
      "### Computation\n",
      "\n",
      "Boosting algorithms are rather fast to train, which is great. But how come they're fast to train since we consider every stump possible and compute exponentials recursively?\n",
      "\n",
      "Well, here's where the magic happens. If we choose properly $$ \\alpha^t $$ and $$ Z $$, the weights that are supposed to change at each step simplify to :\n",
      "\n",
      "$$ \\sum_{positive} = \\frac {1} {2} $$ and $$ \\sum_{negative} = \\frac {1} {2} $$\n",
      "\n",
      "This is a very strong result, and it does not contradict the statement according to which the weights should vary with the iterations, since the number of training samples that are badly classified drops, and their total weights is still 0.5!\n",
      "- No $$ Z $$ to compute\n",
      "- No $$ \\alpha $$\n",
      "- No exponential\n",
      "\n",
      "And there's another trick: any classifier that tries to split 2 well-classified data points will never be optimal. There's no need to even compute it.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/optim_stump.jpg)\n",
      "\n",
      "AdaBoost has for a long time been considered as one of the few algorithms that do not overfit. But lately, it has been proven to overfit at some point, and one should be aware of it. AdaBoost is vastly used in face detection to assess whether there is a face in the video or not. AdaBoost can also be used as a regression algorithm.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/face_detect.jpg)\n",
      "\n",
      "## Let's code!\n",
      "\n",
      "Now, we'll take a quick look at how to use Adaboost in Python using a simple example on a handwritten digit recognition.\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import learning_curve\n",
      "\n",
      "from sklearn.datasets import load_digits\n",
      "```\n",
      "\n",
      "Let's load the data :\n",
      "```python\n",
      "dataset = load_digits()\n",
      "X = dataset['data']\n",
      "y = dataset['target']\n",
      "```\n",
      "\n",
      "$ X $ contains arrays of length 64 which are simply flattened 8x8 images. This dataset aims to recognize handwritten digits. Let's take a look at a given handwritten digit :\n",
      "\n",
      "```\n",
      "plt.imshow(X[4].reshape(8,8))\n",
      "```\n",
      "![image](https://maelfabien.github.io/assets/images/4.jpg)\n",
      "\n",
      "If we stick to a Decision Tree Classifier of depth 1 (a stump), here's how to implement AdaBoost classifier :\n",
      "\n",
      "```python\n",
      "reg_ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1))\n",
      "scores_ada = cross_val_score(reg_ada, X, y, cv=6)\n",
      "scores_ada.mean()\n",
      "```\n",
      "\n",
      "And it should head a result of around 26%, which can largely be improved. One of the key parameters is the depth of the sequential decision tree classifiers. How does accuracy improve with depth of the decision trees?\n",
      "\n",
      "```python\n",
      "score = []\n",
      "for depth in [1,2,10] : \n",
      "    reg_ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=depth))\n",
      "    scores_ada = cross_val_score(reg_ada, X, y, cv=6)\n",
      "    score.append(scores_ada.mean())\n",
      "```\n",
      "\n",
      "And the maximal score is reached for a depth of 10 in this simple example, with an accuracy of 95.8%.\n",
      "\n",
      "The Github repository of this article can be found [here](https://github.com/maelfabien/Machine_Learning_Tutorials).\n",
      "\n",
      "> **Conclusion** : I hope that this article introduced clearly the concept of AdaBoost and that it does now seem clear to you. Don't hesitate to drop a comment if you have any question.\n",
      "\n",
      "*References* :\n",
      "- <a href=\"https://www.courgisera.org/lecture/ml-classification/learning-boosted-decision-stumps-with-adaboost-bx5YA\">Coursera 1</a>. \n",
      "- <a href=\"https://ru.coursera.org/lecture/ml-classification/learning-boosted-decision-stumps-with-adaboost-bx5YA\">Coursera 2</a>. \n",
      "- <a href=\"https://www.youtube.com/watch?v=UHBmv7qCey4\">MIT Course</a>\n",
      "- <a href=\"https://juegosrev.com/nl/wolf-howling-at-the-moon-wallpapers.html\">Header Image</a>. \n",
      "---\n",
      "title: Getting Started with Dev Tools in Elasticsearch\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Elastic Search, Logstash, Kibana\"\n",
      "---\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/els.jpg)\n",
      "\n",
      "# I. Kibana Dev Tools\n",
      "\n",
      "Head to the Dev Tools section in Kibana.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/el_2.jpg)\n",
      "\n",
      "Elasticsearch stores documents in JSON format.\n",
      "\n",
      "## 1. JSON files\n",
      "\n",
      "JSON files are built the following way, according to a key-value structure :\n",
      "\n",
      "```\n",
      "{\n",
      "    \"name\" : \"Mike\",\n",
      "    \"location\" : {\n",
      "        \"city\" : \"California\",\n",
      "        \"geo\" : [39.76, 92.71],\n",
      "    }\n",
      "    \"age\" : 23\n",
      "}\n",
      "```\n",
      "\n",
      "The types of data supported are :\n",
      "- string\n",
      "- number\n",
      "- object\n",
      "- array\n",
      "- boolean\n",
      "- null\n",
      "\n",
      "## 2. Add/index a document\n",
      "\n",
      "Indexing a document means storing it in Elasticsearch. Elasticsearch operates on several REST endpoints. There is a REST API we are going to work with. Then, we'll interact with those REST endpoints to perform certain actions with ElasticSearch. \n",
      "\n",
      "We'll follow the example of the official documentation in which we study a restaurant food safety violation in the city of San Francisco.\n",
      "\n",
      "```\n",
      "POST /inspections/_doc\n",
      "{\n",
      "    \"business_address\" : \"660 Sacramento St\",\n",
      "    \"business_city\" : \"San Francisco\",\n",
      "    \"business_id\" : \"2228\",\n",
      "    \"business_latitude\" : \"37.793698\",\n",
      "    \"business_location\" : {\n",
      "        \"type\": \"Point\",\n",
      "        \"coordinates\" : [\n",
      "        -122.403984,\n",
      "        37.793698\n",
      "        ]\n",
      "    },\n",
      "    \"business_longitude\" : \"-122.403984\",\n",
      "    \"business_name\" : \"Tokyo Express\",\n",
      "    \"business_postal_code\" : \"94111\",\n",
      "    \"business_state\" : \"CA\",\n",
      "    \"inspection_date\" : \"2016-02-04T00:00:00.000\",\n",
      "    \"inspection_id\" : \"2228_20160204\",\n",
      "    \"inspection_type\" : \"Routine\",\n",
      "    \"inspection_score\" : 96,\n",
      "    \"risk_category\" : \"Low Risk\",\n",
      "    \"violation_description\" : \"Unclean nonfood contact surfaces\",\n",
      "    \"violation_id\" : \"2228_20160204_103142\"\n",
      "}\n",
      "```\n",
      "\n",
      "Then, click on the green button to send the request. You should have something like this :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/el_3.jpg)\n",
      "\n",
      "At a given REST endpoint, there are several actions we can make :\n",
      "- POST: to create a new index. Allows to specify the index, here `inspections`, and a type of documents under the index, here `_doc`. Every time we run this command, we add a new index. Elasticsearch will automatically create and add a new `id`. It forces uniqueness for that document.\n",
      "- PUT: to create a new index. Requires an `id` for the document as part of the URL to avoid duplicates.\n",
      "\n",
      "```\n",
      "PUT /inspections/_doc/12345\n",
      "{\n",
      "    \"business_address\" : \"660 Sacramento St\",\n",
      "    \"business_city\" : \"San Francisco\",\n",
      "    \"business_id\" : \"2228\",\n",
      "    \"business_latitude\" : \"37.793698\",\n",
      "    \"business_location\" : {\n",
      "    \"type\": \"Point\",\n",
      "        \"coordinates\" : [\n",
      "            -122.403984,\n",
      "            37.793698\n",
      "        ]\n",
      "    },\n",
      "    \"business_longitude\" : \"-122.403984\",\n",
      "    \"business_name\" : \"Tokyo Express\",\n",
      "    \"business_postal_code\" : \"94111\",\n",
      "    \"business_state\" : \"CA\",\n",
      "    \"inspection_date\" : \"2016-02-04T00:00:00.000\",\n",
      "    \"inspection_id\" : \"2228_20160204\",\n",
      "    \"inspection_type\" : \"Routine\",\n",
      "    \"inspection_score\" : 96,\n",
      "    \"risk_category\" : \"Low Risk\",\n",
      "    \"violation_description\" : \"Unclean nonfood contact surfaces\",\n",
      "    \"violation_id\" : \"2228_20160204_103142\"\n",
      "}\n",
      "```\n",
      "\n",
      "Here, 12345 would be the unique ID. If you run this command several times, the number of documents does not change, but the version of the document is incremented.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/el_4.jpg)\n",
      "\n",
      "- DELETE: To delete the index we created above, for example, ```DELETE /inspections```\n",
      "\n",
      "We can also create an index beforehand to set certain settings.\n",
      "``` \n",
      "PUT /inspections\n",
      "{\n",
      "    \"settings\" : {\n",
      "    \"index.number_of_shards\":1,\n",
      "    \"index.number_of_replicas\":0\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "We have now created an index with 0 documents in it.\n",
      "\n",
      "When we want to add several documents, we should use the bulk API :\n",
      "\n",
      "```\n",
      "PUT /inspections/_doc/_bulk\n",
      "{\"index\":{\"_id\":1}}\n",
      "{\"business_address\" : \"660 Sacramento St\",\"business_city\" : \"San Francisco\",\"business_id\" : \"2228\",\"business_latitude\" : \"37.793698\",\"business_location\" : {\"type\": \"Point\",\"coordinates\" : [-122.403984,37.793698]},\"business_longitude\" : \"-122.403984\",\"business_name\" : \"Tokyo Express San Francisco\",\"business_postal_code\" : \"94111\",\"business_state\" : \"CA\",\"inspection_date\" : \"2016-02-04T00:00:00.000\",\"inspection_id\" : \"2228_20160204\",\"inspection_type\" : \"Routine\",\"inspection_score\" : 96,\"risk_category\" : \"Low Risk\",\"violation_description\" : \"Unclean nonfood contact surfaces\",\"violation_id\" : \"2228_20160204_103142\"}\n",
      "{\"index\":{\"_id\":2}}\n",
      "{\"business_address\" : \"661 Sacramento St\",\"business_city\" : \"San Francisco\",\"business_id\" : \"2229\",\"business_latitude\" : \"37.793698\",\"business_location\" : {\"type\": \"Point\",\"coordinates\" : [-122.403984,37.793698]},\"business_longitude\" : \"-122.403984\",\"business_name\" : \"Soup Paradise\",\"business_postal_code\" : \"94111\",\"business_state\" : \"CA\",\"inspection_date\" : \"2016-02-04T00:00:00.000\",\"inspection_id\" : \"2228_20160204\",\"inspection_type\" : \"Routine\",\"inspection_score\" : 96,\"risk_category\" : \"High Risk\",\"violation_description\" : \"Unclean food contact surfaces\",\"violation_id\" : \"2228_20160204_103142\"}\n",
      "```\n",
      "If everything worked well, you should have the following return :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/el_5.jpg)\n",
      "\n",
      "## 3. Search/get a document\n",
      "\n",
      "The structure for a search query is quite simple: ``` GET /inspections/_doc/_search ```\n",
      "\n",
      "It returns a list of *all* documents that are of the type `_doc` and belong to the index `inspections`.\n",
      "\n",
      "Let's now find all businesses whose name contains the word \"Soup\" :\n",
      "\n",
      "``` \n",
      "GET /inspections/_doc/_search \n",
      "{\n",
      "    \"query\" : {\n",
      "        \"match\" : {\n",
      "            \"business_name\" : \"Soup\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "And it returns the restaurant \"Soup Paradise\", which is exactly what we wanted :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/el_6.jpg)\n",
      "\n",
      "This is the structure of a basic search query in Elasticsearch. You might have noticed the field  `\"max_score\": 0.6931472`. This is a relevance score computed automatically by Elasticsearch. The most relevant documents are displayed first.\n",
      "\n",
      "If you want to match a whole phrase, use the `match_phrase` function :\n",
      "\n",
      "``` \n",
      "GET /inspections/_doc/_search \n",
      "{\n",
      "    \"query\" : {\n",
      "        \"match\" : {\n",
      "            \"business_name\" : \"san francisco\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "Results are once again sorted by `max_score`.\n",
      "\n",
      "We can also combine portions of queries together :\n",
      "\n",
      "``` \n",
      "GET /inspections/_doc/_search \n",
      "{\n",
      "    \"query\" : {\n",
      "        \"bool\" : {\n",
      "            \"must\" : [\n",
      "                {   \n",
      "                    \"match\" : {\n",
      "                        \"business_name\" : \"san francisco\"\n",
      "                    }\n",
      "                },\n",
      "                {   \n",
      "                    \"match\" : {\n",
      "                        \"business_name\" : \"tokyo\"\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "This will return the restaurant Toky Express San Francisco. `bool` is a boolean query operator. The `must` operator states that both of these sub-queries must be true.\n",
      "\n",
      "We can also make sure that the result of our query does not contain a given word :\n",
      "\n",
      "\n",
      "``` \n",
      "GET /inspections/_doc/_search \n",
      "{\n",
      "    \"query\" : {\n",
      "        \"bool\" : {\n",
      "            \"must_not\" : [\n",
      "                {   \n",
      "                    \"match\" : {\n",
      "                        \"business_name\" : \"san francisco\"\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "And this query returns Soup Paradise since we do not want the restaurant's name to contain the phrase \"San Francisco\".\n",
      "\n",
      "If you want to attach some more importance to a given part of the query, the following request adds a `boost` to the work Tokyo :\n",
      "\n",
      "``` \n",
      "GET /inspections/_doc/_search \n",
      "{\n",
      "    \"query\" : {\n",
      "        \"bool\" : {\n",
      "            \"should\" : [\n",
      "                {   \n",
      "                    \"match_phrase\" : {\n",
      "                        \"business_name\" : {\n",
      "                            \"query\" : \"tokyo express\",\n",
      "                            \"boost\" : 3\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                {   \n",
      "                    \"match_phrase\" : {\n",
      "                        \"business_name\" : \"san francisco\"\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "To highlight the field of the result we were looking for :\n",
      "\n",
      "``` \n",
      "GET /inspections/_doc/_search \n",
      "{\n",
      "    \"query\" : {\n",
      "        \"match\" : {\n",
      "            \"business_name\" : \"san francisco\"\n",
      "        }\n",
      "    },\n",
      "    \"highlight\" : {\n",
      "        \"fields\" : {\n",
      "        \"business_name\" : {}\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "We can specify a numerical operator to make a search on the `inspection_score` for example using the `range` operator :\n",
      "\n",
      "``` \n",
      "GET /inspections/_doc/_search \n",
      "{\n",
      "    \"query\" : {\n",
      "        \"range\" : {\n",
      "            \"inspection_score\" : {\n",
      "                \"gte\" : 80\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"sort\" : [\n",
      "        {\"inspection_score\" : \"desc\"}\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "This returns all entries whose inspection score is greater than 80. To find a full list of the possible operators, check <span style=\"color:blue\">[this link](https://www.elastic.co/guide/en/elasticsearch/guide/current/structured-search.html)</span>.\n",
      "\n",
      "## 4. SQL Queries\n",
      "\n",
      "In the Dev Tools console, there is a SQL interpreter which makes it easy to write simple queries. There are several ways actually to write SQL queries in Elasticsearch :\n",
      "- through SQL endpoint\n",
      "- through the command-line interface (CLI) tool in the `bin` directory of Elasticsearch\n",
      "- through JDBC Elasticsearch client\n",
      "\n",
      "Here is an example of the SQL endpoint :\n",
      "\n",
      "```\n",
      "POST /_xpack/sql?format=txt\n",
      "{\n",
      "    \"query\" : \"SELECT business_name, inspection_score FROM inspections ORDER BY inspection_score\"\n",
      "}\n",
      "```\n",
      "\n",
      "It should return the following table :\n",
      "```\n",
      "business_name              |inspection_score\n",
      "---------------------------+----------------\n",
      "Tokyo Express San Francisco|96              \n",
      "Soup Paradise              |96              \n",
      "````\n",
      "\n",
      "## 5. Aggregations\n",
      "\n",
      "Aggregations might be useful for example when a user wants, for example, to find all hotels within a price group, or all restaurant that belongs to a certain group of inspections scores :\n",
      "\n",
      "\n",
      "``` \n",
      "GET /inspections/_doc/_search \n",
      "{\n",
      "    \"query\" : {\n",
      "        \"match\" : {\n",
      "            \"business_name\" : \"Soup\"\n",
      "        }\n",
      "    },\n",
      "    \"aggregations\" : {\n",
      "        \"inspection_score\" : {\n",
      "            \"range\" : {\n",
      "                \"field\" : \"inspection_score\",\n",
      "                \"ranges\" : [\n",
      "                    {\n",
      "                        \"key\" : \"0-80\",\n",
      "                        \"from\" : 0,\n",
      "                        \"to\" : 80\n",
      "                    },\n",
      "                    {\n",
      "                        \"key\" : \"81-90\",\n",
      "                        \"from\" : 81,\n",
      "                        \"to\" : 90\n",
      "                    },\n",
      "                    {\n",
      "                        \"key\" : \"91-100\",\n",
      "                        \"from\" : 91,\n",
      "                        \"to\" : 100\n",
      "                    }\n",
      "                ]\n",
      "            }\n",
      "        }        \n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "You'll notice at the end of the result an aggregation result which is exactly what we were looking for :\n",
      "```\n",
      "\"aggregations\" : {\n",
      "    \"inspection_score\" : {\n",
      "        \"buckets\" : [\n",
      "            {\n",
      "                \"key\" : \"0-80\",\n",
      "                \"from\" : 0.0,\n",
      "                \"to\" : 80.0,\n",
      "                \"doc_count\" : 0\n",
      "            },\n",
      "            {\n",
      "                \"key\" : \"81-90\",\n",
      "                \"from\" : 81.0,\n",
      "                \"to\" : 90.0,\n",
      "                \"doc_count\" : 0\n",
      "            },\n",
      "            {\n",
      "                \"key\" : \"91-100\",\n",
      "                \"from\" : 91.0,\n",
      "                \"to\" : 100.0,\n",
      "                \"doc_count\" : 1\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "## 6. Geo-Search\n",
      "\n",
      "Now, let's imagine that a user is looking for the nearest restaurant. This kind of queries can typically be achieved by geo-search! \n",
      "\n",
      "First of all, we must change the document types to `_mapping` to specify that the coordinates are Geo-JSON points. Mapping makes it more efficient to define the structure of the document, and more efficiently store/search the data within our index. We need to delete out the index and perform our bulk import again :\n",
      "\n",
      "```\n",
      "DELETE /inspections\n",
      "```\n",
      "\n",
      "Then, create it again :\n",
      "\n",
      "```\n",
      "PUT /inspections\n",
      "```\n",
      "\n",
      "```\n",
      "PUT /inspections/_mapping/_doc\n",
      "{\n",
      "    \"properties\" : {\n",
      "        \"business_address\" : {\n",
      "            \"type\" : \"text\",\n",
      "            \"fields\" : {\n",
      "                \"keyword\" : {\n",
      "                    \"type\" : \"keyword\",\n",
      "                    \"ignore_above\" : 256\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"business_city\" : {\n",
      "            \"type\" : \"text\",\n",
      "            \"fields\" : {\n",
      "                \"keyword\" : {\n",
      "                    \"type\" : \"keyword\",\n",
      "                    \"ignore_above\" : 256\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"business_id\" : {\n",
      "            \"type\" : \"text\",\n",
      "            \"fields\" : {\n",
      "                \"keyword\" : {\n",
      "                    \"type\" : \"keyword\",\n",
      "                    \"ignore_above\" : 256\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"business_latitude\" : {\n",
      "            \"type\" : \"text\",\n",
      "            \"fields\" : {\n",
      "                \"keyword\" : {\n",
      "                    \"type\" : \"keyword\",\n",
      "                    \"ignore_above\" : 256\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"business_longitude\" : {\n",
      "            \"type\" : \"text\",\n",
      "            \"fields\" : {\n",
      "                \"keyword\" : {\n",
      "                    \"type\" : \"keyword\",\n",
      "                    \"ignore_above\" : 256\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"business_name\" : {\n",
      "            \"type\" : \"text\",\n",
      "            \"fields\" : {\n",
      "                \"keyword\" : {\n",
      "                    \"type\" : \"keyword\",\n",
      "                    \"ignore_above\" : 256\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"coordinates\" : {\n",
      "            \"type\" : \"geo_point\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "Now, we can execute again the bulk requests to add the data as defined previously :\n",
      "\n",
      "```\n",
      "PUT /inspections/_doc/_bulk\n",
      "{\"index\":{\"_id\":1}}\n",
      "{\"business_address\" : \"660 Sacramento St\",\"business_city\" : \"San Francisco\",\"business_id\" : \"2228\",\"business_latitude\" : \"37.793698\",\"business_location\" : {\"type\": \"Point\",\"coordinates\" : [-122.403984,37.793698]},\"business_longitude\" : \"-122.403984\",\"business_name\" : \"Tokyo Express San Francisco\",\"business_postal_code\" : \"94111\",\"business_state\" : \"CA\",\"inspection_date\" : \"2016-02-04T00:00:00.000\",\"inspection_id\" : \"2228_20160204\",\"inspection_type\" : \"Routine\",\"inspection_score\" : 96,\"risk_category\" : \"Low Risk\",\"violation_description\" : \"Unclean nonfood contact surfaces\",\"violation_id\" : \"2228_20160204_103142\"}\n",
      "{\"index\":{\"_id\":2}}\n",
      "{\"business_address\" : \"661 Sacramento St\",\"business_city\" : \"San Francisco\",\"business_id\" : \"2229\",\"business_latitude\" : \"38.52\",\"business_location\" : {\"type\": \"Point\",\"coordinates\" : [-122.403984,37.793698]},\"business_longitude\" : \"-121.42\",\"business_name\" : \"Soup Paradise\",\"business_postal_code\" : \"94111\",\"business_state\" : \"CA\",\"inspection_date\" : \"2016-02-04T00:00:00.000\",\"inspection_id\" : \"2228_20160204\",\"inspection_type\" : \"Routine\",\"inspection_score\" : 96,\"risk_category\" : \"High Risk\",\"violation_description\" : \"Unclean food contact surfaces\",\"violation_id\" : \"2228_20160204_103142\"}\n",
      "```\n",
      "\n",
      "We can specify the order, the unit (km) and the distance type (here, by plane) from a given input point :\n",
      "\n",
      "``` \n",
      "GET /inspections/_mapping/_search \n",
      "{\n",
      "    \"sort\" : [\n",
      "        {\n",
      "            \"_geo_distance\" : {\n",
      "                \"coordinates\" : {\n",
      "                    \"lat\" : 37.800175,\n",
      "                    \"lon\" : -122.409081\n",
      "                },\n",
      "                \"order\" : \"asc\",\n",
      "                \"unit\" : \"km\",\n",
      "                \"distance_type\" : \"plane\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "## 7. Update documents\n",
      "\n",
      "Suppose we now add a flag to documents and count the number of views per inspection report. We might need to do a partial update if any of those information changes.\n",
      "\n",
      "```\n",
      "GET /inspections/_doc/2/_update\n",
      "{\n",
      "    \"doc\" : {\n",
      "        \"flagged\" : true,\n",
      "        \"views\" : 0\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "The document 2 has now been partially updated!\n",
      "\n",
      "## 8. Delete documents\n",
      "\n",
      "We can delete a document easily :\n",
      "\n",
      "```\n",
      "DELETE /inspections/_doc/2\n",
      "```\n",
      "\n",
      "## 9. Analyzers and tokenizers\n",
      "\n",
      "Dev Tools in Elasticsearch integrates pre-built tokenizers for text processing :\n",
      "\n",
      "```\n",
      "GET /inspections/_analyze\n",
      "{\n",
      "    \"tokenizer\" : \"standard\",\n",
      "    \"text\" : \"my email address is test123@company.com\"\n",
      "}\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/el_7.jpg)\n",
      "\n",
      "Standard tokenizer will remove the `@` within the email address and cut the text at each whitespace. We can force to take only white spaces into account :\n",
      "\n",
      "```\n",
      "GET /inspections/_analyze\n",
      "{\n",
      "    \"tokenizer\" : \"whitespace\",\n",
      "    \"text\" : \"my email address is test123@company.com\"\n",
      "}\n",
      "```\n",
      "\n",
      "We can also use filters on top on those tokens :\n",
      "\n",
      "```\n",
      "GET /inspections/_analyze\n",
      "{\n",
      "    \"tokenizer\" : \"whitespace\",\n",
      "    \"filter\" : [\"lowercase\", \"unique\"],\n",
      "    \"text\" : \"Brown brown brown fox Fox fox Wild wild wild\"\n",
      "}\n",
      "```\n",
      "\n",
      "Which puts to lowercase all words and returns only the first unique word using whitespace tokenizer :\n",
      "\n",
      "```\n",
      "{\n",
      "\"tokens\" : [\n",
      "    {\n",
      "        \"token\" : \"brown\",\n",
      "        \"start_offset\" : 0,\n",
      "        \"end_offset\" : 5,\n",
      "        \"type\" : \"word\",\n",
      "        \"position\" : 0\n",
      "    },\n",
      "    {\n",
      "        \"token\" : \"fox\",\n",
      "        \"start_offset\" : 18,\n",
      "        \"end_offset\" : 21,\n",
      "        \"type\" : \"word\",\n",
      "        \"position\" : 1\n",
      "    },\n",
      "    {\n",
      "        \"token\" : \"wild\",\n",
      "        \"start_offset\" : 30,\n",
      "        \"end_offset\" : 34,\n",
      "        \"type\" : \"word\",\n",
      "        \"position\" : 2\n",
      "    }\n",
      "]\n",
      "}\n",
      "```\n",
      "\n",
      "> *Conclusion *: This was a basic introduction to the Dev Tools console in Elasticsearch! Don't hesitate to drop a comment if you have any question.\n",
      "---\n",
      "title: A day at the Neo4J GraphTour\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Neo4J\"\n",
      "---\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/neo_gt.jpg)\n",
      "\n",
      "On March 19th, 2019, Neo4J organized an event in Paris. This event was part of the Neo4J GraphTour. Other cities included Milan, TelAvis, Madrid or Berlin for example. Over 3500 people attended the 2019 GraphTour. I'll present to you the key points covered during the day and some perspectives discussed throughout the day.\n",
      "\n",
      "# I. Impact and future of graphs\n",
      "\n",
      "## Famous graph use cases\n",
      "\n",
      "You have probably already heard of the Panama papers. This event implied 2.6TB of data, 11.5 million documents, including Emails, Scanned Documents, and Bank Statements... Pretty hard to image a relational database management system (RDBMS) to store and efficiently query all this information. These data have indeed been structured on a Neo4J graph. This had a major impact on the world of finance and offshore companies. This analysis would have not been possible without graph-oriented databases that allow a great data exploration. The graph typically connected an address, to several people and accounts.\n",
      "\n",
      "Make sure to check this [link](https://neo4j.com/blog/analyzing-panama-papers-neo4j/) for the full paper written on Neo4J's blog.\n",
      "\n",
      "Graph oriented databases also allow analyzing fake news propagation on social networks during important political events. Neo4J could help to avoid such cases to happen again in the future :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/trolls.jpg)\n",
      "\n",
      "## State of the graphs\n",
      "\n",
      "In 2014, Forrester predicted that 25% of enterprises would be using graph databases in 2017. As of today (2019), more than 50% of enterprises are using graph databases. Close to 70% of enterprises have or are planning to use graph databases. 76% of Fortune 100 use graph databases.\n",
      "\n",
      "DB engines (insert image) shows that the interest in graph databases has been growing way above any expectation.\n",
      "\n",
      "- 7 of the Top 10 Software companies use Neo4J, including Microsoft, eBay...\n",
      "- 8 of the Top 10 Insurance companies use Neo4J for fraud detection for example...\n",
      "- Over 800 startups use Neo4J \n",
      "\n",
      "Neo4J has over 10 million downloads. Over 50'000 people have been trained and certified Neo4J professionals. Now, you should pretty much be convinced that Neo4J is something pretty big. \n",
      "\n",
      "## When should you use graphs?\n",
      "\n",
      "Graphs should be used when one of these three criteria is met :\n",
      "\n",
      "*a. Complex environment*\n",
      "\n",
      "Volva uses Neo4J for car manufacture. Is a car a complex environment ?\n",
      "- 30'000 pieces\n",
      "- 200 computation units\n",
      "- 100 communication buses\n",
      "- 2000 software components\n",
      "- 10000 signals\n",
      "- ...\n",
      "\n",
      "There is a large variety of data sources, coming from different software... \n",
      "\n",
      "We need to overcome the \"accidental\" complexity, that exists due to incoherence between the need and the tools.\n",
      "We need to focus on the essential complexity, i.e find a solution that fits the needs of the company. \n",
      "\n",
      "A Neo4J project should be implemented step by step. There is an option to load data as native graph data from a SQL database. Then, over time, add new data of new sources and types. There is no need to define the schema at first, so the upload of the data is simple.\n",
      "\n",
      "*b. Need for flexibility*\n",
      "\n",
      "NASA uses a knowledge graph database in their documentation. This provides results way more interesting than classical keyword detection for example.\n",
      "\n",
      "*c. Need for performance*\n",
      "\n",
      "In a native graph architecture, data are computed to be stored only once. Then, any query is a simple read of the data. Marriott offers over 300M dynamic pricing operations per day. Queries during Cyber Mondays are executed in less than 4ms. \n",
      "\n",
      "Let's be concrete. What are the most common use cases?\n",
      "- Social Media and social networks\n",
      "- Telecommunication network\n",
      "- Real-time recommendation (Retail)\n",
      "- Fraud detection (Insurance)\n",
      "- Network management & IT \n",
      "- Master data management \n",
      "- Knowledge graphs (Large documentation e.g NASA)\n",
      "- Access control (Bank)\n",
      "- Dynamic pricing\n",
      "- IoT applications\n",
      "- Supply chain efficiency\n",
      "- ...\n",
      "\n",
      "## A quick history of the development of graphs\n",
      "\n",
      "The **first** wave of development of graph technologies was *graph applications*. Altavista and Lycos disappeared, and Google has since then been rising, using graph search. Monster.com was also replaced LinkedIn, which is using graph technologies too. \n",
      "\n",
      "The **second** wave, in which we currently stand, is *graph platform*. Graph platform is a result of the fact that more and more applications are connected, and more data are being collected. Within a company, several graphs built. Nowadays, companies tend to build graph networks to connect their different connected data. Neo4J launched Neo4j Bloom that gathers :\n",
      "- perspective\n",
      "- visualization\n",
      "- exploration\n",
      "- inspection\n",
      "- editing\n",
      "- search\n",
      "\n",
      "The latest trends include graph algorithms for community detection for example or Cypher for Apache Spark. Make sure to check these great [resources](https://neo4j.com/resources) uploaded by Neo4J.\n",
      "\n",
      "The **third** wave is the *Graph-Enabled and Graph-Enhanced AI*. We are about to face a revolution in the graph industry. The different upcoming technologies include :\n",
      "- evidence-based\n",
      "- ML systems\n",
      "-  Natural Language Generation\n",
      "- ...\n",
      "\n",
      "Graph provides connections and context for AI. A simple graph (Person, drive, born in, loves, married ...) can teach us a lot about a single individual. Relationships are often the strongest predictors of Behavior. \n",
      "\n",
      "The 4 pillars of Graph-Enhanced AI are : \n",
      "- Knowledge graphs (chatbots, NLP...)\n",
      "- Connected features add context to ML for improved accuracy\n",
      "- Filtered Connected Features target some parts of the knowledge graph to improve accuracy and speed\n",
      "- Explainability and Credibility of algorithms are enhanced when we use graphs\n",
      "\n",
      "## Who is Neo4J made for?\n",
      "\n",
      "The main users of connected data are :\n",
      "- data scientists: sentiment analysis, customer segmentation, machine learning, cognitive computing, community detection...\n",
      "- applications: fraud detection, recommendations, master data management...\n",
      "- business users: discovery and visualization for fraud detection, product information management, risk, and portfolio analysis...\n",
      "\n",
      "In summary, pretty much anyone who can gain insights from the company's data.\n",
      "\n",
      "# II. Main features of Neo4J\n",
      "\n",
      "## Properties\n",
      "\n",
      "40 years ago, in 1979, the best storage was still paper forms. The first Relational Database Management System was born at that time. Nowadays, thanks to abundant RAM, Flash & IO Co-processors, we have a high capacity and high-speed storage opportunity. \n",
      "\n",
      "From an IT perspective :\n",
      "- traditional databases are used to store and retrieve data in real-time\n",
      "- big data technology is used to aggregate and filter data (Hadoop, Spark..)\n",
      "- graph technologies are used to explore connections in data\n",
      "\n",
      "Neo4J offers index-free adjacency. At write time, data is connected as it is stored. At read time, queries for data retrieval are up to 1'000x faster while using 10x fewer servers. Above 3 nodes and 2 to 3 connected for each, the queries are already faster. \n",
      "\n",
      "ACID properties are required for graph transactions. This is called a graphing unit of work. ACID consistency guarantees that the whole transaction is a success, otherwise it fails. Data replicated across the cluster should also always be similar on each node, to guarantee the consistency. Neo4J has ACID properties of RDBMS with the speed of NoSQL technologies. \n",
      "\n",
      "Cypher Query Language, the language to make queries in Neo4J, is also easy to read, and way more compact that SQL from example when we explore relations. Cypher is an open-source project that can be used for all graph databases. \n",
      "\n",
      "The foundational components of Neo4J are :\n",
      "- Index-Free adjacency\n",
      "- ACID Foundation\n",
      "- Full-Stack clustering\n",
      "- Language, Drivers, Tooling\n",
      "- Graph Engine\n",
      "- Hardware Optimization\n",
      "\n",
      "Neo4J graph platform includes : \n",
      "- drivers and APIs\n",
      "- discovery and visualization\n",
      "- development and administration\n",
      "- neo4j database, transaction, and analytics\n",
      "- analytics tooling\n",
      "- data integration\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_plat.jpg)\n",
      "\n",
      "## Visualization\n",
      "\n",
      "How can we visualize graphs in Neo4J? \n",
      "- In Bloom, provided by Neo4J, near-natural language search, for individual and small team use\n",
      "- In Graph Viz Solutions using Linkurious or Tom Sawyer for a small team and departmental or cross-departmental use\n",
      "- BI Tools such as Tableau and Qlik, but that are not optimized for graph data and may require special connectors\n",
      "- In Viz Toolkits (d3.js for example) for a custom solution that requires more development\n",
      "\n",
      "## Algorithms\n",
      "\n",
      "Three categories of algorithms have been implemented :\n",
      "- pathfinding and search: find the optimal path, evaluated route availability, and quality\n",
      "- centrality: determines the importance of distinct nodes in the network (e.g PageRank of Google)\n",
      "- community detection: evaluates how a group is clustered or partitioned\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_algo.jpg)\n",
      "\n",
      "## Latest features\n",
      "\n",
      "Neo4J 3.5 added :\n",
      "- full-text search\n",
      "- Go-driver\n",
      "- New graph algorithms\n",
      "- Enhanced performance and security\n",
      "\n",
      "\n",
      "> **Conclusion **: Many other topics and use cases were covered throughout the day. I can only but recommend you to attend the GraphTour! I hope this article gave you a quick idea of what you can expect from the day. Don't hesitate to drop a comment if you have a question.\n",
      "---\n",
      "title: Voice Gender Identification\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "Can we detect the gender of a voice using ML methods? I recently came across [this](https://appliedmachinelearning.blog/2017/06/14/voice-gender-detection-using-gmms-a-python-primer/) article which I found quite interesting in the way it addresses Gender Identification from vocal recordings. \n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "Voice gender identification relies on three important steps:\n",
      "- Extracting from the training set MFCC features (13 per time window usually)\n",
      "- Train two Gaussian Mixture Models (GMMs) on the feature matrices created (N_records x 13), one for each genre\n",
      "- In prediction, compute the likelihood of each gender using the trained GMMs, and pick the most likely gender\n",
      "\n",
      "The Github repository for this article can be found [here](https://github.com/maelfabien/VoiceGenderDetection/blob/master/README.md).\n",
      "\n",
      "The aim of this project is to build a web application using Streamlit in which a user is able to test the trained algorithm on his or her own voice.\n",
      "\n",
      "# Let's build it\n",
      "\n",
      "## Data and imports\n",
      "\n",
      "⚠️ The dataset has been extracted from [AudioSet](https://research.google.com/audioset/dataset/index.html) and can be downloaded from [here directly](https://drive.google.com/file/d/1g64EswaS5PtwIg-Y0ZmWwvSK1DgYvUuc/view?usp=sharing).\n",
      "\n",
      "\n",
      "Start by importing the libraries that we will need to build this application:\n",
      "\n",
      "```python\n",
      "# Data manipulation\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Feature extraction\n",
      "import scipy\n",
      "import librosa\n",
      "import python_speech_features as mfcc\n",
      "import os\n",
      "from scipy.io.wavfile import read\n",
      "\n",
      "# Model training\n",
      "from sklearn.mixture import GaussianMixture as GMM\n",
      "from sklearn import preprocessing\n",
      "import pickle\n",
      "\n",
      "# Live recording\n",
      "import sounddevice as sd\n",
      "import soundfile as sf\n",
      "```\n",
      "\n",
      "If you have not yet understood or seen the concept of Mel Frequency Cepstral Coefficients (MFCC), I recommend that you take a look at [the article I wrote on the topic of Sound Feature Extraction](https://maelfabien.github.io/machinelearning/Speech9).\n",
      "\n",
      "## Feature Extraction\n",
      "\n",
      "The concept behind this approach to gender detection is really simple. We first create a feature matrix from the training audio recordings. MFCCs are extracted on really small time windows (±20ms), and when you run an MFCC feature extraction using `python_speech_features` or Librosa, it automatically creates a matrix for the whole recording.\n",
      "\n",
      "Knowing that, extracting the MFCC of a audio file is really easy:\n",
      "\n",
      "```python\n",
      "def get_MFCC(sr,audio):\n",
      "    \n",
      "    features = mfcc.mfcc(audio, sr, 0.025, 0.01, 13, appendEnergy = False)\n",
      "    features = preprocessing.scale(features)\n",
      "    \n",
      "    return features\n",
      "```\n",
      "\n",
      "I placed the training data in a folder called AudioSet, in which I have two sub-folders: male_clips and female_clips. We can extract the features of the training set simply by running the function above on all files in the training folder. The problem is however that for the moment, both the train and the test set are in the folder. We must, therefore, split these files in two, and run `get_MFCC` iteratively.\n",
      "\n",
      "```python\n",
      "def get_features(source):\n",
      "    \n",
      "    # Split files\n",
      "    files = [os.path.join(source,f) for f in os.listdir(source) if f.endswith('.wav')]\n",
      "    len_train = int(len(files)*0.8)\n",
      "    train_files = files[:len_train]\n",
      "    test_files = files[len_train:]\n",
      "    \n",
      "    # Train features\n",
      "    features_train = []\n",
      "    for f in train_files:\n",
      "        sr, audio = read(f)\n",
      "        vector = get_MFCC(sr,audio)\n",
      "        if len(features_train) == 0:\n",
      "            features_train = vector\n",
      "        else:\n",
      "            features_train = np.vstack((features_train, vector))\n",
      "            \n",
      "    # Test features  \n",
      "    features_test = []\n",
      "    for f in test_files:\n",
      "        sr, audio = read(f)\n",
      "        vector = get_MFCC(sr,audio)\n",
      "        if len(features_test) == 0:\n",
      "            features_test = vector\n",
      "        else:\n",
      "            features_test = np.vstack((features_test, vector))\n",
      "            \n",
      "    return features_train, features_test\n",
      "```\n",
      "\n",
      "## GMM model training\n",
      "\n",
      "> \"A Gaussian Mixture Model (GMM) is a parametric probability density function represented as a weighted sum of Gaussian component densities. ([source](https://github.com/SuperKogito/Voice-based-gender-recognition))\n",
      "\n",
      "GMMs are commonly used as a parametric model of the probability distribution of continuous measurements or features in a biometric system, such as vocal-tract related spectral features in a speaker recognition system. GMM parameters are estimated from training data using the iterative Expectation-Maximization (EM) algorithm or Maximum A Posteriori(MAP) estimation from a well-trained prior model.\"\n",
      "\n",
      "To apply it to the folder containing the Male recordings, simply use this function, extract the train features and train the Gaussian Mixture Model.\n",
      "\n",
      "```python\n",
      "source = \"AudioSet/male_clips\"\n",
      "features_train_male, features_test_male = get_features(source)\n",
      "gmm_male = GMM(n_components = 8, max_iter = 200, covariance_type = 'diag', n_init = 3)\n",
      "gmm_male.fit(features_train_male)\n",
      "```\n",
      "\n",
      "We can repeat the process for Females:\n",
      "\n",
      "```python\n",
      "source = \"AudioSet/female_clips\"\n",
      "features_train_female, features_test_female =  get_features(source)\n",
      "gmm_female = GMM(n_components = 8, max_iter=200, covariance_type='diag', n_init = 3)\n",
      "gmm_female.fit(features_train_female)\n",
      "```\n",
      "\n",
      "Are these features really differentiable for males and females?\n",
      "\n",
      "We can plot the distribution over the MFCC features for random samples of males and females:\n",
      "\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(15,10))\n",
      "for i in range(1, 430000, 1000):\n",
      "    plt.plot(features_train_male[i], c='b', linewidth=0.5, alpha=0.5)\n",
      "    plt.plot(features_train_female[i], c='r', linewidth=0.5, alpha=0.5)\n",
      "plt.plot(features_male[i+1], c='b', label=\"Male\", linewidth=0.5, alpha=0.5)\n",
      "plt.plot(features_female[i+1], c='r', label=\"Female\", linewidth=0.5, alpha=0.5)\n",
      "plt.legend()\n",
      "plt.title(\"MFCC features for Males and Females\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/mfcc_gender.png)\n",
      "\n",
      "There seem to be slight differences in the features extracted, but the rest of the analysis will tell us more about the separability of these distributions.\n",
      "\n",
      "## Model Evaluation\n",
      "\n",
      "It is now time to evaluate the accuracy of the model on the test features that we kept untouched for the moment. The idea is simply that for a given recording, we estimate the likelihood of each time frame and sum it for the whole recording. Therefore, if the likelihood of a male voice is greater, we return 0 as an answer, and 1 otherwise.\n",
      "\n",
      "```python\n",
      "output = []\n",
      "\n",
      "for f in features_test_male:\n",
      "\n",
      "    log_likelihood_male = np.array(gmm_male.score([f])).sum()\n",
      "    log_likelihood_female = np.array(gmm_female.score([f])).sum()\n",
      "    \n",
      "    if log_likelihood_male > log_likelihood_female:\n",
      "        output.append(0)\n",
      "    else:\n",
      "        output.append(1)\n",
      "```\n",
      "\n",
      "The accuracy for the male test set can be computed as:\n",
      "\n",
      "```python\n",
      "accuracy_male = (1 - sum(output)/len(output))\n",
      "accuracy_male\n",
      "```\n",
      "\n",
      "`0.63148`\n",
      "\n",
      "Similarly, the accuracy for the females reaches 0.63808. \n",
      "\n",
      "Overall, the accuracy is not that high for such a task, and we might need to improve the approach in the next article.\n",
      "\n",
      "## Save models\n",
      "\n",
      "We now suppose that our model is ready to move to production and we re-train it on the whole dataset and save the models:\n",
      "\n",
      "```python\n",
      "def get_features(source):\n",
      "    \n",
      "    files = [os.path.join(source,f) for f in os.listdir(source) if f.endswith('.wav')]\n",
      "    \n",
      "    features = []\n",
      "    for f in files:\n",
      "        sr,audio = read(f)\n",
      "        vector   = get_MFCC(sr,audio)\n",
      "        if len(features) == 0:\n",
      "            features = vector\n",
      "        else:\n",
      "            features = np.vstack((features, vector))\n",
      "\n",
      "    return features\n",
      "\n",
      "source_male = \"test_data/AudioSet/male_clips\"\n",
      "features_male = get_features(source_male)\n",
      "\n",
      "gmm_male = GMM(n_components = 8, max_iter=200, covariance_type='diag', n_init = 3)\n",
      "gmm_male.fit(features_male)\n",
      "\n",
      "source_female = \"test_data/AudioSet/female_clips\"\n",
      "features_female = get_features(source_female)\n",
      "\n",
      "gmm_female = GMM(n_components = 8, max_iter=200, covariance_type='diag', n_init = 3)\n",
      "gmm_female.fit(features_female)\n",
      "\n",
      "# Save models\n",
      "pickle.dump(gmm_male, open(\"male.gmm\", \"wb\" ))\n",
      "pickle.dump(gmm_female, open(\"female.gmm\", \"wb\" ))\n",
      "```\n",
      "\n",
      "## Live Prediction\n",
      "\n",
      "The next step, of course, is to build a live predictor that records 3-5 seconds of an audio sample and classifies it. We use sounddevice for this task, and particularly the rec option.\n",
      "\n",
      "```python\n",
      "def record_and_predict(sr=16000, channels=1, duration=3, filename='pred_record.wav'):\n",
      "    \n",
      "    recording = sd.rec(int(duration * sr), samplerate=sr, channels=channels).reshape(-1)\n",
      "    sd.wait()\n",
      "    \n",
      "    features = get_MFCC(sr,recording)\n",
      "    scores = None\n",
      "\n",
      "    log_likelihood_male = np.array(gmm_male.score(features)).sum()\n",
      "    log_likelihood_female = np.array(gmm_female.score(features)).sum()\n",
      "\n",
      "    if log_likelihood_male >= log_likelihood_female:\n",
      "        return(\"Male\")\n",
      "    else:\n",
      "        return(\"Female\")\n",
      "```\n",
      "\n",
      "To test it in your notebook, simply run :\n",
      "\n",
      "```python\n",
      "record_and_predict()\n",
      "```\n",
      "\n",
      "Leave a comment and tell me how good it works! :)\n",
      "\n",
      "Here's what I noticed while using it. The accuracy on the test remains to improve (63%). When a user plays with his or her voice and tries to imitate the other gender, the GMM gets fooled and predicts the wrong gender. This is also due to the training data that it has seen so far which were extracted from AudioSet and Youtube.\n",
      "\n",
      "# Web application\n",
      "\n",
      "Okay, playing with a notebook is quite easy. But we now need to build a dedicated application for this service. Hopefully, this became really easy with [Streamlit](http://streamlit.io/), a light framework to build interactive applications.\n",
      "\n",
      "I won't dive too much in how Streamlit works (this deserves a dedicated article, coming soon :) ), but here's the code of the application that you should place in `app.py`:\n",
      "\n",
      "```python\n",
      "import streamlit as st\n",
      "\n",
      "# Data manipulation\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Feature extraction\n",
      "import scipy\n",
      "import librosa\n",
      "import python_speech_features as mfcc\n",
      "import os\n",
      "from scipy.io.wavfile import read\n",
      "\n",
      "# Model training\n",
      "from sklearn.mixture import GaussianMixture as GMM\n",
      "from sklearn import preprocessing\n",
      "import pickle\n",
      "\n",
      "# Live recording\n",
      "import sounddevice as sd\n",
      "import soundfile as sf\n",
      "\n",
      "st.title(\"Voice Gender Detection\")\n",
      "st.write(\"This application demonstrates a simple Voice Gender Detection. Voice gender identification relies on three important steps.\")\n",
      "st.write(\"- Extracting from the training set MFCC features (13 usually) for each gender\")\n",
      "st.write(\"- Train a GMM on those features\")\n",
      "st.write(\"- In prediction, compute the likelihood of each gender using the trained GMM, and pick the most likely gender\")\n",
      "\n",
      "\n",
      "st.subheader(\"Ready to try it on your voice?\")\n",
      "\n",
      "st.sidebar.title(\"Parameters\")\n",
      "duration = st.sidebar.slider(\"Recording duration\", 0.0, 10.0, 3.0)\n",
      "\n",
      "def get_MFCC(sr,audio):\n",
      "    \"\"\"\n",
      "    Extracts the MFCC audio features from a file\n",
      "    \"\"\"\n",
      "    features = mfcc.mfcc(audio, sr, 0.025, 0.01, 13, appendEnergy = False)\n",
      "    features = preprocessing.scale(features)\n",
      "    return features\n",
      "\n",
      "def record_and_predict(gmm_male, gmm_female, sr=16000, channels=1, duration=3, filename='pred_record.wav'):\n",
      "    \"\"\"\n",
      "    Records live voice and returns the identified gender\n",
      "    \"\"\" \n",
      "    recording = sd.rec(int(duration * sr), samplerate=sr, channels=channels).reshape(-1)\n",
      "    sd.wait()\n",
      "    \n",
      "    features = get_MFCC(sr,recording)\n",
      "    scores = None\n",
      "\n",
      "    log_likelihood_male = np.array(gmm_male.score(features)).sum()\n",
      "    log_likelihood_female = np.array(gmm_female.score(features)).sum()\n",
      "\n",
      "    if log_likelihood_male >= log_likelihood_female:\n",
      "        return(\"Male\")\n",
      "    else:\n",
      "        return(\"Female\")\n",
      "\n",
      "gmm_male = pickle.load(open('male.gmm','rb'))\n",
      "gmm_female = pickle.load(open('female.gmm','rb'))\n",
      "\n",
      "\n",
      "if st.button(\"Start Recording\"):\n",
      "    with st.spinner(\"Recording...\"):\n",
      "        gender = record_and_predict(gmm_male, gmm_female, duration=duration)\n",
      "        st.write(\"The identified gender is: \" + gender)\n",
      "```\n",
      "\n",
      "To launch the app, you must run the command line `streamlit run app.py`:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gender_app.png)\n",
      "\n",
      "> **Conclusion** : I hope that you enjoyed this article and found the approach useful. It has some severe limitations in terms of accuracy and how the user can trick \n",
      "---\n",
      "title: Convolutional Neural Networks\n",
      "layout: post\n",
      "tags: [deeplearning]\n",
      "subtitle : \"Deep Neural Networks\"\n",
      "---\n",
      "\n",
      "Convolutional Neural Networks (CNN) are feed-forward neural networks that are mostly used for computer vision or time series analysis. They offer an automated image pre-treatment as well as a dense neural network part. CNNs are special types of neural networks for processing data with grid-like topology. The architecture of CNNs is inspired by the visual cortex of animals.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/CNN.jpg)\n",
      "\n",
      "# What is a CNN?\n",
      "\n",
      "## Filters\n",
      "\n",
      "The main idea behind using CNNs is to extract more information than the pixel value itself. Indeed, individual pixel values do not bring a lot of information. For this reason, we apply filters on the images. Historically, a great part of the work was to select these filters manually (e.g Gabor filters) and architecture of the filters to extract as much information from the image as possible. The aim of these filters was for example to extract :\n",
      "- edges\n",
      "- contrast zones\n",
      "- dark or light zones\n",
      "- ...\n",
      "\n",
      "How can we extract information from a filter? Well, you should see a filter as a sliding window that creates an operation locally, between all the pixels located in a region. \n",
      "\n",
      "Therefore, we can extract more information than a single pixel value. If you're interested in this, check my articles on computer vision and filtering. Here is an example with some values in a matrix :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Convolution_schematic.gif)\n",
      "\n",
      "## Automatic image processing \n",
      "\n",
      "With the rise of deep learning and greater computation capacities, this work can now be automated. The name of the Convolutional Neural Networks comes from the fact that we convolve the initial image input with a set of filters. This time, we don't have to select the filters anymore. The parameter to choose to remain :\n",
      "- the number of filters to apply, \n",
      "- the dimension of the filters\n",
      "- the step size with which we convolve the filter with the image\n",
      "- the padding\n",
      "\n",
      "The **dimension** of the filter is called the kernel size. Typical values for the stride lie between 2 and 5. For example, on an image of size $$ 100 \\times 100 $$, we can apply a filter of size 5.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/cnn_1.jpg)\n",
      "\n",
      "The **step size** is simply the amount by which we shift the filter at each step. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/cnn_2.jpg)\n",
      "\n",
      "When we convolve a filter with an image, we'll reach the end of the row of pixels at some point. There are two approaches in this case :\n",
      "- either we stop when the extremity of the filter reaches the boundary of the image. This is called: `padding='valid'` in Keras for example. In this case, we lose a bit of the dimension of the image.\n",
      "- otherwise, we convolve the image to keep the original dimension. This is called: `padding='same'` in Keras. Filter window stays at **valid** position inside input map.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/cnn_3.jpg)\n",
      "\n",
      "By adding several filters, we convolve the image with filters many times. In some sense, we are building a convolved output that has a volume. It's no longer a 2-dimensional picture. The filters end up being hardly humanly understandable, especially when we use a lot of them. Some are used to find curves, other edges, other textures... \n",
      "\n",
      "## A bit of maths\n",
      "\n",
      "### Convolution layer\n",
      "\n",
      "Mathematically, the convolution is expressed as such :\n",
      "$$ (f * g)(t) = f(t) * g(t) = \\int_{-\\infty}^{+\\infty} f(\\tau)g(t-\\tau) \\partial \\tau $$\n",
      "\n",
      "The convolution represents the percentage of area of the filter \\(g\\) that overlaps with the input $$ f $$ at time $$ \\tau $$ over all time $$ t $$ . However, since $$ \\tau < 0 $$ and $$ \\tau > t $$ have no meaning, the convolution is described as :\n",
      "\n",
      "$$ (f * g)(t) = \\int_{0}^{t} f(\\tau)g(t-\\tau) \\partial \\tau $$\n",
      "\n",
      "In the discrete case : \n",
      "\n",
      "$$ f[t] * g[t] = \\sum_{-\\infty}^{+\\infty} f[k] g[t-k] $$\n",
      "\n",
      "If we have a function of 2 variables, in the continuous case :\n",
      "\n",
      "$$ f(x,y) * g(x,y) =  \\int_{\\tau_1 = -\\infty}^{+\\infty}  \\int_{\\tau_2 = -\\infty}^{+\\infty} f(\\tau_1, \\tau_2) * g(x-\\tau_1, y-\\tau_2) \\partial \\tau_1 \\partial \\tau_2 $$\n",
      "\n",
      "What kind of filters are usually applied? Wikipedia brings a clear illustration of this :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/cnn_5.jpg)\n",
      "\n",
      "### Activation function\n",
      "\n",
      "At each convolution step, for each input, we apply an activation function (usually ReLU). So far, we have only added dimensionality to our initial image input. The activation function brings non-linearity in a neural network. Otherwise, a neural network would be simply a linear function. There are many types of activation function. The most common ones to apply on the hidden layers are the following :\n",
      "- ReLU (Rectified Linear Unit) : $$ f(x) = max(0,x) $$\n",
      "- Leaky-ReLU : $$ f(x) = 0.001*x $$ if x < 0, $$ x $$ else.\n",
      "\n",
      "There are other activation functions that we use on the final layer :\n",
      "- Softmax : If your output is a one-hot matrix, you might want to transform the output into probabilities for each class. The softmax function is defined as $$ f(x_i) = \\frac {e^{x_i}} { \\sum_i {e^{x_i}}} $$\n",
      "- Sigmoid : If you face a binary classification task, you'll have a single output neuron, and using a sigmoid activation will assign the value of either 0 or 1 to your output. The sigmoid function is defined by : $$ f(x) = \\frac {1} {1 + e^{-u}} $$\n",
      "\n",
      "### Pooling\n",
      "\n",
      "We then apply a so-called pooling. Pooling involves downsampling of features so that we need to learn fewer parameters when training. The most common form of pooling is max-pooling. For each of the dimension of each of the input image, we perform a max-pooling that takes, over a given height and width, typically 2x2, the maximum value among the 4 pixels. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_4.gif)\n",
      "\n",
      "The intuition is that the maximal value has higher chances to be more significant when classifying an image. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/cnn_4.jpg)\n",
      "\n",
      "We have now covered all the ingredients of a convolution neural network :\n",
      "- the convolution layer\n",
      "- the activation\n",
      "- the pooling layer\n",
      "- the fully connected layer, similar to a dense neural network\n",
      "\n",
      "The order of the layers can be switched :\n",
      "\n",
      "$$ ReLU(MaxPool(Conv(X))) = MaxPool(ReLU(Conv(X))) $$\n",
      "\n",
      "In image classification, we usually add several layers of convolution and pooling. This allows us to model more complex structures. Most of the model tuning in deep learning is to determine the optimal model structure. Some famous algorithms developed by Microsoft or Google reach a depth of more than 150 hidden layers. \n",
      "\n",
      "I'll make another article in which I'll cover the maths behind deep learning if you're interested.\n",
      "\n",
      "# Implementing CNNs in Keras\n",
      "\n",
      "Implementing a CNN in Keras can be done the following way. Start by importing the following package :\n",
      "\n",
      "```python\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Conv2D, Flatten, Activation, Dropout, MaxPooling2D\n",
      "from keras.layers.normalization import BatchNormalization\n",
      "import numpy as np\n",
      "```\n",
      "\n",
      "Say that our aim here is to make a binary classification from input images of size $$ 100 \\times 100 $$. It could be a classification of the gender of the person for example. We want to use a sigmoid activation function on the neuron output.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/sigmoid.jpg)\n",
      "\n",
      "We can then build the model in Keras :\n",
      "\n",
      "```python\n",
      "#create model\n",
      "model = Sequential()\n",
      "\n",
      "#add model layers\n",
      "model.add(Conv2D(512, kernel_size = (3,3), input_shape=(100,100,1)))\n",
      "model.add(BatchNormalization())\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
      "model.add(Dropout(0.1))\n",
      "\n",
      "model.add(Conv2D(256, kernel_size = (3,3)))\n",
      "model.add(BatchNormalization())\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
      "model.add(Dropout(0.1))\n",
      "\n",
      "model.add(Conv2D(128, kernel_size = (3,3)))\n",
      "model.add(BatchNormalization())\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
      "model.add(Dropout(0.1))\n",
      "\n",
      "model.add(Flatten())\n",
      "\n",
      "model.add(Dense(1, activation='sigmoid'))\n",
      "```\n",
      "\n",
      "As you can see, we add several convolutions, max pooling, and batch normalization layers, before flattening the output of the layers and adding several dense layers. The final dense layer here contains the number of classes we are working within the inputs.\n",
      "\n",
      "You can print the summary of the model the following way :\n",
      "\n",
      "```python\n",
      "model.summary()\n",
      "```\n",
      "\n",
      "To compile and fit the model, simply run :\n",
      "\n",
      "```python\n",
      "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
      "model.fit(X, y, batch_size=128, epochs=10000, validation_split = 0.2)\n",
      "```\n",
      "\n",
      "You can choose :\n",
      "- the optimizer (we tend to use Adam most often)\n",
      "- the loss (here, binary cross-entropy)\n",
      "- the metric to use (our problem is balanced, so accuracy is just fine)\n",
      "- the training data and the validation split (if you have validation data, you can use them with `validation_data = (X_val, y_val)`)\n",
      "- the batch size, i.e. the number of training samples you use per batch\n",
      "- the number of epochs for the training\n",
      "\n",
      "> **Conclusion **: CNNs are nowadays key elements in computer vision. If you have a question, don't hesitate to comment on the article.\n",
      "\n",
      "---\n",
      "title: Introduction to Git\n",
      "layout: post\n",
      "tags: [da_tuto]\n",
      "subtitle : \"Data Analysis Basics\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# Version Control\n",
      "\n",
      "One of the big strugles when programming in a company vs. on your own project is that you're not alone. Of course, this has a lot of upsides since you can learn from your co-workers and you might not master all technologies involved in the project. The downside is that you need to make sure that the code you are writing is compatible with the code written by your team. \n",
      "\n",
      "This is done by using \"version control\" such as Git that allow you to share code and that prevent your work from erasing other's code. Think of it as a Google Drive but with some security layers on top. You have your local folder, and it gets synchronized online when you execute some command. One of the big advantage of Github is the ability to revert back to a previous version of your application, if your latest change of the app brought a bug for example.\n",
      "\n",
      "On Git, your project is called a **repository**, which gathers the code of your application. This repository has **branches**. Branches are quite similar versions of the application with minor changes on each branch. There is a main branch, usually the version of the application exposed to the end customer, called Master, and each developer can create a branch from this main branch, i.e. a duplicate at a fixed point in time, to modify something on the application, for example adding a feature or improving an algorithm.\n",
      "\n",
      "When you finished adding your changes, we say that you **merge** your changes to the application on the main branch, and make the branch you created disapear. This process usually implies a code review when you do pair programming, i.e. another developer will look at the code and see if something is wrong before merging it into the final version of the application.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/git_0.png)\n",
      "\n",
      "Each feature you add to the application should have its own branch dedicated, (this is a good habit for agile approaches if you have heard of them) with one or few developers working on it. When several developers work on the same branch, and at some point the same file, this might bring **conflicts**. Conflicts happen when two versions of the application co-exist, the one existing on the branch and the one you are trying to overwrite. To avoid conflicts, you need to make sure that you work on different parts of the code, and that you have **pulled** the latest version of the branch, meaning that you have downloaded the latest version of the branch you are working on before editing anything.\n",
      "\n",
      "Once your changes on the application are ready to be put on the branch corresponding to the feature, you can **commit** your changes, i.e. write them on a registery of the changes, and **push** them, i.e. make them avaialable for everybody on the branch.\n",
      "\n",
      "Once the branche has collected all the changes from all the developers working on the feature, as explained before, the branch is merged into the main branch.\n",
      "\n",
      "# How to use Github\n",
      "\n",
      "Git is a distributed version control tool that can manage a development project's source code history, while GitHub is a cloud based platform built around the Git tool.\n",
      "\n",
      "Github's technology has been acquired by Microsoft a few years back. Millions of developers use Github for their team projects, for open-source contributions or for personal projects. On Github, you can browse state-of-the-art repositories when made available publically, but you can also host the code of your company's applicaiton privately. All languages are supported on Github (bash, Python, R, C, C++, JS, ...). Other version control tools exist, such as Gitlab and Bitbucket.\n",
      "\n",
      "You can also download code and projects directly from Github. This is a good starting point for any project.\n",
      "\n",
      "We have installed git in the setup article before, using command line. Now, head to [https://github.com/](https://github.com/). Create an account (it's free), and we will create your first repository. Hit the \"+\" button at the top.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/git_1.png)\n",
      "\n",
      "Call your repository \"my_application\", and create the repository:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/git_2.png)\n",
      "\n",
      "You'll land on a page with code snippets given to you. \n",
      "\n",
      "```bash\n",
      "echo \"# my_application\" >> README.md\n",
      "git init\n",
      "git add README.md\n",
      "git commit -m \"first commit\"\n",
      "git remote add origin https://github.com/maelfabien/my_application.git\n",
      "git push -u origin master\n",
      "```\n",
      "\n",
      "Go to your root from your terminal. In my case, it's `Users/maelfabien`. This is usually where you put your projects. The command given can be executed through your terminal. It will create a new folder that contains the code of your application, create a README (a simple text file that describes your application and the files it contains), add those changes to the commit history, commit those changes (meaning that we mark them in the registery), and push those changes online. \n",
      "\n",
      "Now, if you refresh your Github page, you will see that your repository now contains a single file called README.md. The `.md` extension stands for Markdown, an alternative to `.txt` files you might know and HTML, which includes some layout. We'll get back to this in a further article.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/git_3.png)\n",
      "\n",
      "Alright, we have now pushed our first article online. It is available publically. Our changes are made on the \"Master\" branch, meaning that we work directly on the final branch exposed to a customer if we were to build an application. This is only a simple tutorial, which is why we're doing this.\n",
      "\n",
      "Suppose a friend of yours also works on the project and you want to **pull** the changes he made, i.e. download the changes he made on the application. From the right folder in the terminal, simply type:\n",
      "\n",
      "```bash\n",
      "git pull\n",
      "```\n",
      "\n",
      "This will pull the latest changes. Now, you have made some changes on the application (e.g. updated the README.md file). You need to **add** your changes and commit them.\n",
      "\n",
      "```bash\n",
      "git add .\n",
      "git commit -m \"Update of the README\"\n",
      "```\n",
      "\n",
      "The `add .` means that we add all changes that have been made yet. The commit writes these changes to the registry, and we add a message (-m) to let other developers know what we modified. Commits should be small enough so that the message summarizes well what it contains.\n",
      "\n",
      "Finally, we need to **push** changes online, and make it available to our friend (who will `git pull` your changes), using:\n",
      "\n",
      "```bash\n",
      "git push\n",
      "```\n",
      "\n",
      "These 4 git commands are fundamental. They allow you to work alone with organized code on a repository, or on small projects with other developers. Then, you will need to start using branches.\n",
      "\n",
      "Suppose that the master branch contains the latest version of the application. You want to add a feature and need to create a branch. If another developer already created the branch before you, simply use :\n",
      "\n",
      "```bash\n",
      "git checkout other_branch_name\n",
      "```\n",
      "\n",
      "The **checkout** will allow you to move to that branch. This will automatically update the content of your local files ! You need to have all you changes commited before checking out. Otherwise, if you need to create the branch, simply use:\n",
      "\n",
      "```bash\n",
      "git branch -b other_branch_name\n",
      "```\n",
      "\n",
      "This will create a duplicate of the branch you were on (the Master), and allow you to make changes safely before merging it back into Master. \n",
      "\n",
      "Finally, if you notice a nice repository on Github and wish to download it on your own computer to test it, simply click on the right menu of the repo \"Download\". This will give you the choice between downloading it as a zipped file, or as a link. To **clone** a project from a link, use this simple command line on your terminal:\n",
      "\n",
      "```bash\n",
      "git clone https://github.com/link_to_package\n",
      "```\n",
      "\n",
      "With these 7 commands, you should be able to handle 95% of your Github use cases. Some important notes on Git:\n",
      "- You can use Git to browse repositories and look for scientific paper implementations for example, or open source project's source code\n",
      "- I think it's safe to say that 99% of programming teams worlwide use Git (Github, Gitlab, Bitbucket...). Don't skip it, you'll end up regreting it.\n",
      "- do not push files of more than 100 Mb., you will have to use Git Large File System otherwise\n",
      "- Git is therefore not made to host databases, but simply code (a complete application should be a few dozen Mb. at most)\n",
      "- Use Git to show your projects, this is highly valued by recruiters\n",
      "- Make your code understandable, and write a full Readme\n",
      "- Use git from the start of your project, this is a good habit, and makes tracking your changes easier. Plus, it shows how active you are on your Github account\n",
      "\n",
      "---\n",
      "title: Getting started with Elastic Cloud\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Elastic Search, Logstash, Kibana\"\n",
      "---\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/elc.jpg)\n",
      "\n",
      "# I. Cloud solution\n",
      "\n",
      "The ElasticCloud is a fully managed service for both Kibana and Elasticsearch. It allows one-click install and easy upgrades. It also gives a nice UI for Elasticsearch clusters. The solution is free for a 14 days trial. Some customers of Elastic Cloud include Sprint, Verizon, eBay or Dell.\n",
      "\n",
      "## 1. Elasticsearch\n",
      "\n",
      "First of all, create an account <span style=\"color:blue\">[here](https://cloud.elastic.co/)</span> and log-in to the console. You should see something like this :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_1.jpg)\n",
      "\n",
      "We'll now create a cluster with Elastic Cloud in under 3 minutes. You can get started without a credit card during the first 14 days. The trial version comes with :\n",
      "- 8GB RAM\n",
      "- 240GB storage\n",
      "- High availability over 2 zones\n",
      "\n",
      "Click on \"Create Deployment\". \n",
      "- Give your deployment a name\n",
      "- Select a cloud platform among AWS or GCP\n",
      "- Select a region\n",
      "- Choose a deployment template among the ones proposed\n",
      "- Click on \"Customize Deployment\"\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_2.jpg)\n",
      "\n",
      "Among the proposed templates, some are optimized for aggregations in RAM, others for time series...\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_3.jpg)\n",
      "\n",
      "Once on the customization menu, navigate through the page to see which options you have access to. Everything is pretty intuitive, and adding capacity or enabling services is quite simple. Most menus are however locked since you currently have a trial account.\n",
      "\n",
      "Then, click on \"Create Deployment\".\n",
      "\n",
      "You'll land on the Activity Page. It might take a few minutes for the clusters to be up and running. Make sure to save the password given, as there won't be any way to retrieve it later on.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_4.jpg)\n",
      "\n",
      "## 2. Kibana\n",
      "\n",
      "Now, in the menu on your left, click on the Overview Page, and launch Kibana service :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_5.jpg)\n",
      "\n",
      "Log-in to Kibana using the default user name `elastic` and the password that you saved in the Activity section. Click on \"Explore on my own\".\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_6.jpg)\n",
      "\n",
      "In this tutorial, we'll cover a basic example of retrieving the system metrics of your local machine into Kibana. In the \"Add Data to Kibana\" section, click on Metrics. This can typically be used to collect metrics from the operating system and services running on your servers or your local machine.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_7.jpg)\n",
      "\n",
      "Scroll down, and click on \"System metrics\". \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_8.jpg)\n",
      "\n",
      "I'm on macOS, so I'll just follow the small tutorial for macOS :\n",
      "\n",
      "*Step 1* : Download and install MetricBeat\n",
      "\n",
      "Copy the code snipet :\n",
      "![image](https://maelfabien.github.io/assets/images/ela_9.jpg)\n",
      "\n",
      "Paste this code in your default directory in your terminal.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_10.jpg)\n",
      "\n",
      "*Step 2* : Edit the configuration file\n",
      "\n",
      "Modify `metricbeat.yml` to set the connection information for Elastic Cloud. In your terminal, type :\n",
      "```bash\n",
      "vim metricbeat.yml\n",
      "```\n",
      "\n",
      "The metricbeat.yml will open. To start editing the file, type the letter `i` (for insert). Copy-paste the code snippet in the Elastic Cloud section :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_11.jpg)\n",
      "\n",
      "Make sure to replace the `<password>` section by the password you copy-pasted from the Activity section before. \n",
      "\n",
      "```bash \n",
      "#============================= Elastic Cloud ==================================\n",
      "cloud.id: \"Test:ZXUtY2VudHJhbC0xLmF3cy5jbG91ZC5lcy5pbyQ0ZGU0NmNlZDhkOGQ0NTk2OTZlNTQ0ZmU1ZjMyYjk5OSRlY2I0YTJlZmY0OTA0ZDliOTE5NzMzMmQwOWNjOTY5Ng==\"\n",
      "cloud.auth: \"elastic: PASTE YOUR PASSWORD HERE\"\n",
      "\n",
      "# These settings simplify using metricbeat with the Elastic Cloud (https://cloud.elastic.co/).\n",
      "```\n",
      "\n",
      "We are still using the default user. We'll see later on how to create a user and attach rights to it. To save and quit the file, press ESC, and type `:wq`.\n",
      "\n",
      "*Step 3* : Enable and configure the system module\n",
      "\n",
      "Simply run the following command in the folder you installed MetricBeat in :\n",
      "\n",
      "```bash\n",
      "./metricbeat modules enable system\n",
      "```\n",
      "\n",
      "*Step 4* : Start Metricbeat \n",
      "\n",
      "Then, load the Kibana dashboards in your terminal :\n",
      "```bash\n",
      "./metricbeat setup\n",
      "```\n",
      "\n",
      "You should see this:\n",
      "```\n",
      "Loaded index template\n",
      "Loading dashboards (Kibana must be running and reachable)\n",
      "Loaded dashboards\n",
      "```\n",
      "\n",
      "Finally, run this command :\n",
      "```bash\n",
      "./metricbeat -e\n",
      "```\n",
      "\n",
      "Make sure that the data is received using the \"Check Data\" button :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_12.jpg)\n",
      "\n",
      "You're now ready! Move on to the next step.\n",
      "\n",
      "You now have metrics data from your local machine flowing into Kibana. \n",
      "\n",
      "\n",
      "## 3. Exploring Kibana\n",
      "\n",
      "We'll now go over the different sections of Kibana while our data keeps flowing in.\n",
      "\n",
      "### a. Discover\n",
      "\n",
      "If you're familiar with Tableau software, you'll notice how easy to use this section is. The graph presents the count of data coming in every 30 seconds. 90 to 100 new data are added every 30 seconds so that around 180 to 200 data point per minute. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_15.jpg)\n",
      "\n",
      "You can add or remove fields to explore on the side menu, using the \"Add\" or \"Remove\" button.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_16.jpg)\n",
      "\n",
      "### b. Visualize\n",
      "\n",
      "The \"Visualize\" menu displays some pre-built views to explore your data depending on the most common use cases, including for example :\n",
      "- command rates\n",
      "- average response time\n",
      "- average time in the queue\n",
      "- ...\n",
      "\n",
      "For example, type \"System\" in the research menu and select `CPU Usage [Metricbeat System]`. You'll have access to a pre-built dashboard that displays a time series of your CPU usage.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_17.jpg)\n",
      "\n",
      "### c. Dashboard\n",
      "\n",
      "The dashboard looks like this :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_13.jpg)\n",
      "\n",
      "You can use the \"Host overview\" tab to visualize some more data. The dashboard is made of modules that present the data in a clear and easy to understand way.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_14.jpg)\n",
      "\n",
      "### d. Timelion\n",
      "\n",
      "\"Timelion is the clawing, gnashing, zebra killing, pluggable time series interface for everything. If your datastore can produce a time series, then you have all of the awesome power of Timelion at your disposal. Timeline lets you compare and combine datasets across multiple data sources with one easy-to-master expression syntax. This tutorial focuses on Elasticsearch, but you'll quickly discover that what you learn here applies to any data source Timelion supports.\"\n",
      "\n",
      "We'll try to see how to use Timelion for the number of data points received every minute for example.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_18.jpg)\n",
      "\n",
      "Explore the function reference to see which functions are available. For example, if we are interested in the first difference of the number of data points received every minute :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_19.jpg)\n",
      "\n",
      "The drop in the number of points is because the minute of collecting data is not over yet.\n",
      "\n",
      "### e. Canvas\n",
      "\n",
      "The canvas is an amazing way to build nice-looking dashboard intuitively. It's a Drag&Drop for data visualization.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_20.jpg)\n",
      "\n",
      "Here, I'm using the sample data Kibana provides. You can connect several data sources on the same dashboard, add pictures, several types of graphs, metrics...\n",
      "\n",
      "This is still a Beta functionality while I'm writing this, but the service will become a standard shortly.\n",
      "\n",
      "### f. Machine Learning\n",
      "\n",
      "I won't go into the details of the section, I'll make a dedicated article on this subject.\n",
      "\n",
      "### g. Infrastructure\n",
      "\n",
      "This section is really useful to monitor your infrastructure and identify problems in real-time. You can explore logs, containers, and services. Since we have only one machine connected, I'll display a dashboard of a real infrastructure that allows detecting anomalies.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_21.jpg)\n",
      "\n",
      "### h. Logs\n",
      "\n",
      "We don't have any logging indices for the moment. We'll follow the tutorial to add some (here, for macOS).\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_22.jpg)\n",
      "\n",
      "*Step 1* : Download and install Filebeat\n",
      "Paste the following code in your default directory in your terminal :\n",
      "\n",
      "```\n",
      "curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.6.1-darwin-x86_64.tar.gz\n",
      "tar xzvf filebeat-6.6.1-darwin-x86_64.tar.gz\n",
      "cd filebeat-6.6.1-darwin-x86_64/\n",
      "```\n",
      "\n",
      "*Step 2* : Edit the configuration\n",
      "Modify `filebeat.yml`  to set the connection information for Elastic Cloud:\n",
      "\n",
      "```\n",
      "vim filebeat.yml\n",
      "```\n",
      "Paste the code snippet :\n",
      "```bash \n",
      "#============================= Elastic Cloud ==================================\n",
      "cloud.id: \"Test:ZXUtY2VudHJhbC0xLmF3cy5jbG91ZC5lcy5pbyQ0ZGU0NmNlZDhkOGQ0NTk2OTZlNTQ0ZmU1ZjMyYjk5OSRlY2I0YTJlZmY0OTA0ZDliOTE5NzMzMmQwOWNjOTY5Ng==\"\n",
      "cloud.auth: \"elastic:PASTE YOUR PASSWORD HERE\"\n",
      "# These settings simplify using filebeat with the Elastic Cloud (https://cloud.elastic.co/).\n",
      "```\n",
      "\n",
      "We are still using the previous password for the default user. To save and quit the file, press ESC, and type `:wq`.\n",
      "\n",
      "*Step 3* : Enable and configure the system module\n",
      "\n",
      "From the installation directory, run :\n",
      "```\n",
      "./filebeat modules enable system\n",
      "````\n",
      "\n",
      "*Step 4* : Start Filebeat\n",
      "\n",
      "Finally, start Kibana dashboards and start filebeat using :\n",
      "\n",
      "```\n",
      "./filebeat setup\n",
      "./filebeat -e\n",
      "```\n",
      "\n",
      "If the installation process worked successfully, you should see something like this :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_23.jpg)\n",
      "\n",
      "You should now be able to see all the logs from your Log section on Kibana!\n",
      "\n",
      "### i. APM\n",
      "\n",
      "Application Performance Monitoring (APM) collects in-depth performance metrics and errors from inside your application. It allows you to monitor the performance of thousands of applications in real-time. Since we don't have any app yet, we won't cover this part.\n",
      "\n",
      "### j. Graph\n",
      "\n",
      "If your data involve a network, it might be nice to represent them as graph data. This section will be doing it for you.\n",
      "\n",
      "### k. Devtools\n",
      "\n",
      "Devtools contains development tools that you can use to interact with your data in Kibana.\n",
      "\n",
      "### l. Monitoring\n",
      "\n",
      "The monitoring can be used to check that your services are up and running :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_24.jpg)\n",
      "\n",
      "### m. Management\n",
      "\n",
      "Now, suppose we want to create another user. Go to the management section, and click on \"Users\".\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_25.jpg)\n",
      "\n",
      "Then, click on \"Create New User\" :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_26.jpg)\n",
      "\n",
      "Fill in the form, and give the user the specific access you'd like him to have :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_27.jpg)\n",
      "\n",
      "The password you set will be your new user password. \n",
      "\n",
      "# II. Stop the services\n",
      "\n",
      "Once you have explored your data, you might want to stop the data flow that goes from your machine to Kibana. From the terminal, in the tabs concerned, simply type Ctrl + C.\n",
      "\n",
      "To stop being charged for Elasticsearch, you'll have to delete your deployment. All your data will be lost! Keep this in mind.\n",
      "\n",
      "On your Elastic Cloud interface, click on your deployment :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_28.jpg)\n",
      "\n",
      "And click on \"Delete deployment\".\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ela_29.jpg)\n",
      "\n",
      "> *Conclusion *: I hope this introduction to Elastic Cloud managed solution was helpful. Don't hesitate to drop a comment if you have a question or remark.\n",
      "\n",
      "Sources :\n",
      "- [Infrastructure](https://www.elastic.co/guide/en/kibana/master/xpack-infra.html)\n",
      "- [Canvas](https://www.elastic.co/guide/en/kibana/6.6/canvas.html)\n",
      "- [Cloud Introduction](https://info.elastic.co/es-service-trial-rtp-v6.03-2.html?baymax=rtp&elektra=cloud-footer&storm=everyone&iesrc=ctr)\n",
      "- [Getting Started](https://www.elastic.co/webinars/getting-started-elasticsearch?baymax=rtp&elektra=home&storm=sub1&iesrc=ctr)\n",
      "---\n",
      "title: Basics of Python programming\n",
      "layout: post\n",
      "tags: [da_tuto]\n",
      "subtitle : \"Data Analysis Basics\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# The logic behind Python\n",
      "\n",
      "First, get in your mind that Python is easy. There is no need to have a Computer Science degree to understand Python code and write Python code. It is very close to spoken language, but it just had to be formalized a bit so that every computer interprets it the same way.\n",
      "\n",
      "Open a Jupyter Notebook, and we'll dive in the basics of Python programming.\n",
      "\n",
      "To illustrate how easy Python programming can be, say that you want your computer to print the content \"Hello world\" when you run the cell. How to do that?\n",
      "\n",
      "Well, if we write the instruction, it would be something like `print Hello World!`. It's almost that, but I need to say where the string to print starts, and where it ends. Therefore, simply add quotes:\n",
      "\n",
      "`print \"Hello World!\"`\n",
      "\n",
      "Nice, if you execute that, you'll get an error. Since Python 3, the print statement must have parenthesis. Simply adapt it:\n",
      "\n",
      "```python\n",
      "print(\"Hello World!\")\n",
      "```\n",
      "\n",
      "It's that easy. Run this cell, and it will display under \"Hello World!\". Learning Python (as well as many other programming languages) is just about learning a syntax. The logic behind most languages remains the same.\n",
      "\n",
      "Now, printing information is cool, but we will start defining variables.\n",
      "\n",
      "```python \n",
      "var_a = 1\n",
      "var_b = 1.4\n",
      "var_c = \"Hello World!\"\n",
      "```\n",
      "\n",
      "We define a name, the name of the variable, and attach to it a value. You can naturally read: var_a equals 1.\n",
      "\n",
      "Another example of this logic is the if-statement:\n",
      "\n",
      "```python\n",
      "if var_a == 2:\n",
      "\tprint(var_a)\n",
      "else:\n",
      "\tprint(var_b)\n",
      "```\n",
      "\n",
      "We will come back to this later, but to know what this does, simply read it. If var_a is equal to 2, print var_a, else, print var_b. This is straight forward. Now that you get the logic, let's dive in the code.\n",
      "\n",
      "# Data types\n",
      "\n",
      "Python data types do not have to be declared, they are infered automatically. You create a **variable**, which is an object that takes a value, and can be re-used or modified. The data types can be:\n",
      "\n",
      "- a string : \n",
      "```python \n",
      "var_a = \"Hello World!\"\n",
      "```\n",
      "- an interger : \n",
      "```python \n",
      "var_a = 1\n",
      "```\n",
      "- a float : \n",
      "```python \n",
      "var_a = 1.02\n",
      "```\n",
      "- a boolean, takes values *True* or *False* :\n",
      "```python \n",
      "var_a = True\n",
      "```\n",
      "\n",
      "You can give different names to your variables, and try to make them expressive. For example:\n",
      "\n",
      "```python\n",
      "size_house = 120.35\n",
      "nb_rooms = 5\n",
      "```\n",
      "\n",
      "You can convert one type into another using type converter:\n",
      "\n",
      "```python\n",
      "size_integer = int(size_house)\n",
      "```\n",
      "\n",
      "```\n",
      "120\n",
      "```\n",
      "\n",
      "This will only keep the integer part for example. You can also transform a integer into a float:\n",
      "\n",
      "```python\n",
      "float_nb_rooms = float(nb_rooms)\n",
      "```\n",
      "\n",
      "```\n",
      "5.0\n",
      "```\n",
      "\n",
      "And transform an integer into a string:\n",
      "\n",
      "```python\n",
      "str_nb_rooms = str(nb_rooms)\n",
      "```\n",
      "\n",
      "```\n",
      "'5'\n",
      "```\n",
      "\n",
      "Some transformations will of course give you error, when you try to cast a string into an integer and the string itself is not a number.\n",
      "\n",
      "# Operations \n",
      "\n",
      "You can perform operations on these variables once defined, such as addition, substraction, multiplication, division, exponent, modulo...\n",
      "\n",
      "Take for example:\n",
      "\n",
      "```python\n",
      "a = 5\n",
      "b = 2\n",
      "```\n",
      "\n",
      "## Addition\n",
      "\n",
      "```python\n",
      "a + b \n",
      "```\n",
      "\n",
      "```\n",
      "7\n",
      "```\n",
      "\n",
      "## Substraction\n",
      "\n",
      "```python\n",
      "a - b\n",
      "```\n",
      "\n",
      "```\n",
      "3\n",
      "```\n",
      "\n",
      "## Multiplication\n",
      "\n",
      "```python\n",
      "a * b\n",
      "```\n",
      "\n",
      "```\n",
      "10\n",
      "```\n",
      "\n",
      "## Division\n",
      "\n",
      "```python\n",
      "a / b\n",
      "```\n",
      "\n",
      "```\n",
      "2.5\n",
      "```\n",
      "\n",
      "## Power\n",
      "\n",
      "```python\n",
      "a ** b\n",
      "```\n",
      "\n",
      "```\n",
      "25\n",
      "```\n",
      "\n",
      "> If you found the article useful or see ways in which it could be improved, please leave a comment :)\n",
      "---\n",
      "title: Automated Graphs with Visual Recommendation Systems\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Data Viz\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# Landscape of visual recommendation\n",
      "\n",
      "Visualization is a great way to enrich data exploration and analysis. Visual Recommendation systems aim to suggest automatically visualizations of a dataset.\n",
      "\n",
      "Think about it... How can you automatically create a good visualization? This is a hard question since there are thousands of ways to display information from a data set. The user could want to display specific columns, with a specific kind of graph, specific colors, etc.\n",
      "\n",
      "There are several ways to create graphs automatically :\n",
      "- rule-based methods, which typically requires a lot of tuning\n",
      "- ML-based systems learn the relationship between training data and the visualizations\n",
      "\n",
      "We will focus on the latter. [Viz-ML](https://arxiv.org/pdf/1808.04819.pdf) is one of the most recent evolutions in ML-based systems, and proposes a Neural Network-based approach trained on Plotly's Community feed, a set of more than 200Go of data that map data tables and visualizations. This article will summarize the initial 13 pages of paper. The code of the paper is available on [GitHub](https://github.com/mitmedialab/vizml).\n",
      "\n",
      "VizML reached a visual satisfaction degree (measured on Amazon Mechanical Turk) similar to the one reached by Plotly's users and outperforms other methods.\n",
      "\n",
      "# What is visualization?\n",
      "\n",
      "Building a visualization means making a set of design choices among possible choices. We make choices among $$ C = \\{ c \\} $$ designs, where each parameter $$ c $$ can take a certain number of values. Then, the number of total (possible) combinations is : $$ c_1 \\times c_2 \\times ... \\times c_k $$ where $$ c_1 $$ is the number of possible choices for the first parameter. We measure the effectiveness of visualization by efficiency, accuracy, memorability, engagement, etc. given contextual factors: data, tasks, audience, medium...\n",
      "\n",
      "The user aims to maximize the effectiveness given the constraints :\n",
      "\n",
      "$$ C_max = arg max_C Eff( C \\mid d, T) $$\n",
      "\n",
      "The output of a good visual recommendation system should be to suggest a subset of design choices $$ C_{rec} \\subseteq C $$. The authors defined an objective function that maximizes the likelihood of observing the training output $$ C_d $$. The authors try to approximate the best solution with a neural network model $$ G_C $$.\n",
      "\n",
      "The overall pipeline looks like this :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/pip_viz.jpg)\n",
      "\n",
      "# How does VizML work?\n",
      "\n",
      "## The data\n",
      "\n",
      "The data was collected using the Community Feed of Plotly. The data come as JSON files. `data` contains the source data, `specification` contains the traces, and` layout` defines display configuration.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/source_viz.jpg)\n",
      "\n",
      "The training data is roughly made of 2 million unique visualizations. 95% of the datasets are made of less than 25 columns. \n",
      "\n",
      "## Feature extraction\n",
      "\n",
      "At that point, we still need to extract features from the JSON files before training any model. The idea is to teach the algorithm which columns are relevant for plotting purposes. We need to extract features from these columns to build a training dataset.\n",
      "\n",
      "The features created refer to :\n",
      "- either a single column. There are 81 single-column features\n",
      "- either pair of columns. There are 30 pair features\n",
      "\n",
      "The 81 features extracted concern either :\n",
      "- The Dimension: number of values\n",
      "- The type of data: categorical, string, integer, temporal...\n",
      "- The values: mean, median, skewness, entropy...\n",
      "- The column names: length of the name, number of words, contains...\n",
      "\n",
      "The 30 pairwise columns concern either :\n",
      "- The values (correlation, Kolmogorov-Smirnov test, ANOVA...)\n",
      "- The column names: edit distances (Levenshtein), shared words...\n",
      "\n",
      "The authors created 841 dataset-level features by aggregating these single and pairwise-column features using 16 aggregation functions.\n",
      "\n",
      "## Design extraction\n",
      "\n",
      "The last data pre-processing step is to extract the design type from the JSON file. The idea was to extract an analyst’s design choices by parsing these files, and extracting encoding-level design choices such as mark type (scatter, line, bar) and X or Y column encoding, \n",
      "\n",
      "Some additional cleaning is then done to remove duplicates of datasets and identify a final corpus of 199'000 datasets and 287'000 columns.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/pip_viz_2.jpg)\n",
      "\n",
      "## Prediction tasks\n",
      "\n",
      "To get a good set of basic models, naive Bayes, K-Nearest Neighbors, and Logistic Regressions were used as baselines. The model tuned in VizML is a fully-connected feedforward neural network (NN). The network had 3 hidden layers, each consisting of 1,000 neurons with ReLU activation functions.\n",
      "\n",
      "The accuracy was the chosen metric. The neural network almost systematically outperforms other models.\n",
      "\n",
      "## Model interpretability\n",
      "\n",
      "To interpret the model and justify the feature extraction, we can compute the feature importances :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/feat_viz.jpg)\n",
      "\n",
      "Key messages :\n",
      "- Dimensionality (grey): the length of a column is the second most important feature for predicting whether that column is visualized in a line or a bar trace.\n",
      "- Column types (yellow): consistently important for each prediction task. For example, whether a dataset has a string column is the fifth most important feature for determining whether that dataset is visualized as a bar or a line chart. \n",
      "- Statistical features (blue and orange): Gini, entropy, skewness, and kurtosis are much more important than lower moments such as mean and variance.\n",
      "- Orderedness (green): Sortedness is defined as the element-wise correlation between the sorted and unsorted values of a column. It appears to be quite important since users might have pre-sorted target columns in Plotly.\n",
      "- The scale of variation (red): the linear or logarithmic space sequence coefficients is also important.\n",
      "\n",
      "## Results\n",
      "\n",
      "To measure the performance of the model, participants were recruited through Amazon Mechanical Turk. The outcomes are encouraging since VizML performs as well as Plotly's users on the chosen metric.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/res_viz.jpg)\n",
      "\n",
      "> *Conclusion *: This recent paper offers great perspectives in the field of Visual Recommendation. In the original paper, the authors also suggest a series of future research directions.\n",
      "\n",
      "---\n",
      "title: Who's the painter ?\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Better features, better data\"\n",
      "search: false\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "In this article, we will be using data from the Web Gallery of Art, a virtual museum and searchable database of European fine arts from the $$ 3^{rd} $$ to $$ 19^{th} $$ centuries. The gallery can be accessed  [here](https://www.wga.hu/index1.html).\n",
      "\n",
      "We will create an algorithm to predict the name of the painter based on an intial set of features of the painting, and then gradually including more and more, thus improving the feature engineering, and including pictures.\n",
      "\n",
      "Through this article, we will illustrate:\n",
      "- The importance of good feature engineering;\n",
      "- The importance of data enrichment; and\n",
      "- The impact this can have on accuracy \n",
      "\n",
      "Ready ? Let's get started !\n",
      "\n",
      "# The data\n",
      "\n",
      "To download the data, you can either :\n",
      "- click on [this link](https://www.wga.hu/database/download/data_xls.zip) to download the XLS file directly\n",
      "-  go to Database tab in the website, and click on the last link : *You can download the catalogue for studying or searching off-line*. Select the Excel format of 5.2 Mb. \n",
      "\n",
      "Start off by importing several packages to be used later:\n",
      "\n",
      "```python\n",
      "### Manipulating and plotting data ###\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.image as mpimg\n",
      "import seaborn as sns\n",
      "\n",
      "### Process text ###\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize import word_tokenize\n",
      "import spacy\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "import re\n",
      "nlp = spacy.load('en_core_web_md')\n",
      "\n",
      "### Process images ###\n",
      "import glob\n",
      "import cv2\n",
      "import matplotlib.image as mpimg\n",
      "import urllib.request\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "### Modeling libraries performance ###\n",
      "from sklearn import preprocessing\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\")\n",
      "```\n",
      "\n",
      "The architecture of our folders should be the following:\n",
      "\n",
      "```\n",
      "- Notebook.ipynb\n",
      "- images\n",
      "- catalog.xlsx\n",
      "```\n",
      "\n",
      "*Images* is an empty folder to be used later.\n",
      "\n",
      "Import the file `catalog.xlsx` :\n",
      "\n",
      "```python\n",
      "catalog = pd.read_excel('catalog.xlsx', header=0)\n",
      "catalog.head()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl_0.png)\n",
      "\n",
      "We directly notice that we need to process the data, to make it exploitable. The available columns are :\n",
      "- The author, which we will try to predict\n",
      "- The date of birth and death of the author. We will drop this column since it is directly linked to the author\n",
      "- The title of the painting\n",
      "- The date of the painting, if available\n",
      "- The technique used (Oil on copper, oil on canvas, wood, etc) as well as the size of the painting\n",
      "- The current location of the painting\n",
      "- The URL of the image on the website\n",
      "- The form (painting, ceramics, sculpture, etc). We will only focus on paintings in our case.\n",
      "- The type of painting (mythological, genre, portrait, landscape, etc.)\n",
      "- The school, i.e. the dominant painting style\n",
      "- The timeframe in which experts estimate that the painting was painted\n",
      "\n",
      "# Feature engineering\n",
      "\n",
      "Since the dataset itself is not made for running a ML algorithm, but meant to be a simple catalog, we need some processing. \n",
      "\n",
      "## Date\n",
      "\n",
      "By exploring the data, we notice missing values for the date. When the date is approximative, it is denoted by :\n",
      "- 1590s\n",
      "- or c.1590\n",
      "\n",
      "Moreover, the missing values are denoted by a hyphen. For all these reasons, using a regex to extract the date seems to be appropriate.\n",
      "\n",
      "```python\n",
      "def date_extract(date) :\n",
      "    try :\n",
      "        return re.findall('\\d+', date)[0]\n",
      "    except :\n",
      "        return None\n",
      "\n",
      "catalog['DATE'] = catalog['DATE'].astype(str)\n",
      "catalog['DATE'] = catalog['DATE'].apply(lambda x : date_extract(x))\n",
      "```\n",
      "\n",
      "The time frame is redundant if the date is known. Including both variables would imply adding multi-colinearity in the data. \n",
      "\n",
      "```python\n",
      "catalog = catalog.drop(['TIMEFRAME'], axis=1)\n",
      "```\n",
      "\n",
      "## Technique\n",
      "\n",
      "The \"Technique\" is an interesting feature. It is a string that takes the following form:\n",
      "\n",
      "`Oil on copper, 56 x 47 cm`\n",
      "\n",
      "We can extract several elements from this feature:\n",
      "- The type of painting (oil on copper)\n",
      "- The height\n",
      "- The width\n",
      "\n",
      "We will only focus on paintings, and drop observations that are sculplures or architecture for example.\n",
      "\n",
      "```python\n",
      "catalog = catalog[catalog['FORM'] == 'painting']\n",
      "```\n",
      "\n",
      "We can apply several functions to extract the width and height:\n",
      "\n",
      "```python\n",
      "def height_extract(tech) :\n",
      "    try :\n",
      "        return re.findall('\\d+', tech.split(\" x \")[0])[0]\n",
      "    except :\n",
      "        return None\n",
      "\n",
      "def width_extract(tech) :\n",
      "    try :\n",
      "        return re.findall('\\d+', tech.split(\" x \")[1])[0]\n",
      "    except :\n",
      "        return None\n",
      "        \n",
      "catalog['HEIGHT'] = catalog['TECHNIQUE'].apply(lambda x : height_extract(x))\n",
      "catalog['WIDTH'] = catalog['TECHNIQUE'].apply(lambda x : width_extract(x))\n",
      "```\n",
      "\n",
      "## Width and height\n",
      "\n",
      "In some cases, the \"Technique\" feature does not contain the width nor the height. We might want to fill the missing values. It's not a good idea to fill it with 0's. To minimize the error, we'll set the missing values to the average of each feature.\n",
      "\n",
      "```python\n",
      "catalog['HEIGHT'] = catalog['HEIGHT'].fillna(0).astype(int)\n",
      "catalog['WIDTH'] = catalog['WIDTH'].fillna(0).astype(int)\n",
      "\n",
      "mean_height = sum(catalog[catalog['HEIGHT']>0]['HEIGHT'])/len(catalog[catalog['HEIGHT']>0]['HEIGHT'])\n",
      "mean_width = sum(catalog[catalog['WIDTH']>0]['WIDTH'])/len(catalog[catalog['WIDTH']>0]['WIDTH'])\n",
      "\n",
      "def treat_height(height) :\n",
      "    if height == 0 :\n",
      "        return mean_height\n",
      "    else : \n",
      "        return height\n",
      "\n",
      "def treat_width(width) :\n",
      "    if width == 0 :\n",
      "        return mean_width\n",
      "    else : \n",
      "        return width\n",
      "\n",
      "catalog['HEIGHT'] = catalog['HEIGHT'].apply(lambda x : treat_height(x))\n",
      "catalog['WIDTH'] = catalog['WIDTH'].apply(lambda x : treat_width(x))\n",
      "```\n",
      "\n",
      "## Missing values and useless columns\n",
      "\n",
      "As stated above, we won't exploit the birth nor death of the author, since it's an information that depends on the author.\n",
      "\n",
      "```python\n",
      "catalog = catalog.drop(['BORN-DIED'], axis=1)\n",
      "```\n",
      "\n",
      "At this point we can confidently drop any row that has missing values since the processing is almost over.\n",
      "\n",
      "```\n",
      "catalog = catalog.dropna()\n",
      "```\n",
      "\n",
      "There are many authors in the database (> 3500). To check this, simply run a values count on the author's feature.\n",
      "\n",
      "```python\n",
      "catalog['AUTHOR'].value_counts()\n",
      "```\n",
      "\n",
      "We will need a good number of training samples for each label for the algorithm to be applied. For this reason, all authors with less than 200 observations should be dropped. This is a major limitation in our simple model, but will give a better class balance later on.\n",
      "\n",
      "\n",
      "```python\n",
      "counts = catalog['AUTHOR'].value_counts()\n",
      "catalog = catalog[catalog['AUTHOR'].isin(counts.index[counts > 200])]\n",
      "catalog['AUTHOR'].value_counts()\n",
      "```\n",
      "\n",
      "```python\n",
      "GIOTTO di Bondone                 564\n",
      "GOGH, Vincent van                 332\n",
      "REMBRANDT Harmenszoon van Rijn    315\n",
      "RUBENS, Peter Paul                303\n",
      "RAFFAELLO Sanzio                  289\n",
      "TINTORETTO                        287\n",
      "MICHELANGELO Buonarroti           278\n",
      "CRANACH, Lucas the Elder          275\n",
      "TIZIANO Vecellio                  269\n",
      "VERONESE, Paolo                   266\n",
      "TIEPOLO, Giovanni Battista        249\n",
      "GRECO, El                         245\n",
      "ANGELICO, Fra                     242\n",
      "UNKNOWN MASTER, Italian           236\n",
      "MEMLING, Hans                     209\n",
      "BRUEGEL, Pieter the Elder         205\n",
      "```\n",
      "\n",
      "# A first model\n",
      "\n",
      "The aim of this exercise is to illustrate the need for a good feature engineering and additional data. We won't spend too much time on the optimization of the model itself and we will use a random forest classifier. A label encoding needs to be applied to transform the labels into numeric values that can be understood by our model.\n",
      "\n",
      "The accuracy of a model will be evaluated by the average of the cross validation with 5 folds.\n",
      "\n",
      "```python\n",
      "df = catalog.copy()\n",
      "df = df.drop(['TITLE', 'LOCATION', 'TECHNIQUE', 'URL'], axis=1)\n",
      "\n",
      "le = preprocessing.LabelEncoder()\n",
      "\n",
      "df['AUTHOR'] = le.fit_transform(df['AUTHOR'])\n",
      "df['FORM'] = le.fit_transform(df['FORM'])\n",
      "df['TYPE'] = le.fit_transform(df['TYPE'])\n",
      "df['SCHOOL'] = le.fit_transform(df['SCHOOL'])\n",
      "```\n",
      "\n",
      "```\n",
      "rf = RandomForestClassifier(n_estimators=500)\n",
      "y = df['AUTHOR']\n",
      "X = df.drop(['AUTHOR'], axis=1)\n",
      "\n",
      "cv = cross_val_score(rf, X, y, cv=5)\n",
      "print(cv)  \n",
      "print(np.mean(cv))\n",
      "```\n",
      "\n",
      "```\n",
      "[0.79623477 0.81818182 0.86399108 0.87150838 0.70594837]\n",
      "0.8111728850092941\n",
      "```\n",
      "\n",
      "The mean accuracy during our cross validation reaches 81.1% with our simple random forest model. We can also look at the confusion matrix.\n",
      "\n",
      "```python\n",
      "y_pred = cross_val_predict(rf, X, y, cv=5)\n",
      "conf_mat = confusion_matrix(y, y_pred)\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "sns.heatmap(conf_mat)\n",
      "plt.title(\"Confusion Matrix\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl_1.png)\n",
      "\n",
      "It's easy to understand that mistakes are made more frequently with latest authors, given that we have fewer observations for these.\n",
      "\n",
      "# More feature engineering\n",
      "\n",
      "## Technique\n",
      "\n",
      "Alright, we are now ready to move on and add other variables by improving the feature engineering. Looking at the \"Technique\" feature, you will notice that we have not used the \"type of painting\" variable yet. Indeed, only the width and height have been extracted from this field.\n",
      "\n",
      "The technique is systematically specified before the first comma. We will split the string on the first comma, if there is one, and then select the first word (oil, tempera, wood...).\n",
      "\n",
      "```python\n",
      "def process_tech(tech) :\n",
      "    tech = tech.split(\" \")[0]\n",
      "    try : \n",
      "        return tech.split(\",\")[0]\n",
      "    except :\n",
      "        return tech\n",
      "\n",
      "catalog['TECHNIQUE'] = catalog['TECHNIQUE'].apply(lambda x: process_tech(x))\n",
      "````\n",
      "\n",
      "## Location\n",
      "\n",
      "So far we have not exploited the location field either. The location describes where the painting is being kept. We only extract the name of the city from this field as extracting the name of the museum would lead to an overfitting. The collections of each museum are limited, and we only have at this point around 4'500 training samples.\n",
      "\n",
      "```python\n",
      "def process_loc(loc) :\n",
      "    return loc.split(\",\")[-1]\n",
      "    \n",
      "catalog['LOCATION'] = catalog['LOCATION'].apply(lambda x: process_loc(x))\n",
      "```\n",
      "\n",
      "## Second model\n",
      "\n",
      "After adding these two variables, we can test again the outcome on a cross validation.\n",
      "\n",
      "```python\n",
      "df = catalog.copy()\n",
      "df = df.drop(['TITLE', 'URL'], axis=1)\n",
      "\n",
      "le = preprocessing.LabelEncoder()\n",
      "\n",
      "df['AUTHOR'] = le.fit_transform(df['AUTHOR'])\n",
      "df['TECHNIQUE'] = le.fit_transform(df['TECHNIQUE'])\n",
      "df['FORM'] = le.fit_transform(df['FORM'])\n",
      "df['TYPE'] = le.fit_transform(df['TYPE'])\n",
      "df['SCHOOL'] = le.fit_transform(df['SCHOOL'])\n",
      "df['LOCATION'] = le.fit_transform(df['LOCATION'])\n",
      "df['TECH'] = le.fit_transform(df['TECH'])\n",
      "\n",
      "y = df['AUTHOR']\n",
      "X = df.drop(['AUTHOR'], axis=1)\n",
      "```\n",
      "\n",
      "Then, run the cross validation :\n",
      "\n",
      "\n",
      "```python\n",
      "cv = cross_val_score(rf, X, y, cv=5)\n",
      "print(cv)  \n",
      "print(np.mean(cv))\n",
      "```\n",
      "\n",
      "```\n",
      "[0.83277962 0.83037694 0.88963211 0.88938547 0.7620651 ]\n",
      "0.8408478481785021\n",
      "```\n",
      "\n",
      "And print the confusion matrix :\n",
      "\n",
      "```\n",
      "y_pred = cross_val_predict(rf, X, y, cv=5)\n",
      "conf_mat = confusion_matrix(y, y_pred)\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "sns.heatmap(conf_mat)\n",
      "plt.title(\"Confusion Matrix\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl_2.png)\n",
      "\n",
      "We have gained a significant accuracy by improving the feature engineering !\n",
      "\n",
      "## Process the title\n",
      "\n",
      "Can the processing of the title bring additional accuracy ? It might be interesting to :\n",
      "- Embed the title using a pre-trained model\n",
      "- Reduce the dimension of the embedding using a Principal Component Analysis (PCA)\n",
      "- Use the new dimensions as new features to predict the name of the painter\n",
      "\n",
      "To start, download pre-trained models from Spacy from your terminal :\n",
      "\n",
      "`python -m spacy download en_core_web_md`\n",
      "\n",
      "We will be using a pre-trained Word2Vec model and begin by defining the embedding function:\n",
      "\n",
      "```python\n",
      "def embed_txt(titles) :\n",
      "    list_mean = []\n",
      "\n",
      "    # For each title\n",
      "    for title in titles :\n",
      "    \n",
      "        # Tokenize the title\n",
      "        tokens = word_tokenize(title)\n",
      "        all_embedding = []\n",
      "        arr = np.empty((300,))\n",
      "\n",
      "        # Compute the embedding of each word\n",
      "        for token in tokens :\n",
      "            arr = np.append(arr, np.array(nlp(token).vector), axis=0)     \n",
      "        \n",
      "        # Compute the average embedding of the title\n",
      "        arr = arr.reshape(300, -1)\n",
      "        arr = np.nan_to_num(arr)\n",
      "        mean = np.mean(arr, axis = 1)\n",
      "        list_mean.append(mean)\n",
      "\n",
      "    return list_mean\n",
      "```\n",
      "\n",
      "We then apply our function to the list of titles :\n",
      "\n",
      "```python\n",
      "embedding = np.array(embed_txt(list(catalog['TITLE'])))\n",
      "```\n",
      "\n",
      "We will now reduce the dimension (300 currently) of the embedding to use it as features in our prediction. The Principal Component Analysis (PCA) is sensitive to scaling, and requires a scaling of the embedding values :\n",
      "\n",
      "```python\n",
      "scaler = MinMaxScaler(feature_range=[0, 1])\n",
      "data_rescaled = scaler.fit_transform(embedding)\n",
      "```\n",
      "\n",
      "We can apply the PCA on the rescaled data and see what percentage of the variance we are able to explain :\n",
      "\n",
      "```\n",
      "pca = PCA().fit(data_rescaled)\n",
      "#Plotting the Cumulative Summation of the Explained Variance\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
      "plt.xlabel('Number of Components')\n",
      "plt.ylabel('Variance (%)')\n",
      "plt.title('Explained Variance')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl_3.png)\n",
      "\n",
      "This is a tricky situation. Adding more dimensions seems to smoothly improve the percentage of the explained variance, up to 200 features. This might happen if the embeddings are too similar since the Word2Vec model has been trained on a corpus that uses a more general vocabulary, e.g. \"Scenes from the Life of Christ\" and \"Christ Blessing the Children\" will tend to have similar average embeddings.\n",
      "\n",
      "To confirm this thought, we can try to plot on a scatterplot the embeddings reduced to 2 dimensions by PCA.\n",
      "\n",
      "```python\n",
      "pca = PCA(2).fit_transform(data_rescaled)\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.scatter(pca[:,0], pca[:,1], s=0.3)\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl_4.png)\n",
      "\n",
      "There seems to be no real clustering effect, although a K-Means algorithm could probably detach 3-4 clusters.\n",
      "\n",
      "```python\n",
      "kmeans = KMeans(n_clusters=4, random_state=0).fit(pca)\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.scatter(pca[:,0], pca[:,1], c=kmeans.labels_, s=0.4)\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl_5.png)\n",
      "\n",
      "We might expect the new features derived from the embedding not to improve the overall accuracy. \n",
      "\n",
      "```python\n",
      "catalog_2 = pd.concat([catalog.reset_index(), pd.DataFrame(pca)], axis=1).drop(['index'], axis=1)\n",
      "df = catalog_2.copy()\n",
      "df = df.drop(['TITLE', 'URL'], axis=1)\n",
      "\n",
      "le = preprocessing.LabelEncoder()\n",
      "\n",
      "df['AUTHOR'] = le.fit_transform(df['AUTHOR'])\n",
      "df['TECHNIQUE'] = le.fit_transform(df['TECHNIQUE'])\n",
      "df['FORM'] = le.fit_transform(df['FORM'])\n",
      "df['TYPE'] = le.fit_transform(df['TYPE'])\n",
      "df['SCHOOL'] = le.fit_transform(df['SCHOOL'])\n",
      "df['LOCATION'] = le.fit_transform(df['LOCATION'])\n",
      "\n",
      "y = df['AUTHOR']\n",
      "X = df.drop(['AUTHOR'], axis=1)\n",
      "\n",
      "cv = cross_val_score(rf, X, y, cv=5)\n",
      "print(cv)  \n",
      "print(np.mean(cv))\n",
      "```\n",
      "\n",
      "```\n",
      "[0.82840237 0.84752475 0.8757515  0.85110664 0.75708502]\n",
      "0.8319740564854229\n",
      "```\n",
      "\n",
      "This is indeed the case. Then, should we include the title variable ? A cool feature of the random forest is to be able to apply a feature importance. By checking the feature importance, we notice how many node splits depend on values encountered on a given feature.\n",
      "\n",
      "```python\n",
      "rf.fit(X,y)\n",
      "importances = rf.feature_importances_\n",
      "\n",
      "std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n",
      "axis=0)\n",
      "indices = np.argsort(importances)\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.title(\"Feature importances\")\n",
      "plt.barh(X.columns.astype(str), importances[indices],\n",
      "color=\"r\", xerr=std[indices], align=\"center\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl_6.png)\n",
      "\n",
      "The 2 features extracted by the PCA on the embedding are the most important. Including them at that point might not be a good idea as we would need to fine-tune the Word2Vec embedding for our use case. A similar approach with a PCA on a Tf-Idf has been tested and has given similar results.\n",
      "\n",
      "> This highlights a major limitation in the dataset itself. This open source catolog focuses on European art between the $$ 3^{rd} $$ and the $$ 19^{th} $$ century, and maily includes religious art. Therefore, the titles, the pictures and certain characteristics are quite similar across artists. Pre-trained models require fine-tuning, and feature engineering needs to be done wisely. \n",
      "\n",
      "# Exploiting the images\n",
      "\n",
      "The URL column contains a link to download the images. By clicking on a link, we access the webpage of the painting.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl_8.png)\n",
      "\n",
      "If you click on the image, you can notice how the URL changes. We now have direct access to the image :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl_7.png)\n",
      "\n",
      "In this example, the URL just went from :\n",
      "`https://www.wga.hu/html/a/angelico/00/10fieso1.html`\n",
      "\n",
      "To : \n",
      "`https://www.wga.hu/art/a/angelico/00/10fieso1.jpg`\n",
      "\n",
      "All we need to do is process the URLs so they fit the second template.\n",
      "\n",
      "```python\n",
      "def process_url(url):\n",
      "    start = url.split(\"/html/\")[0]\n",
      "    end = url.split(\"/html/\")[1]\n",
      "    end_2 = end.split(\".html\")[0]\n",
      "    final_url = start + \"/art/\" + end_2 + \".jpg\"\n",
      "    return final_url\n",
      "    \n",
      "catalog['URL'] = catalog['URL'].apply(lambda x : process_url(x))\n",
      "```\n",
      "\n",
      "We are now ready to download all the images. First, create an empty folder called `images` and enter the following script to fetch images from the website directly:\n",
      "\n",
      "```python\n",
      "data = urllib.request.urlretrieve\n",
      "\n",
      "filename = \"images\"\n",
      "i = 0\n",
      "\n",
      "for line in catalog['URL'] :\n",
      "    urllib.request.urlretrieve(line, filename + \"/img_\" + str(i) + \".png\")\n",
      "    if i % 10 == 0 :\n",
      "        print(i)\n",
      "    i+=1\n",
      "```\n",
      "\n",
      "Depending on your WiFi and server response time, it might takes several minutes/hours to download the 4488 images. It might be a good idea to add a `time.sleep(1)` within the *for* loop to avoid errors. At this point, we are faced with the problem where each image has a different size and resolution. We need to scale down the images, and add margins in order to make them all look square.\n",
      "\n",
      "To further reduce the dimension we only use the greyscale version of the images :\n",
      "\n",
      "```python\n",
      "def rgb2gray(rgb):\n",
      "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n",
      "```\n",
      "\n",
      "Run this script to reduce the dimensions of the images to $$ 100 \\times 100 $$ and add margins if needed. We are using OpenCV's resize function in the loop :\n",
      "\n",
      "```python\n",
      "img = []\n",
      "i = 0\n",
      "desired_size = 100\n",
      "\n",
      "for filename in glob.glob('images/*.png'):\n",
      "\n",
      "    im = cv2.imread(filename)\n",
      "    old_size = im.shape[:2]\n",
      "\n",
      "    ratio = float(desired_size)/max(old_size)\n",
      "    new_size = tuple([int(x*ratio) for x in old_size])\n",
      "\n",
      "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
      "\n",
      "    delta_w = desired_size - new_size[1]\n",
      "    delta_h = desired_size - new_size[0]\n",
      "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
      "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
      "\n",
      "    color = [0, 0, 0]\n",
      "    new_img = rgb2gray(cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color))\n",
      "    img.append(new_img)\n",
      "\n",
      "    i += 1\n",
      "    if i % 100 == 0 :\n",
      "        print(i)\n",
      "        plt.imshow(new_img)\n",
      "        plt.show()\n",
      "        \n",
      "img = np.array(img)\n",
      "img = img.reshape(-1, 100*100)\n",
      "```\n",
      "\n",
      "The images have been reduced to a dimension of $$ 100 \\times 100 $$, but that's still 10'000 features to potentially include in the original dataset, and including a value pixel by pixel won't make much sense. PCA finds the eigenvectors of a covariance matrix with the highest eigenvalues. The eigenvectors are then used to project the data into a smaller dimension. PCA is commonly used for feature extraction.\n",
      "\n",
      "Many techniques of computer vision could be applied here but we will simply apply a PCA on the image itself.\n",
      "\n",
      "```python\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "images_scaled = StandardScaler().fit_transform(img)\n",
      "pca = PCA(n_components=1)\n",
      "pca_result = pca.fit_transform(images_scaled)\n",
      "```\n",
      "\n",
      "The number of components to extract has been tested empirically, and 1 component gave additional accuracy :\n",
      "\n",
      "```python\n",
      "catalog_4 = pd.concat([catalog.reset_index(), pd.DataFrame(pca_result)], axis=1).drop(['index'], axis=1)\n",
      "\n",
      "df = catalog_4.copy()\n",
      "df = df.drop(['TITLE', 'URL'], axis=1)\n",
      "\n",
      "le = preprocessing.LabelEncoder()\n",
      "\n",
      "df['AUTHOR'] = le.fit_transform(df['AUTHOR'])\n",
      "df['TECHNIQUE'] = le.fit_transform(df['TECHNIQUE'])\n",
      "df['FORM'] = le.fit_transform(df['FORM'])\n",
      "df['TYPE'] = le.fit_transform(df['TYPE'])\n",
      "df['SCHOOL'] = le.fit_transform(df['SCHOOL'])\n",
      "df['LOCATION'] = le.fit_transform(df['LOCATION'])\n",
      "\n",
      "y = df['AUTHOR']\n",
      "X = df.drop(['AUTHOR'], axis=1)\n",
      "\n",
      "cv = cross_val_score(rf, X, y, cv=5)\n",
      "print(cv)  \n",
      "print(np.mean(cv))\n",
      "```\n",
      "\n",
      "```\n",
      "[0.84939092 0.82926829 0.89632107 0.88826816 0.75757576]\n",
      "0.844164839215148\n",
      "```\n",
      "\n",
      "Half a percentage of accuracy is gained by adding the PCA of the image as a feature.\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "We can summarize by saying that this article shows how a good feature engineering and external data sources can improve the accuracy of a given model.\n",
      "\n",
      "| Model | Description | Accuracy |\n",
      "|---|---|---|\n",
      "| 1 |  Simple feature engineering | 0.81117 |\n",
      "| 2 |  Improved feature engineering | 0.84084 |\n",
      "| 3 |  Add embedding of the title | 0.83197 |\n",
      "| 4 |  Add PCA of the images | 0.84416 |\n",
      "\n",
      "> We improved the accuracy by up to 3.3%. There still is room for better models, deep learning pipelines, computer vision techniques and fine-tuned embedding techniques.\n",
      "\n",
      "---\n",
      "title: Who's the painter ?\n",
      "layout: post\n",
      "tags: [project]\n",
      "search: false\n",
      "---\n",
      "\n",
      "In this article, we will be using data from the Web Gallery of Art, a virtual museum and searchable database of European fine arts from the $$ 3^{rd} $$ to $$ 19^{th} $$ centuries. The gallery can be accessed  [here](https://www.wga.hu/index1.html).\n",
      "\n",
      "We will create an algorithm to predict the name of the painter based on an intial set of features of the painting, and then gradually including more and more, thus improving the feature engineering, and including pictures.\n",
      "\n",
      "Through this article, we will illustrate:\n",
      "- The importance of good feature engineering;\n",
      "- The importance of data enrichment; and\n",
      "- The impact this can have on accuracy \n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "Ready ? Let's get started !\n",
      "\n",
      "# The data\n",
      "\n",
      "To download the data, you can either :\n",
      "- click on [this link](https://www.wga.hu/database/download/data_xls.zip) to download the XLS file directly\n",
      "-  go to Database tab in the website, and click on the last link : *You can download the catalogue for studying or searching off-line*. Select the Excel format of 5.2 Mb. \n",
      "\n",
      "...\n",
      "\n",
      "I have published the [full article on Explorium's blog](https://www.explorium.ai/blog/whos-the-painter/)\n",
      "\n",
      "---\n",
      "title: Time Series Forecasting with Prophet\n",
      "layout: post\n",
      "tags: [statistics]\n",
      "subtitle : \"Time Series\"\n",
      "---\n",
      "\n",
      "Facebook recently open-sourced a time-series forecasting library called Prophet. It is incredibly simple to use, and the outcomes are worth mentioning.\n",
      "\n",
      "The data I'm using can be downloaded from [https://maelfabien.github.io/assets/files/file.csv](https://maelfabien.github.io/assets/files/file.csv).\n",
      "\n",
      "Start by importing the following packages :\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from fbprophet import Prophet\n",
      "```\n",
      "\n",
      "Then, load the data :\n",
      "```python\n",
      "df = pd.read_csv('file.csv', parse_dates=['date'], index_col='date')\n",
      "df.head()\n",
      "```\n",
      "\n",
      "| value | date |\n",
      "| 1991-07-01 |  3.526591 | \n",
      "| 1991-08-01 | 3.180891 | \n",
      "| 1991-09-01 | 3.252221 | \n",
      "| 1991-10-01 |  3.611003 | \n",
      "| 1991-11-01 |  3.565869 | \n",
      "\n",
      "We first need to rename the columns to match Prophet's needs :\n",
      "\n",
      "```python\n",
      "df = pd.DataFrame(df['value'].dropna()).reset_index().rename(columns={'date': 'ds', 'value': 'y'})\n",
      "```\n",
      "\n",
      "Then, declare and fit the model :\n",
      "\n",
      "```python\n",
      "model = Prophet(interval_width=0.95)\n",
      "model.fit(df)\n",
      "```\n",
      "\n",
      "Finally, make the forecast over 3 years here (36 months).\n",
      "\n",
      "```python\n",
      "forecast = model.make_future_dataframe(periods=36, freq='MS')\n",
      "forecast = model.predict(forecast)\n",
      "\n",
      "plt.figure(figsize=(18, 8))\n",
      "model.plot(forecast, xlabel = 'Date', ylabel = 'Consumption')\n",
      "plt.title('Index')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![images](https://maelfabien.github.io/assets/images/ts2_9.jpg)\n",
      "\n",
      "The forecast data frame contains all the predictions. \n",
      "\n",
      "Prophet even shows the decomposition of the series :\n",
      "\n",
      "```python\n",
      "model.plot_components(forecast);\n",
      "```\n",
      "\n",
      "![images](https://maelfabien.github.io/assets/images/ts2_10.jpg)\n",
      "\n",
      "I don't know if I would use such a tool in production, but it's definitely something interesting that you should consider in your Time Series analysis!\n",
      "\n",
      "---\n",
      "title: Using OpenPose on macOS\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "\"OpenPose represents the first real-time multi-person system to jointly detect human body, hand, facial, and foot key points (in total 135 keypoints) on single images.\"\n",
      "OpenPose is a game-changer in pose detection. This library is proposed by the Perceptual Computing Laboratory of the Carnegie Mellon University.\n",
      "\n",
      "OpenPose offers a Python as well as a C++ API. In June 2018, a CPU-only macOS support was released. This is what we'll try right now!\n",
      "This article is a practical approach to the OpenPose library. A second article will develop the theory behind the library. \n",
      "\n",
      "NB: OpenPose License clearly states that any commercial use in the domain of sports is prohibited. This simply is a toy example for personal use.\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "## Functionalities\n",
      "\n",
      "For detailed information, please check the GitHub repository of the [project](https://github.com/CMU-Perceptual-Computing-Lab/openpose).\n",
      "\n",
      "The most important features of OpenPose include :\n",
      "- 2D multi-person real-time keypoint detection (body, hand, foot, face)\n",
      "- 3D single-person real-time keypoint detection\n",
      "\n",
      "The input should be an image, a video, your webcam, or thermic/3D cameras...\n",
      "The output would typically be your basic image/video with key points on top of it, in different file formats (PNG, JPG, AVI...), or simply the key points (JSON, XML, YML...).\n",
      "\n",
      "OpenPose works under Ubuntu (14, 16), Windows (8, 10) and Mac OSX.\n",
      "\n",
      "So what are those keypoints? Well, nothing's better than a visual illustration.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/img1.jpg)\n",
      "\n",
      "The requirements regarding your Mac are the following :\n",
      "- Around 8GB of free RAM memory\n",
      "- a CPU with at least 8 cores\n",
      "\n",
      "## Installation\n",
      "The whole installation guide can be found [here](https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/installation.md).\n",
      "\n",
      "**Step 1)**\n",
      "Clone the repository in the target folder\n",
      "\n",
      "`git clone https://github.com/CMU-Perceptual-Computing-Lab/openpose`\n",
      "\n",
      "**Step 2)**\n",
      "Install CMake GUI :\n",
      "\n",
      "`https://cmake.org/download/`\n",
      "\n",
      "Or :\n",
      "`brew cask install cmake`\n",
      "\n",
      "**Step 3)**\n",
      "Install dependencies :\n",
      "\n",
      "`bash 3rdparty/osx/install_deps.sh`\n",
      "\n",
      "**Step 4)**\n",
      "Generate caffe.pb.h manually using protoc as follows. In the directory, you installed Caffe to\n",
      "\n",
      "`protoc src/caffe/proto/caffe.proto --cpp_out=.`\n",
      " ",
      "`mkdir include/caffe/proto`\n",
      " ",
      "`mv src/caffe/proto/caffe.pb.h include/caffe/proto`\n",
      "\n",
      "\n",
      "See [this issue](https://github.com/BVLC/caffe/issues/1761) for more details.\n",
      "\n",
      "**Step 5)**\n",
      "Open CMake GUI and select the OpenPose directory as project source directory, and a non-existing or empty sub-directory (e.g., build) where the Makefile files (Ubuntu) or Visual Studio solution (Windows) will be generated. If the build does not exist, it will ask you whether to create it. Press Yes. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/img2.jpg)\n",
      "\n",
      "**Step 6)**\n",
      "Make the following adjustments to the CMake config before pressing \"Configure\":\n",
      "\n",
      "```BUILD_CAFFE``` set to false\n",
      "\n",
      "```GPU_MODE set to CPU_ONLY``` (as recommended for MacOS)\n",
      "\n",
      "```Caffe_INCLUDE_DIRS``` set to ```/usr/local/include/caffe```\n",
      "\n",
      "```Caffe_LIBS``` set to ```/usr/local/lib/libcaffe.dylib```\n",
      "\n",
      "See [this issue](https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/677) for more details.\n",
      "\n",
      "**Step 7)**\n",
      "Press configure, wait until ```Configuring Done``` appears. Everything should work well, and you should be able to click the ```Generate``` right after.\n",
      "If ever you get an error here, especially with High Sierra, please check [this issue](https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/809).\n",
      "\n",
      "**Step 8)**\n",
      "Build the project :\n",
      "\n",
      "```cd build/```\n",
      "\n",
      "```make -j`nproc` ```\n",
      "\n",
      "## Run OpenPose\n",
      "This section refers to the [Quick Start](https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/quick_start.md) section of the GitHub of the project.\n",
      "\n",
      "**Run on videos:**\n",
      "\n",
      "*Ubuntu and Mac*\n",
      "`./build/examples/openpose/openpose.bin --video examples/media/video.avi`\n",
      "\n",
      "*With face and hands*\n",
      "`./build/examples/openpose/openpose.bin --video examples/media/video.avi --face --hand`\n",
      "\n",
      "**Running on webcam:**\n",
      "\n",
      "*Ubuntu and Mac*\n",
      "`./build/examples/openpose/openpose.bin`\n",
      "\n",
      "*With face and hands*\n",
      "`./build/examples/openpose/openpose.bin --face —hand`\n",
      "\n",
      "**Running on images :**\n",
      "\n",
      "*Ubuntu and Mac*\n",
      "`./build/examples/openpose/openpose.bin --image_dir examples/media/`\n",
      "\n",
      "*With face and hands*\n",
      "`./build/examples/openpose/openpose.bin --image_dir examples/media/ --face --hand`\n",
      "\n",
      "Great, this should work fine. Try to insert a new image in your media folder and run it on your image or video.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/img3.jpg)\n",
      "\n",
      "How to save the output?\n",
      "[Several options](https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/demo_overview.md) to save the outputs exist. \n",
      "\n",
      "For example, to save a video as a .avi file, use :\n",
      "\n",
      "```./build/examples/openpose/openpose.bin --video examples/media/film.avi --write_video out.avi --write_video_fps 5```\n",
      "\n",
      "Notice that the frames per second specified here will have a dramatic impact on the length of the algorithm execution.\n",
      "For images, you can directly use ```--write_image```\n",
      "\n",
      "To save the keypoints positions as X,Y coordinates, use :\n",
      "\n",
      "```./build/examples/openpose/openpose.bin --video examples/media/film-short.avi --write_json output/ --display 0 --render_pose 0```\n",
      "\n",
      "The ```--display 0 --render_pose 0``` allows the algorithm to run without the video popup. \n",
      "\n",
      "It's time to check the final output of our work!\n",
      "\n",
      "{% include video id=\"ZreEaLSgQcc\" provider=\"youtube\" %}\n",
      "\n",
      "> **Conclusion **: I hope you enjoyed this quick tutorial on OpenPose for macOS. I am looking forward to making a more developed article on the field of pose recognition!\n",
      "\n",
      "---\n",
      "title: Using Google Drive to store your data on Colab\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Google Cloud Platform\"\n",
      "---\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gc.jpg)\n",
      "\n",
      "In this quick tutorial, we'll see how to use Google Drive as a file storage system when working in Google Colab.\n",
      "\n",
      "## Mount Google Drive\n",
      "\n",
      "The first step is to mount Google Drive on your Colab session :\n",
      "\n",
      "```python\n",
      "from google.colab import drive\n",
      "drive.mount('/content/drive')\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gd_init.jpg)\n",
      "\n",
      "Once you run the cell, a link is provided, similar to :\n",
      "```Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=XXX```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gd_connect.jpg)\n",
      "\n",
      "Open the URL. You should now see a page asking for access to the content of your Drive. Allow access.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gd_allow.jpg)\n",
      "\n",
      "Now, simply copy the code given.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gd_copy.jpg)\n",
      "\n",
      "Paste it in your notebook :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gd_valid.jpg)\n",
      "\n",
      "## Check the content of your drive\n",
      "\n",
      "Your drive is accessible through the folder  `drive/My Drive`. To check the content of this folder, run the following cell in your notebook :\n",
      "\n",
      "```\n",
      "!ls \"drive/My Drive\"\n",
      "```\n",
      "\n",
      "## Load a file from your drive\n",
      "\n",
      "Your drive can now be used as a local folder ! For example :\n",
      "\n",
      "```\n",
      "X_train = np.load('drive/My Drive/X_train.npy')\n",
      "```\n",
      "\n",
      "Similarly, you can save your files as you would do locally.\n",
      "\n",
      "> **Conclusion** : I hope this quick tip on Google Drive on Colab was helpful. If you have any question, don't hesitate to drop a comment!\n",
      "\n",
      "---\n",
      "title: Gradient Boosting Classification\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Supervised Learning Algorithms\"\n",
      "---\n",
      "\n",
      "In the previous article, we covered the Gradient Boosting Regression. In this article, we'll get into the Gradient Boosting Classification.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "Let's consider a simple scenario in which we have several features, $$ x_1, x_2, x_3, x_4 $$ and try to predict $$ y $$, a binary output. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/tab_8.jpg)\n",
      "\n",
      "# Gradient Boosting Classification steps\n",
      "\n",
      "**Step 1** : Make the first guess\n",
      "\n",
      "The initial guess of the Gradient Boosting algorithm is to *predict the log of the odds of the target $$ y $$*, the equivalent of the average for the logistic regression. \n",
      "\n",
      "$$ odds = log( \\frac {P(Y=1)} {P(Y=0)} ) = log( \\frac {3} {1} ) = log(3) $$\n",
      "\n",
      "How is this ratio used to make a classification? We apply a softmax transformation!\n",
      "\n",
      "$$ P(Y=1) = \\frac {e^{odds}} {1 + e^{odds}} = \\frac {3} {4} = 0.75 $$\n",
      "\n",
      "If this probability is greater than 0.5, we classify as 1. Else, we classify as 0.\n",
      "\n",
      "**Step 2** : Compute the pseudo-residuals\n",
      "\n",
      "For the variable $$ x_1 $$, we compute the difference between the observations and the prediction we made. This is called the pseudo-residuals.\n",
      "\n",
      "We have now predicted a value for every sample, the same value for all of them. The next step is to compute the residuals :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/tab_9.jpg)\n",
      "\n",
      "**Step 3** : Predict the pseudo-residuals\n",
      "\n",
      "As previously, we use the features 1 to 3 to predict the residuals. Suppose that we build the classification tree to predict the output value of the tree :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/tab_10.jpg)\n",
      "\n",
      "In that case, we cannot use the output of a leaf (or the average output if we have more observations) as the predicted value, since we applied a transformation initiative.\n",
      "\n",
      "We need to apply another transformation :\n",
      "\n",
      "$$ \\gamma_{i+1} = \\frac { \\sum_i Residuals_i } { \\sum(\\gamma_i \\times (1-\\gamma_i))} $$\n",
      "\n",
      "For example, take a case in which we have 1 more observation that falls into a leaf :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/tab_11.jpg)\n",
      "\n",
      "In that case, the output value of the branch that contains 0.25 and -0.75 is :\n",
      "\n",
      "$$ \\frac {0.25 - 0.75} { 0.75 * (1-0.75) + 0.75*(1-0.75)} = -1.33 $$\n",
      "\n",
      "\n",
      "**Step 4** : Make a prediction and compute the residuals\n",
      "\n",
      "We can now compute the new prediction :\n",
      "\n",
      "$$ y_{pred} = odds + lr \\times y_{res} = log(3) + 0.1 * -1.33 = 0.9656 $$\n",
      "\n",
      "We can now convert the new log odds prediction into a probability using the softmax function :\n",
      "\n",
      "$$ P(Y=1) = \\frac {e^{0.9656}} {1 + e^{0.9656}} = 0.7242 $$\n",
      "\n",
      "The probability diminishes compared to before since we had 1 well classified and 1 incorrectly classified sample in this leaf.\n",
      "\n",
      "**Step 5** : Make a second prediction\n",
      "\n",
      "Now, we :\n",
      "- build a second tree\n",
      "- compute the prediction using this second tree\n",
      "- compute the residuals according to the prediction\n",
      "- build the third tree\n",
      "- ...\n",
      "\n",
      "As before, we compute the prediction using : \n",
      "\n",
      "$$ y_{pred} = odds + lr \\times y_{res}  + lr \\times y_{res_2} + lr \\times y_{res_3} + lr \\times y_{res_4} + ... $$\n",
      "\n",
      "And classifiy using :\n",
      "\n",
      "$$ P(Y=1) = \\frac {e^{y_{pred}}} {1 + e^{y_{pred}}} $$\n",
      "\n",
      "> **Conclusion** : I hope this introduction to Gradient Boosting Classification was helpful. The topic can get much more complex over time, and the implementation is Scikit-learn is much more complex than this. In the next article, we'll cover the topic of classification.\n",
      "---\n",
      "title: Multilayer Perceptron\n",
      "layout: post\n",
      "tags: [deeplearning]\n",
      "subtitle : \"Deep Neural Networks\"\n",
      "---\n",
      "\n",
      "Now we have covered the concept on Perceptron, it is time to move on to the so-called Multilayer Perceptron (MLP)\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "## Definition \n",
      "\n",
      "A multilayer perceptron is a feedforward artificial neural network (ANN), made of an input layer, one or several hidden layers, and an output layer.\n",
      "\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/mlp.jpg){:height=\"50%\" width=\"50%\"}\n",
      "\n",
      "\n",
      "The phase of \"learning\" for a multilayer perceptron is reduced to determining for each relationship between each neuron of two consecutive layers :\n",
      "- the weights  $$ w_i $$\n",
      "- the biases $$ b_i $$\n",
      "\n",
      "One major difference with the Rosenblatt perceptron is that the structure, due to the multiplicity of (hidden) layers, is no longer linear and allows us to model more complex data structures.\n",
      "\n",
      "## Maximum Likelihood Estimate (MLE)\n",
      "Such values are determined using the concept of maximum likelihood estimations (MLE). The maximum likelihood estimate  $$ \\hat{\\theta}^{MLE} $$ determines the parameters which maximize the probability of observing $$ \\theta $$ For classification problems, the maximum likelihood optimization can be expressed as follows :\n",
      "\n",
      "$$ \\hat{\\theta}^{MLE} = {argmax}_\\theta P(Y|X, \\theta) $$\n",
      "\n",
      "where $$ \\theta =[W^{(l)}, b^{(l)}] $$. We suppose that we have $$ n $$ training samples, and that each sample has the same number of classes  $$ k $$.\n",
      "\n",
      "Assuming that the $$ Y_n $$ are identically and independently distributed (iid), which will in most cases be the assumption, we know that : \n",
      "\n",
      "$$ Y_n ~^{iid} P(Y | X, \\theta) $$\n",
      "\n",
      "Therefore :\n",
      "\n",
      "$$ \\hat{\\theta}^{MLE} = {argmax}_\\theta \\prod P(Y_n|X_n, \\theta) $$\n",
      "\n",
      "\n",
      "In the context of classification, labels will follow a Bernouilli distribution. We can state that :\n",
      "\n",
      "$$ \\hat{\\theta}^{MLE} = {argmax}_\\theta \\prod_n \\prod_k P(Y_n=k|X_{n}, \\theta)^{t_{n,k}} $$\n",
      "\n",
      "where $$ t_{n,k} $$ is a one-hot encoded version of our labels $$ Y $$. As a sample always belong to a class, we know that :\n",
      "\n",
      "$$ \\sum_k P(Y_n=k|X_{n}, \\theta) = 1 $$\n",
      "\n",
      "We can focus on the log-likelihood to make the computations easier and work with sums instead of products :\n",
      "\n",
      "$$ \\hat{\\theta}^{MLE} = {argmax}_\\theta \\sum_n \\sum_k t_{n,k} log(P(Y_n=k|X_{n}, \\theta)) $$\n",
      "\n",
      "When dealing with classification problems, we want to minimize the classification error, which is expressed as the negative log-likelihood (NLL) :\n",
      "\n",
      "$$ NLL(\\theta) = -l(\\theta) = - \\sum_n \\sum_k t_{n,k} log(P(Y_n=k|X_{n}, \\theta)) $$\n",
      "\n",
      "For a single sample, the measure is called the cross-entropy loss :\n",
      "\n",
      "$$ NLL(\\theta) = -  \\sum_k t_{n,k} log(P(Y_n=k|X_{n}, \\theta)) $$\n",
      "\n",
      "We can indeed prove the equivalence between minimizing the loss and maximizing the likelihood. \n",
      "We need to find \\(\\theta^{MLE}\\) that minimizes this loss. Note that the MLE produces an unbiaised estimator, i.e $$ E(\\hat{\\theta}^{MLE}) = \\theta^* $$\n",
      "\n",
      "As we do not have access to the potential entire data set, but to a sample only, we minimize the cost (the sum of the losses) given the concept of Empirical Risk Minimization (ERM).\n",
      "\n",
      "## Activation functions \n",
      "\n",
      "Activation functions are essential to build a multilayer perceptron. They bring non-linearity in the structure of MLPs. There are several activation functions that can be used. The activation functions can vary depending on the layer we consider. The most popular activation functions are the following :\n",
      "- sigmoid : $$ {\\sigma(z)} = \\frac {1}{1 + e^{-z}} $$\n",
      "- hyperbolic tangent : $$ {g(z)} = \\frac {e^{z} - e^{-z}}{e^{z} + e^{-z}} $$\n",
      "- rectified linear unit (ReLU) : $$ {g(z)} = max(0,z) $$\n",
      "- leaky ReLU : $$ {g(z)} = max(0.01z,z) $$\n",
      "- parametric ReLU (PReLU) : $$ {g(z)} = max(\\alpha z,z) $$\n",
      "- softplus function : $$ {g(z)} = log(1 + e^z) $$\n",
      "\n",
      "In practice, for our face emotion recognition problem, we tend to use the ReLU activation function. Indeed, the ReLU is not subject to the vanishing gradient problem. It has issues dealing with negative inputs, but since our input takes the form of arrays describing colors of pixels, we only have positive inputs.\n",
      "\n",
      "These activations functions are commonly used on the hidden layers. For the output layer, we commonly use another activation function called the $$ softmax $$ activation. The $$ softmax $$ allows a transformation of the output layer into probabilities for each class. For classification purposes, we then only select the maximal probability as the class computed.\n",
      "\n",
      "$$ {g(z)_j} = \\frac {e^{z_j}}{\\sum_k e^{z_k}} $$\n",
      "\n",
      "## Gradient descent\n",
      "There are two ways to solve this minimization problem :\n",
      "- using traditional calculus, but this typically produces long and un-intuitive equations\n",
      "- using stochastic gradient descent\n",
      "\n",
      "Stochastic gradient descent offers a two-step gradient descent (forward propagation, and backpropagation) that allows us to approach the solution in a computationally efficient way. The stochastic gradient descent takes each observation individually and computes a gradient descent. Then, the overall gradient is averaged. On the other hand, the batch gradient descent needs to wait until the optimal gradient has been computed on all training observations.\n",
      "\n",
      "The forward propagation consists of applying a set of weights, bias and activation functions to an input layer to compute the output. Then, while learning from our classification errors, we update the weights by moving backward. This is the backpropagation.\n",
      "\n",
      "We won't cover the details of the back propagation, but for the intuition, it is enough to state that the concept of back propagation is a simple application of the chain rule that allows :\n",
      "\n",
      "$$ \\frac {\\partial}{\\partial t} f(x(t)) = \\frac {\\partial f} {\\partial x} \\frac {\\partial x} {\\partial t} $$\n",
      "\n",
      "In practice, the stochastic gradient descent is however rarely used. Some improved alternatives are preferred, including :\n",
      "- Mini batch gradient descent, which performs an update for every mini-batch of \\(n\\) training examples, but cannot guarantee good convergence due to the choice of the learning rate\n",
      "- Momentum, a method that helps accelerate stochastic gradient descent in the relevant direction and dampens oscillations by adding a fraction of the past time step to the current one\n",
      "- Adaptive Moment Estimation (Adam), that computes adaptive learning rates for each parameter.\n",
      "\n",
      "## Implementation in Tensorflow\n",
      "\n",
      "```python\n",
      "def init_weights_and_biases(shape, stddev=0.1, seed_in=None):\n",
      "\"\"\"\n",
      "This function should return Tensorflow Variables containing the initialized weights and biases of the network,\n",
      "using a normal distribution for the initialization, with stddev of the normal as an input argument\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "shape : tuple, (n_input_features,n_hidden_1, n_hidden_2,n_classes)\n",
      "sizes necessary for defining the weights and biases\n",
      "\n",
      "Returns\n",
      "-------\n",
      "w1, b1, w2, b2, w3, b3 : Tensorflow Variables\n",
      "initialized weights and biases, with correct shapes\n",
      "\"\"\"\n",
      "\n",
      "w1 = tf.Variable(tf.random_normal([shape[0], shape[1]], stddev=stddev), name='w1')\n",
      "b1 = tf.Variable(tf.random_normal([shape[1]]), name='b1')\n",
      "\n",
      "w2 = tf.Variable(tf.random_normal([shape[1], shape[2]], stddev=stddev), name='w2')\n",
      "b2 = tf.Variable(tf.random_normal([shape[2]]), name='b2')\n",
      "\n",
      "w3 = tf.Variable(tf.random_normal([shape[2], shape[3]], stddev=stddev), name='w3')\n",
      "b3 = tf.Variable(tf.random_normal([shape[3]]), name='b3')\n",
      "\n",
      "return w1, b1, w2, b2, w3, b3\n",
      "\n",
      "def forward_prop_multi_layer(X, w1, b1, w2, b2, w3, b3):\n",
      "\"\"\"\n",
      "This function should define the network architecture, explained above\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "X: input to the network\n",
      "w1, w2, w3: Tensorflow Variables\n",
      "network weights\n",
      "b1, b2, b3: Tensorflow Variables\n",
      "network biases \n",
      "\n",
      "Returns\n",
      "-------\n",
      "Y_pred :\n",
      "the output layer of the network, the classification prediction\n",
      "\"\"\"\n",
      "\n",
      "hidden_1 = tf.nn.sigmoid(tf.add(tf.matmul(X,w1), b1))\n",
      "hidden_2 = tf.nn.sigmoid(tf.add(tf.matmul(hidden_1,w2), b2))\n",
      "Y_pred = tf.nn.softmax(tf.add(tf.matmul(hidden_2,w3), b3))\n",
      "\n",
      "return Y_pred\n",
      "```\n",
      "\n",
      "```python\n",
      "RANDOM_SEED = 52\n",
      "tf.set_random_seed(RANDOM_SEED)\n",
      "\n",
      "# Network Parameters\n",
      "n_hidden_1 = 256 # 1st layer number of neurons\n",
      "n_hidden_2 = 256 # 2nd layer number of neurons\n",
      "n_input = X_train.shape[1]\n",
      "n_classes = Y_train.shape[1] # MNIST total classes (0-9 digits)\n",
      "\n",
      "# tf Graph input\n",
      "X_input = tf.placeholder(\"float\", [None, n_input])\n",
      "Y_true = tf.placeholder(\"float\", [None, n_classes])\n",
      "\n",
      "# Weight and bias initialisations\n",
      "stddev = 0.1\n",
      "w1,b1,w2,b2,w3,b3 = init_weights_and_biases([n_input, n_hidden_1, n_hidden_2,n_classes], stddev=0.1, seed_in=RANDOM_SEED)\n",
      "\n",
      "# Construct model\n",
      "Y_pred = forward_prop_multi_layer(X_input,w1,b1,w2,b2,w3,b3)\n",
      "\n",
      "# Define loss and optimizer\n",
      "cross_entropy = -tf.reduce_sum(Y_true * tf.log(Y_pred),axis=1)\n",
      "\n",
      "loss = tf.reduce_mean(cross_entropy)\n",
      "acc = accuracy(Y_pred, Y_true)\n",
      "\n",
      "learning_rate = 0.001\n",
      "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
      "training_variables = optimizer.minimize(loss)\n",
      "\n",
      "# Parameters\n",
      "n_epochs = 20\n",
      "train_accuracy = []\n",
      "test_accuracy = []\n",
      "batch_size = 100\n",
      "display_step = 1\n",
      "n_batches = int(np.ceil(X_train.shape[0]/batch_size))\n",
      "\n",
      "with tf.Session() as sess:\n",
      "# Initializing the variables\n",
      "init = tf.global_variables_initializer()\n",
      "sess.run(init)\n",
      "\n",
      "for epoch in range(n_epochs):\n",
      "# Loop over all batches\n",
      "for batch_idx in range(n_batches):\n",
      "#get the next batch in the MNIST dataset and carry out training\n",
      "#BEGIN STUDENT CODE\n",
      "batch_x = X_train[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
      "batch_y = Y_train[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
      "sess.run(training_variables, feed_dict={X_input:batch_x, Y_true:batch_y})\n",
      "\n",
      "#END STUDENT CODE\n",
      "# calculate accuracy for this epoch\n",
      "train_accuracy.append(sess.run(acc, feed_dict={X_input: X_train,Y_true:Y_train}))\n",
      "test_accuracy.append(sess.run(acc,  feed_dict={X_input: X_test,Y_true:Y_test}))\n",
      "\n",
      "print(\".\", end='')\n",
      "print(\"Training finished\")\n",
      "\n",
      "#plot the accuracy\n",
      "plot_accuracy(train_accuracy,test_accuracy)\n",
      "```\n",
      "\n",
      "The Github repository of this article can be found [here](https://github.com/maelfabien/Machine_Learning_Tutorials).\n",
      "\n",
      "> **Conclusion**: The MLP is the base model for several other deep learning algorithms (CNN, RNN...). These algorithms typically find applications in the field of natural language processing, computer vision, signal processing... We will get more into details into further articles!\n",
      "\n",
      "---\n",
      "title: What to expect from this training?\n",
      "layout: post\n",
      "tags: [da_tuto]\n",
      "subtitle : \"Before we start\"\n",
      "---\n",
      "\n",
      "***Private lessons***: *If you want to set up a private lesson (a few hours per week) to cover this content and apply it to your domain of interest, please [contact me](mailto:mael.fabien@gmail.com).*\n",
      "\n",
      "The training is organized the following way:\n",
      "- there are several courses, each made of several articles\n",
      "- each article contains explanation, code and images\n",
      "- the training is free, but you are invited to leave a comment under the articles to give feedback and allow me to improve its quality over time\n",
      "- when you are done, please comment on the last article how long it took you to go through all the material (working hours vs. time between first and last article)\n",
      "\n",
      "In this training, you'll learn:\n",
      "- What is a terminal and how to use bash commands\n",
      "- How to install Python\n",
      "- How to code in Python (IDEs, Jupyter Notebooks...)\n",
      "- How to write functions and classes in Python\n",
      "- How to share your code on Github\n",
      "- How to install packages from Github or Pypy\n",
      "- How to load and analyze data in Python\n",
      "- How to visualize data in Python\n",
      "- How to make interactive visualizations in Python\n",
      "- How to build your own dashboard\n",
      "- Key concepts of machine learning\n",
      "\n",
      "The articles offer a theoretical approach. In order to gain the necessary skills, you should complete additional exercises and code on your own.\n",
      "\n",
      "There is a final project proposed. This will serve as a certification. You are invited to send me your final project (details in the last article), and after a review, if concluant, you will be awarded a certificate.\n",
      "---\n",
      "title: Build an ETL in Scala for GDELT Data\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"GDelt Project\"\n",
      "---\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/scala.jpg)\n",
      "\n",
      "This part aims to present a simple yet effective ETL for GDELT data processing in Scala. \n",
      "\n",
      "The analysis will be made on a Zeppelin Notebook using some built-in tools. If you have some questions regarding this part, please refer to my article on how to launch <span style=\"color:blue\">[Zeppelin Notebooks](https://maelfabien.github.io/bigdata/zeppelin_emr/)</span>.\n",
      "\n",
      "# Download the data\n",
      "\n",
      "Once you're in your Zeppelin Notebook, import some useful functions :\n",
      "\n",
      "```scala\n",
      "// Imports\n",
      "import sys.process._\n",
      "import java.net.URL\n",
      "import java.io.File\n",
      "import java.io.File\n",
      "import java.nio.file.{Files, StandardCopyOption}\n",
      "import java.net.HttpURLConnection \n",
      "import org.apache.spark.sql.functions._\n",
      "import sqlContext.implicits._\n",
      "import org.apache.spark.input.PortableDataStream\n",
      "import java.util.zip.ZipInputStream\n",
      "import java.io.BufferedReader\n",
      "import java.io.InputStreamReader\n",
      "import org.apache.spark.sql.SQLContext\n",
      "import com.amazonaws.services.s3.AmazonS3Client\n",
      "import com.amazonaws.auth.BasicAWSCredentials\n",
      "import org.apache.spark.sql.cassandra._\n",
      "import com.datastax.spark.connector._\n",
      "import org.apache.spark.sql.types.IntegerType\n",
      "```\n",
      "\n",
      "The GDELT data set is quite special. As explained in a previous article, Zipped CSV files are uploaded every 15 minutes and record key events in the world based essentially on articles. The first step is to grab the list of the CSV files and to put those files in an S3 bucket. \n",
      "\n",
      "We can define a file downloader function to load the data set later on :\n",
      "```scala\n",
      "def fileDownloader(urlOfFileToDownload: String, fileName: String) = {\n",
      "    val url = new URL(urlOfFileToDownload)\n",
      "    val connection = url.openConnection().asInstanceOf[HttpURLConnection]\n",
      "    connection.setConnectTimeout(5000)\n",
      "    connection.setReadTimeout(5000)\n",
      "    connection.connect()\n",
      "\n",
      "    if (connection.getResponseCode >= 400)\n",
      "        println(\"error\")\n",
      "    else\n",
      "        url #> new File(fileName) !!\n",
      "}\n",
      "```\n",
      "\n",
      "There are 2 lists of files to download, one in English, and one international one.\n",
      "\n",
      "```scala\n",
      "fileDownloader(\"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\", \"/tmp/masterfilelist.txt\") // save the list file to the Spark Master\n",
      "fileDownloader(\"http://data.gdeltproject.org/gdeltv2/masterfilelist-translation.txt\", \"/tmp/masterfilelist_translation.txt\") //same for Translation file\n",
      "```\n",
      "\n",
      "You might have to configure your AWS client service first. Once this is done, put those 2 lists in your S3 bucket :\n",
      "```scala\n",
      "awsClient.putObject(\"mys3bucket\", \"masterfilelist.txt\", new File(\"/tmp/masterfilelist.txt\") )\n",
      "awsClient.putObject(\"mys3bucket\", \"masterfilelist_translation.txt\", new File( \"/tmp/masterfilelist_translation.txt\") )\n",
      "```\n",
      "\n",
      "Then, load the list of all the files from the year 2018 :\n",
      "```scala\n",
      "// English Data\n",
      "val list_csv = spark.read.format(\"csv\").option(\"delimiter\", \" \").\n",
      "    csv(\"s3a://fabien-mael-telecom-gdelt2018/masterfilelist.txt\").\n",
      "    withColumnRenamed(\"_c0\",\"size\").\n",
      "    withColumnRenamed(\"_c1\",\"hash\").\n",
      "    withColumnRenamed(\"_c2\",\"url\")\n",
      "val list_2018_tot = list_csv.where(col(\"url\").like(\"%/2018%\"))\n",
      "```\n",
      "\n",
      "And download them all!\n",
      "\n",
      "```scala\n",
      "list_2018_tot.select(\"url\").repartition(100).foreach( r=> {\n",
      "    val URL = r.getAs[String](0)\n",
      "    val fileName = r.getAs[String](0).split(\"/\").last\n",
      "    val dir = \"/mnt/tmp/\"\n",
      "    val localFileName = dir + fileName\n",
      "    fileDownloader(URL,  localFileName)\n",
      "    val localFile = new File(localFileName)\n",
      "    AwsClient.s3.putObject(\"mys3bucket\", fileName, localFile )\n",
      "    localFile.delete()\n",
      "})\n",
      "```\n",
      "\n",
      "We can replicate this for the translated data set :\n",
      "```scala\n",
      "val list_csv_translation = spark.read.format(\"csv\").option(\"delimiter\", \" \").\n",
      "    csv(\"s3a://fabien-mael-telecom-gdelt2018/masterfilelist_translation.txt\").\n",
      "    withColumnRenamed(\"_c0\",\"size\").\n",
      "    withColumnRenamed(\"_c1\",\"hash\").\n",
      "    withColumnRenamed(\"_c2\",\"url\")\n",
      "val list_2018_translation_tot = list_csv_translation.where(col(\"url\").like(\"%/2018%\"))\n",
      "\n",
      "list_2018_translation_tot.select(\"url\").repartition(100).foreach( r=> {\n",
      "    val URL = r.getAs[String](0)\n",
      "    val fileName = r.getAs[String](0).split(\"/\").last\n",
      "    val dir = \"/mnt/tmp/\"\n",
      "    val localFileName = dir + fileName\n",
      "    fileDownloader(URL,  localFileName)\n",
      "    val localFile = new File(localFileName)\n",
      "    AwsClient.s3.putObject(\"fabien-mael-telecom-gdelt2018\", fileName, localFile )\n",
      "    localFile.delete()\n",
      "\n",
      "})\n",
      "```\n",
      "\n",
      "# Create the data frames\n",
      "\n",
      "The two major tables from the GDELT data set are Mentions and Export. All the files are stored zipped CSV files. Part of this pipeline is dedicated to unzip data.\n",
      "\n",
      "```scala\n",
      "// Export English\n",
      "val exportRDD = sc.binaryFiles(\"s3a://mys3bucket/201801*.export.CSV.zip\"). // Use Regex to load some files from 1st month\n",
      "    flatMap {  // unzip files\n",
      "        case (name: String, content: PortableDataStream) =>\n",
      "            val zis = new ZipInputStream(content.open)\n",
      "            Stream.continually(zis.getNextEntry).\n",
      "                takeWhile{ case null => zis.close(); false\n",
      "                    case _ => true }.\n",
      "                flatMap { _ =>\n",
      "                    val br = new BufferedReader(new InputStreamReader(zis))\n",
      "                    Stream.continually(br.readLine()).takeWhile(_ != null)\n",
      "                }\n",
      "    }\n",
      "val exportDF = exportRDD.map(x => x.split(\"\\t\")).map(row => row.mkString(\";\")).map(x => x.split(\";\")).toDF()\n",
      "```\n",
      "\n",
      "We have built an RDD for the first month of 2018 (one can use the whole year too). We can replicate this for the Mentions tables too, and also for the translated table.\n",
      "\n",
      "```scala\n",
      "\n",
      "// Mentions English\n",
      "val mentionsRDD = sc.binaryFiles(\"s3a://mys3bucket/201801*.mentions.CSV.zip\").\n",
      "    flatMap {  // unzip files\n",
      "        case (name: String, content: PortableDataStream) =>\n",
      "            val zis = new ZipInputStream(content.open)\n",
      "            Stream.continually(zis.getNextEntry).\n",
      "                takeWhile{ case null => zis.close(); false\n",
      "                    case _ => true }.\n",
      "                flatMap { _ =>\n",
      "                    val br = new BufferedReader(new InputStreamReader(zis))\n",
      "                    Stream.continually(br.readLine()).takeWhile(_ != null)\n",
      "                }   \n",
      "    }\n",
      "val mentionsDF = mentionsRDD.map(x => x.split(\"\\t\")).map(row => row.mkString(\";\")).map(x => x.split(\";\")).toDF()\n",
      "```\n",
      "\n",
      "```scala\n",
      "// Mentions Translation\n",
      "val mentionsRDD_trans = sc.binaryFiles(\"s3a://mys3bucket/201801*translation.mentions.CSV.zip\"). \n",
      "    flatMap {  \n",
      "        case (name: String, content: PortableDataStream) =>\n",
      "            val zis = new ZipInputStream(content.open)\n",
      "            Stream.continually(zis.getNextEntry).\n",
      "                takeWhile{ case null => zis.close(); false\n",
      "                    case _ => true }.\n",
      "                flatMap { _ =>\n",
      "                    val br = new BufferedReader(new InputStreamReader(zis))\n",
      "                    Stream.continually(br.readLine()).takeWhile(_ != null)\n",
      "                }\n",
      "    }\n",
      "val mentionsDF_trans = mentionsRDD_trans.map(x => x.split(\"\\t\")).map(row => row.mkString(\";\")).map(x => x.split(\";\")).toDF()\n",
      "\n",
      "// Export Translation\n",
      "val exportRDD_trans = sc.binaryFiles(\"s3a://mys3bucket/201801*translation.export.CSV.zip\"). \n",
      "    flatMap { \n",
      "        case (name: String, content: PortableDataStream) =>\n",
      "            val zis = new ZipInputStream(content.open)\n",
      "            Stream.continually(zis.getNextEntry).\n",
      "                takeWhile{ case null => zis.close(); false\n",
      "                    case _ => true }.\n",
      "                flatMap { _ =>\n",
      "                    val br = new BufferedReader(new InputStreamReader(zis))\n",
      "                    Stream.continually(br.readLine()).takeWhile(_ != null)\n",
      "                }\n",
      "    }\n",
      "val exportDF_trans = exportRDD_trans.map(x => x.split(\"\\t\")).map(row => row.mkString(\";\")).map(x => x.split(\";\")).toDF()\n",
      "```\n",
      "\n",
      "> **Conclusion** : Our ETL is now defined. We have downloaded and sorted the data. We have created separated RDDs and unzipped files. In the next part, we'll cover how to optimize the data sets and how to put the data in Cassandra Tables.\n",
      "---\n",
      "title: Full introduction to Neural Nets\n",
      "layout: post\n",
      "tags: [deeplearning]\n",
      "subtitle : \"Deep Learning with PyTorch\"\n",
      "---\n",
      "\n",
      "This series of articles provides a summary of the course : \"Introduction to Deep Learning with PyTorch\" on [Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188).\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# The Perceptron : Key concepts\n",
      "\n",
      "The perceptron can be seen as a mapping of inputs into neurons. Each input is represented as a neuron :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/neuron_4.jpg)\n",
      "\n",
      "(I wrote an article on the topic if you'd like to learn more on this topic)\n",
      "\n",
      "We attach to each input a weight ( $$w_i$$) and notice how we add an input of value 1 with a weight of $$ - \\theta $$. This is called bias. What we are doing is instead of having only the inputs and the weight and compare them to a threshold, we also learn the threshold as a weight for a standard input of value 1.\n",
      "\n",
      "The inputs can be seen as neurons and will be called the **input layer**. Altogether, these neurons and the function (which we'll cover in a minute) form a **perceptron**.\n",
      "\n",
      "How do we make classification using a perceptron then?\n",
      "\n",
      "$$ y = 1 $$ if $$ \\sum_i w_i x_i ≥ 0 $$, else $$ y = 0 $$\n",
      "\n",
      "The classification that checks of the output is greater than 0 is called a **step function**.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Signum_function.svg.jpg)\n",
      "\n",
      "## Logical operators\n",
      "\n",
      "Perceptron can be used to represent logical operators. For example, one can represent the perceptron as an \"AND\" operator.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_1.jpg)\n",
      "\n",
      "A simple \"AND\" perceptron can be built in the following way :\n",
      "\n",
      "```\n",
      "weight1 = 1.0\n",
      "weight2 = 1\n",
      "bias = -1.2\n",
      "\n",
      "linear_combination = weight1 * input_0 + weight2 * input_1 + bias\n",
      "output = int(linear_combination >= 0)\n",
      "```\n",
      "\n",
      "Where `input_0` and `input_1` represent the two feature inputs. We are shifting the bias by 1.2 to isolate the positive case where both inputs are 1.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_2.jpg)\n",
      "\n",
      "However, solving the XOR problem is impossible :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_3.jpg)\n",
      "\n",
      "This is why Multi-layer perceptrons were introduced. In Multi-layer perceptrons, we can build XOR as a combination of logical operators perceptrons :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_4.jpg)\n",
      "\n",
      "## Finding the weights\n",
      "\n",
      "How do we identify the right weights that allow the best split ?\n",
      "\n",
      "Suppose we have an initial equation with random weights set as : $$ 3 X_1 + 4 X_2 - 10 = 0 $$\n",
      "\n",
      "If the point (4,5) is misclassified and should belong to the class -1,  and we have a learning rate of 0.1, what we'll do is update the weights the following way :\n",
      "\n",
      "$$ 3 - 0.1 * 4 = 2.6 $$\n",
      "\n",
      "$$ 4 - 0.1 * 5 = 3.5 $$\n",
      "\n",
      "$$ -10 - 0.1 * 1 = -10.1 $$\n",
      "\n",
      "We are slowly shifting the line towards the point that was misclassified. the new equation is therefore :\n",
      "\n",
      "$$ 2.6 X_1 + 3.5 X_2 - 10.1 = 0 $$\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_5.jpg)\n",
      "\n",
      "If another point is misclassified and should belong to the positive class, we'll apply the same process except we'll add weights. \n",
      "\n",
      "To summarize, the pseudo-code for the Perceptron is the following :\n",
      "\n",
      "- Initialize random weights $$ w_1, ..., w_n, b $$\n",
      "- For every misclassified point $$ X_1, ..., X_n $$ :\n",
      "\t- If the prediction is 0 :\n",
      "\t\t- For $$ i = 1...n $$ :\n",
      "\t\t\t- $$ W_i = W_i + \\alpha X_i $$\n",
      "\t\t- $$ b_i = b_i + \\alpha $$\n",
      "\t- If the prediction is 1 :\n",
      "\t\t- For $$ i = 1...n $$ :\n",
      "\t\t\t- $$ W_i = W_i - \\alpha X_i $$\n",
      "\t\t- $$ b_i = b_i - \\alpha $$\n",
      "\n",
      "\n",
      "## Implementation in Python\n",
      "\n",
      "We can quite simply implement a Perceptron in Python :\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "def stepFunction(t):\n",
      "    if t >= 0:\n",
      "        return 1\n",
      "    return 0\n",
      "\n",
      "def prediction(X, W, b):\n",
      "    return stepFunction((np.matmul(X,W)+b)[0])\n",
      "\n",
      "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
      "    # Fill in code\n",
      "    for i in range(len(y)):\n",
      "        y_hat = prediction(X[i],W,b)\n",
      "        if y[i] - y_hat == 1:\n",
      "            W[0] += learn_rate * X[i][0]\n",
      "            W[1] += learn_rate * X[i][1]\n",
      "            b += learn_rate\n",
      "        elif y[i] - y_hat == -1:\n",
      "            W[0] -= learn_rate * X[i][0]\n",
      "            W[1] -= learn_rate * X[i][1]\n",
      "            b -= learn_rate\n",
      "    return W, b\n",
      "    \n",
      "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
      "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
      "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
      "    W = np.array(np.random.rand(2,1))\n",
      "    b = np.random.rand(1)[0] + x_max\n",
      "\n",
      "    boundary_lines = []\n",
      "    for i in range(num_epochs):\n",
      "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
      "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
      "    return boundary_lines\n",
      "\n",
      "```\n",
      "\n",
      "# The Perceptron Algorithm\n",
      "\n",
      "## Continuous framework\n",
      "\n",
      "We cannot use our discrete examples above in this minimization algorithm, since the gradient descent would only work with continuous values. This is one of the limitations of the **step function** as an activation function. We tend to appy a **softmax function** instead.\n",
      "\n",
      "Using a sigmoid activation will assign the value of a neuron to either 0 if the output is smaller than 0.5, or 1 if the neuron is larger than 0.5. The sigmoid function is defined by : $$ f(x) = \\frac {1} {1 + e^{-u}} $$\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/sigmoid.jpg)\n",
      "\n",
      "This activation function is smooth, differentiable (allows back-propagation) and continuous. We don't have to output a 0 or a 1, but we can output probabilities to belong to a class instead. If you're familiar with it, this version of the perceptron is a logistic regression with 0 hidden layers.\n",
      "\n",
      "The perceptron can be seen as an error minimization algorithm. We choose the softmax function, a differentiable and continuous error function and try to minimize it by applying gradient descent.\n",
      "\n",
      "We usually apply a log-loss error function. This error function applies a penalty to miscalssified points that is proportional to the distance of the boundary.\n",
      "\n",
      "## Multi-class\n",
      "\n",
      "What if our problem involves more than 2 classes ? In such case, we apply a softmax function, that implies an exponential transformation to handle both positive and negative scores.\n",
      "\n",
      "$$ P(z_i \\in C_i) = \\frac {e^{z_i}} {\\sum_j e^{j}} $$\n",
      "\n",
      "The Python implementation of the sotfmax can be done in the following way :\n",
      "\n",
      "```python\n",
      "def softmax(lst):\n",
      "    exp_lst = np.exp(lst)\n",
      "    sum_exp_lst = sum(exp_lst)\n",
      "\n",
      "    result = []\n",
      "    for i in exp_lst:\n",
      "        result.append(float(i)/sum_exp_lst)\n",
      "\n",
      "    return result\n",
      "```\n",
      "\n",
      "## Cross-Entropy\n",
      "\n",
      "We are now back to a 2 class scenario. Since we now have probabilities of having each point belonging to each class. What should we do based on that to select a model or another? We can compute the maximum likelihood, which is simply :\n",
      "\n",
      "$$ \\prod_i p_i $$ for each probability of a point belonging to a class it is assigned to.\n",
      "\n",
      "However, working with products can be challenging and painful when we have a large amount of data. We prefer applying a negative log transformation. This is the **cross-entropy**.\n",
      "\n",
      "A good model has a low cross-entropy, and a bad model has a high cross-entropy. One of the advantage of the cross-entropy is to be able to compute the individual error for each point, due to the summation property. Therefore, a misclassified point has a large individual error.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_9.jpg)\n",
      "\n",
      "Our aim now becomes to minimize the cross-entropy !\n",
      "\n",
      "The cross-entropy can be formalized as such :\n",
      "\n",
      "$$ CE = - \\sum_i y_i \\log{p_i} + (1-y_i) \\log{1-p_i} $$\n",
      "\n",
      "The cross-entropy can be computed in Python :\n",
      "\n",
      "```python\n",
      "def cross_entropy(y, prob):\n",
      "    return -1 * np.sum(y*np.log(prob) + (1-y)*np.log(1-prob))\n",
      "```\n",
      "\n",
      "As previously, we should consider applying the cross-entropy to multi-class cases :\n",
      "\n",
      "$$ CE = - \\sum_j \\sum_i y_{ij} \\log{p_{ij}} $$\n",
      "\n",
      "The main idea behind the variable $$ y_{ij} $$ is that we only add the probabilities of the events that occured. We can also take the average rather than the sum for the cross entropy by convention.\n",
      "\n",
      "The 'prob' given above is actually known by the formula of the perceptron itself :\n",
      "\n",
      "$$ prob = \\sigma (Wx_i + b) $$\n",
      "\n",
      "Therefore, if we replace this value in the binary case :\n",
      "\n",
      "$$ CE = - \\sum_i y_i \\log{\\sigma (Wx_i + b) } + (1-y_i) \\log{1-\\sigma (Wx_i + b)} $$\n",
      "\n",
      "And in the multiclass framework :\n",
      "\n",
      "$$ CE = - \\sum_j \\sum_i y_{ij} \\log{\\sigma(Wx_{ij} + b)} $$\n",
      "\n",
      "## Error minimization\n",
      "\n",
      "Our error function is now fully specified. The next step is to minimize this function through the iterations of the algorithm. We minimize this function by gradient descent. Our goal is to calculate the gradient of $$ E $$ at a point $$ x = (x_1, \\ldots, x_n) $$ given by the partial derivatives\n",
      "\n",
      "$$ E = (\\frac{\\partial}{\\partial w_1}E, \\cdots, \\frac{\\partial}{\\partial w_n}E) $$\n",
      "\n",
      "There is a rather nice trick to apply here for the derivatives computation. Start by computing :\n",
      "\n",
      "$$ \\frac{\\partial}{\\partial w_j} \\hat{y} = \\frac{\\partial}{\\partial w_j} \\sigma (Wx + b) $$\n",
      "\n",
      "$$ = (Wx + b)(1-(Wx + b)) \\times \\frac{\\partial}{\\partial w_j} (Wx + b) $$\n",
      "\n",
      "$$ = \\hat{y}(1-\\hat{y}) \\times \\frac{\\partial}{\\partial w_j} (w_1 x_1 + \\cdots + w_j x_j  + \\cdots + b) $$\n",
      "\n",
      "$$ = \\hat{y} (1-\\hat{y}) x_j $$\n",
      "\n",
      "This building block we be re-used when computing the two partial derivatives of the error function with respect to $$ w_j $$ and $$ b_j $$ :\n",
      "\n",
      "$$ \\frac{\\partial}{\\partial w_j}E = \\frac{\\partial}{\\partial w_j}(-y \\log{\\hat{y}} - (1-y) \\log{1-\\hat{y}}) $$\n",
      "\n",
      "$$ = -y \\frac{\\partial}{\\partial w_j} \\log{\\hat{y}} - (1-y) \\frac{\\partial}{\\partial w_j} \\log{1-\\hat{y}} $$\n",
      "\n",
      "$$ = -y \\frac{1}{\\hat{y}} \\frac{\\partial}{\\partial w_j} \\hat{y} - (1-y) \\frac{1}{1-\\hat{y}} \\frac{\\partial}{\\partial w_j} (1-\\hat{y}) $$\n",
      "\n",
      "$$ = -y \\frac{1}{\\hat{y}} \\frac{\\partial}{\\partial w_j} \\hat{y} - (1-y) \\frac{1}{1-\\hat{y}} \\frac{\\partial}{\\partial w_j} (1-\\hat{y}) $$\n",
      "\n",
      "$$ = -y(1-\\hat{y})x_j - (1-y)(\\hat{y})x_j $$\n",
      "\n",
      "$$ = -(y-\\hat{y})x_j $$\n",
      "\n",
      "Applying similar calculations, we can show that :\n",
      "\n",
      "$$ \\frac{\\partial}{\\partial b}E = = -(y-\\hat{y}) $$\n",
      "\n",
      "Overcall, the gradient can be seen in its vectorized form as :\n",
      "\n",
      "$$ ∇E = -(y-\\hat{y})(x_1, \\cdots, x_n, 1) $$\n",
      "\n",
      "We now have determined the gradients. For a \"gradient descent\", we simply update the weights according to a learning rate in the inverse direction of the gradient :\n",
      "\n",
      "$$ w_j^{(i+1)} = w_j^{i} - \\alpha (-(y-\\hat{y})x_i) $$\n",
      "\n",
      "Which can be rewritten as :\n",
      "\n",
      "$$ w_j^{(i+1)} = w_j^{i} + \\alpha (y-\\hat{y})x_i $$\n",
      "\n",
      "And for the bias :\n",
      "\n",
      "$$ b^{(i+1)} = b^{i} + \\alpha (y-\\hat{y}) $$\n",
      "\n",
      "Where $$ \\alpha $$ represents $$ \\frac{1}{m} $$ of the original gradient since we moved in the average direction.\n",
      "\n",
      "To summarize, the gradient descent algorithm is the following :\n",
      "\n",
      "- Initialize random weights : $$ w_1, \\cdots, w_n, b $$\n",
      "- For every point $$ x_1, \\cdots, x_n $$ :\n",
      "\t- For $$ i \\in 1 \\cdots n $$ :\n",
      "\t\t- Update $$ w_j^* = w_j + \\alpha (y-\\hat{y})x_i $$\n",
      "\t\t- Update $$ b^* = b + \\alpha (y-\\hat{y}) $$\n",
      "- Repeat until the error gets smaller than a threshold\n",
      "\n",
      "In the Perceptron algorithm, we split the weights update depending on the value of $$ y - \\hat{y} $$ :\n",
      "- $$ w_i = w_i + \\alpha X_i $$ if $$ y - \\hat{y} $$ is positive\n",
      "- $$ w_i = w_i - \\alpha X_i $$ if $$ y - \\hat{y} $$ is negative\n",
      "\n",
      "This is the exact same algorithm as the gradient descent !\n",
      "\n",
      "# Building Neural Networks for Non-linear data\n",
      "\n",
      "In most cases, the data is not linearly separable.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_10.jpg)\n",
      "\n",
      "The basic idea is that we combine several linear models :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_11.jpg)\n",
      "\n",
      "How do we combine those models ?\n",
      "- we have a probability for each point in the first model of belonging to the class blue class for example\n",
      "- we also have a probability for each point in the second model of belonging to the class blue\n",
      "- we add those probabilities\n",
      "- we map them in a sigmoid function\n",
      "- we get a probability in return \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_12.jpg)\n",
      "\n",
      "We can weight the models indiviually to assign more weight to a model than to another. Say that we want $$ 2/3 $$ of the overall weight on the first one. We simply apply a factor of 2 to the probabilites in the first model :\n",
      "\n",
      "$$ 2 * 0.7 + 1 * 0.8 $$\n",
      "\n",
      "We can also add a bias to the individual models :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_13.jpg)\n",
      "\n",
      "This is a new Perceptron ! This is the building block of Neural Networks. We take linear combinations of perceptrons to turn them into new perceptrons. We can visually represent this :\n",
      "\n",
      "First we add a new perceptron as a linear combination of the 2 :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_14.jpg)\n",
      "\n",
      "The outputs of the first 2 are the inputs of our perceptron :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_15.jpg)\n",
      "\n",
      "And since the first two perceptrons share the same inputs, we can group them :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_16.jpg)\n",
      "\n",
      "As usual, we can represent the bias outside the neuron, and highlight the fact that we use the sigmoid activation function. This is how we systematically represent neural networks :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_17.jpg)\n",
      "\n",
      "We can generalize this architecture even more into 3 categories :\n",
      "- an input layer\n",
      "- hidden layers\n",
      "- an output layer\n",
      "\n",
      "All together, they form what is called : \"Deep Neural Networks\".\n",
      "\n",
      "The output layer can have as many neurons as we'd like, depending on the nature of the problem.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_18.jpg)\n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "title: NLP on GitHub comments\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "The dataset I am using in this project (`github_comments.tsv`) that carries 4000 comments that were published on pull requests on Github by developer teams.\n",
      "\n",
      "Here is an explanation of the table columns:\n",
      "- Comment: the comment made by a developer on the pull request.\n",
      "- Comment_date: date at which the comment was published\n",
      "- Is_merged: shows whether the pull request on which the comment was made has been accepted (therefore merged) or rejected.\n",
      "- Merged_at: date at which the pull request was merged (if accepted).\n",
      "- Request_changes: each comment is labelled either 1 or 0: if it’s labelled as 1 if the comment is a request for change in the code. If not, it’s labelled as 0.\n",
      "\n",
      "The GitHub of the project can be found here :\n",
      "\n",
      "<div class=\"github-card\" data-github=\"maelfabien/Analyze-Github-Pull-Requests\" data-width=\"100%\" data-height=\"\" data-theme=\"default\"></div>\n",
      "<script src=\"//cdn.jsdelivr.net/github-cards/latest/widget.js\"></script>\n",
      "\n",
      "The goal is to dig deeper into the nature of blockers and analyze the requests for change. If possible, try to answer the following questions:\n",
      "- What are the most common problems that appear in these comments?\n",
      "- Can we cluster the problems by topic/problem type?\n",
      "- How long is the resolution time after a change was requested?\n",
      "\n",
      "## Content\n",
      "- Report.pdf is a PDF report that details my approach.\n",
      "- images is a collection of the images that I included in my report\n",
      "- TopicModelling.ipynb is a Jupyter Notebook in which I have do my analysis in Python\n",
      "- corpus.pkl, dictionary.gensim, and all files starting with model… are files generated in the notebook that I use to avoid re-running some steps.\n",
      "\n",
      "## Theory covered\n",
      "This project covers the concepts of :\n",
      "- Topic Modelling using LDA\n",
      "- Clustering through tf-idf and BoW \n",
      "- Dimension reduction through t-SNE and truncated SVD\n",
      "- Classification and Regression algorithms\n",
      "\n",
      "<embed src=\"https://maelfabien.github.io/assets/images/Report.pdf\" type=\"application/pdf\" width=\"600px\" height=\"500px\" />\n",
      "\n",
      "---\n",
      "title: Leveraging side information for speaker identification with the Enron conversational telephone speech collection\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Criminal Networks\"\n",
      "---\n",
      "\n",
      "In this article, I will discuss and summarize the paper: [\"Leveraging side information for speaker identification with the Enron conversational telephone speech collection\"](http://www.cs.jhu.edu/~mdredze/publications/2017_asru_speakerid.pdf) by Ning Gao,  Gregory Sell, Douglas W. Oard and Mark Dredze.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# Background\n",
      "\n",
      "In conversational data such as the ones that we get in criminal investigation, we collect much more information than just the speech itself. We supposely have 2 identities between 2 nodes, who might have talked previously. We can therefore leverage graph structures to enhance speaker identification.\n",
      "\n",
      "# Data\n",
      "\n",
      "The authors use the Enron conversational telephone speech collection, a database with several thousands of emails, and 1'731 phone calls, for a total of close to 50 hours. The authors manually transcribed 57 conversations, from 41 different speakers, among which 37 sent emails recently. 28 recording were used in trainings, and 29 in test.\n",
      "\n",
      "# Approach\n",
      "\n",
      "For each phone recording, the authors first run a speaker identification system trained only on\n",
      "acoustic evidence to rank each of the candidate speakers according to the probability of their being one of the speakers in a specific call. Then, they add side information (social network features, channel features and ASR to identify known name variants) and re-rank the speaker candidates.\n",
      "\n",
      "The Detection Cost Function (DCF) is the measure chosen to first rank the different speakers:\n",
      "\n",
      "$$ DCF = C_M P_M P_T + C_{FA} P_{FA}(1 − P_T )$$\n",
      "\n",
      "Where $$ C_M $$ is the cost of misses, $$ C_{FA} $$ is the cost of false alarms (both set to 1), and the prior probability $$ P_t $$ is set to 0.03.\n",
      "\n",
      "Then, to evaluate the identification, authors use the classification error which computes how often the correct speaker is given the highest score. The mean reciprocal rank (MRR) is also used as a metric on the produced rankings.\n",
      "\n",
      "Concretely, in a call with 2 speakers, the process is:\n",
      "- a list of speakers is made for speaker 1 in the call\n",
      "- a list of speakers is made for speaker 2 in the call\n",
      "- remove speaker 1 from list 2\n",
      "- remove speaker 2 from list 1\n",
      "- compute the harmonic mean of the ranks of speaker 1 and 2: $$ R = \\frac{n}{\\sum_{i=1}^n \\frac{1}{r_i}} -1 $$\n",
      "\n",
      "Where $$ r_i $$ is the rank of the ground truth speaker in list $$ i $$ and $$ n $$ is the number of list, e.g. 2 in a simple phone call. $$ R $$ now stands for the harmonic expected rank.\n",
      "\n",
      "If all the speakers are in the right position, i.e. 1, we end up with $$ R = \\frac{n}{n} - 1 = 0 $$ which is the best score possible.\n",
      "\n",
      "# Speaker identification\n",
      "\n",
      "First, the recording contain multiple speakers. Speaker diarization was necessary (used i-vector segments to estimate the bounding marks).\n",
      "\n",
      "The resulting audio samples were fed into the speaker identification system using an i-vector baseline. The UBM and the total variability matrix (T) are trained on the Fisher English corpus, and the PLDA is trained on the NIST SRE 04, 05, 06 and 08.\n",
      "\n",
      "This baseline reaches a DCF of 0.67, classification error of 0.56 and harmonic expected rank R of 0.73.\n",
      "\n",
      "# Re-ranking\n",
      "\n",
      "## Social Networks\n",
      "\n",
      "We have past emails for most speakers in the database. Therefore, we can expect the users to be speaking much more frequently over the phone if they exchange a lot of emails. We can therefore build an edge for every email between 2 speakers (or if they were both in CC). The weight of the edge is the frequency of the communication between 2 speakers.\n",
      "\n",
      "A score is then computed for each pair, defined as:\n",
      "\n",
      "$$ s_p = \\frac{1}{2} ((1 + \\frac{e_l}{\\sum e}) s_l + (1 + \\frac{e_r}{\\sum e}) s_r)(1 + \\frac{e_{lr}}{\\sum e}) $$\n",
      "\n",
      "Where:\n",
      "- $$ e_l $$ is the sum of the edge weights connected to the left speaker\n",
      "- $$ e_r $$ is the sum of the edge weights connected to the right speaker\n",
      "- $$ \\sum e $$ is the sum of all edge weights in the network\n",
      "- $$ \\frac{e_l}{\\sum e} $$ is therefore high if the left speaker is a frequent communicant\n",
      "- $$ e_{lr} $$ is the edge weight between left speaker and right speaker\n",
      "- $$ s_l $$ is the acoustic score of left speaker\n",
      "- $$ s_r $$ is the acoustic score of right speaker\n",
      "\n",
      "We end-up with pairs of speakers that are the most likely to have been in this call, by leveraging some simple graph features (number of communications, i.e relative degree, and weights between A and B).\n",
      "\n",
      "The same approach was done by leveraging how often people talk over the phone to build the network (rather than by email). My personal take would have been to use both.\n",
      "\n",
      "Results are displayed here:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/results_graph.png)\n",
      "\n",
      "This table suggests that:\n",
      "- DCF gets worse when using network information\n",
      "- classification error is improved in both cases\n",
      "- the harmonic expected rank is improved\n",
      "\n",
      "## Communication channel\n",
      "\n",
      "Some speakers are often identified on some communication channels, whereas others might talk on different channels. The authors propose a new metric to weight the score by the likelihood of having speaker $$ c $$ talking over channel $$ q $$ depending on how frequently he talked on that channel $$ \\frac{f_i}{\\sum_q f_q} $$ :\n",
      "\n",
      "$$ s_c^{'} = (1 + \\frac{\\lambda f_i}{\\sum_{q=1}^m f_q}) s_c $$\n",
      "\n",
      "Where $$ \\lambda $$ is a simple weight factor.\n",
      "\n",
      "## Name Mention\n",
      "\n",
      "Finally, authors extract information from the text using an ASR system and try to identify the name of the speakers. I won't dive too much into this part, because for criminal networks, we can safely suppose that it is rarely the case.\n",
      "\n",
      "# Results\n",
      "\n",
      "Overall results, combining several re-ranking techniques too, are presented in the table below:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/results_full_graph.png)\n",
      "\n",
      "It is worth noting that:\n",
      "- DCF is hardly improved by the network strcture\n",
      "- Classification error is greatly improved\n",
      "- the harmonic expected rank is greatly improved too\n",
      "\n",
      "# Discussion\n",
      "\n",
      "I would have liked to see a combination of the email and phone networks, since in criminal networks, you would typically combine all sources that you have. I do also believe that there are many other ways to approach this task. However, this work, which is one of the few to address the notion of speaker identification and social networks, shows promising results.\n",
      "---\n",
      "title: Basics of Speaker Verification\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Speech Processing\"\n",
      "---\n",
      "\n",
      "Speaker biometrics is a field of Speech processing which focuses on identifying a unique speaker from several audio recorings. This can be useful for access control or suspect identification for example. Most of my understanding of this field was built from an excellent thesis, \"Speaker Verification using I-vector Features\" by Ahilan Kanagasundaram from Queensland University of Technology, and from the APSIPA Talk : \"Speaker Verification - The present and future of voiceprint based security\".\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# What can you extract from speech?\n",
      "\n",
      "Linguistic features, carrying message and language information:\n",
      "- What is being said?\n",
      "- What language is spoken? \n",
      "\n",
      "Paralinguistic features, carrying emotional and physiological characteristics:\n",
      "- **Who is speaking?**\n",
      "- With what accent?\n",
      "- Gender of the speaker\n",
      "- Age of the speaker\n",
      "- Emotion:\n",
      "\t- Stress level\n",
      "\t- Cognitive load level\n",
      "\t- Depression level\n",
      "\t- Spleepiness detection\n",
      "\t- Is the person inebriated\n",
      "\n",
      "# Overview of Speaker Recognition\n",
      "\n",
      "Speaker biometrics/recognition is split into:\n",
      "- *Speaker identification*: determine an unknown speaker's identify among a group of speakers. We take the audio of the unknown speaker, compare it to the models of all enrolled speakers, and determine the best-matching speaker.\n",
      "- *Speaker verification*: verify the claimed identity of a person through speech signal. We compare the audio sample provided with the claimed speaker model, and decide to accept or reject.\n",
      "- *Speaker diarization*: partition an audio input stream into segments according to the speaker identity.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/bs_0.png)\n",
      "\n",
      "Speaker verification is used in access security, transaction authentification and in suspect identification mainly, and is therefore the most commonly studied problem. This field has been an active field of research since the 1950s. \n",
      "\n",
      "# Overview of Speaker Verification Pipeline\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/bs_1.png)\n",
      "\n",
      "Speaker verification aims at determining whether the identity of the speaker matches the claimed identify, and requires typically 1 comparison. On the other hand, speaker identification among a group of size N requires N comparisons.\n",
      "\n",
      "The main steps when running a Speaker Verification pipeline are the following:\n",
      "- extract features from the audio in what is called the Front End\n",
      "- compare those features to a speaker model in the Back End\n",
      "- make decision based on the output\n",
      "\n",
      "The main known issues in this field of research are:\n",
      "- the amount of data needed\n",
      "- the mismatch between the training data (enrolment) and the testing data (verification), since the channel might change (silent room vs noisy phone environment)\n",
      "\n",
      "There are 2 types of speaker verification techniques:\n",
      "- *Text-dependent*: the speaker must pronounce a known word or phrase. In such case, short training data are enough.\n",
      "- *Text-independent*: users are not restricted to say anything specific. In such cas, the training data must be sufficiently long, but the solution is more flexible.\n",
      "\n",
      "A common example of text-dependant speaker verification would be the \"Hey Siri\" of most iPhones now. The phrase to pronounce is known in advance, and we must verify the identity of the person. Once the identity has been verified, a Speech-to-Text pipeline translates what the user pronounced into a query.\n",
      "\n",
      "The main steps of speaker verification are:\n",
      "- *Development*: learn speaker-idenpendent models using large amount of data. This is a pre-training part, called a Universal Background Model (UBM). It can be gender-specific, in the sense that we have 1 for Males, and 1 for Females.\n",
      "- *Enrollment*: learn distinct characteristics of a speaker's voice. This step typically creates one model per unique speaker considered. This is the training part. \n",
      "- *Verification*: distinct characteristics of a claimant's voice are compared with previously enrolled claimed speaker models. This is the prediction part.\n",
      "\n",
      "The difference between the training and testing data might come from:\n",
      "- the microphones\n",
      "- the environment\n",
      "- the transmission channel (landdline, VoIP...)\n",
      "- the speaker himself\n",
      "\n",
      "The decision process in classic Speaker Verification systems simply compares the likelihood that a speaker comes from the specific model and the likelihood that the speaker comes from the Universal Background model:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/bs_2.png)\n",
      "\n",
      "We call this ratio, the *likelihood ratio*. If the ratio is above the decision threshold, it is more likely that the sample comes from the specific speaker and we therefore accept the identity. Otherwise, we reject it. Note that the UBM might be gender-specific.\n",
      "\n",
      "---\n",
      "title: The decoding graph\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Speech Processing\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "Acoustic modeling allows you to identify phonemes and transitions between them. But we still miss a step to return proper words: a language model that helps identify the correct words.\n",
      "\n",
      "# Language model integration\n",
      "\n",
      "Recall the fundamental equation of speech recognition:\n",
      "\n",
      "$$ W^{\\star} = argmax_W P(W \\mid X) $$\n",
      "\n",
      "This is known as the \"Fundamental Equation of Statistical Speech Processing\". Using Bayes Rule, we can rewrite is as :\n",
      "\n",
      "$$ W^{\\star} = argmax_W \\frac{P(X \\mid W) P(W)}{P(X)} $$\n",
      "\n",
      "Where $$ P(X \\mid W) $$ is the acoustic model (what we have done so far), and $$ P(W) $$ is the language model.\n",
      "\n",
      "In brief, we use pronunciation knowledge to construct HMMs for all possible words, and use the most probable state sequence to recove the most probable word sequence.\n",
      "\n",
      "In continuous speech recogniton, the number of words in the utterance is not known. Word boundaries are not known. We therefore need to add transitions between all word-final and word-initial states.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_32.png)\n",
      "\n",
      "If we then apply Viterbi decoding to find the optimal word sequence, we need to consider $$ {\\mid V \\mid}^2 $$ inter-word transitions at every time step, where $$ V $$ is the number of words in the vocabulary. Needless to say, it can become a problem that is way too long to compute. However, if the HMM models are simples and the size of the vocabulary is small, we can decode the Markov Chain exactly with the Viterbi decoding.\n",
      "\n",
      "So the question becomes: can we speed up the Viterbi search of the best sequence using some kind of information? And the answer is yes, using the Language Model (LM).\n",
      "\n",
      "Recall that in an N-gram language model, you model the probability of observing $$ w_i $$ after a sequence of words $$ w_{i-n}, w_{i-n+1}, ... w_{i-1} $$\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_33.png)\n",
      "\n",
      "However, the exact search is not possible for large vocabulary tasks, especially in the case of the use of cross-words. There are several solutions to this:\n",
      "- beam search : prune low probability hypothesis\n",
      "- tree structured lexicons\n",
      "- language model look-ahead\n",
      "- dynamic search structured\n",
      "- multipass search\n",
      "- best-first search\n",
      "- **Weighted Finite State Transducers** (WFST)\n",
      "\n",
      "## Tree-structured lexicons\n",
      "\n",
      "In tree-structured lexicons, we represent the possible words of a language in a tree, nodes being phones, and leaves being words. Therefore, when we have a sequence of phones, we only need to follow the corresponding branch of the tree, which greatly reduces the number of state transition computations :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_34.png)\n",
      "\n",
      "## Language model look-ahead\n",
      "\n",
      "In a tree-structured decoding, we look ahead to find the best LM scores for any words further down the tree, and states are pruned early if they lead to a low probability. This makes the decoding faster, since many branches are dropped.\n",
      "\n",
      "# Weighted Finite State Transducers (WFSTs)\n",
      "\n",
      "WFSTs are implemented and used in Kaldi. WFSTs transduce input sequences into output sequences. Each transition has an input label, an output label, and weights. \n",
      "\n",
      "Let's get back to the basics. \n",
      "\n",
      "## Finite State Automata\n",
      "\n",
      "**Finite State Automata (FSA)** are extensively used in Speech. We define them as an abstract machine consisting of:\n",
      "- a set of states $$ Q $$, with an initial one $$ I $$ and a final one $$ F $$, often called the accepting state\n",
      "- a set $$ Y $$ of input symbols\n",
      "- a set $$ Z $$ of output symbols\n",
      "- a state transition function $$ q_t = f(y_t, q_{t-1}) $$ that takes the current input event and the previous state $$ q_{t-1} $$ and returns the next state $$ q_t $$\n",
      "- an emission function $$ z_t = g(q_t, q_{t-1}) $$ that returns an output even\n",
      "\n",
      "You migh recognize here a more generic definition of Hidden Markov Models (HMMs). HMMs are in fact a sub-base of Stochastic FSAs. A path in FSA is a series of directed edges.\n",
      "\n",
      "Let us conside the language of a sheep for example: /baa+!/. This language has 5 states:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_35.png)\n",
      "\n",
      "The state labels are mentioned in the circles, and the labels/symbols are on the arcs. More formally, FSAs are usually presented this way:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_36.png)\n",
      "\n",
      "## Finite State Acceptor\n",
      "\n",
      "In our sheep example, the alphabet is made of 3 letters: b, a and !. It has a start state and an accept / final state, as well as 5 transitions. FSA acts as an **Acceptor**, in the sense that it can reject a set of strings/sequence:\n",
      "- abaa! is rejected because of the initial state\n",
      "- baa!b is rejected because of the final state\n",
      "- baaaa! is accepted\n",
      "\n",
      "A string is accepted if:\n",
      "- there is a path with that sequence of symbols on it\n",
      "- the path is successful, starts at the initial state and ends at the final\n",
      "\n",
      "An additional symbol that has a special meaning in FSAs is $$ \\epsilon $$. It means that no symbol is generated. This is a way to go back to a previous state without generating anything:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_37.png)\n",
      "\n",
      "## Composition of FSA\n",
      "\n",
      "Formal languages are just sets of strings, and we can apply set operations on them. There are two main types of operations used on FSA and WFSTs:\n",
      "- Union, or sum of two , denoted : $$ [[T1 ⊕ T2]](x, y) = [[T1]](x, y) ⊕ [[T2]](x, y) $$\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_38.png)\n",
      "\n",
      "- Concatenation, or product, denoted: $$ [[T1 ⊗ T2]](x, y) $$\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_39.png)\n",
      "\n",
      "- Kleene closure, which is an arbitrary repetition\n",
      "\n",
      "## Weighted FSA\n",
      "\n",
      "Weighted FSA simply introduce a notion of weights on arcs and final states. These weights are also refered as costs. The idea is that is multiple paths have the same string, we take the one with the lowest cost. The cost, on the diagram below, is represented after the \"/\".\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_40.png)\n",
      "\n",
      "Weighted FSAs are used in speech, for language modeling or pronunciation modeling:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_41.png)\n",
      "\n",
      "## Weighted Finite State Transducer\n",
      "\n",
      "Finite Automata do no produce an output, whereas **Finite State Transducers (FSTs)** have both inputs and outputs. Now, on the arcs, the notation is the following:\n",
      "\n",
      "a:b/0.3\n",
      "\n",
      "Where:\n",
      "- a is the input\n",
      "- b is the output\n",
      "- 0.3 is the weight\n",
      "\n",
      "Why do we need to produce outputs? For decoding ! We can typically take input phonemes and output words.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_42.png)\n",
      "\n",
      "In this example, the output word is mentioned as the output of the 1st input. In this example, from the phoneme \"d\", we can build 2 words:\n",
      "- data (deytax, daedxax...)\n",
      "- dew (duw)\n",
      "\n",
      "WFSTs can be composed by matching up inner symbols. In the example below, we match A and B in C, denote it C=A◦B, and since in A the inputs a and b and matched with x, and in b x is matched with y, in C, a and b are matched with y:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_43.png)\n",
      "\n",
      "## WFST algorithms\n",
      "\n",
      "There are several algorithms to remember for WFSTs:\n",
      "- composition: Combine transducers T1 and T2 into a single one, where T1 is passed as an input to T2.\n",
      "- determinisation: ensure that each state has no more than a single output transition for a given input label.\n",
      "- minimisation: transform a transfucer into a new one with fewest possible states and transitions\n",
      "- weight pushing: push the weights towards the front of the path\n",
      "\n",
      "The weight pushing process could be illustrated this way:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_51.png)\n",
      "\n",
      "# Decoding with WFSTs\n",
      "\n",
      "The idea of the decoding network is to represent all components that allow us to find the most likely spoken word sequence using WFSTs:\n",
      "\n",
      "HCLG = H◦C◦L◦G\n",
      "\n",
      "Where:\n",
      "- H is the HMM, that takes inputs HMM states and outputs context-dependent phones\n",
      "- C is called context-dependency, takes context dependent phones as an input and outputs phones\n",
      "- L is the pronunciation lexicon that takes phones as an input sequence and outputs words\n",
      "- G is the language model acceptor, a transducer at the word-level that takes words as an input, and outputs words\n",
      "\n",
      "All the components are built separately and composed together.\n",
      "\n",
      "## H: HMMs as WFSTs\n",
      "\n",
      "HMMs can be represented natively as WFSTs the following way:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_44.png)\n",
      "\n",
      "To avoid having too many context-dependent models ($$ N^3 $$), we apply what was mentioned in the HMM-GMM acoustic modeling: clustering based on a phonetic decision tree.\n",
      "\n",
      "## C: Context-dependency transducer\n",
      "\n",
      "HMMs take into account context through context-dependent phones (i.e triphones). To feed it into the pronunciation lexicon, we need to build from these context-dependent phones a set of context-independent phones. To do that, a FST is built and explicits the transitions between triphones. For example, from a triphone \"a/b/c\"(i.e. central phone \"b\" with left context \"a\" and right context \"c\"), the arcs represent the individual phones \"a\", \"b\", \"c\".\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_49.png)\n",
      "\n",
      "## L: Pronunciation model\n",
      "\n",
      "The Pronunciation lexicon L is a transducer that takes as an input context-independent phones and outputs words. The weights of this WFST are defined by the pronounciation probability.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_50.png)\n",
      "\n",
      "## G: Language model as WFSAs\n",
      "\n",
      "We can represent language models (LMs) as WFSAs easily too, and any type of LM will actually be implemented as a WFSA in Kaldi or other softwares. They do no produce any output, so they remain Automatas and not Transducers, since our aim is just to estimate the \"cost\" of a path.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_45.png)\n",
      "\n",
      "In a unigram LM, the probability of each word only depends on that word's own probability in the document, so we only have one-state finite automata as units. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_46.png)\n",
      "\n",
      "In a bigram LM, the probability of a word depends on the previous word too, and in a trigram LM, on the 2 previous words. You would represents a trigram WFSA this way:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_47.png)\n",
      "\n",
      "## The decoding process\n",
      "\n",
      "The different components of the deconding graph have now be explicited, and can be more formally defined as:\n",
      "\n",
      "$$ HCLG = rds(min(det(H ◦ det(C ◦ det( L ◦ G))))) $$\n",
      "\n",
      "Where:\n",
      "- rds means remove disambiguation symbols\n",
      "- min is the minimization (with weight pushing)\n",
      "- det is the determinization\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "If you want to improve this article or have a question, feel free to leave a comment below :)\n",
      "\n",
      "References:\n",
      "- [ASR 09, University of Edimburgh](http://www.inf.ed.ac.uk/teaching/courses/asr/2019-20/asr09-lvcsr.pdf)\n",
      "- [Tutorial on FSA, Idiap](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.3344&rep=rep1&type=pdf)\n",
      "- [Weighted Finite State Transducers in Automatic Speech Recognition - ZRE lecture](https://www.cs.brandeis.edu/~cs136a/CS136a_Slides/zre_lecture_asr_wfst.pdf)\n",
      "\n",
      "---\n",
      "title: I trained a Network to Speak Like Me\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "Over the course of the past months, I wrote over 100 articles on my blog. That's quite a large amount of content. An idea then came to my mind : train a language generation model to **speak like me**. Or more specifically, to write like me. This is the perfect way to illustrate the main concepts of language generation, its implementation using Keras, and the limits of my model.\n",
      "\n",
      "I have found [this Kaggle Kernel](https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms) to be a useful resource.\n",
      "\n",
      "# Language generation\n",
      "\n",
      "Language Generation is a subfield of Natural Language Processing that aims to generate meaningful textual content. Most often, the content is generated as a sequence of individual words. \n",
      "\n",
      "For the big idea, here is how it works :\n",
      "- you train a model to predict the next word of a sequence\n",
      "- you give the trained model an input\n",
      "- and iterate N times so that it generates the next N words\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lgen_6.png)\n",
      "\n",
      "## Dataset Creation\n",
      "\n",
      "The first step is to build a dataset that can be understood by the network we are later on going to build. Start by importing the following packages :\n",
      "\n",
      "```python\n",
      "from keras.preprocessing.sequence import pad_sequences\n",
      "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras.callbacks import EarlyStopping\n",
      "from keras.models import Sequential\n",
      "import keras.utils as ku \n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import string, os \n",
      "```\n",
      "\n",
      "### Load the data\n",
      "\n",
      "The header of each and every article I have written follows this template :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lgen_1.png)\n",
      "\n",
      "This is the type of content we would typically not like to have in our final dataset. We will instead focus on the text itself. First, we need to point to the folder that contains the articles :\n",
      "\n",
      "```python\n",
      "import glob, os\n",
      "\n",
      "os.chdir(\"/MYFOLDER/maelfabien.github.io/_posts/\")\n",
      "```\n",
      "\n",
      "### Sentence Tokenizing\n",
      "\n",
      "Then, open each article, and append the content of each article to a list. However, since our aim is to generate sentences, and not whole articles (so far...), we will split each article into a list of sentences, and append each sentences to the list `all_sentences` :\n",
      "\n",
      "```python\n",
      "all_sentences= []\n",
      "\n",
      "for file in glob.glob(\"*.md\"):\n",
      "    f = open(file,'r')\n",
      "    txt = f.read().replace(\"\\n\", \" \")\n",
      "    try: \n",
      "        sent_text = nltk.sent_tokenize(''.join(txt.split(\"---\")[2]).strip())\n",
      "        for k in sent_text :\n",
      "            all_sentences.append(k)\n",
      "    except : \n",
      "        pass\n",
      "```\n",
      "\n",
      "Overall, we have a little more than 6'800 training sentences :\n",
      "\n",
      "```python\n",
      "len(all_sentences)\n",
      "```\n",
      "\n",
      "`6858`\n",
      "\n",
      "The process so far is the following :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lgen_2.png)\n",
      "\n",
      "### N-gram creation\n",
      "\n",
      "Then, the idea is to create N-grams of words that occur together. To do so, we need to :\n",
      "- fit a tokenizer on the corpus to associate an index to each token\n",
      "- break down each sentence in the corpus as a sequence of tokens\n",
      "- store sequences of tokens that happens together\n",
      "\n",
      "It can be illustrated in the following way :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lgen_3.png)\n",
      "\n",
      "Let's implement this. We first need to fit the tokenizer :\n",
      "\n",
      "```python\n",
      "tokenizer = Tokenizer()\n",
      "tokenizer.fit_on_texts(all_sentences)\n",
      "total_words = len(tokenizer.word_index) + 1\n",
      "```\n",
      "\n",
      "The variable `total_words` contains the total number of different words that have been used. Here, 8976. Then, for each sentence, get the corresponding tokens and generate the N-grams :\n",
      "\n",
      "```python\n",
      "input_sequences = []\n",
      "for sent in all_sentences:\n",
      "    token_list = tokenizer.texts_to_sequences([sent])[0]\n",
      "    for i in range(1, len(token_list)):\n",
      "        n_gram_sequence = token_list[:i+1]\n",
      "        input_sequences.append(n_gram_sequence)\n",
      "```\n",
      "\n",
      "\n",
      "The `token_list` variable contains the sentence as a sequence of tokens :\n",
      "\n",
      "```python\n",
      "[656, 6, 3, 2284, 6, 3, 86, 1283, 640, 1193, 319]\n",
      "[33, 6, 3345, 1007, 7, 388, 5, 2128, 1194, 62, 2731]\n",
      "[7, 17, 152, 97, 1165, 1, 762, 1095, 1343, 4, 656]\n",
      "```\n",
      "\n",
      "Then, the `n_gram_sequences` creates the n-grams. It starts with the first two words, and then gradually adds words :\n",
      "\n",
      "```python\n",
      "[656, 6]\n",
      "[656, 6, 3]\n",
      "[656, 6, 3, 2284]\n",
      "[656, 6, 3, 2284, 6]\n",
      "[656, 6, 3, 2284, 6, 3]\n",
      "...\n",
      "```\n",
      "\n",
      "### Padding\n",
      "\n",
      "We are now facing the following problem : not all sequences have the same length ! How can we solve this ?\n",
      "\n",
      "We will use paddding. Paddings adds sequences of 0's before each line of the variable `input_sequences` so that each line has the same length as the longest line.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lgen_4.png)\n",
      "\n",
      "In order to pad all sentences to the maximum length of the sentences, we must first find the longest sentence :\n",
      "\n",
      "```python\n",
      "max_sequence_len = max([len(x) for x in input_sequences])\n",
      "```\n",
      "\n",
      "It is equal to `792` in my case. Well, that looks quite large for a single sentence ! Since my blog contains some code and tutorials, I expect this single sentence to actually by Python code. Let's plot the histogram of the length of the sequences :\n",
      "\n",
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist([len(x) for x in input_sequences], bins=50)\n",
      "plt.axvline(max_sequence_len, c=\"r\")\n",
      "plt.title(\"Sequence Length\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lgen_8.png)\n",
      "\n",
      "There are indeed very few examples with 200 + words in a single sequence. How about setting the maximal sequence length to 200 ?\n",
      "\n",
      "\n",
      "```python\n",
      "max_sequence_len = 200\n",
      "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
      "```\n",
      "\n",
      "It returns something like :\n",
      "\n",
      "```python \n",
      "array([[   0,    0,    0, ...,    0,  656,    6],\n",
      "       [   0,    0,    0, ...,  656,    6,    3],\n",
      "       [   0,    0,    0, ...,    6,    3, 2284],\n",
      "       ...,\n",
      "```\n",
      "\n",
      "### Split X and y\n",
      "\n",
      "We now have fixed length arrays, most of them are filled with 0's before the actual sequence. Right, how do we turn that into a training set? We need to split X and y! Remember that our aim is to predict the next word of a sequence. We must therefore takes all tokens except for the last one as our `X`, and take the last one as our `y`.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lgen_5.png)\n",
      "\n",
      "In Python, it's as simple as that :\n",
      "\n",
      "```python\n",
      "X, y = input_sequences[:,:-1],input_sequences[:,-1]\n",
      "```\n",
      "\n",
      "We will now see this problem as a multi-class classification task. As usual, we must first one-hot encode the `y` to get a sparse matrix that contains a 1 in the column that corresponds to the token, and 0 eslewhere :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lgen_7.png)\n",
      "\n",
      "In Python, using keras utils `to_categorical` :\n",
      "\n",
      "```python\n",
      "y = ku.to_categorical(y, num_classes=total_words)\n",
      "```\n",
      "\n",
      "Lets us now check the sizes of `X` and `y` :\n",
      "\n",
      "```python\n",
      "X.shape\n",
      "```\n",
      "\n",
      "`(164496, 199)`\n",
      "\n",
      "```python\n",
      "y.shape\n",
      "```\n",
      "\n",
      "`(164496, 8976)`\n",
      "\n",
      "We have 165'000 training samples. X is 199 columns wide since it corresponds to the longest sequence we allow (200) minus one, the label to predict. Y has 8976 columns, which corresponds to a sparse matrix of all the vocabulary words. The dataset is now ready !\n",
      "\n",
      "## Build the model\n",
      "\n",
      "We will be using Long Short-Term Memory networks (LSTM). LSTM have the important advantage of being able to understand depenence over a whole sequence, and therefore, the beginning of a sentence might have an impact on the 15th word to predict. On the other hand, Recurrent Neural Networks (RNN) only imply a dependence on the previous state of the network, and only the previous word would help predict the next one. We would quickly miss context if we chose RNNs, and therefore, LSTMs seem to be the right choice. \n",
      "\n",
      "### Model architecture\n",
      "\n",
      "Since the training can be very (very) (very) (very) (very) (no joke) long, we will build a simple 1 Embedding + 1 LSTM layer + 1 Dense network :\n",
      "\n",
      "```python\n",
      "def create_model(max_sequence_len, total_words):\n",
      "\n",
      "    input_len = max_sequence_len - 1\n",
      "\n",
      "    model = Sequential()\n",
      "    \n",
      "    # Add Input Embedding Layer\n",
      "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
      "    \n",
      "    # Add Hidden Layer 1 - LSTM Layer\n",
      "    model.add(LSTM(100))\n",
      "    model.add(Dropout(0.1))\n",
      "    \n",
      "    # Add Output Layer\n",
      "    model.add(Dense(total_words, activation='softmax'))\n",
      "\n",
      "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
      "    \n",
      "    return model\n",
      "\n",
      "model = create_model(max_sequence_len, total_words)\n",
      "model.summary()\n",
      "```\n",
      "\n",
      "First, we add an embedding layer. We pass that into an LSTM with 100 neurons, add a dropout to control neuron co-adaptation, and end with a dense layer. Notice that we apply a softmax activation function on the last layer to get the probability that the output belongs to each class. The loss used is the categorical cross-entropy, since it is a multi-class classification problem.\n",
      "\n",
      "The summary of the model is :\n",
      "\n",
      "```\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 199, 10)           89760     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8976)              906576    \n",
      "=================================================================\n",
      "Total params: 1,040,736\n",
      "Trainable params: 1,040,736\n",
      "Non-trainable params: 0\n",
      "____________________________\n",
      "\n",
      "```\n",
      "\n",
      "### Train the model\n",
      "\n",
      "We are now (finally) ready to train the model ! \n",
      "\n",
      "```python\n",
      "model.fit(X, y, batch_size=256, epochs=100, verbose=True)\n",
      "```\n",
      "\n",
      "The training of the model will then start :\n",
      "\n",
      "```python\n",
      "Epoch 1/10\n",
      "164496/164496 [==============================] - 471s 3ms/step - loss: 7.0687\n",
      "Epoch 2/10\n",
      "73216/164496 [============>.................] - ETA: 5:12 - loss: 7.0513\n",
      "```\n",
      "\n",
      "On a CPU, a single epoch takes around 8 minutes. On a GPU, you should modify the Keras LSTM network used since it cannot be used on GPU. You would instead need this :\n",
      "\n",
      "```python\n",
      "# Modify Import\n",
      "from keras.layers import Embedding, LSTM, Dense, Dropout, CuDNNLSTM\n",
      "\n",
      "# In the Moddel\n",
      "...\n",
      "    model.add(CuDNNLSTM(100))\n",
      "...\n",
      "```\n",
      "\n",
      "This reduces training time to 2 minutes per epoch, which makes it acceptable. I have personnaly trained this model on Google Colab. I tend to stop the training at several steps to make so sample predictions and control the quality of the model given several values of the cross entropy.\n",
      "\n",
      "Here are my observations :\n",
      "\n",
      "| Loss Value | Sentence Generated |\n",
      "| --- | --- |\n",
      "| ± 7| Generates only the word \"The\" since most frequent |\n",
      "| ± 4| Easily falls into cyclical patterns if the same word occurs twice |\n",
      "| ± 2.8| Becomes interesting e.g \"Machine\" inputs leads to \"Learning algorithms ...\" |\n",
      "\n",
      "## Generating sequences \n",
      "\n",
      "If you have read the article up to here, you basically came here for that : generate sentences ! To generate sentences, we need to apply the same transformations to the input text. We will build a loop that generates for a given number of iterations the next word :\n",
      "\n",
      "```python\n",
      "input_txt = \"Machine\"\n",
      "\n",
      "for _ in range(10):\n",
      "    \n",
      "    # Get tokens\n",
      "    token_list = tokenizer.texts_to_sequences([input_txt])[0]\n",
      "    # Pad the sequence\n",
      "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
      "    # Predict the class\n",
      "    predicted = model.predict_classes(token_list, verbose=0)\n",
      "    \n",
      "    output_word = \"\"\n",
      "    \n",
      "    # Get the corresponding work\n",
      "    for word,index in tokenizer.word_index.items():\n",
      "        if index == predicted:\n",
      "            output_word = word\n",
      "            break\n",
      "            \n",
      "    input_txt += \" \"+output_word\n",
      "```\n",
      "\n",
      "When the loss is around 3.1, here is the sentence it generates with \"Google\" as an input :\n",
      "\n",
      "`Google is a large amount of data produced worldwide`\n",
      "\n",
      "It does not really mean anything, but it sucessfully associates Google to the notion of large amount of data. It's quite impressive since it simply relies on the co-occurence of words, and does not integrate any grammatical notion. If we wait a bit longer in the training and let the loss decrease to 2.6, and give it the input \"In this article\" :\n",
      "\n",
      "`In this article we'll cover the main concepts of the data and the dwell time is proposed mentioning the number of nodes`\n",
      "\n",
      "> I hope this article was useful. I have tried to illustrate the main concepts, challenges and limits of language generation. Larger networks and transfer learning are definitely sources of improvement compared to the approach we discussed in this article. Please leave a comment if you have any question :)\n",
      "\n",
      "Sources :\n",
      "- [Kaggle Kernel](https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms)\n",
      "\n",
      "\n",
      "---\n",
      "title: AWS Cloud Practitioner - Core Services\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Amazon Web Services\"\n",
      "---\n",
      "\n",
      "In this series of articles, I'll present to you the main concepts of Cloud Computing as presented by AWS on their online certification: \"Cloud Practitioner\".\n",
      "\n",
      "<embed src=\"https://maelfabien.github.io/assets/images/AWS_1.pdf\" type=\"application/pdf\" width=\"600px\" height=\"500px\" />\n",
      "\n",
      "---\n",
      "title: Install Spark-Scala and PySpark\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Parallel and Distributed Computing\"\n",
      "---\n",
      "\n",
      "# How to install Spark? \n",
      "\n",
      "We'll explore 2 ways to install Spark :\n",
      "- using Jupyter Notebooks\n",
      "- using the Scala API\n",
      "- using the Python API (PySpark)\n",
      "\n",
      "## Using Jupyter Notebooks\n",
      "\n",
      "Programming in Scala in Jupyter notebooks requires installing a package to activate Scala Kernels:\n",
      "\n",
      "```bash\n",
      "pip install spylon-kernel\n",
      "python -m spylon_kernel install\n",
      "```\n",
      "\n",
      "Then, simply start a new notebook and select the `spylon-kernel`.\n",
      "\n",
      "## Using Scala\n",
      "\n",
      "To install Scala locally, download the Java SE Development Kit “Java SE Development Kit 8u181” from [Oracle's website](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html). Make sure to use version 8, since there are some conflicts with higher vesions. \n",
      "\n",
      "Then, on [Apache Spark website](http://spark.apache.org/downloads.html), download the latest version. When I did the first install, version 2.3.1 for Hadoop 2.7 was the last.\n",
      "\n",
      "Download the release, and save it in your Home repository. To know where it is located, type `echo $HOME` in your Terminal. It usually is `/Users/YourName/`.\n",
      "\n",
      "To make sure that the installation is working, in your terminal, in your Home repository, type `(replace your version) :\n",
      "\n",
      "```bash\n",
      "cd spark-2.3.1-bin-hadoop2.7/bin\n",
      "./spark-shell\n",
      "```\n",
      "\n",
      "Your terminal should look like this :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/spark1.jpg)\n",
      "\n",
      "A user interface, called the Spark Shell application UI, should also be accessible on [localhost:4040](localhost:4040).\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/spark2.jpg)\n",
      "\n",
      "Finally, we need to install SBT, an open-source build tool for Scala and Java projects, similar to Java's Maven and Ant.\n",
      "\n",
      "Its main features are:\n",
      "- Native support for compiling Scala code and integrating with many Scala test frameworks\n",
      "- The continuous compilation, testing, and deployment\n",
      "- Incremental testing and compilation (only changed sources are re-compiled, only affected tests are re-run, etc.)\n",
      "- Build descriptions written in Scala using a DSL\n",
      "- Dependency management using Ivy (which supports Maven-format repositories)\n",
      "- Integration with the Scala interpreter for rapid iteration and debugging\n",
      "- Support for mixed Java/Scala projects\n",
      "\n",
      "Installed in the terminal using :\n",
      "\n",
      "`brew install sbt`\n",
      "\n",
      "To check that the installation is fully working, run :\n",
      "\n",
      "```\n",
      "./spark-shell\n",
      "```\n",
      "\n",
      "You should see a Scala interpreter :\n",
      "```\n",
      "Welcome to\n",
      "   ____              __  \n",
      "  / __/__  ___ _____/ /__\n",
      " _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "/___/ .__/\\_,_/_/ /_/\\_\\   version 2.3.1\n",
      "   /_/\n",
      "\n",
      "Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181)\n",
      "Type in expressions to have them evaluated.\n",
      "Type: help for more information.\n",
      "\n",
      "scala>\n",
      "```\n",
      "\n",
      "## Using PySpark\n",
      "\n",
      "For PySpark, simply run :\n",
      "\n",
      "`pip install pyspark`\n",
      "\n",
      "Then, in your terminal, launch: `pyspark`\n",
      "\n",
      "Observe that you now have access to a Python interpreter instead of a Scala one.\n",
      "\n",
      "```\n",
      "Welcome to\n",
      "   ____              __\n",
      "  / __/__  ___ _____/ /__\n",
      " _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "/__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.1\n",
      "   /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 26 2018 08:42:37)\n",
      "SparkSession available as 'spark'.\n",
      ">>> \n",
      "````\n",
      "\n",
      "Doing this install, your are also able to use PySpark in Jupyter notebooks by running :\n",
      "\n",
      "```python\n",
      "import pyspark\n",
      "```\n",
      "\n",
      "> Conclusion: I hope this tutorial was helpful. I'd be happy to answer any question you might have in the comments section.\n",
      "---\n",
      "title: Loops\n",
      "layout: post\n",
      "tags: [da_tuto]\n",
      "subtitle : \"Data Analysis Basics\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "Loops are a way to execute code iteratively given a certain condition. The most popular loop in Python is the \"For loop\".\n",
      "\n",
      "# For loop\n",
      "\n",
      "Suppose that you want to sum all integers between 1 and 99. You could do it by hand, but don't you thing there's a faster way to do it? With for loops, yes !\n",
      "\n",
      "```python\n",
      "total_sum = 0\n",
      "\n",
      "for i in range(1, 100):\n",
      "\ttotal_sum = total_sum + i\n",
      "```\n",
      "\n",
      "What does this code do?\n",
      "\n",
      "We first define a variable which will contain the total sum. We then loop through values between 1 (included) and 100 (excluded) using the keyword `range`. For each of these values, we add it to the previous sum. \n",
      "\n",
      "There is also a more concise way to write it when we incrementally change the value of a variable:\n",
      "\n",
      "```python\n",
      "total_sum = 0\n",
      "\n",
      "for i in range(1, 100):\n",
      "\ttotal_sum += i\n",
      "```\n",
      "\n",
      "The `+=` sign will take the previous value of the variable and add the value of `i` to it.\n",
      "\n",
      "There is even a more concise way to write this. The `range(1,100)` actually creates a list of values. In front of a list, simply use `sum` to get the overall sum:\n",
      "\n",
      "```python\n",
      "sum(range(1,100))\n",
      "```\n",
      "\n",
      "In general, when dealing with loops, look for a way *not* to write a for-statement since it's rather slow to execute.\n",
      "\n",
      "# List comprehension\n",
      "\n",
      "In a for loop, you can decide to append new elements to a list for example based on a condition:\n",
      "\n",
      "```python\n",
      "my_list = []\n",
      "\n",
      "for i in range(1,100):\n",
      "\tif i%10 == 0:\n",
      "\t\tmy_list.append(i)\n",
      "```\n",
      "\n",
      "This will append only multiples of 10 to the list, for all values between 1 and 99 included.\n",
      "\n",
      "There is a shorter way to write this, and this syntax is called list compregension:\n",
      "\n",
      "```python\n",
      "my_list = [i for in range(1,100) if i%10 == 0]\n",
      "```\n",
      "\n",
      "It does exactly the same thing, in 1 line of code instead of 4. You just have to get used to this compact syntax.\n",
      "\n",
      "# While loops\n",
      "\n",
      "While loops are loops which let you execute a code while a certain condition is met. For example, we can do the exact same operation as before:\n",
      "\n",
      "```python\n",
      "total_sum = 0\n",
      "i = 0\n",
      "\n",
      "while i < 100:\n",
      "\ttotal_sum += i\n",
      "\ti += 1\n",
      "```\n",
      "\n",
      "With this while condition, we must increment both the total sum and the index at each step. It is longer to write, and therefore not often used. However, these conditions are useful to test for a True/False condition over which we have less control in for-loops and would require an if-condition in the code:\n",
      "\n",
      "```python\n",
      "total_sum = 0\n",
      "i = 0\n",
      "\n",
      "while total_sum < 4500:\n",
      "\ttotal_sum += i\n",
      "\ti += 1\n",
      "```\n",
      "\n",
      "This loop stops turning when total_sum hits 4500 or above.\n",
      "\n",
      "\n",
      "> If you found the article useful or see ways in which it could be improved, please leave a comment :)\n",
      "---\n",
      "title: Anomaly Detection\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Supervise and Unsupervised Algorithms\"\n",
      "---\n",
      "\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "> “An anomaly is an observation that deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism.\", Hawkins (1980)\n",
      "\n",
      "Anomaly detection is used in :\n",
      "- network intrusions\n",
      "- creit card fraud detection\n",
      "- insurance\n",
      "- finance\n",
      "- surveillance\n",
      "- rental services\n",
      "- ...\n",
      "\n",
      "There are 3 types of anomaly detection :\n",
      "- supervised : we have labels for both normal data and anomalies\n",
      "- semi-supervised : only normal data is available, no outliers are present\n",
      "- unsupervised : no labels, we suppose that anomalies are rare events\n",
      "\n",
      "The key steps in anomaly detection are the following :\n",
      "- learn a profile of a normal behavior, e.g. patterns, summary statistics...\n",
      "- use that normal profile to build a decision function\n",
      "- detect anomalies among new observations\n",
      "\n",
      "# Unsupervised Anomaly Detection\n",
      "\n",
      "In unsupervised anomaly detection, we make the assumption that anomalies are rare events. The underlying data are unlabeled (no normal/abnormal label), hence the denomination. Anomalies are events supposed to be locates in the tail of the distribution.\n",
      "\n",
      "## Minimum Volume Set\n",
      "\n",
      "We usually estimate the region where the data is the most concentrated as the minimal region containing $$ \\alpha % $$ of the values. This approach is called **Minimum Volume Set**. Samples that do not belong to the Minimum Volume Set are anomalies. For a small value of $$ \\alpha $$ on the other hand, we tend to recover the modes.\n",
      "\n",
      "We can define the MV Set as :\n",
      "\n",
      "$$ Q( \\alpha ) = argmin_{c \\in C} \\{ \\lambda(C), P(X \\in C) ≥ \\alpha \\} $$\n",
      "\n",
      "Where :\n",
      "- $$ \\alpha $$ is a factor close to 1 that reflects the percentage of values in the MV Set.\n",
      "- $$ C $$ classes of measurable sets\n",
      "- $$ \\lambda $$ a Lebesgue measure\n",
      "- $$ \\mu(dx) $$ the unknown probability measure of the observations\n",
      "\n",
      "The goal is to learn a minimum volume set $$ Q(\\alpha) $$ for $$ X_1, \\cdots, X_n $$\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/anomaly_1.jpg)\n",
      "\n",
      "There are theoretical guarantees that there exists a unique MV set at level $$ \\alpha $$.\n",
      "\n",
      "Let's now make the assumption that $$ \\mu $$ has a density $$ h(X) $$ that :\n",
      "- is bounded\n",
      "- has no plateau\n",
      "\n",
      "## Algorithms for anomaly detection\n",
      "\n",
      "### Plug-in technique\n",
      "\n",
      "This technique consists in seeing MV sets as density level sets :\n",
      "\n",
      "$$ G_{\\alpha}* = \\{ x \\in R^d : h(x) ≥ t_{\\alpha} \\} $$\n",
      "\n",
      "Using a naive approach (the 2-split trick), we can :\n",
      "- compute a density estimator using parametric models or local averaging $$ \\hat{h}(x) $$ based on $$ X_1, \\cdots, X_n $$\n",
      "- based on a second sample $$ X_1*, \\cdots, X_n* $$, compute the empirical quantile corresponding to the $$ \\alpha $$ percentile\n",
      "\n",
      "The output is : \n",
      "\n",
      "$$ \\hat{G_{\\alpha}} * = \\{ x: \\hat{h_n(x)} ≥ \\hat{h_n} ( X_{n \\alpha}' ) \\} $$\n",
      "\n",
      "### Unsupervised as binary classification\n",
      "\n",
      "We can turn the unsupervised anomaly detection problem as a binary classification. \n",
      "- Suppose that our data lie into $$ [0,1]^d $$, or that we scaled the data before. We have $$ n $$ points.\n",
      "- We then generate a new sample from the uniform distribution $$ U([0,1]^d) $$. We generate $$ m $$ points.\n",
      "- We assign a negative label to the generated sample.\n",
      "- We assign a positive label to the true sample.\n",
      "\n",
      "We have : \n",
      "\n",
      "$$ \\frac{n}{n+m} = p $$ approximately.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/anomaly_2.jpg)\n",
      "\n",
      "The solution of this binary classification mimics the Bayes Classifier where we predict $$ 1 $$ on the set $$ \\{ x \\in [0,1]^d : h(x) ≥ \\frac{1-p}{p} \\} $$, and $$ -1 $$ otherwise.\n",
      "\n",
      "This solution provides a MV set at level $$ \\alpha = P(h(X) ≥ \\frac{1}{p-1}) $$. We select $$ \\alpha $$ manually, and we tune $$ p $$ using a grid search.\n",
      "\n",
      "### Histograms\n",
      "\n",
      "In histograms, we suppose that we have a compact feature space, and that we build partitions $$ C_1, \\cdots, C_K $$ formed of measurable subsets of same volume :\n",
      "\n",
      "$$ \\lambda(C_1) = \\cdots = \\lambda(C_K) $$\n",
      "\n",
      "The class $$ G_P $$ is the ensemble composed of unions of $$ C_K $$. We want to get the minimal number of partitions to keep in order to isolate the normal data and exclude the anomalies. More formally, we are looking for the solution of :\n",
      "\n",
      "$$ \\min_{G \\in P : \\hat{\\mu_n}(G) ≥ \\alpha - \\phi} \\lambda(G) $$\n",
      "\n",
      "There is a simple and fast procedure to do this :\n",
      "- For each class $$ k = 1 \\cdots K $$, we compute $$ \\hat{\\mu_n}(C_k) $$, i.e the proportion of data in each class \n",
      "- Sort the cells by decresing order of $$ \\hat{\\mu_n}(C_k) $$\n",
      "- Find the minimal value of k such that : $$ \\hat{k} = argmin \\{ k : \\sum_i \\hat{\\mu_n}(C_i) ≥ \\alpha - \\phi \\} $$. In other words, we exclude $$ \\alpha $$ percent of the values based on this histogram.\n",
      "- The output is $$ \\hat{G_{\\alpha}}* = U_i^{\\hat{k}} C_{(i)} $$\n",
      "\n",
      "Where $$ \\phi $$ is a tolerance level. However, such technique is usually not flexible enough.\n",
      "\n",
      "### Decision Trees \n",
      "\n",
      "The partition should be determined based on the **data**. We adopt a top-down strategy, and start from the root node $$ C_{0,0} $$. The volume of any cell $$ C_{j,k} $$ can be computed in a recursive manner.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/anomaly_3.jpg)\n",
      "\n",
      "Hyperrectangless with the label 1 are subsets of the MV set estimate. At each node $$ C_{j,k} $$, the split leading to siblings $$ C_{j+1,2k} $$ and $$ C_{j+1,2k+1} $$ minimizes recursively the volume of the current ddecision set G under the following constraint :\n",
      "\n",
      "$$ \\hat{\\mu_n}(G) ≥ \\alpha(-\\phi) $$\n",
      "\n",
      "To avoid overfitting, we generally prune the tree after growing it to avoid overfitting.\n",
      "\n",
      "### Isolation Forest\n",
      "\n",
      "In insolation forests, we build a forest of isolation trees under the assumption that anomalies are more susceptible to isolation under random partitioning.\n",
      "\n",
      "The main idea behind an **isolation tree** is to recursively (top-down) build a tree that :\n",
      "- chooses randomly a splitting variable \n",
      "- splits at value $$ t $$\n",
      "- splits the cells if the obsevation is larger or lower than $$ t $$ \n",
      "\n",
      "This process stops when a depth limit is reached usually :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/anomaly_4.jpg)\n",
      "\n",
      "# Supervised Anomaly Detection\n",
      "\n",
      "In the supervised Setup, we have pairs of data $$ (X,Y) $$ values in $$ R^d \\times \\{-1,+1\\} $$. If a data point has a positive label (Y=1), it is an anomaly. This is a classic binary classification task. \n",
      "\n",
      "We also define a randking among anomalies, with the first data points the ones that have the highest probability to be anomalies. This is called *Bipartite Ranking*. It is a different approach from classification, since classification remains a local task, whereas ranking is global.\n",
      "\n",
      "We rank and score a set of instances through a scoring function $$ s(X) $$ in which large number instances with label 1 appear on top of the list with high probabilities.\n",
      "\n",
      "How do we measure the quality of a ranking?\n",
      "\n",
      "A natural estimate of the ranking performance : \n",
      "\n",
      "$$ U(s) = P \\{ (s(X) -s(X'))(Y-Y')>0\\} $$\n",
      "\n",
      "is the U-Statistic :\n",
      "\n",
      "$$ \\hat{U}_n(s) = \\frac{2}{n(n-1)} \\sum_{1≤i<j≤n}1 \\{ (s(X_i) - s(X_j))(Y_i - Y_j) > 0 \\} $$\n",
      "\n",
      "This measure is also called the **rate of concording pairs**, or **Kendall's association coefficient**. One of the issues is that the computation of the gradients typically require to average over $$ O(n^2) $$ pairs. \n",
      "\n",
      "The criterion we want to maximize is :\n",
      "\n",
      "$$ L(s) = P \\{s(X^{(1)}) < \\cdots < s(X^{(k)}) \\mid Y^{(1)} = 1, \\cdots, Y^{(k)} = K \\} $$\n",
      "\n",
      "The empirical counterpart of $$ L(s) $$ is really prohibitive to compute, and the maximization is even unfeasible. To overcome such issues, generalized U-Statistics using Kernels or incomplete U-Statistics using only a sample of terms can be used.\n",
      "\n",
      "---\n",
      "title: Hadoop with the HortonWorks Sandbox (1/4)\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Parallel and Distributed Computing\"\n",
      "---\n",
      "\n",
      "In the next 4 articles, we'll launch Hadoop MapReduce jobs (WordCount on a large file) using the HortonWorks Sandbox.\n",
      "\n",
      "## Getting started with the VM\n",
      "\n",
      "### Install and launch\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/hort.jpg)\n",
      "\n",
      "The Sandbox by Hortonworks is a straightforward, pre-configured, learning environment that contains the latest developments from Apache Hadoop, specifically the Hortonworks Data Platform (HDP). The Sandbox comes packaged in a virtual environment that can run in the cloud or on your machine. Sandbox also offers a data-in-motion framework for IoT solutions called Hortonworks Data Flow (HDF). To configure Hadoop from scratch on a Linux VM, this tutorial might be useful: https://www.tutorialspoint.com/hadoop/hadoop_enviornment_setup.htm\n",
      "\n",
      "\n",
      "Hortonworks offers a way to use Hadoop Tools connecting to a Virtual Machine in SSH for command lines interfaces. Numerous web interfaces are also available. \n",
      "\n",
      "The sandbox can be downloaded from [here](https://www.cloudera.com/downloads/hortonworks-sandbox.html).\n",
      "\n",
      "Download the **HDP** Sandbox. This Sandbox makes it easy to get started with Apache Hadoop, Apache Spark, Apache Hive, Apache HBase, Druid and Data Analytics Studio (DAS). Choose the VirtualBox installation type.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/26.jpg)\n",
      "\n",
      "Fill in the form and download the **version 2.6.5** of the Sandbox. The sandbox requires around 15 Go of space.\n",
      "\n",
      "Once this is done (the download might take a long time) :\n",
      "- Open VirtualBox\n",
      "- Click on \"New\"\n",
      "- Go in \"File -> Import a Virtual Machine\".\n",
      "- Select the image of the Sandbox you just downloaded.\n",
      "- Start the VM\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/33.jpg)\n",
      "\n",
      "The first boot takes a while, so time for a break!\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/28.jpg)\n",
      "\n",
      "### Access the Sandbox\n",
      "\n",
      "The application is now ready to be used :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/29.jpg)\n",
      "\n",
      "**User** \n",
      "\n",
      "Once started, the Sandbox is accessible from your local computer, in SSH, on the Port 2222. We will be using the username: `raj_ops` and password `raj_ops` as it is pre-registered. \n",
      "\n",
      "There are many pre-configured users for the HortonWorks Sandbox, including :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/27.jpg)\n",
      "\n",
      "**SSH**\n",
      "\n",
      "Launch your terminal and access the Sandbox using SSH :\n",
      "\n",
      "`ssh raj_ops@localhost -p 2222`\n",
      "\n",
      "If you are asked whether you'd like to permanently add 2222 to the list of known hosts, say Yes.\n",
      "\n",
      "**Splash Page**\n",
      "\n",
      "We should be able to access a graphical view of the services available on a Hadoop cluster. It's called the VirtualBox Splash Page, and it can be accessed on :\n",
      "\n",
      "`http://localhost:8080`\n",
      "\n",
      "You will be asked to log-in :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/34.jpg)\n",
      "\n",
      "Then, after loading, the page should look like this :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/35.jpg)\n",
      "\n",
      "You are now connected to the Sandbox and you can access the HDFS file system. We'll dive deeper into this in the next articles.\n",
      "\n",
      "> Conclusion: I hope this tutorial was clear and helpful. I'd be happy to answer any question you might have in the comments section.\n",
      "---\n",
      "title: Introduction to Computer Vision\n",
      "layout: post\n",
      "tags: [computervision]\n",
      "subtitle : \"Computer Vision\"\n",
      "---\n",
      "\n",
      "In this series of articles, I'll explore computer vision, starting with classical techniques for image processing, and progressively introducing deep learning techniques and the improvements they bring These articles are inspired by the Computer Vision course of TTI Chicago.\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "## What is computer vision?\n",
      "\n",
      "Computer vision is a field of computer science whose role is to analyze pictures (and videos) to develop some sort of understanding of the image, for example finding edges, detecting objects, tracking someone...\n",
      "\n",
      "Classical models to extract that information include :\n",
      "- physical models (geometry, light...)\n",
      "- probabilistic models\n",
      "\n",
      "Latest approaches include Deep Learning and offer outstanding results.\n",
      "\n",
      "Computer vision can be seen as an inverse problem in which we describe the world that we see in an image and try to reconstruct its properties (shape, illumination, color distribution...).\n",
      "\n",
      "Computer vision was initiated around 1966 at MIT and was first applied to geometry. Computer vision however suffered from a lack of data and computing power. \n",
      "\n",
      "## What can computer vision be used for?\n",
      "\n",
      "The main fields in which computer vision is applied are :\n",
      "- Object verification: \"Is that a lamp ?\"\n",
      "- Detection: \"Where are the people ?\"\n",
      "- Activity recognition: \"What are they doing ?\"\n",
      "- Pose detection: \"Which pose do they have ?\"\n",
      "- Description of attributes and relations: \"Crowded square in China\"\n",
      "- Image enhancing / super-resolution / denoising\n",
      "- Image uncropping / increasing field of view\n",
      "- Image completion\n",
      "- Fingerprint Recognition\n",
      "- Face detection / recognition\n",
      "- Facial emotions recognition\n",
      "- 3d Reconstruction \n",
      "- Medical Imaging\n",
      "- Assisted Driving and smart cars\n",
      "- ...\n",
      "\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_1.jpg)\n",
      "\n",
      "> **Conclusion **: I hope this quick introduction to autoencoder was clear. Don't hesitate to drop a comment if you have any question.\n",
      "---\n",
      "title: Gradient Boosting Regression\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Supervised Learning Algorithms\"\n",
      "---\n",
      "\n",
      "In the previous article, I presented AdaBoost, a powerful boosting algorithm which brings some modifications compared to bagging algorithms. In this article, I'll present the key concepts of Gradient Boosting. Regression and classification are quite different concepts for Gradient Boosting. In this article, we'll focus on regression.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# Gradient Boosting vs. AdaBoost\n",
      "\n",
      "Gradient Boosting can be compared to AdaBoost, but has a few differences :\n",
      "\n",
      "- Instead of growing a forest of stumps, we initially predict the average (since it's regression here) of the y-column and build a decision tree based on that value.\n",
      "- Like in AdaBoost, the next tree depends on the error of the previous one.\n",
      "- But unlike AdaBoost, the tree we grow is not only a stump but a real decision tree.\n",
      "- As in AdaBoost, there is a weight associated with the trees, but the scale factor is applied to all the trees.\n",
      "\n",
      "# Gradient Boosting steps\n",
      "\n",
      "Let's consider a simple scenario in which we have several features, $$ x_1, x_2, x_3, x_4 $$ and try to predict $$ y $$. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/tab_1.jpg)\n",
      "\n",
      "**Step 1** : Make the first guess\n",
      "\n",
      "The initial guess of the Gradient Boosting algorithm is to *predict the average value of the target $$ y $$*. For example, if our features are the age $$ x_1 $$ and the height $$ x_2 $$ of a person... and we want to predict the weight of the person.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/tab_2.jpg)\n",
      "\n",
      "**Step 2** : Compute the pseudo-residuals\n",
      "\n",
      "For the variable $$ x_1 $$, we compute the difference between the observations and the prediction we made. This is called the pseudo-residuals.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/tab_3.jpg)\n",
      "\n",
      "We compute the pseudo-residuals for the first feature  $$ x_1 $$.\n",
      "\n",
      "**Step 3** : Predict the pseudo-residuals\n",
      "\n",
      "Then, we will be using the features $$ x_1, x_2,x_3, x_4 $$ to predict the pseudo-residuals column.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/tab_4.jpg)\n",
      "\n",
      "We can now predict the pseudo-residuals using a tree, that typically has 8 to 32 leaves (so larger than a stump). By restricting the number of leaves of the tree we build, we obtain less leaves than residuals. Therefore, the outcome of a given branch of the tree is the average of the columns that lead to this leaf, as in a regression tree.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/tab_5.jpg)\n",
      "\n",
      "**Step 4** : Make a prediction and compute the residuals\n",
      "\n",
      "To make a prediction, we say that the average is 13.39. Then, we take our observation, run in through the tree, get the value of the leaf, and add it to 13.39. \n",
      "\n",
      "If we stop here, we will most probably overfit. Gradient Boost applies a learning rate $$ lr $$ to scale the contribution from a new tree, by applying a factor between 0 and 1.\n",
      "\n",
      "$$ y_{pred} = \\bar{y_{train}} + lr \\times res_{pred} $$\n",
      "\n",
      "The idea behind the learning rate is to make a small step in the right direction. This allows an overall lower variance.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/tab_7.jpg)\n",
      "\n",
      "Notice how all the residuals got smaller now.\n",
      "\n",
      "**Step 5** : Make a second prediction\n",
      "\n",
      "Now, we :\n",
      "- build a second tree\n",
      "- compute the prediction using this second tree\n",
      "- compute the residuals according to the prediction\n",
      "- build the third tree\n",
      "- ...\n",
      "\n",
      "Let's just cover how to compute the prediction. We are still using the features $$ x_1, x_2, x_3, x_4 $$ to predict the new residuals Pseudo_Res_2. \n",
      "\n",
      "We build a tree to estimate those residuals. Once we have this tree (with a limited number of leaves), we are ready to make the new prediction :\n",
      "\n",
      "$$ y_{pred} = \\bar{y_{train}} + lr \\times res_{pred_1} + lr \\times res_{pred_2} $$\n",
      "\n",
      "The prediction is equal to :\n",
      "- the average value initially computed\n",
      "- plus LR * the predicted residuals at step 1\n",
      "- plus LR * the predicted residuals at step 2\n",
      "\n",
      "Notice how we always apply the same Learning Rate. We are now ready to compute the new residuals, fit the 3rd tree on it, compute the 4th residuals... and so on, until :\n",
      "- we reach the maximum number of trees specified\n",
      "- or we don't learn significantly anymore\n",
      "\n",
      "# Full Pseudo-code\n",
      "\n",
      "The algorithm can be then described as the following, on a dataset $$ (x,y) $$ with $$ x $$ the features and $$ y $$ the targets, with a differentiable loss function $$ \\cal{L} $$:\n",
      "\n",
      "$$ \\cal{L} = \\frac {1} {2} (Obs - Pred)^2 $$, called the Squared Residuals. Notice that since the function is differentiable, we have :\n",
      "\n",
      "$$ \\frac { \\delta } {\\delta Pred} \\cal{L} = - 1 \\times (Obs - Pred) $$\n",
      "\n",
      "**Step 1** : Initialize the model with a constant value : $$ F_0(x) = argmin_{\\gamma} \\sum_i \\cal{L}(y_i, \\gamma) $$. We simply want to minimize the sum of the squared residuals (SSR) by choosing the best prediction $$ \\gamma $$.\n",
      "\n",
      "If we derive the optimal value for  $$ \\gamma $$ :\n",
      "\n",
      "$$ \\frac { \\delta } {\\delta \\gamma } \\sum_i \\cal{L}(y_i, \\gamma) = -(y_1 - \\gamma) + -(y_2 - \\gamma) + -(y_3 - \\gamma) + ... = 0 $$\n",
      "\n",
      "$$ \\sum_i y_i - n * \\gamma = 0 $$\n",
      "\n",
      "$$ \\gamma = \\frac{ \\sum_i y_i }{n} = \\bar{y} $$\n",
      "\n",
      "This is simply the average of the observations. This justifies our previous constant initialization. In other words, we created a leaf that predicts all samples will weight the average of the samples.\n",
      "\n",
      "**Step 2** : For m = 1 to M (the maximum number of trees specified, e.g 100) \n",
      "\n",
      "- a) Compute the pseudo-residuals for every sample :\n",
      "\n",
      "$$ r_{im} = - \\frac {\\delta \\cal{L} (y_i, F(x_i)) } {\\delta F(x_i)} = - ( - 1 \\times (Obs - F_{m-1}(x)) ) = (Obs - F_{m-1}(x)) = (Obs - Pred) $$\n",
      "\n",
      "This derivative is called the Gradient. The Gradient Boost is named after this.\n",
      "\n",
      "- b) Fit a regression tree to the $$ r_{im} $$ values and create terminal regions $$ R_{jm} $$ for j = 1, ... , $$ J_m $$, i.e create the leaves of the tree. At that point, we still need to compute the output value of each leaf.\n",
      "\n",
      "- c) For each leaf j = 1... $$ J_m $$, compute the output value that minimized the SSR : $$ \\gamma_{jm} = argmin_{\\gamma} \\sum_{x_i \\in R_{ij}} \\cal{L}(y_i, F_{m-1} + \\gamma) $$. In other words, we will simply predict the output of all the samples stored in a certain leaf.\n",
      "\n",
      "- d) Make a new prediction for each sample by updating, accoridng to a learning rate $$ lr \\in (0,1) $$ :\n",
      "$$ F_m(x) = F_{m-1}(x) + lr \\times \\sum_j \\gamma_{jm} I(x \\in R_{jm} ) $$. We compute the new value by summing the previous prediction and all the predictions $$ \\gamma $$ into which our sample falls.\n",
      "\n",
      "# Implement a high-level Gradient Boosting in Python\n",
      "\n",
      "Since the pseudo-code detailed above might be a bit tricky to understand, I've tried to summarize a high-level idea of Gradient Boosting, and we'll be implementing it in Python.\n",
      "\n",
      "**Step 1** : Initialize the model with a constant value : $$ \\gamma = \\frac{ \\sum_i y_i }{n} = \\bar{y} $$\n",
      "\n",
      "This is simply the average of the observations. \n",
      "\n",
      "**Step 2** : For each tree m = 1 to M (the maximum number of trees specified, e.g 100) \n",
      "\n",
      "- a) Compute the pseudo-residuals for every sample, i.e the true value - the predicted value :\n",
      "\n",
      "$$ r_{im}  = (Obs - Pred) $$\n",
      "\n",
      "- b) Fit a regression tree on the residuals, and predict the residuals $$ r_t $$\n",
      "\n",
      "- c) Update the prediction : \n",
      "\n",
      "$$ Pred_t(x) = Pred_{t-1}(x) + lr \\times r_t $$\n",
      "\n",
      "## Data generation\n",
      "\n",
      "We start by generating some data for our regression :\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "x = np.arange(0,50)\n",
      "x = pd.DataFrame({'x':x})\n",
      "\n",
      "y1 = np.random.uniform(10,15,10)\n",
      "y2 = np.random.uniform(20,25,10)\n",
      "y3 = np.random.uniform(0,5,10)\n",
      "y4 = np.random.uniform(30,32,10)\n",
      "y5 = np.random.uniform(13,17,10)\n",
      "\n",
      "y = np.concatenate((y1,y2,y3,y4,y5))\n",
      "y = y[:,None]\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/tab_12.jpg)\n",
      "\n",
      "## Fit a simple decision tree\n",
      "\n",
      "To illustrate the limits of decision trees, we can try to fit a simple decision tree with a maximal depth of 1, called a stump.\n",
      "\n",
      "```python\n",
      "from sklearn import tree\n",
      "\n",
      "clf = tree.DecisionTreeRegressor(max_depth=1)\n",
      "model = clf.fit(x,y)\n",
      "pred = model.predict(x)\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(x, pred, c='red')\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/tab_13.jpg)\n",
      "\n",
      "This is the starting point for our estimation. Now, we need to go further and make our model more complex by implementing gradient boosting.\n",
      "\n",
      "## Implement Gradient Boosting\n",
      "\n",
      "```python\n",
      "xi = x.copy()\n",
      "yi = y.copy()\n",
      "\n",
      "# Initialize error to 0\n",
      "ei = 0\n",
      "n = len(yi)\n",
      "\n",
      "# Initialize predictions with average\n",
      "predf = np.ones(n) * np.mean(yi)\n",
      "\n",
      "lr = 0.3\n",
      "\n",
      "# Iterate according to the number of iterations chosen\n",
      "for i in range(101):\n",
      "\n",
      "    # Step 2.a)\n",
      "    # Fit the decision tree / stump (max_depth = 1) on xi, yi\n",
      "\n",
      "    clf = tree.DecisionTreeRegressor(max_depth=1)\n",
      "    model = clf.fit(xi, yi)\n",
      "\n",
      "    # Use the fitted model to predict yi\n",
      "\n",
      "    predi = model.predict(xi)\n",
      "\n",
      "    # Step 2.c)\n",
      "    # Compute the new prediction (learning rate !)\n",
      "    # Compute the new residuals, \n",
      "    # Set the new yi equal to the residuals\n",
      "\n",
      "    predf = predf + lr * predi\n",
      "    ei = y.reshape(-1,) - predf\n",
      "    yi = ei\n",
      "\n",
      "    # Every 10 iterations, plot the prediction vs the actual data\n",
      "    if i % 10 == 0 :\n",
      "        plt.figure(figsize=(12,8))\n",
      "        plt.plot(x, predf, c='r')\n",
      "        plt.scatter(x, y)\n",
      "        plt.title(\"Iteration \" + str(i))\n",
      "        plt.show()\n",
      "\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/fit_gbc.jpg)\n",
      "\n",
      "By increasing the learning rate, we tend to overfit. However, if the learning rate is too low, it takes a large number of iterations to even approach the underlying structure of the data.\n",
      "\n",
      "> **Conclusion** : I hope this introduction to Gradient Boosting was helpful. The topic can get much more complex over time, and the implementation is Scikit-learn is much more complex than this. In the next article, we'll cover the topic of classification.\n",
      "---\n",
      "title: Self-training and pre-training, understanding the wav2vec series\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Speech Processing\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "You might have already heard of Fairseq, a sequence-to-sequence toolkit written in PyTorch by FacebookAI. One of the most common applications of Fairseq among speech processing enthusiasts is wav2vec (and all the variants), a framework that aims to extract new types of input vectors for acoustic models from raw audio, using pre-training and self-supervised learning. \n",
      "\n",
      "In this article, we will cover the 4 core papers of this wav2vec series, all of them coming from Facebook AI. All these papers are building blocks of what could be a great innovation in speech recognition but also a lot of other downstream tasks related to speech:\n",
      "- [wav2vec paper](https://arxiv.org/abs/1904.05862)\n",
      "- [vq - wav2vec](https://arxiv.org/abs/1910.05453)\n",
      "- [wav2vec2.0 paper](https://arxiv.org/abs/2006.11477)\n",
      "- [Self-training and Pre-training are Complementary for Speech Recognition](https://arxiv.org/abs/2010.11430)\n",
      "\n",
      "# 1. wav2vec\n",
      "\n",
      "It is not new that speech recognition tasks require huge amounts of data, commonly hundreds of hours of labeled speech. Pre-training of neural networks has proven to be a great way to overcome limited amount of data on a new task.\n",
      "\n",
      "## a. What is pre-training?\n",
      "\n",
      "What we mean by **pre-training** is the fact of training a first neural network on a task where lots of data are available, saving the weights, and creating a second neural network by initializing the weights as the ones saved from the first one. This learns general representations on huge amounts of data, and can supposedly improve the performance on the new task with limited data. This has been applied extensively in Computer Vision, Natural Language Processing, and more recently, for certain speech tasks.\n",
      "\n",
      "When pre-training, you can either do it:\n",
      "- in a supervised fashion\n",
      "- or in an unsupervised fashion\n",
      "\n",
      "Supervised pre-training is clear. This is similar to transfer learning where you pre-train a model, knowing you $$ X $$ and $$ y $$. But for unsupervised pre-training, you learn a representation of speech. **wav2vec**, is a convolutional neural network (CNN) that takes raw audio as input and computes a general representation that can be input to a speech recognition system. The objective is a contrastive loss that requires distinguishing a true future audio sample from negatives.\n",
      "\n",
      "## b. The model\n",
      "\n",
      "Given an input signal context (speech up to a certain time-stamp), the aim is to predict the next observations from this speech sample.\n",
      "\n",
      "*Problem:* This usually requires being able to properly model $$ p(x) $$, the distribution of speech samples.\n",
      "\n",
      "*Solution:* Lower the dimensionality of the speech sample through an \"encoder network\", and then use a *context network* to predict the next values. wav2vec learns representations of audio data by solving a self-supervised context-prediction task.\n",
      "\n",
      "More formally, given audio samples $$x_i \\in X$$, we:\n",
      "- learn a first *encoder network*, based on a CNN, that maps $$ X $$ to $$ Z $$: \n",
      "\n",
      "$$ f:X \\to Z $$\n",
      "\n",
      "- learn a second *context network*, based on a CNN too, that maps $$ Z $$ to a single contextualized tensor $$ C $$:\n",
      "\n",
      "$$ g:Z \\to C $$\n",
      "\n",
      "A representation of these 2 networks is presented in the figure below:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/wav_0.png)\n",
      "\n",
      "Here are the network details of wav2vec implementation:\n",
      "- the encoder is a 5-layer CNN, with kernel sizes (10, 8, 4, 4, 4) and strides (5, 4, 2, 2, 2), and covers 30ms of audio. Layers have 512 channels, a group normalization layer, and a ReLU nonlinearity. \n",
      "- the context network has nine layers with kernel size three and stride one, and the total receptive field of the context network is about 210 ms. Layers have 512 channels, a group normalization layer, and a ReLU nonlinearity. \n",
      "\n",
      "## c. Loss function\n",
      "\n",
      "The model learns to distinguish a true sample $$z_{ik} $$, $$ k $$ steps in the future, from a proposal distribution $$ p_n $$, called a contrastive loss, defined by:\n",
      "\n",
      "$$ \\mathcal{L}_{k}=-\\sum_{i=1}^{T-k}\\left(\\log \\sigma\\left(\\mathbf{z}_{i+k}^{\\top} h_{k}\\left(\\mathbf{c}_{i}\\right)\\right)+\\underset{\\tilde{\\mathbf{z}} \\sim p_{n}}{\\mathbb{E}}\\left[\\log \\sigma\\left(-\\mathbf{\\tilde { z }}^{\\top} h_{k}\\left(\\mathbf{c}_{i}\\right)\\right)\\right]\\right) $$\n",
      "\n",
      "\n",
      "We then optimize the loss over several time steps:\n",
      "\n",
      "$$ \\mathcal{L} = \\sum_{k=1}^K \\mathcal{L}_k $$\n",
      "\n",
      "\n",
      "To get the expectation of the proposal distribution, we sample ten negative examples by choosing uniformly distractors from these negative audio sequences. In order words, we average ten uniformly sampled values from different audio samples. $$ \\lambda $$ is the number of negative samples (10 lead to the best performance).\n",
      "\n",
      "By predicting the next steps, we perform a task, called self-supervised training for speech. But it is also widely applied in NLP and CV.\n",
      "\n",
      "## d. Self-supervised learning\n",
      "\n",
      "In self-supervised learning, we train a model using labels that are naturally part of the input data, rather than requiring separate external labels. For example, in an NLP model, we train a model to predict the next words. This information is part of the training data itself, and the model learns some information on the nature of language. Think of models like GPT or ULMFiT that do this in NLP. GPT-3 appears to be so good at this that you can use it for Question Answering on generic topics without fine-tuning, and get proper replies.\n",
      "\n",
      "In Computer Vision, the implementation is slightly different. We still need to train a model in a self-supervised way using a \"context task\", with the idea to focus on a \"downstream task\". Several pretext tasks can be used, such as colorization, placing images in patches, placing frames in the right order... For videos, for example, a common workflow is to train a model on one or multiple pretext tasks with unlabelled videos and then feed one intermediate feature layer of this model to fine-tune a simple model on downstream tasks of action classification, segmentation, or object tracking.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/wav_2.png)\n",
      "\n",
      "Self-supervised training can allow you to use 1000x less training data for a given downstream task.\n",
      "\n",
      "## e. Acoustic Models\n",
      "\n",
      "wav2vec is used as an input to an acoustic model. The vector supposedly carries more representation information than other types of features. It can be used as an input in a phoneme or grapheme-based wav2letter ASR model. The model then predicts the probabilities over 39-dimensional phoneme or 31-dimensional graphemes.\n",
      "\n",
      "## f. Decoding\n",
      "\n",
      "Regarding the language model decoding, the authors considered a 4-gram language model, a word-based convolutional language model, and a character-based convolutional language model. Word sequences are decoded using beam-search.\n",
      "\n",
      "## g. Results\n",
      "\n",
      "Pre-training reduces WER by 36 % on nov92 when only about eight hours of transcribed data is available. It also improved the PER on the TIMIT database compared to a baseline system, and the more pre-training data, the better the results were (Librispeech + WSJ in their best system). Results on TIMIT are presented in the table below.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/wav_1.png)\n",
      "\n",
      "# 2. vq-wav2vec \n",
      "\n",
      "vq-wav2vec introduces self-supervised learning of discrete speech representations. What we mean by discrete here is that we do not have vectors that take continuous values, but a set of given values only. Discretization, rather than enables the direct application of algorithms from the NLP community which require discrete inputs. In other words, vq-wav2vec, learns vector quantized (VQ) representations of audio data using a future time-step prediction task.\n",
      "\n",
      "## a. Model architecture\n",
      "\n",
      "The model is said to word on discrete speech representations since it adds an additional layer of quantization. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/wav_3.png)\n",
      "\n",
      "This additional layer $$ q : Z \\to \\tilde{Z} $$ that takes $$ Z $$, a dense representation learned by the first CNN, and outputs a discrete representation, using either K-means clustering or the Gumbel-Softmax as constraints in a Vector Quantized Variational Autoencoders. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/wav_4.png)\n",
      "\n",
      "Finally, this discrete representation is fed as an input to the context network $$ g : \\tilde{Z} \\to C $$.\n",
      "\n",
      "## b. What to do with this discrete representation?\n",
      "\n",
      "The discrete speech representation is then fed to train a BERT architecture. BERT is a pre-training approach for NLP tasks, which uses a transformer encoder model to build a representation of text. Transformers use self-attention to encode the input sequence as well as an optional source sequence.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/wav_5.png)\n",
      "\n",
      "\n",
      "In prediction, we fetch speech representations that are used as inputs to an acoustic model.\n",
      "\n",
      "## c. Acoustic Model\n",
      "\n",
      "Authors used wav2letter as an acoustic model and trained for 1,000 epochs on 8\n",
      "GPUs for both TIMIT and WSJ. Language models used were a 4-gram KenLM language model and a character-based convolutional language model.\n",
      "## d. Results\n",
      "\n",
      "Results on the character-based convolution language model and a Gumbel + Bert base vq-wav2vec reach state-of-the-art on the WSJ dataset.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/wav_6.png)\n",
      "\n",
      "Another important contribution of this paper is that, by exploring quantization, highly reduces the bitrate for a given model performance. Acoustic models on vq-wav2vec achieve the best results across most bitrate settings.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/wav_7.png)\n",
      "\n",
      "## e. End-to-end model\n",
      "\n",
      "One last important work, although results were not that good, is the sequence-to-sequence model that authors explain in a small paragraph. Rather than training BERT on discretized speech, and inputting it in the acoustic model, one could solve speech recognition as an end-to-end task. \n",
      "\n",
      "This is solved by taking the discrete speech representation as an input to a Transformer architecture. Hence, no acoustic model is required. No language model or data augmentation was used, which might explain the limited results. But this work is an important step towards what the next papers of this series explore.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/wav_8.png)\n",
      "\n",
      "# 3. wav2vec 2.0\n",
      "\n",
      "wav2vec 2.0 leverages self-supervised training, like vq-wav2vec, but in a continuous framework from raw audio data. It builds context representations over continuous speech representations and self-attention captures dependencies over the entire sequence of latent representations end-to-end.\n",
      "\n",
      "## a. Model architecture\n",
      "\n",
      "Inspired by the end-to-end version of the vq-wav2vec paper, the authors further explored this idea with a novel model architecture:\n",
      "- the feature encoder, made of several blocks of temporal convolution followed by layer normalization and a GELU activation function, learns a latent representation: $$ f : X \\to Z $$. Part of the output of this layer is masked before being used as an input to the context network.\n",
      "- the output of the feature encoder is fed to a context network that follows the Transformer architecture and uses convolutional layers to learn relative positional embedding. $$ g : Z \\to C $$\n",
      "- the output of the feature encoder (and not of the context transformer) is discretized in parallel using a quantization module that relies on product quantization. Product quantization amounts to choosing quantized representations from multiple codebooks and concatenating them, which is then used as an input to a Gumbel softmax to select the quantized representations.\n",
      "\n",
      "The model architecture is summarized below: \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/wav_9.png)\n",
      "\n",
      "## b. Loss function\n",
      "\n",
      "As you might have seen, the loss depends on 2 components:\n",
      "- a contrastive loss $$ L_m $$, where the model needs to identify the true quantized latent speech representation, and distractors. Distractors are uniformly sampled from other masked time steps of the same utterance.\n",
      "- a diversity loss $$ L_d $$ to encourage the model to use the codebook entries equally often\n",
      "\n",
      "The overall loss is defined by :\n",
      "\n",
      "$$ L = L_m + \\alpha L_d $$ \n",
      "\n",
      "## c. Fine-tuning for downstream speech recognition task\n",
      "\n",
      "The authors added a randomly initialized linear projection on top of the context network (transformer) into $$ C $$ classes representing the vocabulary of the task.\n",
      "For Librispeech, we have 29 tokens for character targets plus a word boundary token. Models are optimized by minimizing a CTC loss.\n",
      "\n",
      "## d. Language models\n",
      "\n",
      "2 types of language models were considered:\n",
      "- a 4-gram model\n",
      "- and a transformer trained on Librispeech\n",
      "\n",
      "Beam search decoding was used.\n",
      "\n",
      "## e. Results\n",
      "\n",
      "If a pre-trained model captures the structure of speech, then it should require few labeled examples to fine-tune it for speech recognition. \n",
      "\n",
      "The LARGE model pre-trained on LibriVox (LV-60k) and fine-tuned on only **10 minutes** of labeled data achieves a word error rate of 5.2/8.6 on the Librispeech clean/other test sets. This is definitely a major milestone in speech recognition for ultra-low resource language. This demonstrates that ultra-low resource speech recognition is possible with self-supervised learning on unlabeled data\n",
      "\n",
      "Of course, the more labeled data is processed for fine-tuning, the better the model performance.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/wav_10.png)\n",
      "\n",
      "But what happens when lots of training data are available? Does it beat SOTA?\n",
      "\n",
      "The ten-minute models without lexicon and language model tend to spell words phonetically and omit repeated letters, e.g., will → wil. Spelling errors decrease with more labeled data. \n",
      "\n",
      "When even more training data are available, e.g. the whole of Librispeech, the architecture reaches SOTA results.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/wav_11.png)\n",
      "\n",
      "Finally, the authors mention that self-training is likely complimentary to pre-training and their combination may yield even better results. However, it is not\n",
      "clear whether they learn similar patterns or if they can be effectively combined.\n",
      "\n",
      "# 4. Self-training and Pre-training are Complementary for Speech Recognition\n",
      "\n",
      "This last work combines both self-supervised training and pre-training for speech recognition. It gained a lot of attention lately, especially on Twitter with this headline that just 10 minutes of labeled speech can reach the same WER than a recent system trained on 960 hours of data, from just a year ago.\n",
      "\n",
      "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Great progress in speech recognition: wav2vec 2.0 pre-training + self-training with just 10 minutes of labeled data rivals the best published systems trained on 960 hours of labeled data from just a year ago.<br><br>Paper: <a href=\"https://t.co/niBzDiei1j\">https://t.co/niBzDiei1j</a><br>Models: <a href=\"https://t.co/frCK1GJMZQ\">https://t.co/frCK1GJMZQ</a> <a href=\"https://t.co/AjEWdna6J1\">pic.twitter.com/AjEWdna6J1</a></p>&mdash; Michael Auli (@MichaelAuli) <a href=\"https://twitter.com/MichaelAuli/status/1320755019432427520?ref_src=twsrc%5Etfw\">October 26, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
      "\n",
      "In their paper, the authors explore the complementarity of self-training and pre-training.\n",
      "\n",
      "The pre-training approach adopted so far relied on self-supervised learning. This whole block will now be referred to as unsupervised pre-training. This paper introduces the notion of self-training.\n",
      "\n",
      "## a. Self-training\n",
      "\n",
      "In self-training, we train an initial acoustic model on the available labeled data and then label the unlabeled data with the initial model as well as a language model in a step we call pseudo-labeling. Finally, a new acoustic model is trained on the pseudo-labeled data as well as the original labeled data.\n",
      "\n",
      "Self-training and pre-training are then mixed in the following way:\n",
      "- first, pre-train a wav2vec 2.0 model on the unlabeled\n",
      "data (using self-supervised learning approach), \n",
      "- fine-tune it on the available labeled data in an end-to-end fashion using CTC loss and a letter-based output vocabulary, \n",
      "- then use the model to label the unlabeled data using self-training\n",
      "- and finally, use the pseudo-labeled data to train the final model. \n",
      "\n",
      "For the self-training part,\n",
      "- first, generate a list of candidate transcriptions by combining wav2vec 2.0 and the standard Librispeech 4-gram language model during beam-search\n",
      "- the n-best list is pruned to the 50 highest scoring entries and then rescored with a Transformer LM trained on the Librispeech language corpus \n",
      "\n",
      "The final model trains a Transformer-based sequence to sequence model with log-Mel filterbank inputs after pseudo-labeling using wav2letter++. It uses a 10k word piece output vocabulary computed from the training transcriptions if the whole Librispeech training set is used as labeled data.\n",
      "\n",
      "## b. Results\n",
      "\n",
      "Results achieved on only 10 minutes of data are even better than wav2vec 2.0.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/wav_12.png)\n",
      "\n",
      "Finally, the authors explore to what extent self-training and pre-training are complementary. According to the table below, it appears when respecting a ratio of 8.6 times more unlabeled speech that labeled one, self-training keeps improving results by more than 7% on average.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/wav_13.png)\n",
      "\n",
      "# 5. Conclusion\n",
      "\n",
      "## a. Brief summary\n",
      "\n",
      "We have seen in this article:\n",
      "- that wav2vec can be used as a new representation of speech, which can itself be used as inputs to other downstream tasks such as speech recognition\n",
      "- that for ASR, wav2vec, i.e. pre-training using self-supervised learning on a large amount of unlabeled data can help the model performance on a limited amount of data later\n",
      "- that end-to-end models with CTC loss using wav2vec 2.0 inputs work well\n",
      "- that self-training and pre-training appear to be complementary\n",
      "- that 10 minutes of labeled speech might be sufficient to train a good ASR system \n",
      "\n",
      "initial self-supervised learning \n",
      "\n",
      "## b. What is really embedded in this speech representation?\n",
      "\n",
      "Well, this is an open research question. I am really interested in exploring this. This is what I have currently found:\n",
      "- wav2vec embeds language id information, which can be then used for language classification: [Comparison of Deep Learning Methods for Spoken Language Identification](https://link.springer.com/chapter/10.1007/978-3-030-60276-5_23)\n",
      "- wav2vec, in a modified version, might directly contain information related to speakers, and could therefore be used for speaker verification tasks, or fused with X-vectors: [Wav2Spk: A Simple DNN Architecture for Learning Speaker Embeddings from Waveforms](http://www.eie.polyu.edu.hk/~mwmak/papers/interspeech20b.pdf)\n",
      "\n",
      "Speech representations can be used for several downstream tasks. I am rather convinced that it is a matter of time before these representations, learned from raw waveforms, approach SOTA or even beat it on other tasks such as speaker identification, voice activity detection, language identification, pathological speech detection... I see it as a very powerful tool.\n",
      "\n",
      "# Final word\n",
      "\n",
      "I hope this wav2vec series summary was useful. Feel free to leave a comment \n",
      "\n",
      "All references:\n",
      "- [wav2vec paper](https://arxiv.org/abs/1904.05862)\n",
      "- [vq - wav2vec](https://arxiv.org/abs/1910.05453)\n",
      "- [wav2vec2.0 paper](https://arxiv.org/abs/2006.11477)\n",
      "- [Self-training and Pre-training are Complementary for Speech Recognition](https://arxiv.org/abs/2010.11430)\n",
      "- [wav2vec explained, on YouTube](https://www.youtube.com/watch?v=XkUVOijzAt8)\n",
      "- [wav2vec 2.0, on YouTube](https://www.youtube.com/watch?v=aUSXvoWfy3w)\n",
      "- [Self-supervised training in CV](https://www.fast.ai/2020/01/13/self_supervised/#:~:text=We%20would%20like%20something%20which,than%20requiring%20separate%20external%20labels.)\n",
      "- [More self-supervised learning in CV](https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html)\n",
      "\n",
      "---\n",
      "title: Handle Missing Values in Time Series\n",
      "layout: post\n",
      "tags: [statistics]\n",
      "subtitle : \"Time Series\"\n",
      "---\n",
      "\n",
      "In this quick article, we will review the two basic techniques to handle missing values in Time Series. \n",
      "\n",
      "The two techniques are the following :\n",
      "- take the last known value and make a \"forward fill\", i.e to fill the values with the last known value until a new value is met\n",
      "- take the first known value after the missing values, and full the values backward\n",
      "\n",
      "Both techniques can be illustrated this way :\n",
      "\n",
      "![images](https://maelfabien.github.io/assets/images/ts2_11.jpg)\n",
      "\n",
      "To perform a forward fill, run the following command on your data frame :\n",
      "\n",
      "```python\n",
      "df.ffill(axis = 0)\n",
      "```\n",
      "\n",
      "| value | date |\n",
      "| 1991-07-01 | 3.526591 |\n",
      "| 1991-08-01 | 3.180891 |\n",
      "| 1991-09-01 | 3.252221 |\n",
      "\n",
      "To perform a backward fill, run the following command on your data frame :\n",
      "\n",
      "```python\n",
      "df.bfill(axis = 0)\n",
      "```\n",
      "\n",
      "---\n",
      "title: Social network analysis as a tool for criminal intelligence: Understanding its potential from the perspectivesof intelligence analysts\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Criminal Networks\"\n",
      "---\n",
      "\n",
      "In this article, I will discuss and summarize the paper: [\"Social network analysis as a tool for criminal intelligence:understanding its potential from the perspectivesof intelligence analysts\"](https://www.researchgate.net/publication/318037428_Social_network_analysis_as_a_tool_for_criminal_intelligence_Understanding_its_potential_from_the_perspectives_of_intelligence_analysts) by Morgan Burcher and Chad Whelan.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# Background\n",
      "\n",
      "More and more research focuses on social network analysis (SNA) in criminal networks. However, are these methods acutally used by Law Enforcement Agencies (LEAs) and can they leverage it properly? This paper goes some way towards addressing these issues by drawing on qualitative interviews with criminal intelligence analysts from two Australian state law enforcement agencies.\n",
      "\n",
      "Some past studies have shown that there is sometimes a missalignment between the suspects identified/followed by LEAs and the actual vital characters as identified by SNA. It is hard for researchers to access real world data for security reasons, and it is hard to track how efficient a SNA system actually is in production. \n",
      "\n",
      "# The interviews\n",
      "\n",
      "Semi-structured interviews were conducted among 2 Australian state-police, with respectively 10 and 17 participants, betweeen 2015-2016. These state police have budgets exceeding 3 billion $ each, and more than 10'000 employees each.\n",
      "\n",
      "Participants are analysts working with digital tools, most of them learned about SNA through internal training. \n",
      "\n",
      "# Findings\n",
      "\n",
      "The understanding of SNA for most analysts is limited to the understanding of the network structure (who knows who), and some basic metrics like the centrality. Most of them do not have a proper idea of what the difference between \"betweenness centrality\" and \"degree centrality\" is, it's rather just buttons on the sidebar. Few tools that they use involve disrupting networks or predicting hidden links.\n",
      "\n",
      "Some participants suggest that SNA is used to identify suspects that were not on the radar of LEAs so far. Also, some suggest that it helps identify if a new information is important or not.\n",
      "\n",
      "Challenges can be grouped in 4 categories:\n",
      "- the *size of the network*, which can become huge. Too much information can distract the analyst.\n",
      "- the *incompleteness of the data*, by the collection or the time during which analysts can keep the data (often limited)\n",
      "- wasting scarce resources by not defining properly who to include in the SNA or not, know as *fuzzy boundaries*\n",
      "- the information collected is *dynamic* by nature, meaning that the structure of the network might change quickly\n",
      "\n",
      "No or few feedbacks are provided on the tool. When a case is over, additional studies should be conducted and feedback should be given (could be automatic, re-train algorithms...). However, no organization is doing that for the moment. \n",
      "\n",
      "The lack of training and the technical gap between the software capability and the understanding of analysts is also an issue.\n",
      "\n",
      "# My personal take on that\n",
      "\n",
      "I've had several meetings with LEAs these pasts months, and start to understand how they use SNAs tools. Quite often, the lack of training could be partly filled by simple videos explaining how betweenness centrality works for example. I'm pretty sure few softwares currently include that.\n",
      "\n",
      "Then, I do believe in continual/incremental learning. This is a different learning paradigm for algorithms, since it does require constant feedback on the tool. Clicks (such as merging nodes) should re-train a matching algorithm automatically for example.\n",
      "\n",
      "\n",
      "---\n",
      "title: Run jobs on Dataproc - Week 1 Module 2\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Road to Google Cloud Platform Certification\"\n",
      "---\n",
      "\n",
      "# Run jobs on Dataproc\n",
      "\n",
      "Dataproc comes with pre-installed softwares such as :\n",
      "- Apache Hive : for SQL-like processing of structured data. HiveQL is an imperative language.\n",
      "- Pig : for cleaning data and turning semi-structured data into structured data. Pig is a declarative language, and does not decide of the resource allocation. It can fit better in a pipeline.\n",
      "- and Spark : for data processing and pipelines, ideal for unstructured data\n",
      "\n",
      "To submit a job, we can establish a SSH tunnel to the cluster and run Pig/Spark.\n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "title: Introduction to Kaldi\n",
      "layout: post\n",
      "tags: [signal]\n",
      "subtitle : \"Speech Processing\"\n",
      "---\n",
      "\n",
      "I am currently getting to know Kaldi for my Ph.D. work. I thought that documenting the process would be interesting.\n",
      "\n",
      "# What is Kaldi?\n",
      "\n",
      "Kaldi is a speech recognition tool written in C++, available on Github right [here](https://github.com/kaldi-asr/kaldi). The homepage of the project can be found [here](https://kaldi-asr.org/).\n",
      "\n",
      "Kaldi is a tool user for many speech-related tasks, such as:\n",
      "- Automatic Speech Recogniton (ASR)\n",
      "- Speaker Verification (SV)\n",
      "- Speaker Diarization \n",
      "\n",
      "It implements low-level efficient algorithms and makes them available to the end-user through bash and Python scripts. Kaldi is developped by Johns Hopkins University, and Idiap is a large contributor. The project started in 2009. Many ASR or speech-related companies rely today on Kaldi. \n",
      "\n",
      "# What documentation to read?\n",
      "\n",
      "Kaldi has itself a great documentation. I'll be presenting here some notes I took through the process of getting used to Kaldi and working on various speech tasks. You can find the documentation [here](https://kaldi-asr.org/doc/).\n",
      "\n",
      "Otherwise, it's quite hard to find external resources on Kaldi. Some articles on Medium can help getting a general overview, like [this one](https://towardsdatascience.com/how-to-start-with-kaldi-and-speech-recognition-a9b7670ffff6).\n",
      "\n",
      "If you have questions on Kaldi, refer to the [help group](https://groups.google.com/forum/#!forum/kaldi-help).\n",
      "\n",
      "# Knowledge Requirements\n",
      "\n",
      "If you are interested in Kaldi, there are chances that you want to do some Speech Processing for a project or research. Apart from a high-level idea of the process of training an ASR or a Speaker Verification model, one should be familiar with bash scripting and Python. C++ is not necessary to get started, although good to know if you want to dive deeper in Kaldi afterwards.\n",
      "\n",
      "# Setup Requirements\n",
      "\n",
      "Kaldi runs best on Unix environments. At Idiap, we use Debian. But I also installed it on my MacOS environment using Docker. Kaldi is computationally intensive by the nature of the jobs it will run. It is advised to work on a cluster of Linux machines on the grid, and have access to GPUs. This is however not required to get started in this article.\n",
      "\n",
      "Kaldi will require you to install several packages. Some of them are required, such as git, wget, bash, perl, awk, grep, make... There are chances that you already have them all installed.\n",
      "\n",
      "Kaldi will also install other softwares (OpenFst, IRSTLM, SRILM...). Refer to [this page](https://kaldi-asr.org/doc/dependencies.html) if you want to know more about what is installed.\n",
      "\n",
      "Another option, which I will present below, is to simply rely on Docker to do the job :)\n",
      "\n",
      "# File formats you'll encounter\n",
      "\n",
      "In Kaldi, you will encounter many file formats, among which:\n",
      "- .sh for bash scripts\n",
      "- .py for Python scripts\n",
      "- .cc for C++ code\n",
      "- .h for header files, containing variables, functions... used by various C++ files\n",
      "- .pl for Perl scripts, useful to process text files\n",
      "\n",
      "# Install Kaldi\n",
      "\n",
      "## Install Kaldi using Docker\n",
      "\n",
      "Docker is a good option if you don't want to bother with all dependencies for your machine. I am running Kaldi on MacOS for example. The image of the Kaldi ASR tookit is available on DockerHub, right [here](https://hub.docker.com/r/kaldiasr/kaldi). Supposing that you have Docker installed and are signed in to pull the image, simply run:\n",
      "\n",
      "```bash\n",
      "docker pull kaldiasr/kaldi\n",
      "```\n",
      "\n",
      "If everything goes well, the 2.5Gb of the project will be downloaded, and you will obtain:\n",
      "\n",
      "```bash\n",
      "Status: Downloaded newer image for kaldiasr/kaldi:latest\n",
      "```\n",
      "\n",
      "Make sure that the image is available in your Docker images:\n",
      "\n",
      "```bash\n",
      "docker images\n",
      "\n",
      "REPOSITORY           TAG                 IMAGE ID            CREATED             SIZE\n",
      "kaldiasr/kaldi       latest              314e2e8353b4        8 hours ago         11.5GB\n",
      "```\n",
      "\n",
      "Ok, you are now ready to access Kaldi by launching the container (-it stands for interactive and will give you access to a terminal window).\n",
      "\n",
      "```bash\n",
      "docker run -it kaldiasr/kaldi\n",
      "```\n",
      "\n",
      "If everything worked fine, you terminal should display:\n",
      "\n",
      "```bash\n",
      "root@b28f0647d1f2:/opt/kaldi#\n",
      "```\n",
      "\n",
      "## Install Kaldi through Git\n",
      "\n",
      "To install Kaldi through Git, you will first need to clone the project.\n",
      "\n",
      "```bash\n",
      "git clone https://github.com/kaldi-asr/kaldi.git kaldi --origin upstream\n",
      "cd kaldi\n",
      "```\n",
      "\n",
      "Then, go to tools:\n",
      "\n",
      "```bash\n",
      "cd tools\n",
      "```\n",
      "\n",
      "The file INSTALL gathers all the instructions. Check it out using Vim:\n",
      "\n",
      "```bash\n",
      "vim INSTALL\n",
      "```\n",
      "\n",
      "Read the fill completely, as it provides warning messages and how to solve potential issues. If everything goes well, the following commands should get you ready:\n",
      "\n",
      "```bash\n",
      "extras/check_dependencies.sh\n",
      "make\n",
      "```\n",
      "\n",
      "# What's in Kaldi\n",
      "\n",
      "By running ```ls```, the folders are:\n",
      "\n",
      "```bash\n",
      "README.txt  cmd.sh  conf  diarization  local  path.sh  run.sh  sid  steps  utils\n",
      "```\n",
      "\n",
      "The most important directories are:\n",
      "- `egs`, which stands for examples\n",
      "- `tools`, which contains Kaldi dependencies and setup instructions\n",
      "- `src`, which contains the source code\n",
      "\n",
      "For the sake of completeness, the other directories are:\n",
      "- `windows` to run Kaldi on Windows\n",
      "- `misc` which contains additional tools\n",
      "\n",
      "## Tools\n",
      "\n",
      "Tools, apart from containing setup instructions and makefiles, also contains OpenFST, which is the library used for computing Weighted Finite State Transfucer. \n",
      "\n",
      "## Src\n",
      "\n",
      "Src contains all the internal code needed for the various Kaldi algorithms and functionalities. All folders ending in \"bin\" contain executables. To check that your setup is ready, simply type:\n",
      "\n",
      "```bash\n",
      "make test\n",
      "```\n",
      "\n",
      "This will run tests of internal src code and let you know if there is an issue.\n",
      "\n",
      "## Egs\n",
      "\n",
      "Egs contains examples. For example, `wsj` is the famous Wall Street Journal Corpus for Speech Recognition, `callhome_diarization` is a speaker diarization challenge. Each directory corresponds to a challenge for which scripts were built. Most of them require a Linguistic Data Consortium membership (LDC) but some of them are free (e.g voxforge). Read more about it in the README.txt file.\n",
      "\n",
      "In the next article, we'll move on to a concrete example of speaker verification.\n",
      "---\n",
      "title: The final step of the training\n",
      "layout: post\n",
      "tags: [da_tuto]\n",
      "subtitle : \"Final step\"\n",
      "---\n",
      "\n",
      "You have now finished the Data Analysis training, congratulations. If you submitted your assessment, you will soon get your certificate. This is the moment to leave a final review in the comments on the content of the course and the format. Don't hesitate to tell me what to improve, which content to add... For example, I am currently thinking of adding videos for some courses.\n",
      "\n",
      "I sincerly hope you enjoyed the training. Please spread the word if you liked it, and move on to the Machine Learning training ;)\n",
      "---\n",
      "title: Graph Analysis\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Graph Analysis and Graph Learning\"\n",
      "---\n",
      "\n",
      "Let's now dig deeper into graph analysis! What are the main kinds of graphs? What are their properties?\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "## Introduction\n",
      "\n",
      "For what comes next, open a Jupyter Notebook and import the following packages :\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import random\n",
      "import networkx as nx\n",
      "from IPython.display import Image\n",
      "import matplotlib.pyplot as plt\n",
      "```\n",
      "\n",
      "If you have not already installed the `networkx` package, simply run :\n",
      "\n",
      "```bash\n",
      "pip install networkx\n",
      "```\n",
      "\n",
      "The following articles will be using the latest version  `2.x` of  `networkx`. NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\n",
      "\n",
      "We can analyze a graph at different scales :\n",
      "- using the global properties of the network\n",
      "- using communities and clusters\n",
      "- or by looking at individual nodes\n",
      "\n",
      "The main descriptive measures we'll explore will be :\n",
      "- the degree distribution\n",
      "- the clustering coefficient\n",
      "- the \"small world\" phenomena\n",
      "- the centrality of a node\n",
      "- the diameter of the graph\n",
      "...\n",
      "\n",
      "We'll now cover the most common types of graph models :\n",
      "\n",
      "## Erdos-Rényi model\n",
      "\n",
      "### Definition\n",
      "\n",
      "In an Erdos-Rényi model, we build a *random* graph model with $$ n $$ nodes. The graph is generated by drawing an edge between a pair of nodes $$ (i,j) $$ independently with probability $$ p $$. We, therefore, have 2 parameters: $$ n $$ the number of nodes and $$ p $$\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_8.jpg)\n",
      "\n",
      "In Python, the `networkx` package has a built-in function to generate Erdos-Rényi graphs.\n",
      "\n",
      "```python\n",
      "# Generate the graph\n",
      "n = 50\n",
      "p = 0.2\n",
      "G_erdos = nx.erdos_renyi_graph(n,p, seed =100)\n",
      "\n",
      "# Plot the graph\n",
      "plt.figure(figsize=(12,8))\n",
      "nx.draw(G_erdos, node_size=10)\n",
      "```\n",
      "\n",
      "You'll get a result pretty similar to this one :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_9.jpg)\n",
      "\n",
      "### Degree distribution\n",
      "\n",
      "Let $$ p_k $$ the probability that a randomly selected node has a degree $$ k $$. Due to the random way the graphs are built, the distribution of the degrees of the graph is binomial :\n",
      "\n",
      "$$ p_k = {n-1 \\choose k} p^k (1-p)^{n-1-k} $$\n",
      "\n",
      "The distribution of the number of degrees per node should be close to the mean. The probability of high nodes decreases exponentially. \n",
      "\n",
      "```\n",
      "degree_freq = np.array(nx.degree_histogram(G_erdos)).astype('float')\n",
      "\n",
      "plt.figure(figsize=(12, 8))\n",
      "plt.stem(degree_freq)\n",
      "plt.ylabel(\"Frequence\")\n",
      "plt.xlabel(\"Degree\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "To visualize the distribution, I have increased $$ n $$ to 200.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_10.jpg)\n",
      "\n",
      "### Descriptive statistics\n",
      "\n",
      "- The average degree is given by $$ n \\times p $$. With $$ p = 0.2 $$ and $$ n = 200 $$, we are centered around 40.\n",
      "- The degree expectation is given by $$ (n-1) \\times p $$\n",
      "- The maximum degree is concentrated around the average\n",
      "\n",
      "```\n",
      "# Get the list of the degrees\n",
      "degree_sequence_erdos = list(G_erdos.degree())\n",
      "\n",
      "nb_nodes = n\n",
      "nb_arr = len(G_erdos.edges())\n",
      "\n",
      "avg_degree = np.mean(np.array(degree_sequence_erdos)[:,1])\n",
      "med_degree = np.median(np.array(degree_sequence_erdos)[:,1])\n",
      "\n",
      "max_degree = max(np.array(degree_sequence_erdos)[:,1])\n",
      "min_degree = np.min(np.array(degree_sequence_erdos)[:,1])\n",
      "\n",
      "esp_degree = (n-1)*p\n",
      "\n",
      "print(\"Number of nodes : \" + str(nb_nodes))\n",
      "print(\"Number of edges : \" + str(nb_arr))\n",
      "\n",
      "print(\"Maximum degree : \" + str(max_degree))\n",
      "print(\"Minimum degree : \" + str(min_degree))\n",
      "\n",
      "print(\"Average degree : \" + str(avg_degree))\n",
      "print(\"Expected degree : \" + str(esp_degree))\n",
      "print(\"Median degree : \" + str(med_degree))\n",
      "```\n",
      "\n",
      "This should give you something similar to :\n",
      "\n",
      "```python\n",
      "Number of nodes: 200\n",
      "Number of edges: 3949\n",
      "Maximum degree: 56\n",
      "Minimum degree: 25\n",
      "Average degree: 39.49\n",
      "Expected degree: 39.800000000000004\n",
      "Median degree: 39.5\n",
      "```\n",
      "\n",
      "The average and the expected degrees are close since there is only a small factor between the two.\n",
      "\n",
      "## Barabasi-Albert model\n",
      "\n",
      "### Definition\n",
      "\n",
      "In a Barabasi-Albert model, we build a *random* graph model with $$ n $$ nodes. The graph is generated by the following algorithm :\n",
      "- Step 1: With a probability $$ p $$, move to the second step. Else, move to the third step.\n",
      "- Step 2: Connect a new node to existing nodes chosen uniformly at random\n",
      "- Step 3: Connect the new node to $$ n $$ existing nodes with a probability proportional to their degree\n",
      "\n",
      "Such a graph aims to model *preferential attachment*, which is often observed in real networks.\n",
      "\n",
      "In Python, the `networkx` package has also a built-in function to generate Barabasi-Albert graphs.\n",
      "\n",
      "```python\n",
      "# Generate the graph\n",
      "n = 150\n",
      "m = 3\n",
      "G_barabasi = nx.barabasi_albert_graph(n,m)\n",
      "\n",
      "# Plot the graph\n",
      "plt.figure(figsize=(12,8))\n",
      "nx.draw(G_barabasi, node_size=10)\n",
      "```\n",
      "\n",
      "You'll get a result pretty similar to this one :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_11.jpg)\n",
      "\n",
      "You can easily notice how some nodes appear to have a much larger degree than others now!\n",
      "\n",
      "### Degree distribution\n",
      "\n",
      "Let $$ p_k $$ the probability that a randomly selected node has a degree $$ k $$. The degree distribution follows a power law :\n",
      "\n",
      "$$ p_k \\propto k^{-\\alpha} $$\n",
      "\n",
      "The distribution is now heavy-tailed. There is a large number of nodes that have a small degree, but a significant number of nodes have a high degree. \n",
      "\n",
      "\n",
      "```python \n",
      "degree_freq = np.array(nx.degree_histogram(G_barabasi)).astype('float')\n",
      "\n",
      "plt.figure(figsize=(12, 8))\n",
      "plt.stem(degree_freq)\n",
      "plt.ylabel(\"Frequence\")\n",
      "plt.xlabel(\"Degree\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_12.jpg)\n",
      "\n",
      "The distribution is said to be scale-free, in the sense that the average degree is not informative.\n",
      "\n",
      "### Descriptive statistics\n",
      "\n",
      "- The average degree is constant if $$ \\alpha ≤ 2 $$, else, it diverges\n",
      "- The maximum degree is $$ O(n^{ \\frac {1} { \\alpha -1}}) $$\n",
      "\n",
      "```\n",
      "# Get the list of the degrees\n",
      "degree_sequence_erdos = list(G_erdos.degree())\n",
      "\n",
      "nb_nodes = n\n",
      "nb_arr = len(G_erdos.edges())\n",
      "\n",
      "avg_degree = np.mean(np.array(degree_sequence_erdos)[:,1])\n",
      "med_degree = np.median(np.array(degree_sequence_erdos)[:,1])\n",
      "\n",
      "max_degree = max(np.array(degree_sequence_erdos)[:,1])\n",
      "min_degree = np.min(np.array(degree_sequence_erdos)[:,1])\n",
      "\n",
      "esp_degree = (n-1)*p\n",
      "\n",
      "print(\"Number of nodes : \" + str(nb_nodes))\n",
      "print(\"Number of edges : \" + str(nb_arr))\n",
      "\n",
      "print(\"Maximum degree : \" + str(max_degree))\n",
      "print(\"Minimum degree : \" + str(min_degree))\n",
      "\n",
      "print(\"Average degree : \" + str(avg_degree))\n",
      "print(\"Expected degree : \" + str(esp_degree))\n",
      "print(\"Median degree : \" + str(med_degree))\n",
      "```\n",
      "\n",
      "This should give you something similar to :\n",
      "\n",
      "```python\n",
      "Number of nodes: 200\n",
      "Number of edges: 3949\n",
      "Maximum degree: 56\n",
      "Minimum degree: 25\n",
      "Average degree: 39.49\n",
      "Expected degree: 39.800000000000004\n",
      "Median degree: 39.5\n",
      "```\n",
      "\n",
      "The average and the expected degrees are close since there is only a small factor between the two.\n",
      "\n",
      "In the next article, we'll cover the main Graph Algorithms used for fraud detection or in a social network for example.\n",
      "\n",
      "> **Conclusion** : I hope that this article introduced clearly the basis of graphs analysis and that it does now seem clear to you. Don't hesitate to drop a comment if you have any question.\n",
      "---\n",
      "title: Grid Search vs. Randomized Search\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Advanced Machine Learning\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "Grid Search and Randomized Search are the two most popular methods for hyper-parameter optimization of any model. In both cases, the aim is to test a set of parameters whose range has been specified by the users, and observe the outcome in terms of the metric used (accuracy, precision...). \n",
      "\n",
      "For a Decision Tree, we would typically set the range of parameters to look for to :\n",
      "- `criterion`: Gini or entropy\n",
      "- `max_depth`: between 5 and 50\n",
      "- `min_samples_split`: between 2 and 5\n",
      "- ...\n",
      "\n",
      "However, the way the parameters are tested is quite different between Grid Search and Randomized Search.\n",
      "\n",
      "# Grid Search\n",
      "\n",
      "In GridSearch, we try every combination of the set of parameters defined above. This means that we will test the following combinations for example :\n",
      "\n",
      "| Criterion | Max Depth | Min Samples Split |\n",
      "| Gini | 5 | 2 |\n",
      "| Gini | 5 | 3 |\n",
      "| Gini | 5 | 4 |\n",
      "| Gini | 5 | 5 |\n",
      "| Gini | 10 | 2 |\n",
      "| Gini | 10 | 3 |\n",
      "| Gini | 10 | 4 |\n",
      "| Gini | 10 | 5 |\n",
      "| .. | .. | .. |\n",
      "| Entropy | 50 | 5 |\n",
      "\n",
      "We can visually represent the grid search on 2 features as a sequential way to test, in order, all the combinations :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/grid_1.jpg)\n",
      "\n",
      "As you might guess, grid search does not scale well. There is a huge number of combinations we end up testing for just a few parameters. For example, if we have 4 parameters, and we want to test 10 values for each parameter, there are : $$ 10 \\times 10 \\times 10 \\times 10 = 10'000 $$ combinations possible.\n",
      "\n",
      "Grid search is implemented in scikit-learn under the name of GridSearchCV (for cross validation) :\n",
      "\n",
      "```python\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "param_grid = [\n",
      "    'n_estimators': [3, 10, 30], \n",
      "    'max_features': [2, 4, 6, 8], \n",
      "    'bootstrap' : [True, False]\n",
      "]\n",
      "\n",
      "rf = RandomForestRegressor()\n",
      "\n",
      "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='mean_squared_error', return_train_score=True)\n",
      "\n",
      "grid_search.fit(X_val, y_val)\n",
      "````\n",
      "\n",
      "To optimize the hyper-parameters, we tend to use a validation set (if available) to limit the overfitting on the train set.\n",
      "\n",
      "# Randomized Search\n",
      "\n",
      "Randomized Search follows the same goal. However, we won't test sequentially all the combinations. Instead, we try random combinations among the range of values specified for the hyper-parameters. We initially specify the number of random configurations we want to test in the parameter space.\n",
      "\n",
      "The main advantage is that we can try a broader range of values or hyperparameters within the same computation time as grid search, or test the same ones in much less time. We are however not guaranteed to identify the best combination since not all combinations will be tested.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/grid_2.jpg)\n",
      "\n",
      "The implementation in scikit-learn is also straight forward :\n",
      "\n",
      "```python\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "\n",
      "param_grid = [\n",
      "'n_estimators': [3, 10, 30], \n",
      "'max_features': [2, 4, 6, 8], \n",
      "'bootstrap' : [True, False]\n",
      "]\n",
      "\n",
      "rf = RandomForestRegressor()\n",
      "\n",
      "rnd_search = RandomizedSearchCV(rf, param_grid, cv=5, scoring='mean_squared_error', return_train_score=True)\n",
      "\n",
      "rnd_search.fit(X_val, y_val)\n",
      "````\n",
      "\n",
      "> **Conclusion** : There is a tradeoff to make between the guarantee to identify the best combination of parameters and the computation time. A simple trick could be to start with a randomized search to reduce the parameters space and then launch a grid search to select the optimal features within this space.\n",
      "\n",
      "---\n",
      "title: Text classification from few training examples\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Natural Language Processing\"\n",
      "---\n",
      "\n",
      "I recently came across a good blog post article published by Nicolas Thiebaut right [here](https://data4thought.com/fewshot_learning_nlp.html). This article addresses the task of classifying texts when we have few training examples. I started to read some litterature on this topic, and it's impressive to notice that you can count on your fingertips the number of articles that adress this topic. \n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# Problem definition\n",
      "\n",
      "This kind of problem is however easily encountered. Suppose that your company gets customer emails, that are not labeled. You work in the data science department, and you want to automatically label the emails by saying whether they are important or not. It's a simple binary classification. Labeling data might be incredibly long and cumbersome. You might even never reach enough labeled data for classical NLP classification tasks. \n",
      "\n",
      "This kind of problem needs to be adressed in another way. Let's start by defining scenarios that might occur :\n",
      "- **Zero-shot learning** : you have some labeled observations per classes, but some classes don't have observations. It's a tricky case that we won't address in this article\n",
      "- **One-shot learning** : we have one labeled observation per class\n",
      "- **Few-shot learning** : we have few observations per class\n",
      "\n",
      "It's now much easier to think of your email classification as a One-Shot or Few-Shot learning problem. Indeed, you could easily ask a business user to classify, say 10 emails, 5 important, and 5 not important, and take that as input data. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_fs_0.png)\n",
      "\n",
      "The data labeling would take 5 minutes at most. Now, the question becomes : what the hell can I do with 10 training examples?\n",
      "\n",
      "# Solutions\n",
      "\n",
      "Few shot learning is largely studied in the field of computer vision. [Papers](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) published in this field quite often rely on Siamese Networks. A typical application of such problem would be to build a Face Recognition algorithm. You have 1 or 2 pictures per person, and need to assess who is on the video the camera is filming. However, in the domain of Natural Language Processing, this problem is less common. \n",
      "\n",
      "In most few shot learning problems, there is a notion of **distance** that arises at some point. In Siamese networks, we want to minimize the distance between the anchor and the other positive example, and maximize the distance between the anchor and negative example.\n",
      "\n",
      "I have seen several approaches to few shot learning in recent papers :\n",
      "- either use a Siamese Network based on LSTMs rather than CNNs, and use this for One-shot learning\n",
      "- learn word embeddings in one-shot or few-shot and classify on top\n",
      "- or use a pre-trained word / document embedding network, and build a metric on top\n",
      "\n",
      "We will focus on the last solution. This article is an implementation of a recent paper, [Few-Shot Text Classification with Pre-Trained Word Embeddings and a Human in the Loop by Katherine Bailey and Sunny Chopra Acquia](https://arxiv.org/pdf/1804.02063.pdf). A simple presentation of the paper can paper can be found [here](https://katbailey.github.io/talks/Few-shot%20text%20classification.pdf).\n",
      "\n",
      "# Few-Shot with Human in the Loop\n",
      "\n",
      "## Concept \n",
      "\n",
      "Let's start by formalizing the main idea behind the paper. The fact that there is a \"Human in the loop\" simply refers to the fact that we have a potentially large corpus of unlabeled data and require the user to label a few examples of each class.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_fs_1.png)\n",
      "\n",
      "Then, using a pre-trained Word Embedding model (Word2Vec, Glove..), we compute the average embedding of each email / short text in the training examples :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_fs_2.png)\n",
      "\n",
      "At this point, we compute the avereage embedding for each class :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_fs_3.png)\n",
      "\n",
      "This average embedding per class can be seen as a centroid in a high dimensional space. From that point, when a new observation comes in, we simply have to check how far it is from both centroids, and take the closest. The distance metric used in the paper is the cosine distance :\n",
      "\n",
      "$$ similarity = cos(\\theta) = \\frac {  A \\dot B } { \\mid \\mid A  \\mid \\mid  \\mid \\mid B  \\mid \\mid } $$\n",
      "\n",
      "Here is the process when a new sentence to classify comes in :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_fs_4.png)\n",
      "\n",
      "## Implementation\n",
      "\n",
      "Start by importing the following packages :\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from random import seed\n",
      "from random import sample\n",
      "\n",
      "seed(42)\n",
      "np.random.seed(42)\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import gensim.downloader as api\n",
      "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.metrics import accuracy_score\n",
      "from scipy import spatial\n",
      "\n",
      "from nltk.corpus import stopwords\n",
      "```\n",
      "\n",
      "We will use pre-trained models from Gensim :\n",
      "\n",
      "```python\n",
      "#model = api.load('glove-twitter-25') Not used\n",
      "model2 = api.load('word2vec-google-news-300')\n",
      "```\n",
      "\n",
      "As you can see, I have tried this exercise with both Glove and Word2Vec, and chose to stick to Word2Vec. Let us now load a dataset that would suit. I have found a short text classification dataset from StackOverflow, where the corresponding categories are actually the language/forum category the question falls into.\n",
      "\n",
      "You can download it [here](https://github.com/jacoxu/StackOverflow). I called it \"Stack\" for what comes next, since the first step was to concatenate the texts and the labels (skipped here to keep it short) :\n",
      "\n",
      "```python\n",
      "df = pd.read_csv(\"stack.csv\")\n",
      "df.head()\n",
      "```\n",
      "\n",
      "| | Text | Label |\n",
      "| 0  | How do I fill a DataSet or a DataTable from a ...  |  18 |\n",
      "| 1 |   How do you page a collection with LINQ?    18 |\n",
      "| 2  | Best Subversion clients for Windows Vista (64bit)   | 3 |\n",
      "| 3  |  Best Practice: Collaborative Environment, Bin ...   | 3 |\n",
      "| 4  |  Visual Studio Setup Project - Per User Registr...   | 7 |\n",
      "\n",
      "We can now clean the texts in order to keep only the characters :\n",
      "\n",
      "```python\n",
      "def get_only_chars(line):\n",
      "\n",
      "    clean_line = \"\"\n",
      "\n",
      "    line = line.replace(\"’\", \"\")\n",
      "    line = line.replace(\"'\", \"\")\n",
      "    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n",
      "    line = line.replace(\"\\t\", \" \")\n",
      "    line = line.replace(\"\\n\", \" \")\n",
      "    line = line.lower()\n",
      "\n",
      "    for char in line:\n",
      "        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
      "            clean_line += char\n",
      "        else:\n",
      "            clean_line += ' '\n",
      "\n",
      "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
      "    if clean_line[0] == ' ':\n",
      "        clean_line = clean_line[1:]\n",
      "    return clean_line\n",
      "```\n",
      "\n",
      "Then, apply the function to the text column :\n",
      "\n",
      "```python\n",
      "df['Text'] = df['Text'].apply(lambda x: get_only_chars(x))\n",
      "```\n",
      "\n",
      "Then, there are 2 variables which we'll have to control :\n",
      "- the number of classes we consider (since the dataset has many classes)\n",
      "- the number of labeled sampled we'll require from the user \n",
      "\n",
      "We set the by default to :\n",
      "\n",
      "```python\n",
      "num_classes = 2\n",
      "sample_size = 3\n",
      "```\n",
      "\n",
      "We then must generate samples that contain $$ K = 3 $$ training samples per class, and $$ M = 2 $$ classes :\n",
      "\n",
      "```python\n",
      "# Generate samples that contains K samples of each class\n",
      "\n",
      "def gen_sample(sample_size, num_classes):\n",
      "\n",
      "    df_1 = df[(df[\"Label\"]<num_classes + 1)].reset_index().drop([\"index\"], axis=1).reset_index().drop([\"index\"], axis=1)\n",
      "    train = df_1[df_1[\"Label\"] == np.unique(df_1['Label'])[0]].sample(sample_size)\n",
      "\n",
      "    train_index = train.index.tolist()\n",
      "\n",
      "    for i in range(1,num_classes):\n",
      "        train_2 = df_1[df_1[\"Label\"] == np.unique(df_1['Label'])[i]].sample(sample_size)\n",
      "        train = pd.concat([train, train_2], axis=0)\n",
      "        train_index.extend(train_2.index.tolist())\n",
      "\n",
      "    test = df_1[~df_1.index.isin(train_index)]\n",
      "\n",
      "    return train, test\n",
      "```\n",
      "\n",
      "\n",
      "Apply that to the dataframe :\n",
      "\n",
      "```python\n",
      "train, test = gen_sample(sample_size, num_classes)\n",
      "\n",
      "X_train = train['Text']\n",
      "y_train = train['Label'].values\n",
      "X_test = test['Text']\n",
      "y_test = test['Label'].values\n",
      "```\n",
      "\n",
      "`train` now contains only 6 observations, 3 from each class :\n",
      "\n",
      "| | Text | Label |\n",
      "| 1421  |  Lighttp and wordpress URL rewrite   | 1 |\n",
      "| 1275  |  wordpress plug-ins, themes and widgets tips ...  |  1 |\n",
      "| 1638  |  Is there an easier way to add menu items to a ...  |  1 |\n",
      "| 735  |  Oracle deadlock detection tool  |  2 |\n",
      "| 1074  |  Index not used due to type conversion?  |  2 |\n",
      "| 311  |  UTF 8 from Oracle tables  | 2 |\n",
      "\n",
      "At that point, we need to get the aveage embeddings by first getting the token id in the model vocabulary, and then getting the corresponding word embedding, before finally average over the whole sentence.\n",
      "\n",
      "```python\n",
      "# Text processing (split, find token id, get embedidng)\n",
      "def transform_sentence(text, model):\n",
      "\n",
      "    \"\"\"\n",
      "    Mean embedding vector\n",
      "    \"\"\"\n",
      "\n",
      "    def preprocess_text(raw_text, model=model):\n",
      "\n",
      "        \"\"\" \n",
      "        Excluding unknown words and get corresponding token\n",
      "        \"\"\"\n",
      "\n",
      "        raw_text = raw_text.split()\n",
      "\n",
      "        return list(filter(lambda x: x in model.vocab, raw_text))\n",
      "\n",
      "    tokens = preprocess_text(text)\n",
      "\n",
      "    if not tokens:\n",
      "        return np.zeros(model.vector_size)\n",
      "\n",
      "    text_vector = np.mean(model[tokens], axis=0)\n",
      "\n",
      "    return np.array(text_vector)\n",
      "```\n",
      "\n",
      "Apply this to both the train and the test :\n",
      "\n",
      "```python\n",
      "X_train_mean = X_train.apply(lambda x : transform_sentence(x, model2))\n",
      "X_test_mean = X_test.apply(lambda x : transform_sentence(x, model2))\n",
      "\n",
      "X_train_mean = pd.DataFrame(X_train_mean)['Text'].apply(pd.Series)\n",
      "X_test_mean = pd.DataFrame(X_test_mean)['Text'].apply(pd.Series)\n",
      "```\n",
      "\n",
      "We will now use cosine similarity between the embeddings of the text to classify and the average embeddings of all classes. A new example therefore belongs to the class it is the closest to.\n",
      "\n",
      "```python\n",
      "# Use cosine similarity to find closest class\n",
      "def classify_txt(txt, mean_embedding):\n",
      "\n",
      "    best_dist = 1\n",
      "    best_label = -1\n",
      "\n",
      "    for cl in range(num_classes):\n",
      "\n",
      "        dist = spatial.distance.cosine(transform_sentence(txt, model2), mean_embedding[cl])\n",
      "\n",
      "        if dist < best_dist :\n",
      "            best_dist = dist\n",
      "            best_label = cl+1\n",
      "\n",
      "    return best_label\n",
      "```\n",
      "\n",
      "The point in doing all this is to see how the classification accuracy reacts to the number of classes and to the number of training examples. Let us now create a function that gathers all the previous steps and iterates on the test set to compute the accuracy :\n",
      "\n",
      "```python\n",
      "# Process text and predict on the test set\n",
      "def return_score(sample_size, num_classes):\n",
      "\n",
      "    train, test = gen_sample(sample_size, num_classes)\n",
      "\n",
      "    X_train = train['Text']\n",
      "    y_train = train['Label'].values\n",
      "    X_test = test['Text']\n",
      "    y_test = test['Label'].values\n",
      "\n",
      "    X_train_mean = X_train.apply(lambda x : transform_sentence(x, model2))\n",
      "    X_test_mean = X_test.apply(lambda x : transform_sentence(x, model2))\n",
      "\n",
      "    X_train_mean = pd.DataFrame(X_train_mean)['Text'].apply(pd.Series)\n",
      "    X_test_mean = pd.DataFrame(X_test_mean)['Text'].apply(pd.Series)\n",
      "\n",
      "    mean_embedding = {}\n",
      "    for cl in range(num_classes):\n",
      "        mean_embedding[cl] = np.mean((X_train_mean[y_train == cl + 1]), axis=0)\n",
      "\n",
      "    y_pred = [classify_txt(t, mean_embedding) for t in test['Text'].values]\n",
      "\n",
      "    return accuracy_score(y_pred, y_test)\n",
      "```\n",
      "\n",
      "Now, we will iterate on the number of classes (between 2 and 7) and the number of samples (between 1 andd 50). We will consider that labeling more than 50 training examples **per class** is too long.\n",
      "\n",
      "```python\n",
      "all_accuracy = {2:[],3:[],4:[],5:[],6:[],7:[]}\n",
      "\n",
      "for num_samples in range(1,50):\n",
      "    for num_cl in range(2, 7):\n",
      "        all_accuracy[num_cl].append(return_score(num_samples,num_cl))\n",
      "```\n",
      "\n",
      "This will take several minutes to run. Once we are done, we can plot the accuracy for each number of class (the color) depending on the number of training examples (the y-axis) :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(all_accuracy[2], label=\"2 classes\")\n",
      "plt.plot(all_accuracy[3], label=\"3 classes\")\n",
      "plt.plot(all_accuracy[4], label=\"4 classes\")\n",
      "plt.plot(all_accuracy[5], label=\"5 classes\")\n",
      "plt.plot(all_accuracy[6], label=\"6 classes\")\n",
      "plt.axvline(7, c='black', alpha=0.5)\n",
      "plt.title(\"Accuracy depending on the number of samples and classes\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_fs_5.png)\n",
      "\n",
      "For 2 classes, we rapidly reach an accuracy of about 90%. Notice how the accuracy decreases with the number of classes, but remain above the \"random\" allocation threshold. For 4 classes, the accuracy is indeed of 40%, and remains above 25% if we were to classify randomly. \n",
      "\n",
      "The results are quite encouraging, but the approach of distance metric is quite restrictive since it relies on comparing the embedding of a sentence (which is itself an average of word embeddings), with the average of the embedding of all training examples within a class. That makes a lot of averages. Couldn't we go a little further and avoid averaging the embedding of the training examples ?\n",
      "\n",
      "# Pre-trained Word2Vec and K-NN\n",
      "\n",
      "We are going to use the same logic as defined above. However, instead of averaging the embeddings for each class, we will keep the embeddings as separate observations. Instead, once a new observation comes in, we will apply a K-Nearest Neighbors classifier for the classification task.\n",
      "\n",
      "Let's suppose that the embeding dimension is only 2 (or that we apply a PCA with 2 components) to represent this problem graphically. The classification task with the KNN is the following :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_fs_6.png)\n",
      "\n",
      "One important note about the parameter choice for the K-NN : I think that looking at as many neighbors as we have of training samples per class is intuitive. Say if we have 5 samples per class, and we must classify a new observation, then if the classes are different enough, we should be able to have the 5 neighbors with the same label. If we were to set this number of neighbors to consider manually, we would risk to take too many points into consideration.\n",
      "\n",
      "Let's modify the `return_score` function :\n",
      "\n",
      "```python\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "def return_score(sample_size, num_classes):\n",
      "\n",
      "    train, test = gen_sample(sample_size, num_classes)\n",
      "\n",
      "    X_train = train['Text']\n",
      "    y_train = train['Label'].values\n",
      "    X_test = test['Text']\n",
      "    y_test = test['Label'].values\n",
      "\n",
      "    X_train_mean = X_train.apply(lambda x : transform_sentence(x, model2))\n",
      "    X_test_mean = X_test.apply(lambda x : transform_sentence(x, model2))\n",
      "\n",
      "    X_train_mean = pd.DataFrame(X_train_mean)['Text'].apply(pd.Series)\n",
      "    X_test_mean = pd.DataFrame(X_test_mean)['Text'].apply(pd.Series)\n",
      "\n",
      "    clf = KNeighborsClassifier(n_neighbors=sample_size, p=2)\n",
      "    clf.fit(X_train_mean, y_train)\n",
      "\n",
      "    y_pred = clf.predict(X_test_mean)\n",
      "\n",
      "    return accuracy_score(y_pred, y_test)\n",
      "```\n",
      "\n",
      "And run the same loop as above :\n",
      "\n",
      "```python\n",
      "all_accuracy_knn = {2:[],3:[],4:[],5:[],6:[],7:[]}\n",
      "\n",
      "for num_samples in range(1,50):\n",
      "\n",
      "    for num_cl in range(2, 7):\n",
      "\n",
      "        all_accuracy_knn[num_cl].append(return_score(num_samples,num_cl))\n",
      "```\n",
      "\n",
      "Then, plot the results :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(all_accuracy_knn[2], label=\"2 classes\")\n",
      "plt.plot(all_accuracy_knn[3], label=\"3 classes\")\n",
      "plt.plot(all_accuracy_knn[4], label=\"4 classes\")\n",
      "plt.plot(all_accuracy_knn[5], label=\"5 classes\")\n",
      "plt.plot(all_accuracy_knn[6], label=\"6 classes\")\n",
      "plt.title(\"Accuracy depending on the number of samples and classes\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_fs_7.png)\n",
      "\n",
      "When we have really few training samples (less than 10), the cosine-based approach seems to outperform the K-NN. However, once we consider more than 10 training samples, the accuracy with 4 classes improves from 40% to 67% !\n",
      "\n",
      "# Summary\n",
      "\n",
      "We can now group this into a summary table :\n",
      "\n",
      "```python\n",
      "df_results = pd.DataFrame({\n",
      "    'Nb Classes':[2,3,4,5], \n",
      "    'min K-NN':[min(all_accuracy_knn[2]), \n",
      "        min(all_accuracy_knn[3]), \n",
      "        min(all_accuracy_knn[4]), \n",
      "        min(all_accuracy_knn[5])],\n",
      "    'min Cosine':[min(all_accuracy[2]), \n",
      "        min(all_accuracy[3]), \n",
      "        min(all_accuracy[4]), \n",
      "        min(all_accuracy[5])],\n",
      "    'mean K-NN':[np.mean(all_accuracy_knn[2]), \n",
      "        np.mean(all_accuracy_knn[3]), \n",
      "        np.mean(all_accuracy_knn[4]), \n",
      "        np.mean(all_accuracy_knn[5])],\n",
      "    'mean Cosine':[np.mean(all_accuracy[2]), \n",
      "        np.mean(all_accuracy[3]), \n",
      "        np.mean(all_accuracy[4]), \n",
      "        np.mean(all_accuracy[5])],\n",
      "    'max K-NN':[max(all_accuracy_knn[2]), \n",
      "        max(all_accuracy_knn[3]), \n",
      "        max(all_accuracy_knn[4]), \n",
      "        max(all_accuracy_knn[5])],\n",
      "    'max Cosine':[max(all_accuracy[2]), \n",
      "        max(all_accuracy[3]), \n",
      "        max(all_accuracy[4]), \n",
      "        max(all_accuracy[5])]\n",
      "    })\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_fs_8.png)\n",
      "\n",
      "On average, the K-NN is better if there are more than 2 classes, and a sufficient amount of training samples.\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "We have covered in this article a really simple implementation of Few-Shot Text Classification with Pre-Trained Word Embeddings and a Human in the Loop. This paper is interesting since it addresses a concrete problem you might encounter. The solution proposed by the authors (although I skipped the PCA part) seems to perform well if we have few classes and few trainign examples. On the other hand, the extension that I developped with K-NNs outperforms the solution once we consider more samples and more classes. There seems to be a trade-off to make here when choosing the right approach.\n",
      "\n",
      "I hope this article was interesting. If you implement it on your own, please share your results and improvements of the current solution !\n",
      "\n",
      "---\n",
      "title: Neural Network acoustic modeling\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Speech Processing\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "So far, we have covered HMM-GMM acoustic modeling and some practical issues related to context and less frequent phonemes.\n",
      "\n",
      "# Introduction to Neural Network acoustic modeling\n",
      "\n",
      "There is an alternative way to build an acoustic model, and it consists in using neural networks to:\n",
      "- take an input acoustic frame\n",
      "- and output a score for each phone\n",
      "\n",
      "Let's consider a single-layer neural network, that takes as an input an acoustic frame $$ X_t $$ and outputs phonetic scores $$ f(t) $$ (one score for each phone).\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_30.png)\n",
      "\n",
      "It can be expressed as:\n",
      "\n",
      "$$ f = Wx + b $$\n",
      "\n",
      "Where $$ W $$ is the weight matrix made of weights $$ w_{ij} $$ that reflect the weifht between input $$ i $$ and output $$ j $$, and $$ b $$ is the bias term. \n",
      "\n",
      "How do we learn the parameters $$ W $$ and $$ b $$? We target the minimization of the error function $$ E $$, the Mean Square Error (MSE) between the output and the target:\n",
      "\n",
      "$$ E = 0.5 \\times \\frac{1}{T} \\sum_{t=1}^T {\\mid \\mid f(x_t) - r(t) \\mid \\mid}^2 $$\n",
      "\n",
      "Where $$ r(t) $$ are the target outputs.\n",
      "\n",
      "The error minimization is typically done using gradient descent, and we must compute the terms:\n",
      "\n",
      "$$ \\frac{d E}{d W} $$ and $$ \\frac{d E}{d b} $$\n",
      "\n",
      "*Reminder*: Stochastic gradient descent (SGD)\n",
      "\n",
      "In SGD, we:\n",
      "- intialize weights and biases with small random numbers\n",
      "- randomise the order of training data example\n",
      "- then for each epoch:\n",
      "\t- take a minibact\n",
      "\t- compute network outputs\n",
      "\t- backpropagate and update the weights\n",
      "\n",
      "The network that predicts phonetic scores is a classifier, so we need to take a softmax to force output values to act as probabilities:\n",
      "\n",
      "$$ y_j = \\frac{exp(f(x_j))}{\\sum_{k=1}^K exp(f(x_k))} $$\n",
      "\n",
      "Where $$ f(x_j) = \\sum_{d=1}^D w_{jd} x_d + b_j $$\n",
      "\n",
      "However, the MSE is not the wisest choice when working with probabilities. We can directly maximize the log probability of observing the correct label using the Cross-Entropy (CE) error function:\n",
      "\n",
      "$$ E_t = - \\sum{j=1}^J r_j^t \\ln y_j^t $$\n",
      "\n",
      "Using CE, the gradients of the outputs weights simplify to:\n",
      "\n",
      "$$ \\frac{dE^t}{dW_{jd}} = (y_j^t - r_j^t) x_d $$\n",
      "\n",
      "There are several extensions possible to this very very simple model:\n",
      "- use more context by taking multiple frames into account\n",
      "- add more hidden layers to obtains DNNs\n",
      "- use activation functions such as ReLU\n",
      "\n",
      "This looks interesting, and overall simpler than the HMM-GMM. But there is a major limitation, since we cannot do speech recognition with this approach. There are several phone recognition tasks:\n",
      "- frame classification: classify each frame of data\n",
      "- phone classification: classify each segment of data (what Neural Networks can do)\n",
      "- phone recongition: segment the data and label the segment\n",
      "\n",
      "Using only DNN, we lack the notion of data segmentation that HMMs are good at doing. But can't we mix HMMs and DNNs ?\n",
      "\n",
      "# HMM-DNN acoustic modeling\n",
      "\n",
      "In an HMM-GMM, replacing the GMM by a DNN to estimate output pdfs build a so-called HMM-DNN architecture. In a HMM-DNN, we consider one-state per phone, and train a NN as a phone-state classifier.\n",
      "\n",
      "It can be shown that the outputs corresponding to class $$ j $$ given an input $$ x_t $$ are an estimate of the posterior probability $$ P(q_t = j \\mid x_t) $$, $$ q_t $$ being a state, because we have softmax outputs and use a CE loss function.\n",
      "\n",
      "And using Bayes Rule, we can relate the posterior $$ P(q_t = j \\mid x_t) $$ to the likelihood $$ P(x_t \\mid q_t = j) $$:\n",
      "\n",
      "$$ P(q_t \\mid x_t) = \\frac{P(x_t \\mid q_t = j) P(q_t = j)}{P(X_t)} $$\n",
      "\n",
      "If we want HMM-DNNs to output probabilities, we should scale the likelihoods:\n",
      "\n",
      "$$ \\frac{P(q_t = j \\mid x_t)}{P(q_t = j)} = \\frac{P(x_t \\mid q_t = j)}{P(x_t)} $$\n",
      "\n",
      "This means we can obtain scaled likelihoods by dividing each network output by the prior, i.e. the relative frequency of class $$ j $$ in training data.\n",
      "\n",
      "There are several approaches to continuous speech recognition with HMM-DNN:\n",
      "- 1 state per phone (each NN can output typically 60 classes if 60 phone classes)\n",
      "- 3 state context-independent (CI) models, where each phone has 3 states, modeled by 3 NNs\n",
      "- State-clustered context-dependent (CD) models, 1 NN output per tied state\n",
      "\n",
      "The architecture of a HMM-DNN is presented below:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_31.png)\n",
      "\n",
      "NN are more flexible, learn richer representations and handle correlated features. In terms of speech features, experiments indicate that mel-scaled filter bank features (FBANK) work better than MFCCs, and results in better clustering when applying t-SNE on the hidden layers. Indeed, in FBANK the useful information is distributed over all the features, whereas in MFCC it is concentrated in the first few.\n",
      "\n",
      "# Modelling phonetic context with DNNs\n",
      "\n",
      "Modling the phonetic context with DNNs was considered hard until 2011, but a simple solution emerged: use the state-tying process from a GMM system, and train a HMM-DNN on it.\n",
      "\n",
      "More precisely, the context-dependent hybrid HMM/DNN approach is:\n",
      "- Train a GMM/GMM system on your data\n",
      "- Use a phonetic decision tree to determine the HMM tied states for infrequent states\n",
      "- Perform Viterbi alignment using the trained HMM/GMM and the training data\n",
      "- Train a neural network to map the input speech features to a label representing a context-dependent tied HMM-state, instead of contenxt-independent phones\n",
      "- this increases the number of possible labels, and each frame is labelled with Viterbi aligned tied states\n",
      "- we then train the NN using gradient descent\n",
      "\n",
      "Concretely, we model the acoustic context by including neighbour frames in the input layer. \n",
      "\n",
      "We use richer Neural Network models for acoustic context:\n",
      "- Recurrent Neural Networks (RNNs)\n",
      "- Time-delay Neural Networks (TDNNs)\n",
      "\n",
      "## Time-delay Neural Networks\n",
      "\n",
      "In TDNNs, higher hidden layers take input from a larger acoustic context, and lower hidden layers from narrower contexts. TDNNs can be seen as a 1D convolutional network. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_52.png)\n",
      "\n",
      "A TDNN with a context (-2, 2) has 5 times more weights than a regular DNN. For this reason, sub-sampled TDNNs are explored. Due to the large overlaps between input contexts at adjacent time steps, which are likely to be correlated, we take a sub-sample window of hidden unit activations. It reduces computation time and model weights.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_53.png)\n",
      "\n",
      "## Recurrent Neural Networks\n",
      "\n",
      "In its unfolded representation, the RNN for a sequence of T inputs is a T-layer network with shared weights. It can keep information through time.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_54.png)\n",
      "\n",
      "LSTMs avoid the vanishing gradient problem of RNNs. Bidirectional RNNs consider both the right and the left context, in a forward layer and a backward layer. Deep RNNs have several hidden layers, and deep bidirectional LSTM combine the advantages of all these methods:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_55.png)\n",
      "\n",
      "Here is an example of a Bidirectional LSTM Acoustic Model training on the Switchboard dataset:\n",
      "\n",
      "- LSTM has 4-6 bidirectional layers with 1024 cells/layer (512 each direction)\n",
      "- 256 unit linear bottleneck layer\n",
      "- 32k context-dependent state outputs\n",
      "- Input features:\n",
      "\t- 40-dimension linearly transformed MFCCs (plus ivector)\n",
      "\t- 64-dimension log mel filter bank features (plus first and second derivatives)\n",
      "\t- concatenation of MFCC and FBANK features\n",
      "- Training: 14 passes frame-level cross-entropy training, 1 pass sequence training (2 weeks on a K80 GPU)\n",
      "\n",
      "LSTMs + feature fusion currently reach close to state-of-the-art.\n",
      "# Conclusion\n",
      "\n",
      "If you want to improve this article or have a question, feel free to leave a comment below :)\n",
      "\n",
      "References:\n",
      "- [ASR 07, University of Edimburgh](http://www.inf.ed.ac.uk/teaching/courses/asr/2019-20/asr07-nnintro.pdf)\n",
      "- [ASR 08, University of Edimburgh](http://www.inf.ed.ac.uk/teaching/courses/asr/2019-20/asr08-hybrid_hmm_nn.pdf)\n",
      "- [ASR 08, University of Edimburgh](http://www.inf.ed.ac.uk/teaching/courses/asr/2019-20/asr11-dnn-tdnn-lstm.pdf)\n",
      "---\n",
      "title: Unsupervised Learning Cheat Sheet\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Machine Learning Basics\"\n",
      "---\n",
      "\n",
      "Let's move on to unsupervised part ! This cheatsheet covers the key concepts, illustrations, otpimisaton program and limitations for the most common types of algorithms. Don't hesitate to drop a comment !\n",
      "\n",
      "We'll cover :\n",
      "- Principal Component Analysis (PCA)\n",
      "- Kernel PCA\n",
      "- Factor Analysis\n",
      "- K-Means\n",
      "- Gaussian Mixture Model (GMM)\n",
      "- Expectation Maximization (EM)\n",
      "- Hierarchical Clustering\n",
      "- Nearest Neighbor Chain\n",
      "- Density Based Spatial Clustering of Applications with noise (DBSCAN)\n",
      "- Non-negative Matrix Factorisation model (NMF)\n",
      "- Independent Component Analysis (ICA)\n",
      "\n",
      "Click on the image below to load the PDF summary : \n",
      "\n",
      "<a href=\"https://github.com/maelfabien/Machine_Learning_Tutorials/blob/master/Images/unsupervised.pdf\">![image](https://maelfabien.github.io/assets/images/unsup.jpg){:height=\"30%\" width=\"30%\"}</a>\n",
      "\n",
      "---\n",
      "title: Install Zookeeper on EC2 instances\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Amazon Web Services\"\n",
      "---\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ec2_zk.jpg)\n",
      "\n",
      "ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications. Zookeeper is useful if you would like to secure your architecture a little more and prevent from the consequences of the fall of your Masters for example.\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "Zookeeper (ZK) may be installed on its own on a node, or together with Spark/Cassandra on a worker node. Each ZK node should be aware of other ZK instances to form a quorum of 3. We chose to install zookeeper before Spark since the configuration is lighter.\n",
      "\n",
      "## SSH Connection to the nodes \n",
      "\n",
      "The first step is to establish an SSH connection with the nodes on which you would like to install Zookeeper. For example, Master 1, 2, and 2 slaves. Recall :\n",
      "``` bash\n",
      "ssh -i \"<path to your keyPair directory>/Cluster_test_Key_Pair.pem\" ubuntu@<copy the public DNS> \n",
      "```\n",
      "\n",
      "## Install Apache-Zookeeper on your instances\n",
      "\n",
      "Copy this link :\n",
      "<span style=\"color:blue\">[https://www-eu.apache.org/dist/zookeeper/zookeeper-3.4.13/](https://www-eu.apache.org/dist/zookeeper/zookeeper-3.4.13/)</span>\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Zookeeper_DL.jpg)\n",
      "\n",
      "On each instance, go to the repository : ``` /ubuntu/home/ ```.\n",
      "\n",
      "a. Download the .tar.gz file :\n",
      "\n",
      "Once you're in the good directory, execute the following command :\n",
      "``` wget https://www-eu.apache.org/dist/zookeeper/zookeeper-3.4.13/zookeeper-3.4.13.tar.gz ```\n",
      "\n",
      "You should see something like this :\n",
      "![image](https://maelfabien.github.io/assets/images/Zookeeper_Wget.jpg)\n",
      "\n",
      "Then, extract the software by executing the command below : \n",
      "``` tar -xv zookeeper-3.4.13.tar.gz ```\n",
      "\n",
      "We can remove the ```.tar.gz```  file :\n",
      "```rm zookeeper-3.4.13.tar.gz```\n",
      "\n",
      "The terminal of the nodes 1 & 2 looks like this :\n",
      "![image](https://maelfabien.github.io/assets/images/Zookeeper_Extract.jpg)\n",
      "\n",
      "Finally, we can execute the same commands for each install we would like to install Zookeeper on.\n",
      "\n",
      "## Configure your nodes\n",
      "\n",
      "The steps toward the configuration of your nodes are the following :\n",
      "- modify ```zoo-sample.cfg```\n",
      "- modify ```spark-default.sh``` (covered in the next tutorial)\n",
      "- rename the directory ```zookeeper-3.4.13```\n",
      "- create the directory ```logs``` and ```data```\n",
      "- create a file ```myid``` in the new ```data``` dirèctory\n",
      "\n",
      "a. Rename the directory :\n",
      "\n",
      "Make sure you are in the home directory: ```/ubuntu/home/ ```\n",
      "\n",
      "Depending on the node you are working on, execute the following command by changing the digit at the end: ```mv zookeeper-3.4.13/ zookeeper_1```\n",
      "- 1: if you're working on worker node 1\n",
      "- 2: if you're working on worker node 2\n",
      "- 3: if you're working on your node \"Zookeeper\"\n",
      "\n",
      "b. Create the directory ```logs``` and ```data```  :\n",
      "Now, you need to create those 2 directories :\n",
      "```mkdir data``` \n",
      "```mkdir logs``` \n",
      "\n",
      "c. Create a new file  ```myid``` :\n",
      "Go on the new directory ```data``` and create a new file which contains only a digit between 1 and 3.\n",
      "\n",
      "```cd data``` \n",
      "```> myid.txt``` \n",
      "```vi myid.txt``` \n",
      "\n",
      "Depending on the node you are working on, add the digit in the text file :\n",
      "- 1: if you're working on worker node 1\n",
      "- 2: if you're working on worker node 2\n",
      "- 3: if you're working on your node \"Zookeeper\"\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Zookeeper_myid.jpg)\n",
      "\n",
      "d. Modify the file ```zoo-sample.cfg``` :\n",
      "Make sure you are on the “conf” directory : \n",
      "```cd ..```\n",
      "```cd conf```\n",
      "\n",
      "Copy the file ```zoo-sample.cfg``` as ```zoo.cfg``` :\n",
      "```cp zoo-sample.cfg zoo.cfg```\n",
      "\n",
      "We will modify the file ```zoo.cfg``` file :\n",
      "```vi zoo.cfg```\n",
      "\n",
      "- add the line :```clientPort= 218X``` : Replace X by the digit defined above\n",
      "- add the line : ```server.1=<PRIVATE.DNS.1>:2891:3881 ; server.2=<PRIVATE.DNS.2>:2892:3882 ; server.3=<PRIVATE.DNS.3>:2893:3883```\n",
      "- add the line: ```datadir= <Path to the data dir>```\n",
      "\n",
      "Here's an example :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Zookeeper_zoo.jpg)\n",
      "\n",
      "Save (ESC + :wq) and quit.\n",
      "\n",
      "e. Copy configuration files :\n",
      "On the home directory (zookeeper_X), execute this line :\n",
      "```java -cp zookeeper-3.4.13.jar:lib/log4j-1.2.17.jar:lib/slf4j-log4j12-1.7.25.jar:lib/slf4j-api-1.7.25.jar:conf org.apache.zookeeper.server.quorum.QuorumPeerMain conf/zoo.cfg >> logs/zookeeper.log & ```\n",
      "\n",
      "Repeat those steps on the three nodes. Do not forget that you need “OpenJDK-8” on each node. \n",
      "\n",
      "## Launch Zookeeper on each node\n",
      "\n",
      "Right now, your configuration is ready on your Zookeeper Cluster. To execute Zookeeper, go on the ```bin``` directory and execute this command on each node : \n",
      "```./zkServer.sh start```\n",
      "\n",
      "> *Conclusion *: Your quorum of 3 nodes with Zookeeper is now ready. The next step is to install Apache-Spark. \n",
      "---\n",
      "title: Using Spark-Scala for Machine Learning\n",
      "layout: post\n",
      "author_profile: false\n",
      "tags: [bigdata]\n",
      "subtitle : \"Parallel and Distributed Computing\"\n",
      "---\n",
      "\n",
      "As we discussed before, Spark-Scala API is the most widely used API and is a production-ready solution.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Scala.jpg)\n",
      "\n",
      "# What is Scala?\n",
      "\n",
      "Scala is a functional programming language. It is a powerful programming concept that offers more robustness than imperative or object programming. Scala is not purely functional but would rather be in a grey area between the two.\n",
      "\n",
      "Before diving in Scala for Machine Learning, I would recommand simply following the basic overview of Scala from the [official documentation](https://docs.scala-lang.org/tour/basics.html). It takes 10 minutes andd introduces the basics of Scala and some syntax.\n",
      "\n",
      "Spark-Scala API might be close to 10 times faster than PySpark, since Spark is written in Scala. Scala is a language developed by EPFL and become really popular a few years ago. It is quite similar to Java in some parts. \n",
      "\n",
      "One of the main feature of Scala is the function compostion. You might be used to it in PySpark, an this is where it comes from. For example, you can apply sequential functions to a dataframe this way.\n",
      "\n",
      "```scala\n",
      "val newDF = df\n",
      ".select(\"cleanUrls\",\"tags\")\n",
      ".dropDuplicates\n",
      ".groupBy(\"cleanUrls\")\n",
      ".count\n",
      ".filter(col(\"count\")===1)\n",
      ".select(\"cleanUrls\")\n",
      "```\n",
      "\n",
      "In this example, every function (groupBy, dropDuplicates...) is applied to input the dataframe, and each function returns a new data frame.\n",
      "\n",
      "# Scala for Machine Learning\n",
      "\n",
      "Through Spark, Scala is a great ML tool that data scientists should master. It's a great way to use simple high-level APIs for ML and apply it at scale.\n",
      "\n",
      "In the example below, we'll try to predict the price of some houses given several features. \n",
      "\n",
      "## Basic Exploration\n",
      "\n",
      "First, open a Jupyter Notebook with a Spylon Kernel (see my previous article) or whatever IDE you'd like, and import the following packages:\n",
      "\n",
      "```scala\n",
      "import org.apache.spark.sql.SparkSession\n",
      "import org.apache.spark.sql.DataFrame\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.ml.feature.VectorAssembler\n",
      "import org.apache.spark.ml.feature.StringIndexer\n",
      "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
      "import org.apache.spark.ml.regression.GBTRegressor\n",
      "```\n",
      "\n",
      "Then, create your new Spark Session:\n",
      "\n",
      "```scala\n",
      "val spark = SparkSession.builder\n",
      "    .appName(\"SparkMLScala\") \n",
      "    .master(\"local[4]\") \n",
      "    .getOrCreate\n",
      "```\n",
      "\n",
      "We then read the input CSV file:\n",
      "\n",
      "```scala\n",
      "val df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data/house.csv\")\n",
      "df.show(2)\n",
      "```\n",
      "\n",
      "```\n",
      "+---+--------------------+--------------------+--------------------+---------+--------+------------+------------+-------------+---------+-----------+---------------+----------+-----------+---------+---------+-----------+-------+-----------+\n",
      "|_c0|             address|                info|           z_address|bathrooms|bedrooms|finishedsqft|lastsolddate|lastsoldprice| latitude|  longitude|   neighborhood|totalrooms|    usecode|yearbuilt|zestimate|zindexvalue|zipcode|       zpid|\n",
      "+---+--------------------+--------------------+--------------------+---------+--------+------------+------------+-------------+---------+-----------+---------------+----------+-----------+---------+---------+-----------+-------+-----------+\n",
      "|  2|Address: 1160 Mis...| San FranciscoSal...|1160 Mission St U...|      2.0|     2.0|      1043.0|  02/17/2016|    1300000.0|37.778705|-122.412635|South of Market|       4.0|Condominium|   2007.0|1167508.0|    975,700|94103.0|8.3152781E7|\n",
      "|  5|Address: 260 King...| San FranciscoSal...|260 King St UNIT 475|      1.0|     1.0|       903.0|  02/17/2016|     750000.0|37.777641|-122.393417|South of Market|       3.0|Condominium|   2004.0| 823719.0|    975,700|94107.0|6.9819817E7|\n",
      "```\n",
      "\n",
      "The layout of these dataframes is not always the best. We can print the columns using:\n",
      "\n",
      "```scala\n",
      "df.columns\n",
      "```\n",
      "\n",
      "```\n",
      "Array(_c0, address, info, z_address, bathrooms, bedrooms, finishedsqft, lastsolddate, lastsoldprice, latitude, longitude, neighborhood, totalrooms, usecode, yearbuilt, zestimate, zindexvalue, zipcode, zpid)\n",
      "```\n",
      "\n",
      "## SQL Exploration\n",
      "\n",
      "If you are familiar with SQL, it is extremely simple to run SQL queries using Spark-Scala. You must first register the template dataframe as a table, and then use `spark.sql` to run queries.\n",
      "\n",
      "Let's print the average price of the houses in the database:\n",
      "\n",
      "```scala\n",
      "df.registerTempTable(\"housing\")\n",
      "val sqlDF = spark.sql(\"SELECT AVG(lastsoldprice) FROM housing\")\n",
      "sqlDF.show()\n",
      "```\n",
      "\n",
      "```\n",
      "+------------------+\n",
      "|avg(lastsoldprice)|\n",
      "+------------------+\n",
      "|1263928.1871138571|\n",
      "+------------------+\n",
      "```\n",
      "\n",
      "We can also display the most popular ZIPCodes among our database:\n",
      "\n",
      "```scala\n",
      "val sqlDF2 = spark.sql(\"SELECT zipcode, COUNT(*) AS `num` FROM housing GROUP BY zipcode ORDER BY num DESC\")\n",
      "sqlDF2.show(5)\n",
      "```\n",
      "\n",
      "```\n",
      "+-------+---+\n",
      "|zipcode|num|\n",
      "+-------+---+\n",
      "|94110.0|935|\n",
      "|94112.0|877|\n",
      "|94107.0|857|\n",
      "|94131.0|687|\n",
      "|94116.0|655|\n",
      "+-------+---+\n",
      "only showing top 5 rows\n",
      "```\n",
      "\n",
      "You can make sure that the type of the data has been well identified during the `inferSchema` at import.\n",
      "\n",
      "```scala\n",
      "df.printSchema()\n",
      "```\n",
      "\n",
      "```\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- info: string (nullable = true)\n",
      " |-- z_address: string (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- bedrooms: double (nullable = true)\n",
      " |-- finishedsqft: double (nullable = true)\n",
      " |-- lastsolddate: string (nullable = true)\n",
      " |-- lastsoldprice: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- neighborhood: string (nullable = true)\n",
      " |-- totalrooms: double (nullable = true)\n",
      " |-- usecode: string (nullable = true)\n",
      " |-- yearbuilt: double (nullable = true)\n",
      " |-- zestimate: double (nullable = true)\n",
      " |-- zindexvalue: string (nullable = true)\n",
      " |-- zipcode: double (nullable = true)\n",
      " |-- zpid: double (nullable = true)\n",
      "```\n",
      "\n",
      "## Pre-processing\n",
      "\n",
      "There are many features which we won't use, such as the address since this is a string input and the aim is not to dive in Natural Language Processing, but there is a feature `usecode` which is categorial and should be transformed to numerical.\n",
      "\n",
      "To do so, we define a string indexer which matches an input string category with a given numeric index.\n",
      "\n",
      "```scala\n",
      "val indexer = new StringIndexer().setInputCol(\"usecode\").setOutputCol(\"usecode2\")\n",
      "\n",
      "val df2 = indexer.fit(df).transform(df)\n",
      "```\n",
      "\n",
      "In order to use SparkML, we should build a column which contains all features we are going to use to make the prediction. Think about it as grouping all features in a list, and creating a single column called \"features\". This is done with a Vector Assembler.\n",
      "\n",
      "```scala\n",
      "val assembler = new VectorAssembler().\n",
      "       setInputCols(df2.drop(\"lastsoldprice\", \"zindexvalue\", \"_c0\", \"address\", \"info\", \"z_address\", \"lastsolddate\", \"neighborhood\", \"usecode\").columns).\n",
      "       setOutputCol(\"features\")\n",
      "\n",
      "val df3 = assembler.transform(df2)\n",
      "```\n",
      "\n",
      "We can now check the different columns of the dataframe:\n",
      "\n",
      "```scala\n",
      "df3.columns\n",
      "```\n",
      "\n",
      "```scala\n",
      "Array[String] = Array(_c0, address, info, z_address, bathrooms, bedrooms, finishedsqft, lastsolddate, lastsoldprice, latitude, longitude, neighborhood, totalrooms, usecode, yearbuilt, zestimate, zindexvalue, zipcode, zpid, usecode2, features)\n",
      "```\n",
      "\n",
      "We do indeed have a new column! The last step before builing our model is to split our data into train and test:\n",
      "\n",
      "```scala\n",
      "val Array(train, test) = df3.randomSplit(Array(0.8, 0.2), seed = 30)\n",
      "```\n",
      "\n",
      "## Build a model\n",
      "\n",
      "We will use a simple Gradient Boosted Tree Regression model with default parameter and at most 10 iterations.\n",
      "\n",
      "```scala\n",
      "val gbt = new GBTRegressor()\n",
      "  .setLabelCol(\"lastsoldprice\")\n",
      "  .setFeaturesCol(\"features\")\n",
      "  .setMaxIter(10)\n",
      "```\n",
      "\n",
      "Then, fit the model:\n",
      "\n",
      "```scala\n",
      "val model = gbt.fit(train)\n",
      "```\n",
      "\n",
      "## Make predictions\n",
      "\n",
      "Let's now make some predictions on the test set and assess the performance of our model:\n",
      "\n",
      "```scala\n",
      "val predictions = model.transform(test)\n",
      "predictions.select(\"prediction\", \"lastsoldprice\").show(5)\n",
      "```\n",
      "\n",
      "```\n",
      "+------------------+-------------+\n",
      "|        prediction|lastsoldprice|\n",
      "+------------------+-------------+\n",
      "|1613879.2210632167|    1530000.0|\n",
      "|1389284.2393296428|    1440000.0|\n",
      "| 1369447.861598761|    1700000.0|\n",
      "| 770113.3958960483|     700000.0|\n",
      "|1062512.6163005617|    1525000.0|\n",
      "+------------------+-------------+\n",
      "```\n",
      "\n",
      "To evaluate the performance of a model, we should simply call a regression evaluator. We can pick the metric of our choice, for example the Root Mean Squared Error:\n",
      "\n",
      "```scala\n",
      "val evaluator = new RegressionEvaluator()\n",
      "  .setLabelCol(\"lastsoldprice\")\n",
      "  .setPredictionCol(\"prediction\")\n",
      "  .setMetricName(\"rmse\")\n",
      "\n",
      "val rmse = evaluator.evaluate(predictions)\n",
      "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")\n",
      "```\n",
      "\n",
      "```\n",
      "Root Mean Squared Error (RMSE) on test data = 695538.99\n",
      "```\n",
      "\n",
      "> Conclusion: I hope this first approach to Spark-Scala was clear and helpful. I'd be happy to answer any question you might have in the comments section.\n",
      "---\n",
      "title: Functions\n",
      "layout: post\n",
      "tags: [da_tuto]\n",
      "subtitle : \"Data Analysis Basics\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "Writing code in Python is quite simple, but we need to make it re-useable if we apply the same code over and over again during our project. We do so by wrapping our code into functions.\n",
      "\n",
      "# Functions\n",
      "\n",
      "This about this transformation that you want to apply to a string:\n",
      "\n",
      "```python\n",
      "start_string = \"12-12-XXBC\"\n",
      "```\n",
      "\n",
      "You want to extract the first two letters of this code, here \"XX\", and put them in lowercase.\n",
      "\n",
      "```python\n",
      "start_string = start_string[6:8]\n",
      "start_string = start_string.lower()\n",
      "```\n",
      "\n",
      "```\n",
      "xx\n",
      "```\n",
      "\n",
      "Now, you need to repeat that 5 times. Writing this code 5 times for 5 different variable names would be cumbersome. You can wrap it in a function using the keyword `def`:\n",
      "\n",
      "```python\n",
      "def extract_code(x):\n",
      "\treturn x[6:8].lower()\n",
      "```\n",
      "\n",
      "This function has a name: `extract_code` that I chose arbitrarly. It takes one input, here `x`. This input name does not point to any variable, it's simply here to say that we'll later apply this function on some input. And for this given `x`, it extracts the right part, puts it into lowercase and returns this value.\n",
      "\n",
      "You can then call this function and specifiy the input we want:\n",
      "\n",
      "```python\n",
      "string_1 = extract_code(\"12-12-XXBC\")\n",
      "string_1\n",
      "```\n",
      "\n",
      "```\n",
      "xx\n",
      "```\n",
      "\n",
      "You can apply whatever transformation you want with a function, on whatever input data type. You can also sum two integers easily:\n",
      "\n",
      "```python\n",
      "def add_integers(a, b):\n",
      "\treturn a + b\n",
      "```\n",
      "\n",
      "The inputs can take a default value, and if its value is not specified by the user when we calls the function, it will use this default value in the function:\n",
      "\n",
      "```python\n",
      "def add_integers(a, b=1):\n",
      "\treturn a + b\n",
      "```\n",
      "\n",
      "```python\n",
      "add_integers(2)\n",
      "```\n",
      "\n",
      "```\n",
      "3\n",
      "```\n",
      "\n",
      "\"a\" takes the value 2, which is specified by the user, while \"b\" is not specified and therefore takes the value 1 by default.\n",
      "\n",
      "> If you found the article useful or see ways in which it could be improved, please leave a comment :)\n",
      "---\n",
      "title: Hadoop Distributed File System (HDFS)\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Parallel and Distributed Computing\"\n",
      "---\n",
      "\n",
      "## What is HDFS?\n",
      "\n",
      "HDFS stands for Hadoop Distributed File System. It is a sub-project of Hadoop. HDFS lets you connect nodes contained within clusters over which data files are distributed, overall being fault-tolerant. \n",
      "\n",
      "You can then access and store the data files as one seamless file system. HDFS has many goals, among which:\n",
      "- Fault tolerance + automatic recovery\n",
      "- Data access via MapReduce streaming\n",
      "- Simple and robust coherency model\n",
      "- Portability across heterogeneous operating systems + hardware\n",
      "- Scalability to reliably store and process large amounts of data\n",
      "- ...\n",
      "\n",
      "HDFS is written in Java, so any machine supporting Java can run HDFS.\n",
      "\n",
      "## How can you access data stored in HDFS?\n",
      "\n",
      "- Using the Native Java API\n",
      "- Using a C-wrapper \n",
      "- Using a web-browser interface\n",
      "\n",
      "## What is HDFS made of?\n",
      "\n",
      "HDFS is an interconnected cluster of nodes where files and directories reside. HDFS is an interconnected cluster of nodes where files and directories reside. HDFS splits input data into blocks of 64 or 128MB and stores them on computers called DataNodes.\n",
      "\n",
      "\n",
      "\n",
      "An HDFS cluster consists of :\n",
      "- A single node, known as a NameNode, that manages the file system namespace and regulates client access to files. It manages file system namespace operations like opening, closing, and renaming files and directories. A name node also maps data blocks to data nodes, which handle read and write requests from HDFS clients. \n",
      "- DataNodes, that create, delete and replicate data blocks according to instructions from the governing name node.\n",
      "\n",
      "Data nodes continuously loop, asking the name node for instructions. The file system is similar to most other existing file systems; you can create, rename, relocate, and remove files, and put them in directories. HDFS also supports third-party file systems such as CloudStore and Amazon Simple Storage Service (S3).\n",
      "\n",
      "## Process\n",
      "\n",
      "When a client creates a file in HDFS :\n",
      "- it first caches the data into a temporary local file. \n",
      "- It then redirects subsequent writes to the temporary file. \n",
      "- When the temporary file accumulates enough data to fill an HDFS block, the client reports this to the name node, which converts the file to a permanent data node. \n",
      "- The client then closes the temporary file and flushes any remaining data to the newly created data node. \n",
      "- The name node then commits the data node to disk.\n",
      "\n",
      "## Data Storage Reliability\n",
      "\n",
      "Remember that HDFS needs to be reliable even when failures occur within :\n",
      "- name nodes, \n",
      "- data nodes, \n",
      "- or network partitions.\n",
      "\n",
      "How do we control if a node is failing? We use a **heartbeat** process, a small message sent by the node to the name node. If the message is received, then the node is up. Once we don't receive the message anymore, we know that the node is down. HDFS guarantees **Transaction** and **Data Integrity** processes. It uses **checksum** validation on the contents of HDFS files by storing computed checksums in separate, hidden files in the same namespace as the actual data. When a client retrieves file data, it can verify that the data received matches the checksum stored in the associated file.\n",
      "\n",
      "> Conclusion: I hope this high-level overview was clear and helpful. I'd be happy to answer any question you might have in the comments section.\n",
      "---\n",
      "title: Launch a MapReduce Job (3/4)\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Parallel and Distributed Computing\"\n",
      "---\n",
      "\n",
      "In this article, we'll get to the core of the exercise: launch a MapReduce WordCount job.\n",
      "\n",
      "## Launch the Job\n",
      "\n",
      "There are several commands we can use over `hadoop`:\n",
      "\n",
      "- `namenode -format` : Formats the DFS filesystem.\n",
      "- `secondarynamenode` : Runs the DFS secondary namenode.\n",
      "- `namenode` : Runs the DFS namenode.\n",
      "- `datanode` : Runs a DFS datanode.\n",
      "- `dfsadmin` : Runs a DFS admin client.\n",
      "- `mradmin` : Runs a Map-Reduce admin client.\n",
      "- `fsck` : Runs a DFS filesystem checking utility.\n",
      "- `fs` : Runs a generic filesystem user client.\n",
      "- `balancer` : Runs a cluster balancing utility.\n",
      "- `oiv` : Applies the offline fsimage viewer to an fsimage.\n",
      "- `fetchdt` : Fetches a delegation token from the NameNode.\n",
      "- `jobtracker` : Runs the MapReduce job Tracker node.\n",
      "- `pipes` : Runs a Pipes job.\n",
      "- `tasktracker` : Runs a MapReduce task Tracker node.\n",
      "- `historyserver` : Runs job history servers as a standalone daemon.\n",
      "- `job` : Manipulates the MapReduce jobs.\n",
      "- `queue` : Gets information regarding JobQueues.\n",
      "- `version` : Prints the version.\n",
      "- `jar <jar>` : Runs a jar file.\n",
      "- `distcp <srcurl> <desturl>` : Copies file or directories recursively.\n",
      "- `distcp2 <srcurl> <desturl>` : DistCp version 2.\n",
      "- `archive -archiveName NAME -p <parent path> <src>* <dest>` : Creates a hadoop archive.\n",
      "- `classpath` : Prints the class path needed to get the Hadoop jar and the required libraries.\n",
      "- `daemonlog` : Get/Set the log level for each daemon\n",
      "\n",
      "To launch the Hadoop MapReduce job, you should simply type the following command from the VM's terminal :\n",
      "\n",
      "`hadoop jar wc.jar WordCount TP/input TP/output`\n",
      "\n",
      "You can see the cluster work from this page : [http://localhost:8088/](http://localhost:8088/).\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/36.jpg)\n",
      "\n",
      "## YARN\n",
      "\n",
      "This page displays the work done on YARN. Apache Hadoop YARN is the resource management and job scheduling technology in the open-source Hadoop distributed processing framework. One of Apache Hadoop's core components, YARN is responsible for allocating system resources to the various applications running in a Hadoop cluster and scheduling tasks to be executed on different cluster nodes.\n",
      "\n",
      "YARN stands for Yet Another Resource Negotiator, but it's commonly referred to by the acronym alone.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/37.jpg)\n",
      "\n",
      "Apache Hadoop YARN decentralizes execution and monitoring of processing jobs by separating the various responsibilities into these components:\n",
      "- A global ResourceManager that accepts job submissions from users, schedules the jobs and allocates resources to them\n",
      "- A NodeManager slave that's installed at each node and functions as a monitoring and reporting agent of the ResourceManager\n",
      "- An ApplicationMaster that's created for each application to negotiate for resources and work with the NodeManager to execute and monitor tasks\n",
      "- Resource containers that are controlled by NodeManagers and assigned the system resources allocated to individual applications\n",
      "\n",
      "## Output\n",
      "\n",
      "First of all, go the file viewer from [http://localhost:8088/](http://localhost:8088/).\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/38.jpg)\n",
      "\n",
      "Go to `user > raj_ops > TP > output` and click on `part-r-00000` to view the file :\n",
      "\n",
      "You should see something like this :\n",
      "\n",
      "```\n",
      "\"#Muscular    1\n",
      "\"'Come    1\n",
      "\"'Dieu    1\n",
      "\"'Dio    1\n",
      "\"'From    1\n",
      "\"'Grant    1\n",
      "\"'I    4\n",
      "\"'No    1\n",
      "...\n",
      "\"Anna    2\n",
      "\"Annette,    1\n",
      "\"Announce    1\n",
      "\"Another    7\n",
      "\"Any    2\n",
      "\"Anybody    1\n",
      "\"Anyhow    1\n",
      "\"Anyhow,    2\n",
      "\"Anything    1\n",
      "\"Appeal    1\n",
      "\"Apropos,    1\n",
      "\"Arakcheev    1\n",
      "\"Are    30\n",
      "\"Aren't    2\n",
      "\"Arguing?    1\n",
      "\"Arinka!    1\n",
      "\"Arnauts!\"    1\n",
      "\"Arrange    1\n",
      "\"Arranging    1\n",
      "\"Arthur    1\n",
      "\"As    36\n",
      "```\n",
      "\n",
      "And this is it! We managed to get the results of the MapReduce WordCount job. In the next article, we'll see how to submit Python scripts for the Map and the Reduce Parts.\n",
      "\n",
      "\n",
      "> Conclusion: I hope this tutorial was clear and helpful. I'd be happy to answer any question you might have in the comments section.\n",
      "---\n",
      "title: Virtual Machines with Virtual Box\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Parallel and Distributed Computing\"\n",
      "---\n",
      "\n",
      "Hadoop runs only on GNU/Linux platforms. Therefore, if you have another OS, you need to install Virtual Box. Virtual Box is a software that lets you create and run Virtual Machines.\n",
      "\n",
      "A virtual machine is a machine that takes part of the resources of your computer (according to initial parameters you chose), and for which you can choose the OS to boot on within the software.\n",
      "\n",
      "## Step 1: Install Virtual Box\n",
      "\n",
      "The first step of this exercise is to install Virtual Box if you don't have a Linux operating system.\n",
      "- Go to: https://www.virtualbox.org/ and download VirtualBox\n",
      "- Follow the installation steps\n",
      "\n",
      "## Step 2: Configure the VM\n",
      "\n",
      "In this part, we'll try to understand how to install a complete Ubuntu VM on your computer. Go to [this link](https://www.ubuntu.com/download/desktop) and download the Ubuntu Desktop file.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/13.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/14.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/15.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/16.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/17.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/18.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/19.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/20.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/21.jpg)\n",
      "\n",
      "Here's what you should see :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/22.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/23.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/24.jpg)\n",
      "\n",
      "Follow the steps until you have to restart the VM. You might need to quit the installation and restart the VM from the menu. You should finally see your user interface :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/25.jpg)\n",
      "\n",
      "> Conclusion: I hope this tutorial was clear and helpful. I'd be happy to answer any question you might have in the comments section.\n",
      "---\n",
      "title: Large Scale Kernel Methods \n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Supervised Learning Algorithms\"\n",
      "---\n",
      "\n",
      "Kernel methods such as Kernel SVM have some major issues regarding scalability. You might have encountered some issues when trying to apply RBF Kernel SVMs on a large amount of data. \n",
      "\n",
      "Two major algorithms allow to easily scale Kernel methods :\n",
      "- Random Kernel features\n",
      "- Nyström approximation\n",
      "\n",
      "We'll recall what Kernel methods are, and cover both methods.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "For what comes next, you might want to open a Jupyter Notebook and import the following packages :\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from scipy import linalg\n",
      "import matplotlib.pyplot as plt\n",
      "plt.style.use('ggplot')\n",
      "\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.datasets import load_svmlight_file\n",
      "from sklearn.datasets import make_classification\n",
      "\n",
      "from sklearn.svm import SVC, LinearSVC\n",
      "from time import time\n",
      "\n",
      "from scipy.sparse.linalg import svds\n",
      "from scipy.linalg import svd\n",
      "from scipy.sparse import csc_matrix\n",
      "from numpy.linalg import multi_dot\n",
      "from numpy.linalg import norm\n",
      "\n",
      "from math import pi\n",
      "```\n",
      "\n",
      "# I. Recall on Kernel Methods\n",
      "\n",
      "## SVM Classifier\n",
      "\n",
      "We'll consider a binary classification framework. Suppose we have training observations $$ X_1, ..., X_n \\subset R^p $$ , and training labels $$ y_1, ..., y_n ∈ {-1,1} $$ .\n",
      "\n",
      "We can define the non-linearly separable SVM framework as follows :\n",
      "\n",
      "$$ min_{w ∈ R^p, b ∈ R, \\epsilon ∈ R^n} \\frac {1} {2} { {\\mid \\mid w \\mid \\mid }_2 }^2 + C \\sum_i {\\epsilon}_i $$\n",
      "\n",
      "subject to :\n",
      "\n",
      "$$ y_i ( w^T X + b) ≥ 1 - {\\epsilon_i}, i = 1 ... n$$\n",
      "\n",
      "$$ {\\epsilon_i} ≥ 0, i = 1 ... n $$\n",
      "\n",
      "We can rewrite this as a dual problem using a Lagrange formulation :\n",
      "\n",
      "$$ max_{\\alpha ∈ R^n} {\\sum}_i {\\alpha}_i - \\frac {1} {2} {\\alpha}_i {\\alpha}_j y_i y_j {X_i}^T X_j $$\n",
      "\n",
      "subject to :\n",
      "\n",
      "$$ 0 ≤ {\\alpha}_i ≤ C, i = 1 ... n $$\n",
      "\n",
      "$$ {\\epsilon}_i , i = 1 ... n $$\n",
      "\n",
      "The binary classifier is : $$ f(x) = sign( \\sum_i {\\alpha}_i y_i {X_i}^T X_i ) $$\n",
      "\n",
      "## The kernel trick\n",
      "\n",
      "A symmetric function $$ K : χ \\times χ → R $$ is a kernel if there exists a mapping function $$ \\phi : χ → R $$ from the instance space $$ χ $$ to a Hilbert space $$ H $$ such that $$ K $$ can be written as an inner product in $$ H $$ :\n",
      "\n",
      "$$ K(X, X') = < \\phi(X), \\phi(X') > $$\n",
      "\n",
      "The Kernel trick can be visualized as a projection of an initial problem with a complex decision frontier into feature space in which the decision frontier is way easier and faster to build.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/kernel_trick.jpg)\n",
      "\n",
      "The Kernel SVM can be expressed as :\n",
      "\n",
      "$$ max_{\\alpha ∈ R^n} {\\sum}_i {\\alpha}_i - \\frac {1} {2} {\\alpha}_i {\\alpha}_j y_i y_j K ({X_i}^T X_j) $$\n",
      "\n",
      "subject to :\n",
      "\n",
      "$$ 0 ≤ {\\alpha}_i ≤ C, i = 1 ... n $$\n",
      "\n",
      "$$ \\sum_i {\\alpha}_i  y_i = 0, i = 1 ... n $$\n",
      "\n",
      "The binary classifier is : $$ f(x) = sign( \\sum_i {\\alpha}_i y_i K({X_i}^T X_i )) $$\n",
      "\n",
      "## Types of kernels\n",
      "\n",
      "What types of kernels can be used?\n",
      "\n",
      "- Linear kernel : $$ K(X,X') = X^T X' $$\n",
      "- Polynomial kernel : $$ K(X,X') = (X^T X' + c)^d $$\n",
      "- Gaussian RBF kernel : $$ K(X,X') = exp( - \\gamma { { \\mid \\mid X - X' \\mid \\mid}_2 }^2 ) $$\n",
      "- Laplace RBF kernel : $$ K(X,X') = exp( - \\gamma { \\mid \\mid X - X' \\mid \\mid}_1 ) $$\n",
      "\n",
      "Kernels allow non-linear variants for many linear machine learning algorithms :\n",
      "- SVM\n",
      "- Ridge Regression\n",
      "- PCA\n",
      "- K-Means\n",
      "- and others ...\n",
      "\n",
      "## In Python\n",
      "\n",
      "First of all, we'll generate some articifical data :\n",
      "\n",
      "```python\n",
      "X, y = make_classification(n_samples=100000)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
      "```\n",
      "\n",
      "We now have 75'000 training data and 25'000 test data. We first scale the data.\n",
      "\n",
      "```python\n",
      "scaler = StandardScaler()\n",
      "X_train = scaler.fit_transform(X_train)\n",
      "X_test = scaler.transform(X_test)\n",
      "\n",
      "n1, p = X_train.shape\n",
      "n2 = X_test.shape[0]\n",
      "\n",
      "print(\"Training samples :\", n1)\n",
      "print(\"Test samples:\", n2)\n",
      "print(\"Features:\", p)\n",
      "```\n",
      "\n",
      "### Linear Support Vector Classifier\n",
      "\n",
      "```python\n",
      "# Train\n",
      "t0 = time()\n",
      "clf_lin = LinearSVC(dual=False)\n",
      "clf_lin.fit(X_train, y_train)\n",
      "print(\"done in %0.3fs\" % (time() - t0))\n",
      "```\n",
      "`done in 0.334s`\n",
      "\n",
      "```python\n",
      "# Test\n",
      "t1 = time()\n",
      "timing_linear = time() - t1\n",
      "y_pred = clf_lin.predict(X_test)\n",
      "print(\"done in %0.3fs\" % (time() - t1))\n",
      "```\n",
      "`done in 0.016s`\n",
      "\n",
      "```python\n",
      "# Accuracy\n",
      "accuracy_linear = accuracy_score(y_pred, y_test)\n",
      "print(\"classification accuracy: %0.3f\" % accuracy_linear)\n",
      "```\n",
      "`classification accuracy: 0.868`\n",
      "\n",
      "### Gaussian RBF Kernel Support Vector Classifier\n",
      "\n",
      "```python\n",
      "# Train\n",
      "t0 = time()\n",
      "clf = SVC(kernel='rbf')\n",
      "clf.fit(X_train, y_train)\n",
      "print(\"done in %0.3fs\" % (time() - t0))\n",
      "```\n",
      "`done in 375.102s`\n",
      "\n",
      "```python\n",
      "# Test\n",
      "t1 = time()\n",
      "y_pred = clf.predict(X_test)\n",
      "timing_kernel = time() - t1\n",
      "print(\"done in %0.3fs\" % (time() - t1))\n",
      "```\n",
      "`done in 40.148s`\n",
      "\n",
      "```python\n",
      "# Accuracy\n",
      "accuracy_kernel = accuracy_score(y_pred, y_test)\n",
      "print(\"classification accuracy: %0.3f\" % accuracy_kernel)\n",
      "```\n",
      "`classification accuracy: 0.891`\n",
      "\n",
      "The classification accuracy improves when we use the Gaussian RBF. However, the training and prediction times are now much longer.\n",
      "\n",
      "# II. Limits of Kernel methods\n",
      "\n",
      "Kernel methods rely on Gram Matrix : $$ G ∈ R^{n \\times n} $$\n",
      "\n",
      "The Gram martix has the following form :\n",
      "\n",
      "$$ \\begin{pmatrix} K(X_1, X_1) & K(X_1, X_2) & .. & K(X_1, X_n) \\\\ ... & ... & ... & ... \\\\ K(X_n, X_1) & K(X_n, X_2) & .. & K(X_n, X_n) \\end{pmatrix} $$\n",
      "\n",
      "The complexity of the kernel evaluation in the training is $$ O(n^2) $$.\n",
      "\n",
      "The complexity of the prediction is $$ O(n) $$. Overall, this becomes infeasible for large $$ n $$.\n",
      "\n",
      "We'll now cover the two most common ways to overcome this problem :\n",
      "- Random Kernel features: approximate the kernel function\n",
      "- Nyström approximation: approximate the Gram matrix\n",
      "\n",
      "# III. Random Kernel features\n",
      "\n",
      "## Principle\n",
      "\n",
      "If we don't apply the Kernel SVM, the problem can be expressed the following way :\n",
      "\n",
      "$$ min_{w,b} \\frac {1} {2} { { \\mid \\mid w \\mid \\mid }_2 }^2 + C \\sum_i [ y_i (W^T \\phi(X) + b)]_+ $$\n",
      "\n",
      "where $$ [a]_+ = max(0, 1-a) $$ is the hingle loss function.\n",
      "\n",
      "Usually, $$ \\phi(X) $$ is unknown and potentially infinite-dimensional, and implies $$ O(n^2) $$ or $$ O(n^3) $$ complexity.\n",
      "\n",
      "The ***idea*** of Randon Kernel Features is to find a finite dimensional feature map $$ \\hat{ \\phi } (X)  ∈ R^c $$ such that :\n",
      "\n",
      "$$ K(X, X') ≈ < \\hat{\\phi}(X), \\hat{\\phi}(X') > $$\n",
      "\n",
      "We should be able to solve the primal form to get $$ w $$ and $$ b $$, and use the approximated kernel in a binary classification : $$ f(x) = sign (w^T \\hat{ \\phi } (X) + b) $$ .\n",
      "\n",
      "## Botchner's Theorem\n",
      "\n",
      "A kernel is said to be shift-invariant if and only if for any $$ a ∈ R^p $$ and any $$ (x,x') ∈ R^p \\times R^p $$ :\n",
      "\n",
      "$$ K (x-a, x'-a) = K (x, x') $$\n",
      "\n",
      "$$ K (x, x') = K (x-x') = K ( \\Delta) $$\n",
      "\n",
      "> We'll consider shift-invariant kernels $$ K (x-x') = K ( \\Delta) $$ (Gaussian RBF and Laplace RBF) in order to apply Bochner's theorem. This theorem states that a continuous shift-invariant kernel is positive definite if and only if $$ K ( \\Delta) $$ is the Fourier transform of a non-negative probability measure. \n",
      "\n",
      "It can be shown (the demonstration is skipped for this article) that :\n",
      "\n",
      "$$ K (x, x') = E_{w \\sim P, b \\sim U[0, 2 \\pi]} [ \\sqrt{2} cos (w^T x + b) \\sqrt{2} cos (w^T x' + b)] $$\n",
      "\n",
      "The kernel is an infinite sum since we consider all values of $$ w $$ and $$ b $$. The kernel has, therefore, an infinite dimension. \n",
      "\n",
      "## Kernel approximation\n",
      "\n",
      "A usual technique to approximate such problem is random sampling! If we know the distributions of $$ w $$ and $$ b $$, by Monte-Carlo principle, we'll approach the result of the RBF Kernel!\n",
      "\n",
      "The distributions are the following :\n",
      "- $$ b $$ follows a uniform distribution : $$ b \\sim U[0, 2 \\pi] $$\n",
      "- $$ w $$ follows $$ P (w) $$ the scaled fourier transform of  $$ K(\\Delta) $$\n",
      "\n",
      "If the Kernel is Gaussian, $$ P $$ is Gaussian itself : $$ P \\sim N(0, 2 \\gamma) $$, where the default value of $$ \\gamma $$ is $$ \\frac {1} {p} $$ , $$ p $$ being the number of features . If the Kernel is Laplacian, $$ P $$ is a Cauchy distribution.\n",
      "\n",
      "## Pseudo-Code\n",
      "\n",
      "1. Set the number of random kernel features to $$ c $$\n",
      "2. Draw $$ w_1, ..., w_c \\sim P(w) $$ and $$ b_1, ..., b_c \\sim U [0, 2 \\pi] $$\n",
      "3. Map training points $$ x_1, ..., x_n ∈ R^p $$ to their random kernel features $$ \\hat{\\phi} (X_1), ...,  \\hat{\\phi} (X_n) ∈ R^c $$ where $$ \\hat{\\phi} (X_i) = \\sqrt{ \\frac {2} {c} } cos ( {w_i}^T X + b_j), j ∈ [1, ... , c] $$. $$ c $$ is present in the fraction to create a mean.\n",
      "4. Train a linear model (such as Linear SVM) on transformed data $$ \\hat{\\phi} (X_1), ...,  \\hat{\\phi} (X_n) ∈ R^c $$\n",
      "\n",
      "In other words, to speed up the whole training process and get results that tend to be similar to RBF kernel, we pre-process the data and apply a linear SVM on top. It can be shown that this approximation will converge to the RBF Kernel. \n",
      "\n",
      "Moreover, the kernel approximation error uniformly decreases in $$ O( \\sqrt { \\frac {1} {c} } ) $$.\n",
      "\n",
      "## In Python \n",
      "\n",
      "Let's define the `random_features` function that will return the modified training data according to the pseudo-code above :\n",
      "\n",
      "```python\n",
      "def random_features(X_train, X_test, gamma, c=300, seed=42):\n",
      "    rng = np.random.RandomState(seed)\n",
      "    n_samples, n_features = X_train.shape\n",
      "\n",
      "    W = np.random.normal(0, np.sqrt(2*gamma), (n_features, c))\n",
      "    b = np.random.uniform(0, 2*pi, (1,c))\n",
      "\n",
      "    X_new_train = np.sqrt(2/n_features) * np.cos(np.dot(X_train, W) + b)\n",
      "    X_new_test = np.sqrt(2/n_features) * np.cos(np.dot(X_test, W) + b)\n",
      "\n",
      "    return X_new_train, X_new_test\n",
      "```\n",
      "\n",
      "As defined above, the default value of $$ \\gamma $$ is $$ \\frac {1} {p} $$ :\n",
      "\n",
      "```python\n",
      "n_samples, n_features = X_train.shape\n",
      "gamma = 1. / n_features\n",
      "```\n",
      "\n",
      "Then, modify the input data using the random kernel feature :\n",
      "\n",
      "```python\n",
      "Z_train, Z_test = random_features(X_train, X_test, gamma, c=800)\n",
      "```\n",
      "\n",
      "We'll now assess the efficiency of this technique :\n",
      "\n",
      "```python\n",
      "t0 = time()\n",
      "clf = LinearSVC(dual=False)\n",
      "clf.fit(Z_train, y_train)\n",
      "print(\"done in %0.3fs\" % (time() - t0))\n",
      "```\n",
      "`done in 39.525s`\n",
      "\n",
      "```python\n",
      "t1 = time()\n",
      "accuracy = clf.score(Z_test, y_test)\n",
      "print(\"done in %0.3fs\" % (time() - t1))\n",
      "print(\"classification accuracy: %0.3f\" % accuracy)\n",
      "```\n",
      "`done in 0.089s`\n",
      "`classification accuracy: 0.881`\n",
      "\n",
      "The classification is very close to the one achieved by RBF. However, the computation time has been divided by 10 overall.\n",
      "\n",
      "# IV. Nyström Approximation\n",
      "\n",
      "The essence of the Nyström approximation is to offer an approximation of the Gram matrix involved in the computation by spectral decomposition.\n",
      "\n",
      "Let $$ G ∈ R^{n \\times n} $$ be a Gram matrix such that $$ G_{i,j} = K(X_i, X_j) $$ . When $$ n $$ gets large, we want to approximate $$ G $$ with a lower rank matrix. \n",
      "\n",
      "## Spectral decomposition\n",
      "\n",
      "Recall that the spectral decomposition is defined as : $$ G = U Λ U^T $$ where :\n",
      "- $$ U = [u_1, ... , u_n]^T ∈ R^{n \\times n} $$ a set of eigenvectors\n",
      "- $$ Λ = diag( \\lambda_1, ..., \\lambda_n) $$ a set of eigenvalues\n",
      "\n",
      "The best rank-k approximation $$ G_k $$ of $$ G $$ is given by $$ G_k = U_k Λ_k {U_k}^T $$ where :\n",
      "- $$ U_k  ∈ R^{n \\times k} $$\n",
      "- $$ Λ = diag( \\lambda_1, ..., \\lambda_k) $$\n",
      "\n",
      "We only keep the $$ k^{th} $$ largest eigenvalues.\n",
      "\n",
      "## Limitations and motivation\n",
      "\n",
      "However, in our case, this is useless. We need to construct $$ G $$ in $$ O(n^2) $$ time and compute its $$ k^{th} $$ best rank approximation in $$ O(n^2) $$ to $$ O(n^3) $$ depending on the value of $$ k $$. \n",
      "\n",
      "Our goal is therefore to find a good approximation $$ \\hat {G_k} $$ of $$ G_k $$ in $$ O(n) $$ time.\n",
      "\n",
      "For this reason, we introduce the Nyström approximation :\n",
      "\n",
      "$$ \\hat{G_k} = C W^+ C^T $$ where :\n",
      "- $$ C ∈ R^{n \\times c} $$\n",
      "- $$ W ∈ R^{c \\times c} $$\n",
      "- $$ W^+ $$ the Moore-Penrose (pseudo) inverse\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/schema_nystrom.jpg)\n",
      "\n",
      "This decomposition might seem a bit weird since we sample the columns and the rows of the Gram matrix. First of all, it can only be applied to Gram matrices, not to any kind of matrix. Suppose that we take a look at a matrix of distances between different cities. Would you need all the distances between all the cities to provide a pretty accurate estimate of the distance between 2 cities? Well, there's definitely some pieces of information that have a little importance and bring few additional precision. This is exactly what we're doing here on the Gram matrix.\n",
      "\n",
      "## Pseudo-code\n",
      "\n",
      "1. Sample a set $$ I $$ of $$ c $$ indices uniformly in $$ {1,...,n} $$\n",
      "2. Compute $$ c ∈ R^{n \\times c} $$ with $$ c_{ij} = K(X_i, X_j), i ∈ {1,...,n}, j ∈ I $$\n",
      "3. Form a matrix $$ W ∈ R^{c \\times c} $$ with $$ W_{ij} = K(X_i, X_j), i, j ∈ I $$\n",
      "4. Compute $$ W_k ∈ R^{c \\times c} $$ the best rank-k approximation of $$ W $$\n",
      "5. Compute the final rank-k matrix of G : $$ \\hat{G_k} = C {W_k}^+ C^T ∈ R^{n \\times n} $$\n",
      "\n",
      "The complexity is $$ O(c^3 + nck) $$ . The convergence of this approximation has also been demonstrated.\n",
      "\n",
      "## In Python\n",
      "\n",
      "Define the function corresponding to the Nyström approximation :\n",
      "```python\n",
      "def nystrom(X_train, X_test, gamma, c=500, k=200, seed=44):\n",
      "\n",
      "    rng = np.random.RandomState(seed)\n",
      "    n_samples = X_train.shape[0]\n",
      "    idx = rng.choice(n_samples, c)\n",
      "\n",
      "    X_train_idx = X_train[idx, :]\n",
      "    W = rbf_kernel(X_train_idx, X_train_idx, gamma=gamma)\n",
      "\n",
      "    u, s, vt = linalg.svd(W, full_matrices=False)\n",
      "    u = u[:,:k]\n",
      "    s = s[:k]\n",
      "    vt = vt[:k, :]\n",
      "\n",
      "    M = np.dot(u, np.diag(1/np.sqrt(s)))\n",
      "\n",
      "    C_train = rbf_kernel(X_train, X_train_idx, gamma=gamma)\n",
      "    C_test = rbf_kernel(X_test, X_train_idx, gamma=gamma)\n",
      "\n",
      "    X_new_train = np.dot(C_train, M)\n",
      "    X_new_test = np.dot(C_test, M)\n",
      "\n",
      "    return X_new_train, X_new_test\n",
      "```\n",
      "\n",
      "Modify the input data :\n",
      "\n",
      "```python\n",
      "Z_train, Z_test = nystrom(X_train, X_test, gamma, c=500, k=300, seed=44)\n",
      "```\n",
      "\n",
      "Fit the model :\n",
      "\n",
      "```python\n",
      "t0 = time()\n",
      "clf = LinearSVC(dual=False)\n",
      "clf.fit(Z_train, y_train)\n",
      "print(\"done in %0.3fs\" % (time() - t0))\n",
      "```\n",
      "`done in 15.260s`\n",
      "\n",
      "And compute the accuracy :\n",
      "\n",
      "```python\n",
      "t1 = time()\n",
      "accuracy = clf.score(Z_test, y_test)\n",
      "print(\"done in %0.3fs\" % (time() - t1))\n",
      "print(\"classification accuracy: %0.3f\" % accuracy)\n",
      "```\n",
      "`done in 0.021s`\n",
      "`classification accuracy: 0.886`\n",
      "\n",
      "The results are overall better than Linear SVC and random kernel features, and the computation time is way smaller.\n",
      "\n",
      "# V. Performance overview\n",
      "\n",
      "In this section, we'll compare the performances of the different versions of the classifier in terms of accuracy and computation time :\n",
      "\n",
      "```python\n",
      "ranks = np.arange(20, 600, 50)\n",
      "n_ranks = len(ranks)\n",
      "\n",
      "timing_rkf = np.zeros(n_ranks)\n",
      "timing_nystrom = np.zeros(n_ranks)\n",
      "timing_linear = np.zeros(n_ranks)\n",
      "timing_rbf = np.zeros(n_ranks)\n",
      "\n",
      "accuracy_nystrom = np.zeros(n_ranks)\n",
      "accuracy_rkf = np.zeros(n_ranks)\n",
      "accuracy_linear = np.zeros(n_ranks)\n",
      "accuracy_rbf = np.zeros(n_ranks)\n",
      "\n",
      "print(\"Training SVMs for various values of c...\")\n",
      "\n",
      "for i, c in enumerate(ranks):\n",
      "\n",
      "    print(i, c)\n",
      "\n",
      "    ## Nystorm\n",
      "    Z_ny_train, Z_ny_test = nystrom(X_train, X_test, gamma, c=c, k=300, seed=44)\n",
      "\n",
      "    t0 = time()\n",
      "    clf = LinearSVC(dual=False)\n",
      "    clf.fit(Z_ny_train, y_train)\n",
      "    accuracy_nystrom[i] = clf.score(Z_ny_test, y_test)\n",
      "    timing_nystrom[i] = time() - t0\n",
      "\n",
      "    ## Random Kernel Feature\n",
      "    Z_rkf_train, Z_rkf_test = random_features(X_train, X_test, gamma, c=c, seed=44)\n",
      "    t0 = time()\n",
      "    clf = LinearSVC(dual=False)\n",
      "    clf.fit(Z_rkf_train, y_train)\n",
      "    accuracy_rkf[i] = clf.score(Z_rkf_test, y_test)\n",
      "    timing_rkf[i] = time() - t0\n",
      "\n",
      "    ## Linear\n",
      "    t0 = time()\n",
      "    clf = LinearSVC(dual=False)\n",
      "    clf.fit(X_train, y_train)\n",
      "    accuracy_linear[i] = clf.score(X_test, y_test)\n",
      "    timing_linear[i] = time() - t0\n",
      "\n",
      "    ## RBF\n",
      "    t0 = time()\n",
      "    clf = SVC(kernel='rbf')\n",
      "    clf.fit(X_train, y_train)\n",
      "    accuracy_rbf[i] = clf.score(X_test, y_test)\n",
      "    timing_rbf[i] = time() - t0\n",
      "```\n",
      "\n",
      "If we plot the time and the accuracy depending on the number of features included, we obtain :\n",
      "\n",
      "```python\n",
      "f, axes = plt.subplots(ncols=1, nrows=2, figsize=(10,6))\n",
      "ax1, ax2 = axes.ravel()\n",
      "\n",
      "ax1.plot(ranks-10, timing_nystrom, '-', label='Nystrom')\n",
      "ax1.plot(ranks, timing_rkf, '-', label='RKF')\n",
      "ax1.plot(ranks, timing_linear * np.ones(n_ranks), '-', label='LinearSVC')\n",
      "ax1.plot(ranks, timing_kernel * np.ones(n_ranks), '-', label='RBF')\n",
      "\n",
      "ax1.set_xlabel('Number of features')\n",
      "ax1.set_ylabel('Time')\n",
      "ax1.legend(loc='lower right')\n",
      "\n",
      "ax2.plot(ranks-10, accuracy_nystrom, '-', label='Nystrom')\n",
      "ax2.plot(ranks, accuracy_rkf, '-', label='RKF')\n",
      "ax2.plot(ranks, accuracy_linear * np.ones(n_ranks), '-', label='LinearSVC')\n",
      "ax2.plot(ranks, accuracy_kernel * np.ones(n_ranks), '-', label='RBF')\n",
      "ax2.set_xlabel('Number of features')\n",
      "ax2.set_ylabel('Accuracy')\n",
      "ax2.legend(loc='lower right')\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/perf_kernel.jpg)\n",
      "\n",
      "We observe the convergence in terms of the accuracy of random kernel features and Nyström methods. The computation time is also smaller up to a certain number of features.\n",
      "\n",
      "The Github repository of this article can be found [here](https://github.com/maelfabien/Machine_Learning_Tutorials).\n",
      "\n",
      "> **Conclusion** : I hope that that this article on large scale kernel methods was useful to you at some point. Don't hesitate to drop a comment if you have any question.\n",
      "---\n",
      "title: Introduction to the ElasticStack\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Elastic Search, Logstash, Kibana\"\n",
      "---\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/els.jpg)\n",
      "\n",
      "You may have already heard of Elasticsearch and Kibana. Elasticsearch is an open-source distributed, RESTful search and analytics engine capable of solving a growing number of use cases. Elasticsearch has been downloaded over 250 million times and has an active community of more than 100'000 members.\n",
      "\n",
      "# I. The Elastic Stack\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/elastic_stack.jpg)\n",
      "\n",
      "Kibana is used to visualize data, and Elasticsearch is used to store, search and analyze the data. \n",
      "\n",
      "Logstash and Beats are used to ingest the data and put them into Elasticsearch :\n",
      "- Beats is a lightweight data shipper that you can put on your applications\n",
      "- Logstash is an ETL tool to enrich and process the data before putting it into Elasticsearch\n",
      "\n",
      "Elasticsearch became so popular because :\n",
      "- it is scalable and distributed, and can easily handle millions of documents or a large number of requests per second\n",
      "- the data model in Elasticsearch is flexible, and data can, therefore, be represented in more than 2 different ways. It mimics how an application will consume the data. \n",
      "- it is highly available and fault tolerant\n",
      "- it is also developer-friendly\n",
      "\n",
      "There are two ways to deploy Elasticsearch :\n",
      "- using the fully managed cloud solution accessible <span style=\"color:blue\">[here](https://cloud.elastic.co/)</span>\n",
      "- locally, using <span style=\"color:blue\">[this link](https://www.elastic.co/downloads/elasticsearch)</span>\n",
      "\n",
      "In further articles, we'll cover both deployment methods.\n",
      "\n",
      "---\n",
      "title: Machine Learning Explainability\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Advanced Machine Learning\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "In this series, I will summarize the course \"Machine Learning Explaibnability\" from Kaggle Learn. The full course is available [here](https://www.kaggle.com/learn/machine-learning-explainability).\n",
      "\n",
      "First of all, it is important to define the difference between machine learning explainability and interpretability. According to [KDnuggets](https://www.kdnuggets.com/2018/12/machine-learning-explainability-interpretability-ai.html) :\n",
      "- Interpretability is about the extent to which a cause and effect can be observed within a system. Or, to put it another way, it is the extent to which you can predict what is going to happen, given a change in input or algorithmic parameters.\n",
      "- Explainability, meanwhile, is the extent to which the internal mechanics of a machine or deep learning system can be explained in human terms. \n",
      "\n",
      "Explainability and interpretability are key elements today if we want to deploy ML algorithms in healthcare, banking, and other domains.\n",
      "\n",
      "# I. Use cases for model insights\n",
      "\n",
      "In this course, we will answer the following questions on model insights extraction :\n",
      "- What features in the data did the model think are most important?\n",
      "- For any single prediction from a model, how did each feature in the data affect that particular prediction?\n",
      "- How does each feature affect the model's predictions in a big-picture sense (what is its typical effect when considered over a large number of possible predictions)?\n",
      "\n",
      "These insights are valuable since they have many use cases.\n",
      "\n",
      "## Debugging\n",
      "\n",
      "Understanding the patterns a model is finding helps us identify when these patterns are odds. This is the first step to track bugs, unreliable and dirty data. \n",
      "\n",
      "## Informing feature engineering\n",
      "\n",
      "Feature engineering is a great way to improve model accuracy. It implies a transformation of the existing features. But what happens when we have up to 100 features, when we don't have the right background to create smart features or when for privacy reasons the column names are not available?\n",
      "\n",
      "By identifying the most important features, it is then much easier to simply create an addition, a subtraction or a multiplication between 2 features for example.\n",
      "\n",
      "## Directing future data collection\n",
      "\n",
      "Many businesses can expand the types of data they collect. Model-based insights show you what are the most important features to collect, and helps you reason about what new values may be most useful.\n",
      "\n",
      "## Informing human decision-making\n",
      "\n",
      "For many human decisions that cannot (yet?) be made automatically by an algorithm, insights on the model prediction can bring explainability to support a decision.\n",
      "\n",
      "## Building trust\n",
      "\n",
      "Many people won't assume they can trust your model for important decisions without verifying some basic facts. Showing the right insights, even to people with few data science knowledge, is important.\n",
      "\n",
      "# II. Permutation importance\n",
      "\n",
      "What features have the biggest impact on predictions? There are many ways to compute feature importance. We will focus on permutation importance, which is :\n",
      "- fast to compute\n",
      "- widely used\n",
      "- consistent with the properties needed\n",
      "\n",
      "## How does it work?\n",
      "\n",
      "Permutation importance is computed after a model has been fitted. It answers the following question :\n",
      "If with randomly shuffle a single column of the validation data, leaving the target and all other columns in place, how would that affect the accuracy?\n",
      "\n",
      "For example, say we want to predict the height of a person at age 20 based on a set of features, including some less relevant ones (the number of socks owned at age 10):\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/perm.jpg)\n",
      "\n",
      "Randomly re-ordering a single column should decrease the accuracy. Depending on how relevant the feature is, it will more or less impact the accuracy. From the impact on accuracy, we can determine the importance of a feature.\n",
      "\n",
      "## Example\n",
      "\n",
      "In this example, we will try to predict the \"Man of the Game\" of a football match based on a set of features of a player in a match.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "data = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')\n",
      "\n",
      "y = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"/\"No\" to binary\n",
      "feature_names = [i for i in data.columns if data[i].dtype in [np.int64]]\n",
      "X = data[feature_names]\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
      "my_model = RandomForestClassifier(random_state=0).fit(X_train, y_train)\n",
      "```\n",
      "\n",
      "We can then compute the Permutation Importance with [Eli5 library](https://eli5.readthedocs.io/en/latest/). Eli5 is a Python library which allows to visualize and debug various Machine Learning models using unified API. It has built-in support for several ML frameworks and provides a way to explain black-box models.\n",
      "\n",
      "```python\n",
      "import eli5\n",
      "from eli5.sklearn import PermutationImportance\n",
      "\n",
      "perm = PermutationImportance(my_model, random_state=1).fit(X_test, y_test)\n",
      "eli5.show_weights(perm, feature_names = val_X.columns.tolist())\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/perm2.jpg)\n",
      "\n",
      "In our example, the most important feature was Goals scored. The first number in each row shows how much model performance decreased with a random shuffling (in this case, using \"accuracy\" as the performance metric). We measure the randomness by repeating the process with multiple shuffles.\n",
      "\n",
      "A negative value for importance occurs when the feature is not important at all.\n",
      "\n",
      "# III. Partial dependence plots\n",
      "\n",
      "While feature importance shows what variables most affect predictions, partial dependence plots show how a feature affects predictions.\n",
      "\n",
      "Partial dependence plots can be interpreted similarly to coefficients in linear or logistic regression models but can capture more complex patterns than simple coefficients.\n",
      "\n",
      "We can use partial dependence plots to answer questions like :\n",
      "- Controlling for all other house features, what impact do longitude and latitude have on home prices? To restate this, how would similarly sized houses be priced in different areas?\n",
      "- Are predicted health differences between the two groups due to differences in their diets, or due to some other factor?\n",
      "\n",
      "## How does it work?\n",
      "\n",
      "Partial dependence plots are calculated after a model has been fit. How do we then disentangle the effects of several features?\n",
      "\n",
      "We start by selecting a single row. We will use the fitted model to predict our outcome of that row. But we repeatedly **alter the value** for **one variable** to make a series of predictions.\n",
      "\n",
      "For example, in the football example used above, we could predict the outcome if the team had the ball 40% of the time, but also 45, 50, 55, 60, ...\n",
      "\n",
      "We build the plot by:\n",
      "- representing on the horizontal axis the value change in the ball possession for example\n",
      "- and on the horizontal axis the change of the outcome\n",
      "\n",
      "We don't use only a single row, but many rows to do that. Therefore, we can represent a confidence interval and an average value, just like on this graph:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/perm3.jpg)\n",
      "\n",
      "The blue shaded area indicates the level of confidence.\n",
      "\n",
      "## Example\n",
      "\n",
      "Back to our FIFA Man of the Game example :\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "data = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')\n",
      "\n",
      "y = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"/\"No\" to binary\n",
      "feature_names = [i for i in data.columns if data[i].dtype in [np.int64]]\n",
      "X = data[feature_names]\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
      "tree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(X_train, y_train)\n",
      "```\n",
      "\n",
      "Then, we can plot the Partial Dependence Plot using [PDPbox](https://pdpbox.readthedocs.io/en/latest/). The goal of this library is to visualize the impact of certain features towards model prediction for any supervised learning algorithm using partial dependence plots. The PDP for the number of goals scored is the following :\n",
      "\n",
      "```python\n",
      "from matplotlib import pyplot as plt\n",
      "from pdpbox import pdp, get_dataset, info_plots\n",
      "\n",
      "# Create the data that we will plot\n",
      "pdp_goals = pdp.pdp_isolate(model=tree_model, dataset=X_test, model_features=feature_names, feature='Goal Scored')\n",
      "\n",
      "# plot it\n",
      "pdp.pdp_plot(pdp_goals, 'Goal Scored')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/perm4.jpg)\n",
      "\n",
      "From this particular graph, we see that scoring a goal substantially increases your chances of winning \"Man of The Match.\" But extra goals beyond that appear to have little impact on predictions.\n",
      "\n",
      "We can pick a more complex model and another feature to illustrate the changes :\n",
      "\n",
      "```python\n",
      "# Build Random Forest model\n",
      "rf_model = RandomForestClassifier(random_state=0).fit(X_train, y_train)\n",
      "\n",
      "pdp_dist = pdp.pdp_isolate(model=rf_model, dataset=X_test, model_features=feature_names, feature=feature_to_plot)\n",
      "\n",
      "pdp.pdp_plot(pdp_dist, feature_to_plot)\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/perm5.jpg)\n",
      "\n",
      "## 2D Partial Dependence Plots\n",
      "\n",
      "We can also plot interactions between features on a 2D graph.\n",
      "\n",
      "```python\n",
      "# Similar to previous PDP plot except we use pdp_interact instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plot\n",
      "\n",
      "features_to_plot = ['Goal Scored', 'Distance Covered (Kms)']\n",
      "\n",
      "inter1  =  pdp.pdp_interact(model=tree_model, dataset=X_test, model_features=feature_names, features=features_to_plot)\n",
      "\n",
      "pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/perm6.jpg)\n",
      "\n",
      "In this example, each feature can only take a limited number of values. What happens if we have continuous variables? The level frontiers bring value on the interaction between the 2 variables.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/perm7.jpg)\n",
      "\n",
      "# IV. SHAP Values\n",
      "\n",
      "We have seen so far techniques to extract general insights from a machine learning model. What if you want to break down how the model works for an individual prediction?\n",
      "\n",
      "SHAP Values (an acronym from SHapley Additive exPlanations) break down a prediction to show the impact of each feature.\n",
      "\n",
      "This could be used for :\n",
      "- banking automatic decision making \n",
      "- healthcare risk factor assessment for a single person\n",
      "\n",
      "In summary, we use SHAP values to explain individual predictions.\n",
      "\n",
      "## How does it work?\n",
      "\n",
      "SHAP values interpret the impact of having a certain value for a given feature in comparison to the prediction we'd make if that feature took some baseline value.\n",
      "\n",
      "In our football example, we could wonder how much was a prediction driven by the fact that the team scored 3 goals, instead of some baseline number of goals?\n",
      "\n",
      "We can decompose a prediction with the following equation:\n",
      "\n",
      "`sum(SHAP values for all features) = pred_for_team - pred_for_baseline_values`\n",
      "\n",
      "The SHAP Value can be represented visually as follows :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/shap.jpg)\n",
      "\n",
      "The output value is 0.70. This is the prediction for the selected team. The base value is 0.4979. Feature values causing increased predictions are in pink, and their visual size shows the magnitude of the feature's effect. Feature values decreasing the prediction are in blue. The biggest impact comes from Goal Scored being 2. Though the ball possession value has a meaningful effect decreasing the prediction.\n",
      "\n",
      "If you subtract the length of the blue bars from the length of the pink bars, it equals the distance from the base value to the output.\n",
      "\n",
      "## Example\n",
      "\n",
      "We will use the [SHAP library](https://github.com/slundberg/shap). As previously, we import the Football game example. We will look at SHAP values for a single row of the dataset (we arbitrarily chose row 5).\n",
      "\n",
      "```python\n",
      "import shap  # package used to calculate Shap values\n",
      "\n",
      "row_to_show = 5\n",
      "data_for_prediction = X_test.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\n",
      "data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n",
      "\n",
      "# Create object that can calculate shap values\n",
      "explainer = shap.TreeExplainer(my_model)\n",
      "\n",
      "# Calculate Shap values\n",
      "shap_values = explainer.shap_values(data_for_prediction)\n",
      "```\n",
      "\n",
      "The `shap_values` is a list with two arrays. It's cumbersome to review raw arrays, but the shap package has a nice way to visualize the results.\n",
      "\n",
      "```python\n",
      "shap.initjs()\n",
      "shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/shap_2.jpg)\n",
      "\n",
      "The output prediction is 0.7, which means that the team is 70% likely to have a player win the award.\n",
      "\n",
      "If we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset:\n",
      "\n",
      "```python\n",
      "# visualize the training set predictions\n",
      "shap.force_plot(explainer.expected_value, shap_values, X)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/shap_7.jpg)\n",
      "\n",
      "So far, we have used `shap.TreeExplainer(my_model)`. The package has other explainers for every type of model :\n",
      "- `shap.DeepExplainer` works with Deep Learning models.\n",
      "- `shap.KernelExplainer` works with all models, though it is slower than other Explainers and it offers an approximation rather than exact Shap values.\n",
      "\n",
      "## Advanced uses of SHAP Values\n",
      "\n",
      "### Summary plots\n",
      "\n",
      "Permutation importance creates simple numeric measures to see which features mattered to a model. But it doesn't tell you how each features matter. If a feature has medium permutation importance, that could mean it has :\n",
      "- a large effect for a few predictions, but no effect in general, or\n",
      "- a medium effect for all predictions.\n",
      "\n",
      "SHAP summary plots give us a birds-eye view of feature importance and what is driving it. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/shap_3.jpg)\n",
      "\n",
      "Each dot has 3 characteristics :\n",
      "- Vertical location shows what feature it is depicting\n",
      "- The color shows whether the feature was high or low for that row of the dataset\n",
      "- Horizontal location shows whether the effect of that value caused a higher or lower prediction\n",
      "\n",
      "In this specific example, the model ignored `Red` and `Yellow & Red` features. High values of goal scored caused higher predictions, and low values caused low predictions.\n",
      "\n",
      "Summary plots can be built the following way :\n",
      "\n",
      "```python\n",
      "# Create an object that can calculate shap values\n",
      "explainer = shap.TreeExplainer(my_model)\n",
      "\n",
      "# Calculate shap_values for all of X_test rather than a single row, to have more data for plot.\n",
      "shap_values = explainer.shap_values(X_test)\n",
      "\n",
      "# Make plot. Index of [1] is explained in text below.\n",
      "shap.summary_plot(shap_values[1], X_test)\n",
      "```\n",
      "\n",
      "Computing SHAP values can be slow on large datasets.\n",
      "\n",
      "### SHAP Dependence Contribution plots\n",
      "\n",
      "Partial Dependence Plots to show how a single feature impacts predictions. But they don't show the distribution of the effects for example. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/shap_4.jpg)\n",
      "\n",
      "Each dot represents a row of data. The horizontal location is the actual value from the dataset, and the vertical location shows what having that value did to the prediction. The fact this slopes upward says that the more you possess the ball, the higher the model's prediction is for winning the Man of the Match award.\n",
      "\n",
      "The spread suggests that other features must interact with Ball Possession %. For the same ball possession, we encounter SHAP values that range from -0.05 to 0.07.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/shap_5.jpg)\n",
      "\n",
      "We can also notice outliers that stand out spatially as being far away from the upward trend.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/shap_6.jpg)\n",
      "\n",
      "We can find an interpretation for this: In general, having the ball increases a team's chance of having their player win the award. But if they only score one goal, that trend reverses and the award judges may penalize them for having the ball so much if they score that little.\n",
      "\n",
      "To implement Dependence Contribution plots, we can use the following code :\n",
      "\n",
      "\n",
      "```python\n",
      "# Create an object that can calculate shap values\n",
      "explainer = shap.TreeExplainer(my_model)\n",
      "\n",
      "# calculate shap values. This is what we will plot.\n",
      "shap_values = explainer.shap_values(X)\n",
      "\n",
      "# make plot.\n",
      "shap.dependence_plot('Ball Possession %', shap_values[1], X, interaction_index=\"Goal Scored\")\n",
      "```\n",
      "\n",
      "### Summary plots\n",
      "\n",
      "We can also just take the mean absolute value of the SHAP values for each feature to get a standard bar plot (produces stacked bars for multi-class outputs):\n",
      "\n",
      "```python\n",
      "shap.summary_plot(shap_values, X, plot_type=\"bar\")\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/shap_8.jpg)\n",
      "\n",
      "### Interaction plots\n",
      "\n",
      "We can represent the interaction effect for two features and the effect on the SHAP Value they have. This can be done by plotting a dependence plot between the interaction values. Let's take another random example in which we consider the interaction between the age and the white blood cells, and the effect this has on the SHAP interaction values :\n",
      "\n",
      "```\n",
      "shap.dependence_plot(\n",
      "(\"Age\", \"White blood cells\"),\n",
      "shap_interaction_values, X.iloc[:2000,:],\n",
      "display_features=X_display.iloc[:2000,:]\n",
      ")\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/shap_9.jpg)\n",
      "\n",
      "> **Conclusion **: That's it for this introduction to Machine Learning Explainability! Don't hesitate to drop a comment if you have any question.\n",
      "\n",
      "---\n",
      "title: Disrupting Resilient criminal networks through data analysis\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Criminal Networks\"\n",
      "---\n",
      "\n",
      "In this article, I will re-implement and discuss the paper: [\"Disrupting resilient criminal networks through data analysis: the case of Sicilian Mafia\"](https://arxiv.org/pdf/2003.05303.pdf) by Lucia Cavallaro and Annamaria Ficara and Pasquale De Meo and Giacomo Fiumara and Salvatore Catanese and Ovidiu Bagdasar and Antonio Liotta.\n",
      "\n",
      "They published real world data, and I will also explore it on my side. The Github of the project with the code and data can be found [here](https://github.com/lcucav/networkdistruption). \n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# Background\n",
      "\n",
      "This study focuses on the structure of the Sicilian Mafia as a criminal network. The whole network is made of sub-groups referred as families.\n",
      "\n",
      "The authors have built two real-world anonymized datasets, based on raw data derived from juridical acts, relating to a Mafia gang that operated in Sicily (Italy) during the first decade of 2000s: \n",
      "- a dataset of phonecalls \n",
      "- a dataset of physical meetings\n",
      "\n",
      "The authors apply techniques of Social Network Analysis (SNA) to identify the best technique to disrupt the network (which member to arrest, in what order...).\n",
      "\n",
      "Strategies to disrupt networks rely on:\n",
      "- human capital, a strategy in which we investigate the resources possessed by actors of a network\n",
      "- social capital, a strategy in which we arrest actors with the largest social link in the network (e.g brokers that are bridges between two markets)\n",
      "\n",
      "In social capital, the removal of the actors with the largest betweenness centrality tends to be the best technique. After disruption, the network might still be exist. We talk about network resilience. This is what this paper focuses on.\n",
      "\n",
      "# Data\n",
      "\n",
      "For this part, I won't cover too much what the authors did and I will explore this dataset using my approach/visualization. I will rely on networkx (quite common), but also on Nx_altair, a library (which I think isn't maintained anymore) that mixes Altair and Networkx.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "# Visualization\n",
      "import matplotlib.pyplot as plt\n",
      "import altair as alt\n",
      "\n",
      "# Networks\n",
      "import networkx as nx\n",
      "import nx_altair as nxa\n",
      "from networkx.algorithms.community import greedy_modularity_communities\n",
      "```\n",
      "\n",
      "We will first focus on the physical meeting dataset and build the corresponding graph:\n",
      "\n",
      "```bash\n",
      "file = open('Datasets/Montagna_meetings_edgelist.csv', 'r')\n",
      "\n",
      "G = nx.Graph()\n",
      "for row in file:\n",
      "    r = row.split()\n",
      "    n1, n2, w = int(r[0]), int(r[1]), int(r[2])\n",
      "    G.add_node(n1)\n",
      "    G.add_node(n2)\n",
      "    G.nodes[n1]['id'] = n1\n",
      "    G.nodes[n2]['id'] = n2\n",
      "    G.add_edge(n1, n2)\n",
      "    G.edges[(n1,n2)]['weight']=w\n",
      "    G.edges[(n1,n2)]['method']='meetings'\n",
      "```\n",
      "\n",
      "We attributed a weight to each edge based on how frequently two nodes met.\n",
      "\n",
      "Now, let's make a first plot of the network and the meetings between the nodes.\n",
      "\n",
      "<script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega@5\"></script>\n",
      "<script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega-lite@4.0.2\"></script>\n",
      "<script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega-embed@6\"></script>\n",
      "\n",
      "```python\n",
      "pos = nx.spring_layout(G)\n",
      "\n",
      "chart = nxa.draw_networkx(\n",
      "    G=G,\n",
      "    pos=pos,\n",
      "    width='weight:N',\n",
      "    node_tooltip=['id:N']\n",
      ").properties(title=\"Sicilian Mafia physical meetings\", height=500, width=500\n",
      ").interactive()\n",
      "\n",
      "chart\n",
      "```\n",
      "\n",
      "<div id=\"vis\"></div>\n",
      "<script>\n",
      "  (function(vegaEmbed) {\n",
      "    var spec = {\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"domain\": false, \"grid\": false, \"labels\": false, \"ticks\": false}}, \"layer\": [{\"data\": {\"name\": \"data-9e6588b82645676b6f4de9075628f490\"}, \"mark\": {\"type\": \"line\", \"color\": \"black\", \"opacity\": 1}, \"encoding\": {\"detail\": {\"type\": \"quantitative\", \"field\": \"edge\"}, \"size\": {\"type\": \"nominal\", \"field\": \"weight\", \"legend\": null}, \"x\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"\"}, \"field\": \"x\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"\"}, \"field\": \"y\"}}, \"selection\": {\"selector009\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}}, {\"data\": {\"name\": \"data-321a68357ea3610409b01fd605cd8fd1\"}, \"mark\": {\"type\": \"point\", \"fill\": \"red\", \"opacity\": 1, \"size\": 300}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"x\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"y\"}}}], \"height\": 500, \"title\": \"Sicilian Mafia physical meetings\", \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-9e6588b82645676b6f4de9075628f490\": [{\"x\": 0.7742874486799499, \"source\": 0, \"target\": 1, \"weight\": 1, \"y\": 0.5118736526538851, \"pair\": [0, 1], \"edge\": 0}, {\"x\": 0.8275919628216338, \"source\": 0, \"target\": 1, \"weight\": 1, \"y\": 0.5652124335182597, \"pair\": [0, 1], \"edge\": 0}, {\"x\": 0.7742874486799499, \"source\": 0, \"target\": 2, \"weight\": 1, \"y\": 0.5118736526538851, \"pair\": [0, 2], \"edge\": 1}, {\"x\": 0.831945869665898, \"source\": 0, \"target\": 2, \"weight\": 1, \"y\": 0.5319223294473572, \"pair\": [0, 2], \"edge\": 1}, {\"x\": 0.8275919628216338, \"source\": 1, \"target\": 2, \"weight\": 1, \"y\": 0.5652124335182597, \"pair\": [1, 2], \"edge\": 2}, {\"x\": 0.831945869665898, \"source\": 1, \"target\": 2, \"weight\": 1, \"y\": 0.5319223294473572, \"pair\": [1, 2], \"edge\": 2}, {\"x\": -0.2713646677481388, \"source\": 3, \"target\": 4, \"weight\": 1, \"y\": 0.16813658099006523, \"pair\": [3, 4], \"edge\": 3}, {\"x\": -0.2902234045391699, \"source\": 3, \"target\": 4, \"weight\": 1, \"y\": 0.21010482716733897, \"pair\": [3, 4], \"edge\": 3}, {\"x\": -0.2713646677481388, \"source\": 3, \"target\": 5, \"weight\": 1, \"y\": 0.16813658099006523, \"pair\": [3, 5], \"edge\": 4}, {\"x\": -0.25016631544945583, \"source\": 3, \"target\": 5, \"weight\": 1, \"y\": 0.14104479809456952, \"pair\": [3, 5], \"edge\": 4}, {\"x\": -0.2713646677481388, \"source\": 3, \"target\": 6, \"weight\": 1, \"y\": 0.16813658099006523, \"pair\": [3, 6], \"edge\": 5}, {\"x\": -0.23749646566634144, \"source\": 3, \"target\": 6, \"weight\": 1, \"y\": 0.14468595974833082, \"pair\": [3, 6], \"edge\": 5}, {\"x\": -0.2713646677481388, \"source\": 3, \"target\": 7, \"weight\": 1, \"y\": 0.16813658099006523, \"pair\": [3, 7], \"edge\": 6}, {\"x\": -0.24841023810360466, \"source\": 3, \"target\": 7, \"weight\": 1, \"y\": 0.22314649054010852, \"pair\": [3, 7], \"edge\": 6}, {\"x\": -0.2713646677481388, \"source\": 3, \"target\": 8, \"weight\": 1, \"y\": 0.16813658099006523, \"pair\": [3, 8], \"edge\": 7}, {\"x\": -0.27607783527041946, \"source\": 3, \"target\": 8, \"weight\": 1, \"y\": 0.2262309599729506, \"pair\": [3, 8], \"edge\": 7}, {\"x\": -0.2713646677481388, \"source\": 3, \"target\": 9, \"weight\": 1, \"y\": 0.16813658099006523, \"pair\": [3, 9], \"edge\": 8}, {\"x\": -0.26271436173988244, \"source\": 3, \"target\": 9, \"weight\": 1, \"y\": 0.21084895828993597, \"pair\": [3, 9], \"edge\": 8}, {\"x\": -0.2713646677481388, \"source\": 3, \"target\": 11, \"weight\": 1, \"y\": 0.16813658099006523, \"pair\": [3, 11], \"edge\": 9}, {\"x\": -0.25462573199231797, \"source\": 3, \"target\": 11, \"weight\": 1, \"y\": 0.06931654778861604, \"pair\": [3, 11], \"edge\": 9}, {\"x\": -0.2713646677481388, \"source\": 3, \"target\": 12, \"weight\": 1, \"y\": 0.16813658099006523, \"pair\": [3, 12], \"edge\": 10}, {\"x\": -0.27470517992732346, \"source\": 3, \"target\": 12, \"weight\": 1, \"y\": 0.06938321781282342, \"pair\": [3, 12], \"edge\": 10}, {\"x\": -0.2902234045391699, \"source\": 4, \"target\": 5, \"weight\": 1, \"y\": 0.21010482716733897, \"pair\": [4, 5], \"edge\": 11}, {\"x\": -0.25016631544945583, \"source\": 4, \"target\": 5, \"weight\": 1, \"y\": 0.14104479809456952, \"pair\": [4, 5], \"edge\": 11}, {\"x\": -0.2902234045391699, \"source\": 4, \"target\": 6, \"weight\": 1, \"y\": 0.21010482716733897, \"pair\": [4, 6], \"edge\": 12}, {\"x\": -0.23749646566634144, \"source\": 4, \"target\": 6, \"weight\": 1, \"y\": 0.14468595974833082, \"pair\": [4, 6], \"edge\": 12}, {\"x\": -0.2902234045391699, \"source\": 4, \"target\": 7, \"weight\": 1, \"y\": 0.21010482716733897, \"pair\": [4, 7], \"edge\": 13}, {\"x\": -0.24841023810360466, \"source\": 4, \"target\": 7, \"weight\": 1, \"y\": 0.22314649054010852, \"pair\": [4, 7], \"edge\": 13}, {\"x\": -0.2902234045391699, \"source\": 4, \"target\": 8, \"weight\": 1, \"y\": 0.21010482716733897, \"pair\": [4, 8], \"edge\": 14}, {\"x\": -0.27607783527041946, \"source\": 4, \"target\": 8, \"weight\": 1, \"y\": 0.2262309599729506, \"pair\": [4, 8], \"edge\": 14}, {\"x\": -0.2902234045391699, \"source\": 4, \"target\": 9, \"weight\": 1, \"y\": 0.21010482716733897, \"pair\": [4, 9], \"edge\": 15}, {\"x\": -0.26271436173988244, \"source\": 4, \"target\": 9, \"weight\": 1, \"y\": 0.21084895828993597, \"pair\": [4, 9], \"edge\": 15}, {\"x\": -0.25016631544945583, \"source\": 5, \"target\": 6, \"weight\": 1, \"y\": 0.14104479809456952, \"pair\": [5, 6], \"edge\": 16}, {\"x\": -0.23749646566634144, \"source\": 5, \"target\": 6, \"weight\": 1, \"y\": 0.14468595974833082, \"pair\": [5, 6], \"edge\": 16}, {\"x\": -0.25016631544945583, \"source\": 5, \"target\": 7, \"weight\": 1, \"y\": 0.14104479809456952, \"pair\": [5, 7], \"edge\": 17}, {\"x\": -0.24841023810360466, \"source\": 5, \"target\": 7, \"weight\": 1, \"y\": 0.22314649054010852, \"pair\": [5, 7], \"edge\": 17}, {\"x\": -0.25016631544945583, \"source\": 5, \"target\": 8, \"weight\": 1, \"y\": 0.14104479809456952, \"pair\": [5, 8], \"edge\": 18}, {\"x\": -0.27607783527041946, \"source\": 5, \"target\": 8, \"weight\": 1, \"y\": 0.2262309599729506, \"pair\": [5, 8], \"edge\": 18}, {\"x\": -0.25016631544945583, \"source\": 5, \"target\": 9, \"weight\": 1, \"y\": 0.14104479809456952, \"pair\": [5, 9], \"edge\": 19}, {\"x\": -0.26271436173988244, \"source\": 5, \"target\": 9, \"weight\": 1, \"y\": 0.21084895828993597, \"pair\": [5, 9], \"edge\": 19}, {\"x\": -0.25016631544945583, \"source\": 5, \"target\": 11, \"weight\": 1, \"y\": 0.14104479809456952, \"pair\": [5, 11], \"edge\": 20}, {\"x\": -0.25462573199231797, \"source\": 5, \"target\": 11, \"weight\": 1, \"y\": 0.06931654778861604, \"pair\": [5, 11], \"edge\": 20}, {\"x\": -0.25016631544945583, \"source\": 5, \"target\": 12, \"weight\": 1, \"y\": 0.14104479809456952, \"pair\": [5, 12], \"edge\": 21}, {\"x\": -0.27470517992732346, \"source\": 5, \"target\": 12, \"weight\": 1, \"y\": 0.06938321781282342, \"pair\": [5, 12], \"edge\": 21}, {\"x\": -0.25016631544945583, \"source\": 5, \"target\": 28, \"weight\": 1, \"y\": 0.14104479809456952, \"pair\": [5, 28], \"edge\": 22}, {\"x\": -0.22745410678189415, \"source\": 5, \"target\": 28, \"weight\": 1, \"y\": 0.09591285457938409, \"pair\": [5, 28], \"edge\": 22}, {\"x\": -0.25016631544945583, \"source\": 5, \"target\": 25, \"weight\": 1, \"y\": 0.14104479809456952, \"pair\": [5, 25], \"edge\": 23}, {\"x\": -0.18008173051450233, \"source\": 5, \"target\": 25, \"weight\": 1, \"y\": 0.005015555628553944, \"pair\": [5, 25], \"edge\": 23}, {\"x\": -0.23749646566634144, \"source\": 6, \"target\": 7, \"weight\": 1, \"y\": 0.14468595974833082, \"pair\": [6, 7], \"edge\": 24}, {\"x\": -0.24841023810360466, \"source\": 6, \"target\": 7, \"weight\": 1, \"y\": 0.22314649054010852, \"pair\": [6, 7], \"edge\": 24}, {\"x\": -0.23749646566634144, \"source\": 6, \"target\": 8, \"weight\": 1, \"y\": 0.14468595974833082, \"pair\": [6, 8], \"edge\": 25}, {\"x\": -0.27607783527041946, \"source\": 6, \"target\": 8, \"weight\": 1, \"y\": 0.2262309599729506, \"pair\": [6, 8], \"edge\": 25}, {\"x\": -0.23749646566634144, \"source\": 6, \"target\": 9, \"weight\": 1, \"y\": 0.14468595974833082, \"pair\": [6, 9], \"edge\": 26}, {\"x\": -0.26271436173988244, \"source\": 6, \"target\": 9, \"weight\": 1, \"y\": 0.21084895828993597, \"pair\": [6, 9], \"edge\": 26}, {\"x\": -0.23749646566634144, \"source\": 6, \"target\": 11, \"weight\": 1, \"y\": 0.14468595974833082, \"pair\": [6, 11], \"edge\": 27}, {\"x\": -0.25462573199231797, \"source\": 6, \"target\": 11, \"weight\": 1, \"y\": 0.06931654778861604, \"pair\": [6, 11], \"edge\": 27}, {\"x\": -0.23749646566634144, \"source\": 6, \"target\": 12, \"weight\": 1, \"y\": 0.14468595974833082, \"pair\": [6, 12], \"edge\": 28}, {\"x\": -0.27470517992732346, \"source\": 6, \"target\": 12, \"weight\": 1, \"y\": 0.06938321781282342, \"pair\": [6, 12], \"edge\": 28}, {\"x\": -0.23749646566634144, \"source\": 6, \"target\": 28, \"weight\": 1, \"y\": 0.14468595974833082, \"pair\": [6, 28], \"edge\": 29}, {\"x\": -0.22745410678189415, \"source\": 6, \"target\": 28, \"weight\": 1, \"y\": 0.09591285457938409, \"pair\": [6, 28], \"edge\": 29}, {\"x\": -0.23749646566634144, \"source\": 6, \"target\": 25, \"weight\": 1, \"y\": 0.14468595974833082, \"pair\": [6, 25], \"edge\": 30}, {\"x\": -0.18008173051450233, \"source\": 6, \"target\": 25, \"weight\": 1, \"y\": 0.005015555628553944, \"pair\": [6, 25], \"edge\": 30}, {\"x\": -0.24841023810360466, \"source\": 7, \"target\": 8, \"weight\": 1, \"y\": 0.22314649054010852, \"pair\": [7, 8], \"edge\": 31}, {\"x\": -0.27607783527041946, \"source\": 7, \"target\": 8, \"weight\": 1, \"y\": 0.2262309599729506, \"pair\": [7, 8], \"edge\": 31}, {\"x\": -0.24841023810360466, \"source\": 7, \"target\": 9, \"weight\": 1, \"y\": 0.22314649054010852, \"pair\": [7, 9], \"edge\": 32}, {\"x\": -0.26271436173988244, \"source\": 7, \"target\": 9, \"weight\": 1, \"y\": 0.21084895828993597, \"pair\": [7, 9], \"edge\": 32}, {\"x\": -0.27607783527041946, \"source\": 8, \"target\": 9, \"weight\": 1, \"y\": 0.2262309599729506, \"pair\": [8, 9], \"edge\": 33}, {\"x\": -0.26271436173988244, \"source\": 8, \"target\": 9, \"weight\": 1, \"y\": 0.21084895828993597, \"pair\": [8, 9], \"edge\": 33}, {\"x\": -0.3434189120122944, \"source\": 10, \"target\": 11, \"weight\": 1, \"y\": 0.0708219335108565, \"pair\": [10, 11], \"edge\": 34}, {\"x\": -0.25462573199231797, \"source\": 10, \"target\": 11, \"weight\": 1, \"y\": 0.06931654778861604, \"pair\": [10, 11], \"edge\": 34}, {\"x\": -0.3434189120122944, \"source\": 10, \"target\": 12, \"weight\": 1, \"y\": 0.0708219335108565, \"pair\": [10, 12], \"edge\": 35}, {\"x\": -0.27470517992732346, \"source\": 10, \"target\": 12, \"weight\": 1, \"y\": 0.06938321781282342, \"pair\": [10, 12], \"edge\": 35}, {\"x\": -0.3434189120122944, \"source\": 10, \"target\": 13, \"weight\": 1, \"y\": 0.0708219335108565, \"pair\": [10, 13], \"edge\": 36}, {\"x\": -0.313332756284518, \"source\": 10, \"target\": 13, \"weight\": 1, \"y\": 0.08915317997595999, \"pair\": [10, 13], \"edge\": 36}, {\"x\": -0.3434189120122944, \"source\": 10, \"target\": 14, \"weight\": 1, \"y\": 0.0708219335108565, \"pair\": [10, 14], \"edge\": 37}, {\"x\": -0.30426009642291757, \"source\": 10, \"target\": 14, \"weight\": 1, \"y\": 0.10291503297841009, \"pair\": [10, 14], \"edge\": 37}, {\"x\": -0.3434189120122944, \"source\": 10, \"target\": 15, \"weight\": 1, \"y\": 0.0708219335108565, \"pair\": [10, 15], \"edge\": 38}, {\"x\": -0.31397478751285507, \"source\": 10, \"target\": 15, \"weight\": 1, \"y\": 0.07150056265632794, \"pair\": [10, 15], \"edge\": 38}, {\"x\": -0.25462573199231797, \"source\": 11, \"target\": 12, \"weight\": 1, \"y\": 0.06931654778861604, \"pair\": [11, 12], \"edge\": 39}, {\"x\": -0.27470517992732346, \"source\": 11, \"target\": 12, \"weight\": 1, \"y\": 0.06938321781282342, \"pair\": [11, 12], \"edge\": 39}, {\"x\": -0.25462573199231797, \"source\": 11, \"target\": 13, \"weight\": 2, \"y\": 0.06931654778861604, \"pair\": [11, 13], \"edge\": 40}, {\"x\": -0.313332756284518, \"source\": 11, \"target\": 13, \"weight\": 2, \"y\": 0.08915317997595999, \"pair\": [11, 13], \"edge\": 40}, {\"x\": -0.25462573199231797, \"source\": 11, \"target\": 14, \"weight\": 2, \"y\": 0.06931654778861604, \"pair\": [11, 14], \"edge\": 41}, {\"x\": -0.30426009642291757, \"source\": 11, \"target\": 14, \"weight\": 2, \"y\": 0.10291503297841009, \"pair\": [11, 14], \"edge\": 41}, {\"x\": -0.25462573199231797, \"source\": 11, \"target\": 15, \"weight\": 2, \"y\": 0.06931654778861604, \"pair\": [11, 15], \"edge\": 42}, {\"x\": -0.31397478751285507, \"source\": 11, \"target\": 15, \"weight\": 2, \"y\": 0.07150056265632794, \"pair\": [11, 15], \"edge\": 42}, {\"x\": -0.25462573199231797, \"source\": 11, \"target\": 18, \"weight\": 1, \"y\": 0.06931654778861604, \"pair\": [11, 18], \"edge\": 43}, {\"x\": -0.1211203561774911, \"source\": 11, \"target\": 18, \"weight\": 1, \"y\": -0.0747314451859159, \"pair\": [11, 18], \"edge\": 43}, {\"x\": -0.25462573199231797, \"source\": 11, \"target\": 25, \"weight\": 1, \"y\": 0.06931654778861604, \"pair\": [11, 25], \"edge\": 44}, {\"x\": -0.18008173051450233, \"source\": 11, \"target\": 25, \"weight\": 1, \"y\": 0.005015555628553944, \"pair\": [11, 25], \"edge\": 44}, {\"x\": -0.25462573199231797, \"source\": 11, \"target\": 28, \"weight\": 1, \"y\": 0.06931654778861604, \"pair\": [11, 28], \"edge\": 45}, {\"x\": -0.22745410678189415, \"source\": 11, \"target\": 28, \"weight\": 1, \"y\": 0.09591285457938409, \"pair\": [11, 28], \"edge\": 45}, {\"x\": -0.25462573199231797, \"source\": 11, \"target\": 85, \"weight\": 1, \"y\": 0.06931654778861604, \"pair\": [11, 85], \"edge\": 46}, {\"x\": -0.33966858175826775, \"source\": 11, \"target\": 85, \"weight\": 1, \"y\": 0.10297130105798322, \"pair\": [11, 85], \"edge\": 46}, {\"x\": -0.27470517992732346, \"source\": 12, \"target\": 13, \"weight\": 2, \"y\": 0.06938321781282342, \"pair\": [12, 13], \"edge\": 47}, {\"x\": -0.313332756284518, \"source\": 12, \"target\": 13, \"weight\": 2, \"y\": 0.08915317997595999, \"pair\": [12, 13], \"edge\": 47}, {\"x\": -0.27470517992732346, \"source\": 12, \"target\": 14, \"weight\": 2, \"y\": 0.06938321781282342, \"pair\": [12, 14], \"edge\": 48}, {\"x\": -0.30426009642291757, \"source\": 12, \"target\": 14, \"weight\": 2, \"y\": 0.10291503297841009, \"pair\": [12, 14], \"edge\": 48}, {\"x\": -0.27470517992732346, \"source\": 12, \"target\": 15, \"weight\": 2, \"y\": 0.06938321781282342, \"pair\": [12, 15], \"edge\": 49}, {\"x\": -0.31397478751285507, \"source\": 12, \"target\": 15, \"weight\": 2, \"y\": 0.07150056265632794, \"pair\": [12, 15], \"edge\": 49}, {\"x\": -0.27470517992732346, \"source\": 12, \"target\": 18, \"weight\": 1, \"y\": 0.06938321781282342, \"pair\": [12, 18], \"edge\": 50}, {\"x\": -0.1211203561774911, \"source\": 12, \"target\": 18, \"weight\": 1, \"y\": -0.0747314451859159, \"pair\": [12, 18], \"edge\": 50}, {\"x\": -0.27470517992732346, \"source\": 12, \"target\": 21, \"weight\": 1, \"y\": 0.06938321781282342, \"pair\": [12, 21], \"edge\": 51}, {\"x\": -0.2143547239351559, \"source\": 12, \"target\": 21, \"weight\": 1, \"y\": 0.003791796648513245, \"pair\": [12, 21], \"edge\": 51}, {\"x\": -0.27470517992732346, \"source\": 12, \"target\": 25, \"weight\": 2, \"y\": 0.06938321781282342, \"pair\": [12, 25], \"edge\": 52}, {\"x\": -0.18008173051450233, \"source\": 12, \"target\": 25, \"weight\": 2, \"y\": 0.005015555628553944, \"pair\": [12, 25], \"edge\": 52}, {\"x\": -0.27470517992732346, \"source\": 12, \"target\": 28, \"weight\": 1, \"y\": 0.06938321781282342, \"pair\": [12, 28], \"edge\": 53}, {\"x\": -0.22745410678189415, \"source\": 12, \"target\": 28, \"weight\": 1, \"y\": 0.09591285457938409, \"pair\": [12, 28], \"edge\": 53}, {\"x\": -0.27470517992732346, \"source\": 12, \"target\": 84, \"weight\": 1, \"y\": 0.06938321781282342, \"pair\": [12, 84], \"edge\": 54}, {\"x\": -0.3748038713496283, \"source\": 12, \"target\": 84, \"weight\": 1, \"y\": 0.135461209452502, \"pair\": [12, 84], \"edge\": 54}, {\"x\": -0.27470517992732346, \"source\": 12, \"target\": 85, \"weight\": 2, \"y\": 0.06938321781282342, \"pair\": [12, 85], \"edge\": 55}, {\"x\": -0.33966858175826775, \"source\": 12, \"target\": 85, \"weight\": 2, \"y\": 0.10297130105798322, \"pair\": [12, 85], \"edge\": 55}, {\"x\": -0.27470517992732346, \"source\": 12, \"target\": 86, \"weight\": 1, \"y\": 0.06938321781282342, \"pair\": [12, 86], \"edge\": 56}, {\"x\": -0.3893719061692324, \"source\": 12, \"target\": 86, \"weight\": 1, \"y\": 0.11779598919048392, \"pair\": [12, 86], \"edge\": 56}, {\"x\": -0.27470517992732346, \"source\": 12, \"target\": 87, \"weight\": 1, \"y\": 0.06938321781282342, \"pair\": [12, 87], \"edge\": 57}, {\"x\": -0.3904170530999228, \"source\": 12, \"target\": 87, \"weight\": 1, \"y\": 0.09492560497673808, \"pair\": [12, 87], \"edge\": 57}, {\"x\": -0.313332756284518, \"source\": 13, \"target\": 14, \"weight\": 2, \"y\": 0.08915317997595999, \"pair\": [13, 14], \"edge\": 58}, {\"x\": -0.30426009642291757, \"source\": 13, \"target\": 14, \"weight\": 2, \"y\": 0.10291503297841009, \"pair\": [13, 14], \"edge\": 58}, {\"x\": -0.313332756284518, \"source\": 13, \"target\": 15, \"weight\": 2, \"y\": 0.08915317997595999, \"pair\": [13, 15], \"edge\": 59}, {\"x\": -0.31397478751285507, \"source\": 13, \"target\": 15, \"weight\": 2, \"y\": 0.07150056265632794, \"pair\": [13, 15], \"edge\": 59}, {\"x\": -0.313332756284518, \"source\": 13, \"target\": 85, \"weight\": 1, \"y\": 0.08915317997595999, \"pair\": [13, 85], \"edge\": 60}, {\"x\": -0.33966858175826775, \"source\": 13, \"target\": 85, \"weight\": 1, \"y\": 0.10297130105798322, \"pair\": [13, 85], \"edge\": 60}, {\"x\": -0.30426009642291757, \"source\": 14, \"target\": 15, \"weight\": 2, \"y\": 0.10291503297841009, \"pair\": [14, 15], \"edge\": 61}, {\"x\": -0.31397478751285507, \"source\": 14, \"target\": 15, \"weight\": 2, \"y\": 0.07150056265632794, \"pair\": [14, 15], \"edge\": 61}, {\"x\": -0.30426009642291757, \"source\": 14, \"target\": 85, \"weight\": 1, \"y\": 0.10291503297841009, \"pair\": [14, 85], \"edge\": 62}, {\"x\": -0.33966858175826775, \"source\": 14, \"target\": 85, \"weight\": 1, \"y\": 0.10297130105798322, \"pair\": [14, 85], \"edge\": 62}, {\"x\": -0.31397478751285507, \"source\": 15, \"target\": 85, \"weight\": 1, \"y\": 0.07150056265632794, \"pair\": [15, 85], \"edge\": 63}, {\"x\": -0.33966858175826775, \"source\": 15, \"target\": 85, \"weight\": 1, \"y\": 0.10297130105798322, \"pair\": [15, 85], \"edge\": 63}, {\"x\": -0.5809357508719802, \"source\": 16, \"target\": 17, \"weight\": 1, \"y\": 0.861922036965538, \"pair\": [16, 17], \"edge\": 64}, {\"x\": -0.5413522544552076, \"source\": 16, \"target\": 17, \"weight\": 1, \"y\": 0.79657033546486, \"pair\": [16, 17], \"edge\": 64}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 19, \"weight\": 2, \"y\": -0.0747314451859159, \"pair\": [18, 19], \"edge\": 65}, {\"x\": -0.2414026991695374, \"source\": 18, \"target\": 19, \"weight\": 2, \"y\": -0.1098668430474719, \"pair\": [18, 19], \"edge\": 65}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 20, \"weight\": 1, \"y\": -0.0747314451859159, \"pair\": [18, 20], \"edge\": 66}, {\"x\": -0.22345641289191176, \"source\": 18, \"target\": 20, \"weight\": 1, \"y\": -0.1337582341921364, \"pair\": [18, 20], \"edge\": 66}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 21, \"weight\": 1, \"y\": -0.0747314451859159, \"pair\": [18, 21], \"edge\": 67}, {\"x\": -0.2143547239351559, \"source\": 18, \"target\": 21, \"weight\": 1, \"y\": 0.003791796648513245, \"pair\": [18, 21], \"edge\": 67}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 22, \"weight\": 2, \"y\": -0.0747314451859159, \"pair\": [18, 22], \"edge\": 68}, {\"x\": -0.0481799605476668, \"source\": 18, \"target\": 22, \"weight\": 2, \"y\": -0.16819239953321818, \"pair\": [18, 22], \"edge\": 68}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 31, \"weight\": 1, \"y\": -0.0747314451859159, \"pair\": [18, 31], \"edge\": 69}, {\"x\": -0.09694618546715744, \"source\": 18, \"target\": 31, \"weight\": 1, \"y\": -0.1674817286605503, \"pair\": [18, 31], \"edge\": 69}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 34, \"weight\": 2, \"y\": -0.0747314451859159, \"pair\": [18, 34], \"edge\": 70}, {\"x\": -0.18520660198138159, \"source\": 18, \"target\": 34, \"weight\": 2, \"y\": -0.06276800852037102, \"pair\": [18, 34], \"edge\": 70}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 35, \"weight\": 1, \"y\": -0.0747314451859159, \"pair\": [18, 35], \"edge\": 71}, {\"x\": -0.2044802321985295, \"source\": 18, \"target\": 35, \"weight\": 1, \"y\": -0.07765320517369118, \"pair\": [18, 35], \"edge\": 71}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 23, \"weight\": 1, \"y\": -0.0747314451859159, \"pair\": [18, 23], \"edge\": 72}, {\"x\": -0.15823874374769129, \"source\": 18, \"target\": 23, \"weight\": 1, \"y\": -0.10858330989437703, \"pair\": [18, 23], \"edge\": 72}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 25, \"weight\": 2, \"y\": -0.0747314451859159, \"pair\": [18, 25], \"edge\": 73}, {\"x\": -0.18008173051450233, \"source\": 18, \"target\": 25, \"weight\": 2, \"y\": 0.005015555628553944, \"pair\": [18, 25], \"edge\": 73}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 58, \"weight\": 1, \"y\": -0.0747314451859159, \"pair\": [18, 58], \"edge\": 74}, {\"x\": -0.18492941415722705, \"source\": 18, \"target\": 58, \"weight\": 1, \"y\": -0.17625837164480054, \"pair\": [18, 58], \"edge\": 74}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 59, \"weight\": 1, \"y\": -0.0747314451859159, \"pair\": [18, 59], \"edge\": 75}, {\"x\": -0.1640947579319588, \"source\": 18, \"target\": 59, \"weight\": 1, \"y\": -0.188092295651261, \"pair\": [18, 59], \"edge\": 75}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 60, \"weight\": 1, \"y\": -0.0747314451859159, \"pair\": [18, 60], \"edge\": 76}, {\"x\": -0.11891552607506636, \"source\": 18, \"target\": 60, \"weight\": 1, \"y\": -0.18305219276498932, \"pair\": [18, 60], \"edge\": 76}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 63, \"weight\": 1, \"y\": -0.0747314451859159, \"pair\": [18, 63], \"edge\": 77}, {\"x\": -0.056039092246434385, \"source\": 18, \"target\": 63, \"weight\": 1, \"y\": -0.14026048849165834, \"pair\": [18, 63], \"edge\": 77}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 64, \"weight\": 2, \"y\": -0.0747314451859159, \"pair\": [18, 64], \"edge\": 78}, {\"x\": -0.030641572558633177, \"source\": 18, \"target\": 64, \"weight\": 2, \"y\": -0.07765524168918447, \"pair\": [18, 64], \"edge\": 78}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 29, \"weight\": 2, \"y\": -0.0747314451859159, \"pair\": [18, 29], \"edge\": 79}, {\"x\": 0.003499004701826287, \"source\": 18, \"target\": 29, \"weight\": 2, \"y\": -0.0710581432720969, \"pair\": [18, 29], \"edge\": 79}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 65, \"weight\": 1, \"y\": -0.0747314451859159, \"pair\": [18, 65], \"edge\": 80}, {\"x\": -0.20405387216101264, \"source\": 18, \"target\": 65, \"weight\": 1, \"y\": -0.1562100301487121, \"pair\": [18, 65], \"edge\": 80}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 43, \"weight\": 1, \"y\": -0.0747314451859159, \"pair\": [18, 43], \"edge\": 81}, {\"x\": -0.0032126685304665905, \"source\": 18, \"target\": 43, \"weight\": 1, \"y\": -0.01971618392164536, \"pair\": [18, 43], \"edge\": 81}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 27, \"weight\": 1, \"y\": -0.0747314451859159, \"pair\": [18, 27], \"edge\": 82}, {\"x\": 0.015155338771232518, \"source\": 18, \"target\": 27, \"weight\": 1, \"y\": -0.0237949812938176, \"pair\": [18, 27], \"edge\": 82}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 47, \"weight\": 1, \"y\": -0.0747314451859159, \"pair\": [18, 47], \"edge\": 83}, {\"x\": 0.07117801885355013, \"source\": 18, \"target\": 47, \"weight\": 1, \"y\": 0.018805982704749424, \"pair\": [18, 47], \"edge\": 83}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 71, \"weight\": 1, \"y\": -0.0747314451859159, \"pair\": [18, 71], \"edge\": 84}, {\"x\": -0.0669225814703642, \"source\": 18, \"target\": 71, \"weight\": 1, \"y\": -0.08646780054287956, \"pair\": [18, 71], \"edge\": 84}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 76, \"weight\": 1, \"y\": -0.0747314451859159, \"pair\": [18, 76], \"edge\": 85}, {\"x\": -0.02174063302018561, \"source\": 18, \"target\": 76, \"weight\": 1, \"y\": -0.17943032558075114, \"pair\": [18, 76], \"edge\": 85}, {\"x\": -0.1211203561774911, \"source\": 18, \"target\": 100, \"weight\": 1, \"y\": -0.0747314451859159, \"pair\": [18, 100], \"edge\": 86}, {\"x\": -0.16839743229162535, \"source\": 18, \"target\": 100, \"weight\": 1, \"y\": -0.049218488592912786, \"pair\": [18, 100], \"edge\": 86}, {\"x\": -0.2414026991695374, \"source\": 19, \"target\": 20, \"weight\": 1, \"y\": -0.1098668430474719, \"pair\": [19, 20], \"edge\": 87}, {\"x\": -0.22345641289191176, \"source\": 19, \"target\": 20, \"weight\": 1, \"y\": -0.1337582341921364, \"pair\": [19, 20], \"edge\": 87}, {\"x\": -0.2414026991695374, \"source\": 19, \"target\": 32, \"weight\": 1, \"y\": -0.1098668430474719, \"pair\": [19, 32], \"edge\": 88}, {\"x\": -0.35638156860897063, \"source\": 19, \"target\": 32, \"weight\": 1, \"y\": -0.15275275651976816, \"pair\": [19, 32], \"edge\": 88}, {\"x\": -0.2414026991695374, \"source\": 19, \"target\": 33, \"weight\": 2, \"y\": -0.1098668430474719, \"pair\": [19, 33], \"edge\": 89}, {\"x\": -0.3179738240402524, \"source\": 19, \"target\": 33, \"weight\": 2, \"y\": -0.15371013649137227, \"pair\": [19, 33], \"edge\": 89}, {\"x\": -0.2414026991695374, \"source\": 19, \"target\": 34, \"weight\": 1, \"y\": -0.1098668430474719, \"pair\": [19, 34], \"edge\": 90}, {\"x\": -0.18520660198138159, \"source\": 19, \"target\": 34, \"weight\": 1, \"y\": -0.06276800852037102, \"pair\": [19, 34], \"edge\": 90}, {\"x\": -0.2414026991695374, \"source\": 19, \"target\": 35, \"weight\": 1, \"y\": -0.1098668430474719, \"pair\": [19, 35], \"edge\": 91}, {\"x\": -0.2044802321985295, \"source\": 19, \"target\": 35, \"weight\": 1, \"y\": -0.07765320517369118, \"pair\": [19, 35], \"edge\": 91}, {\"x\": -0.2414026991695374, \"source\": 19, \"target\": 23, \"weight\": 1, \"y\": -0.1098668430474719, \"pair\": [19, 23], \"edge\": 92}, {\"x\": -0.15823874374769129, \"source\": 19, \"target\": 23, \"weight\": 1, \"y\": -0.10858330989437703, \"pair\": [19, 23], \"edge\": 92}, {\"x\": -0.2414026991695374, \"source\": 19, \"target\": 25, \"weight\": 1, \"y\": -0.1098668430474719, \"pair\": [19, 25], \"edge\": 93}, {\"x\": -0.18008173051450233, \"source\": 19, \"target\": 25, \"weight\": 1, \"y\": 0.005015555628553944, \"pair\": [19, 25], \"edge\": 93}, {\"x\": -0.2414026991695374, \"source\": 19, \"target\": 88, \"weight\": 1, \"y\": -0.1098668430474719, \"pair\": [19, 88], \"edge\": 94}, {\"x\": -0.3392136300652572, \"source\": 19, \"target\": 88, \"weight\": 1, \"y\": -0.18622094080637322, \"pair\": [19, 88], \"edge\": 94}, {\"x\": -0.0481799605476668, \"source\": 22, \"target\": 23, \"weight\": 2, \"y\": -0.16819239953321818, \"pair\": [22, 23], \"edge\": 95}, {\"x\": -0.15823874374769129, \"source\": 22, \"target\": 23, \"weight\": 2, \"y\": -0.10858330989437703, \"pair\": [22, 23], \"edge\": 95}, {\"x\": -0.0481799605476668, \"source\": 22, \"target\": 29, \"weight\": 2, \"y\": -0.16819239953321818, \"pair\": [22, 29], \"edge\": 96}, {\"x\": 0.003499004701826287, \"source\": 22, \"target\": 29, \"weight\": 2, \"y\": -0.0710581432720969, \"pair\": [22, 29], \"edge\": 96}, {\"x\": -0.0481799605476668, \"source\": 22, \"target\": 30, \"weight\": 1, \"y\": -0.16819239953321818, \"pair\": [22, 30], \"edge\": 97}, {\"x\": 0.04224440922701651, \"source\": 22, \"target\": 30, \"weight\": 1, \"y\": -0.16153126726475023, \"pair\": [22, 30], \"edge\": 97}, {\"x\": -0.0481799605476668, \"source\": 22, \"target\": 31, \"weight\": 2, \"y\": -0.16819239953321818, \"pair\": [22, 31], \"edge\": 98}, {\"x\": -0.09694618546715744, \"source\": 22, \"target\": 31, \"weight\": 2, \"y\": -0.1674817286605503, \"pair\": [22, 31], \"edge\": 98}, {\"x\": -0.0481799605476668, \"source\": 22, \"target\": 60, \"weight\": 1, \"y\": -0.16819239953321818, \"pair\": [22, 60], \"edge\": 99}, {\"x\": -0.11891552607506636, \"source\": 22, \"target\": 60, \"weight\": 1, \"y\": -0.18305219276498932, \"pair\": [22, 60], \"edge\": 99}, {\"x\": -0.0481799605476668, \"source\": 22, \"target\": 63, \"weight\": 2, \"y\": -0.16819239953321818, \"pair\": [22, 63], \"edge\": 100}, {\"x\": -0.056039092246434385, \"source\": 22, \"target\": 63, \"weight\": 2, \"y\": -0.14026048849165834, \"pair\": [22, 63], \"edge\": 100}, {\"x\": -0.0481799605476668, \"source\": 22, \"target\": 72, \"weight\": 1, \"y\": -0.16819239953321818, \"pair\": [22, 72], \"edge\": 101}, {\"x\": -0.054223668595532834, \"source\": 22, \"target\": 72, \"weight\": 1, \"y\": -0.21785962815549784, \"pair\": [22, 72], \"edge\": 101}, {\"x\": -0.0481799605476668, \"source\": 22, \"target\": 76, \"weight\": 2, \"y\": -0.16819239953321818, \"pair\": [22, 76], \"edge\": 102}, {\"x\": -0.02174063302018561, \"source\": 22, \"target\": 76, \"weight\": 2, \"y\": -0.17943032558075114, \"pair\": [22, 76], \"edge\": 102}, {\"x\": -0.0481799605476668, \"source\": 22, \"target\": 77, \"weight\": 2, \"y\": -0.16819239953321818, \"pair\": [22, 77], \"edge\": 103}, {\"x\": 0.005126176174485453, \"source\": 22, \"target\": 77, \"weight\": 2, \"y\": -0.2228619899787118, \"pair\": [22, 77], \"edge\": 103}, {\"x\": -0.0481799605476668, \"source\": 22, \"target\": 75, \"weight\": 2, \"y\": -0.16819239953321818, \"pair\": [22, 75], \"edge\": 104}, {\"x\": 0.007469284917281426, \"source\": 22, \"target\": 75, \"weight\": 2, \"y\": -0.17092042012771036, \"pair\": [22, 75], \"edge\": 104}, {\"x\": -0.0481799605476668, \"source\": 22, \"target\": 78, \"weight\": 1, \"y\": -0.16819239953321818, \"pair\": [22, 78], \"edge\": 105}, {\"x\": 0.02163463999097515, \"source\": 22, \"target\": 78, \"weight\": 1, \"y\": -0.25544958595372547, \"pair\": [22, 78], \"edge\": 105}, {\"x\": -0.0481799605476668, \"source\": 22, \"target\": 98, \"weight\": 1, \"y\": -0.16819239953321818, \"pair\": [22, 98], \"edge\": 106}, {\"x\": -0.011975080761383637, \"source\": 22, \"target\": 98, \"weight\": 1, \"y\": -0.2612569049501908, \"pair\": [22, 98], \"edge\": 106}, {\"x\": -0.0481799605476668, \"source\": 22, \"target\": 99, \"weight\": 1, \"y\": -0.16819239953321818, \"pair\": [22, 99], \"edge\": 107}, {\"x\": -0.013738548332712283, \"source\": 22, \"target\": 99, \"weight\": 1, \"y\": -0.3030810853390118, \"pair\": [22, 99], \"edge\": 107}, {\"x\": -0.15823874374769129, \"source\": 23, \"target\": 24, \"weight\": 1, \"y\": -0.10858330989437703, \"pair\": [23, 24], \"edge\": 108}, {\"x\": -0.23532760622592114, \"source\": 23, \"target\": 24, \"weight\": 1, \"y\": -0.21459847371502708, \"pair\": [23, 24], \"edge\": 108}, {\"x\": -0.15823874374769129, \"source\": 23, \"target\": 34, \"weight\": 1, \"y\": -0.10858330989437703, \"pair\": [23, 34], \"edge\": 109}, {\"x\": -0.18520660198138159, \"source\": 23, \"target\": 34, \"weight\": 1, \"y\": -0.06276800852037102, \"pair\": [23, 34], \"edge\": 109}, {\"x\": -0.15823874374769129, \"source\": 23, \"target\": 35, \"weight\": 1, \"y\": -0.10858330989437703, \"pair\": [23, 35], \"edge\": 110}, {\"x\": -0.2044802321985295, \"source\": 23, \"target\": 35, \"weight\": 1, \"y\": -0.07765320517369118, \"pair\": [23, 35], \"edge\": 110}, {\"x\": -0.15823874374769129, \"source\": 23, \"target\": 25, \"weight\": 2, \"y\": -0.10858330989437703, \"pair\": [23, 25], \"edge\": 111}, {\"x\": -0.18008173051450233, \"source\": 23, \"target\": 25, \"weight\": 2, \"y\": 0.005015555628553944, \"pair\": [23, 25], \"edge\": 111}, {\"x\": -0.15823874374769129, \"source\": 23, \"target\": 100, \"weight\": 1, \"y\": -0.10858330989437703, \"pair\": [23, 100], \"edge\": 112}, {\"x\": -0.16839743229162535, \"source\": 23, \"target\": 100, \"weight\": 1, \"y\": -0.049218488592912786, \"pair\": [23, 100], \"edge\": 112}, {\"x\": -0.18008173051450233, \"source\": 25, \"target\": 26, \"weight\": 1, \"y\": 0.005015555628553944, \"pair\": [25, 26], \"edge\": 113}, {\"x\": -0.08374405782833477, \"source\": 25, \"target\": 26, \"weight\": 1, \"y\": 0.032132162872611586, \"pair\": [25, 26], \"edge\": 113}, {\"x\": -0.18008173051450233, \"source\": 25, \"target\": 27, \"weight\": 1, \"y\": 0.005015555628553944, \"pair\": [25, 27], \"edge\": 114}, {\"x\": 0.015155338771232518, \"source\": 25, \"target\": 27, \"weight\": 1, \"y\": -0.0237949812938176, \"pair\": [25, 27], \"edge\": 114}, {\"x\": -0.18008173051450233, \"source\": 25, \"target\": 28, \"weight\": 1, \"y\": 0.005015555628553944, \"pair\": [25, 28], \"edge\": 115}, {\"x\": -0.22745410678189415, \"source\": 25, \"target\": 28, \"weight\": 1, \"y\": 0.09591285457938409, \"pair\": [25, 28], \"edge\": 115}, {\"x\": -0.18008173051450233, \"source\": 25, \"target\": 34, \"weight\": 2, \"y\": 0.005015555628553944, \"pair\": [25, 34], \"edge\": 116}, {\"x\": -0.18520660198138159, \"source\": 25, \"target\": 34, \"weight\": 2, \"y\": -0.06276800852037102, \"pair\": [25, 34], \"edge\": 116}, {\"x\": -0.18008173051450233, \"source\": 25, \"target\": 35, \"weight\": 1, \"y\": 0.005015555628553944, \"pair\": [25, 35], \"edge\": 117}, {\"x\": -0.2044802321985295, \"source\": 25, \"target\": 35, \"weight\": 1, \"y\": -0.07765320517369118, \"pair\": [25, 35], \"edge\": 117}, {\"x\": -0.18008173051450233, \"source\": 25, \"target\": 100, \"weight\": 1, \"y\": 0.005015555628553944, \"pair\": [25, 100], \"edge\": 118}, {\"x\": -0.16839743229162535, \"source\": 25, \"target\": 100, \"weight\": 1, \"y\": -0.049218488592912786, \"pair\": [25, 100], \"edge\": 118}, {\"x\": -0.08374405782833477, \"source\": 26, \"target\": 27, \"weight\": 1, \"y\": 0.032132162872611586, \"pair\": [26, 27], \"edge\": 119}, {\"x\": 0.015155338771232518, \"source\": 26, \"target\": 27, \"weight\": 1, \"y\": -0.0237949812938176, \"pair\": [26, 27], \"edge\": 119}, {\"x\": 0.015155338771232518, \"source\": 27, \"target\": 38, \"weight\": 1, \"y\": -0.0237949812938176, \"pair\": [27, 38], \"edge\": 120}, {\"x\": 0.11163817201568127, \"source\": 27, \"target\": 38, \"weight\": 1, \"y\": -0.041736627061624114, \"pair\": [27, 38], \"edge\": 120}, {\"x\": 0.015155338771232518, \"source\": 27, \"target\": 45, \"weight\": 3, \"y\": -0.0237949812938176, \"pair\": [27, 45], \"edge\": 121}, {\"x\": 0.10041775413522544, \"source\": 27, \"target\": 45, \"weight\": 3, \"y\": 0.002684926951578756, \"pair\": [27, 45], \"edge\": 121}, {\"x\": 0.015155338771232518, \"source\": 27, \"target\": 51, \"weight\": 1, \"y\": -0.0237949812938176, \"pair\": [27, 51], \"edge\": 122}, {\"x\": 0.08879501531574102, \"source\": 27, \"target\": 51, \"weight\": 1, \"y\": 0.02014889049051504, \"pair\": [27, 51], \"edge\": 122}, {\"x\": 0.015155338771232518, \"source\": 27, \"target\": 43, \"weight\": 1, \"y\": -0.0237949812938176, \"pair\": [27, 43], \"edge\": 123}, {\"x\": -0.0032126685304665905, \"source\": 27, \"target\": 43, \"weight\": 1, \"y\": -0.01971618392164536, \"pair\": [27, 43], \"edge\": 123}, {\"x\": 0.015155338771232518, \"source\": 27, \"target\": 47, \"weight\": 2, \"y\": -0.0237949812938176, \"pair\": [27, 47], \"edge\": 124}, {\"x\": 0.07117801885355013, \"source\": 27, \"target\": 47, \"weight\": 2, \"y\": 0.018805982704749424, \"pair\": [27, 47], \"edge\": 124}, {\"x\": 0.015155338771232518, \"source\": 27, \"target\": 29, \"weight\": 3, \"y\": -0.0237949812938176, \"pair\": [27, 29], \"edge\": 125}, {\"x\": 0.003499004701826287, \"source\": 27, \"target\": 29, \"weight\": 3, \"y\": -0.0710581432720969, \"pair\": [27, 29], \"edge\": 125}, {\"x\": 0.015155338771232518, \"source\": 27, \"target\": 64, \"weight\": 1, \"y\": -0.0237949812938176, \"pair\": [27, 64], \"edge\": 126}, {\"x\": -0.030641572558633177, \"source\": 27, \"target\": 64, \"weight\": 1, \"y\": -0.07765524168918447, \"pair\": [27, 64], \"edge\": 126}, {\"x\": 0.015155338771232518, \"source\": 27, \"target\": 68, \"weight\": 2, \"y\": -0.0237949812938176, \"pair\": [27, 68], \"edge\": 127}, {\"x\": 0.06273653542802236, \"source\": 27, \"target\": 68, \"weight\": 2, \"y\": -0.07677518063870974, \"pair\": [27, 68], \"edge\": 127}, {\"x\": 0.015155338771232518, \"source\": 27, \"target\": 44, \"weight\": 1, \"y\": -0.0237949812938176, \"pair\": [27, 44], \"edge\": 128}, {\"x\": 0.014177116932700448, \"source\": 27, \"target\": 44, \"weight\": 1, \"y\": 0.045118703412298324, \"pair\": [27, 44], \"edge\": 128}, {\"x\": 0.015155338771232518, \"source\": 27, \"target\": 61, \"weight\": 1, \"y\": -0.0237949812938176, \"pair\": [27, 61], \"edge\": 129}, {\"x\": -0.11966546773701857, \"source\": 27, \"target\": 61, \"weight\": 1, \"y\": -0.011954973820502089, \"pair\": [27, 61], \"edge\": 129}, {\"x\": 0.015155338771232518, \"source\": 27, \"target\": 89, \"weight\": 4, \"y\": -0.0237949812938176, \"pair\": [27, 89], \"edge\": 130}, {\"x\": 0.08419189704703227, \"source\": 27, \"target\": 89, \"weight\": 4, \"y\": -0.02097242429132012, \"pair\": [27, 89], \"edge\": 130}, {\"x\": 0.015155338771232518, \"source\": 27, \"target\": 70, \"weight\": 3, \"y\": -0.0237949812938176, \"pair\": [27, 70], \"edge\": 131}, {\"x\": 0.06874944094237588, \"source\": 27, \"target\": 70, \"weight\": 3, \"y\": -0.047239773073776005, \"pair\": [27, 70], \"edge\": 131}, {\"x\": 0.015155338771232518, \"source\": 27, \"target\": 97, \"weight\": 1, \"y\": -0.0237949812938176, \"pair\": [27, 97], \"edge\": 132}, {\"x\": 0.09474699958454538, \"source\": 27, \"target\": 97, \"weight\": 1, \"y\": -0.09247765010836596, \"pair\": [27, 97], \"edge\": 132}, {\"x\": 0.003499004701826287, \"source\": 29, \"target\": 30, \"weight\": 1, \"y\": -0.0710581432720969, \"pair\": [29, 30], \"edge\": 133}, {\"x\": 0.04224440922701651, \"source\": 29, \"target\": 30, \"weight\": 1, \"y\": -0.16153126726475023, \"pair\": [29, 30], \"edge\": 133}, {\"x\": 0.003499004701826287, \"source\": 29, \"target\": 64, \"weight\": 4, \"y\": -0.0710581432720969, \"pair\": [29, 64], \"edge\": 134}, {\"x\": -0.030641572558633177, \"source\": 29, \"target\": 64, \"weight\": 4, \"y\": -0.07765524168918447, \"pair\": [29, 64], \"edge\": 134}, {\"x\": 0.003499004701826287, \"source\": 29, \"target\": 54, \"weight\": 1, \"y\": -0.0710581432720969, \"pair\": [29, 54], \"edge\": 135}, {\"x\": 0.08276473970865998, \"source\": 29, \"target\": 54, \"weight\": 1, \"y\": -0.03758480137070527, \"pair\": [29, 54], \"edge\": 135}, {\"x\": 0.003499004701826287, \"source\": 29, \"target\": 43, \"weight\": 2, \"y\": -0.0710581432720969, \"pair\": [29, 43], \"edge\": 136}, {\"x\": -0.0032126685304665905, \"source\": 29, \"target\": 43, \"weight\": 2, \"y\": -0.01971618392164536, \"pair\": [29, 43], \"edge\": 136}, {\"x\": 0.003499004701826287, \"source\": 29, \"target\": 68, \"weight\": 1, \"y\": -0.0710581432720969, \"pair\": [29, 68], \"edge\": 137}, {\"x\": 0.06273653542802236, \"source\": 29, \"target\": 68, \"weight\": 1, \"y\": -0.07677518063870974, \"pair\": [29, 68], \"edge\": 137}, {\"x\": 0.003499004701826287, \"source\": 29, \"target\": 47, \"weight\": 2, \"y\": -0.0710581432720969, \"pair\": [29, 47], \"edge\": 138}, {\"x\": 0.07117801885355013, \"source\": 29, \"target\": 47, \"weight\": 2, \"y\": 0.018805982704749424, \"pair\": [29, 47], \"edge\": 138}, {\"x\": 0.003499004701826287, \"source\": 29, \"target\": 51, \"weight\": 2, \"y\": -0.0710581432720969, \"pair\": [29, 51], \"edge\": 139}, {\"x\": 0.08879501531574102, \"source\": 29, \"target\": 51, \"weight\": 2, \"y\": 0.02014889049051504, \"pair\": [29, 51], \"edge\": 139}, {\"x\": 0.003499004701826287, \"source\": 29, \"target\": 71, \"weight\": 1, \"y\": -0.0710581432720969, \"pair\": [29, 71], \"edge\": 140}, {\"x\": -0.0669225814703642, \"source\": 29, \"target\": 71, \"weight\": 1, \"y\": -0.08646780054287956, \"pair\": [29, 71], \"edge\": 140}, {\"x\": 0.003499004701826287, \"source\": 29, \"target\": 75, \"weight\": 1, \"y\": -0.0710581432720969, \"pair\": [29, 75], \"edge\": 141}, {\"x\": 0.007469284917281426, \"source\": 29, \"target\": 75, \"weight\": 1, \"y\": -0.17092042012771036, \"pair\": [29, 75], \"edge\": 141}, {\"x\": 0.003499004701826287, \"source\": 29, \"target\": 76, \"weight\": 1, \"y\": -0.0710581432720969, \"pair\": [29, 76], \"edge\": 142}, {\"x\": -0.02174063302018561, \"source\": 29, \"target\": 76, \"weight\": 1, \"y\": -0.17943032558075114, \"pair\": [29, 76], \"edge\": 142}, {\"x\": -0.35638156860897063, \"source\": 32, \"target\": 33, \"weight\": 1, \"y\": -0.15275275651976816, \"pair\": [32, 33], \"edge\": 143}, {\"x\": -0.3179738240402524, \"source\": 32, \"target\": 33, \"weight\": 1, \"y\": -0.15371013649137227, \"pair\": [32, 33], \"edge\": 143}, {\"x\": -0.3179738240402524, \"source\": 33, \"target\": 88, \"weight\": 1, \"y\": -0.15371013649137227, \"pair\": [33, 88], \"edge\": 144}, {\"x\": -0.3392136300652572, \"source\": 33, \"target\": 88, \"weight\": 1, \"y\": -0.18622094080637322, \"pair\": [33, 88], \"edge\": 144}, {\"x\": -0.18520660198138159, \"source\": 34, \"target\": 35, \"weight\": 1, \"y\": -0.06276800852037102, \"pair\": [34, 35], \"edge\": 145}, {\"x\": -0.2044802321985295, \"source\": 34, \"target\": 35, \"weight\": 1, \"y\": -0.07765320517369118, \"pair\": [34, 35], \"edge\": 145}, {\"x\": -0.18520660198138159, \"source\": 34, \"target\": 100, \"weight\": 1, \"y\": -0.06276800852037102, \"pair\": [34, 100], \"edge\": 146}, {\"x\": -0.16839743229162535, \"source\": 34, \"target\": 100, \"weight\": 1, \"y\": -0.049218488592912786, \"pair\": [34, 100], \"edge\": 146}, {\"x\": 0.1824583313460389, \"source\": 36, \"target\": 37, \"weight\": 1, \"y\": 0.009782518718294362, \"pair\": [36, 37], \"edge\": 147}, {\"x\": 0.34056236700902254, \"source\": 36, \"target\": 37, \"weight\": 1, \"y\": 0.009326450386316241, \"pair\": [36, 37], \"edge\": 147}, {\"x\": 0.1824583313460389, \"source\": 36, \"target\": 45, \"weight\": 2, \"y\": 0.009782518718294362, \"pair\": [36, 45], \"edge\": 148}, {\"x\": 0.10041775413522544, \"source\": 36, \"target\": 45, \"weight\": 2, \"y\": 0.002684926951578756, \"pair\": [36, 45], \"edge\": 148}, {\"x\": 0.1824583313460389, \"source\": 36, \"target\": 46, \"weight\": 1, \"y\": 0.009782518718294362, \"pair\": [36, 46], \"edge\": 149}, {\"x\": 0.16007765352359848, \"source\": 36, \"target\": 46, \"weight\": 1, \"y\": 0.04134962494994794, \"pair\": [36, 46], \"edge\": 149}, {\"x\": 0.1824583313460389, \"source\": 36, \"target\": 47, \"weight\": 2, \"y\": 0.009782518718294362, \"pair\": [36, 47], \"edge\": 150}, {\"x\": 0.07117801885355013, \"source\": 36, \"target\": 47, \"weight\": 2, \"y\": 0.018805982704749424, \"pair\": [36, 47], \"edge\": 150}, {\"x\": 0.1824583313460389, \"source\": 36, \"target\": 48, \"weight\": 2, \"y\": 0.009782518718294362, \"pair\": [36, 48], \"edge\": 151}, {\"x\": 0.11705337669328504, \"source\": 36, \"target\": 48, \"weight\": 2, \"y\": 0.03095482957258405, \"pair\": [36, 48], \"edge\": 151}, {\"x\": 0.1824583313460389, \"source\": 36, \"target\": 91, \"weight\": 1, \"y\": 0.009782518718294362, \"pair\": [36, 91], \"edge\": 152}, {\"x\": 0.29405420144270583, \"source\": 36, \"target\": 91, \"weight\": 1, \"y\": 0.03906983535776226, \"pair\": [36, 91], \"edge\": 152}, {\"x\": 0.1824583313460389, \"source\": 36, \"target\": 92, \"weight\": 1, \"y\": 0.009782518718294362, \"pair\": [36, 92], \"edge\": 153}, {\"x\": 0.2953894253991343, \"source\": 36, \"target\": 92, \"weight\": 1, \"y\": 0.005390318607781721, \"pair\": [36, 92], \"edge\": 153}, {\"x\": 0.1824583313460389, \"source\": 36, \"target\": 68, \"weight\": 1, \"y\": 0.009782518718294362, \"pair\": [36, 68], \"edge\": 154}, {\"x\": 0.06273653542802236, \"source\": 36, \"target\": 68, \"weight\": 1, \"y\": -0.07677518063870974, \"pair\": [36, 68], \"edge\": 154}, {\"x\": 0.34056236700902254, \"source\": 37, \"target\": 67, \"weight\": 1, \"y\": 0.009326450386316241, \"pair\": [37, 67], \"edge\": 155}, {\"x\": 0.44511627014077465, \"source\": 37, \"target\": 67, \"weight\": 1, \"y\": 0.012483678992233512, \"pair\": [37, 67], \"edge\": 155}, {\"x\": 0.11163817201568127, \"source\": 38, \"target\": 93, \"weight\": 2, \"y\": -0.041736627061624114, \"pair\": [38, 93], \"edge\": 156}, {\"x\": 0.1254586362084501, \"source\": 38, \"target\": 93, \"weight\": 2, \"y\": -0.011845745356646708, \"pair\": [38, 93], \"edge\": 156}, {\"x\": 0.11163817201568127, \"source\": 38, \"target\": 89, \"weight\": 4, \"y\": -0.041736627061624114, \"pair\": [38, 89], \"edge\": 157}, {\"x\": 0.08419189704703227, \"source\": 38, \"target\": 89, \"weight\": 4, \"y\": -0.02097242429132012, \"pair\": [38, 89], \"edge\": 157}, {\"x\": 0.13372815220552836, \"source\": 39, \"target\": 40, \"weight\": 1, \"y\": 0.1605162712739612, \"pair\": [39, 40], \"edge\": 158}, {\"x\": 0.19236976386643478, \"source\": 39, \"target\": 40, \"weight\": 1, \"y\": 0.2663563059221274, \"pair\": [39, 40], \"edge\": 158}, {\"x\": 0.13372815220552836, \"source\": 39, \"target\": 41, \"weight\": 1, \"y\": 0.1605162712739612, \"pair\": [39, 41], \"edge\": 159}, {\"x\": 0.16922811540721625, \"source\": 39, \"target\": 41, \"weight\": 1, \"y\": 0.2618595044608058, \"pair\": [39, 41], \"edge\": 159}, {\"x\": 0.13372815220552836, \"source\": 39, \"target\": 42, \"weight\": 1, \"y\": 0.1605162712739612, \"pair\": [39, 42], \"edge\": 160}, {\"x\": 0.15465578426260826, \"source\": 39, \"target\": 42, \"weight\": 1, \"y\": 0.2797908640906716, \"pair\": [39, 42], \"edge\": 160}, {\"x\": 0.13372815220552836, \"source\": 39, \"target\": 47, \"weight\": 1, \"y\": 0.1605162712739612, \"pair\": [39, 47], \"edge\": 161}, {\"x\": 0.07117801885355013, \"source\": 39, \"target\": 47, \"weight\": 1, \"y\": 0.018805982704749424, \"pair\": [39, 47], \"edge\": 161}, {\"x\": 0.13372815220552836, \"source\": 39, \"target\": 49, \"weight\": 1, \"y\": 0.1605162712739612, \"pair\": [39, 49], \"edge\": 162}, {\"x\": 0.1177549863041608, \"source\": 39, \"target\": 49, \"weight\": 1, \"y\": 0.10661320320752536, \"pair\": [39, 49], \"edge\": 162}, {\"x\": 0.13372815220552836, \"source\": 39, \"target\": 50, \"weight\": 1, \"y\": 0.1605162712739612, \"pair\": [39, 50], \"edge\": 163}, {\"x\": 0.09034633976307598, \"source\": 39, \"target\": 50, \"weight\": 1, \"y\": 0.05953513091318408, \"pair\": [39, 50], \"edge\": 163}, {\"x\": 0.13372815220552836, \"source\": 39, \"target\": 48, \"weight\": 1, \"y\": 0.1605162712739612, \"pair\": [39, 48], \"edge\": 164}, {\"x\": 0.11705337669328504, \"source\": 39, \"target\": 48, \"weight\": 1, \"y\": 0.03095482957258405, \"pair\": [39, 48], \"edge\": 164}, {\"x\": 0.19236976386643478, \"source\": 40, \"target\": 41, \"weight\": 1, \"y\": 0.2663563059221274, \"pair\": [40, 41], \"edge\": 165}, {\"x\": 0.16922811540721625, \"source\": 40, \"target\": 41, \"weight\": 1, \"y\": 0.2618595044608058, \"pair\": [40, 41], \"edge\": 165}, {\"x\": 0.19236976386643478, \"source\": 40, \"target\": 42, \"weight\": 1, \"y\": 0.2663563059221274, \"pair\": [40, 42], \"edge\": 166}, {\"x\": 0.15465578426260826, \"source\": 40, \"target\": 42, \"weight\": 1, \"y\": 0.2797908640906716, \"pair\": [40, 42], \"edge\": 166}, {\"x\": 0.16922811540721625, \"source\": 41, \"target\": 42, \"weight\": 1, \"y\": 0.2618595044608058, \"pair\": [41, 42], \"edge\": 167}, {\"x\": 0.15465578426260826, \"source\": 41, \"target\": 42, \"weight\": 1, \"y\": 0.2797908640906716, \"pair\": [41, 42], \"edge\": 167}, {\"x\": -0.0032126685304665905, \"source\": 43, \"target\": 44, \"weight\": 1, \"y\": -0.01971618392164536, \"pair\": [43, 44], \"edge\": 168}, {\"x\": 0.014177116932700448, \"source\": 43, \"target\": 44, \"weight\": 1, \"y\": 0.045118703412298324, \"pair\": [43, 44], \"edge\": 168}, {\"x\": -0.0032126685304665905, \"source\": 43, \"target\": 47, \"weight\": 3, \"y\": -0.01971618392164536, \"pair\": [43, 47], \"edge\": 169}, {\"x\": 0.07117801885355013, \"source\": 43, \"target\": 47, \"weight\": 3, \"y\": 0.018805982704749424, \"pair\": [43, 47], \"edge\": 169}, {\"x\": -0.0032126685304665905, \"source\": 43, \"target\": 64, \"weight\": 2, \"y\": -0.01971618392164536, \"pair\": [43, 64], \"edge\": 170}, {\"x\": -0.030641572558633177, \"source\": 43, \"target\": 64, \"weight\": 2, \"y\": -0.07765524168918447, \"pair\": [43, 64], \"edge\": 170}, {\"x\": -0.0032126685304665905, \"source\": 43, \"target\": 68, \"weight\": 1, \"y\": -0.01971618392164536, \"pair\": [43, 68], \"edge\": 171}, {\"x\": 0.06273653542802236, \"source\": 43, \"target\": 68, \"weight\": 1, \"y\": -0.07677518063870974, \"pair\": [43, 68], \"edge\": 171}, {\"x\": -0.0032126685304665905, \"source\": 43, \"target\": 51, \"weight\": 1, \"y\": -0.01971618392164536, \"pair\": [43, 51], \"edge\": 172}, {\"x\": 0.08879501531574102, \"source\": 43, \"target\": 51, \"weight\": 1, \"y\": 0.02014889049051504, \"pair\": [43, 51], \"edge\": 172}, {\"x\": -0.0032126685304665905, \"source\": 43, \"target\": 61, \"weight\": 1, \"y\": -0.01971618392164536, \"pair\": [43, 61], \"edge\": 173}, {\"x\": -0.11966546773701857, \"source\": 43, \"target\": 61, \"weight\": 1, \"y\": -0.011954973820502089, \"pair\": [43, 61], \"edge\": 173}, {\"x\": 0.014177116932700448, \"source\": 44, \"target\": 47, \"weight\": 1, \"y\": 0.045118703412298324, \"pair\": [44, 47], \"edge\": 174}, {\"x\": 0.07117801885355013, \"source\": 44, \"target\": 47, \"weight\": 1, \"y\": 0.018805982704749424, \"pair\": [44, 47], \"edge\": 174}, {\"x\": 0.10041775413522544, \"source\": 45, \"target\": 46, \"weight\": 1, \"y\": 0.002684926951578756, \"pair\": [45, 46], \"edge\": 175}, {\"x\": 0.16007765352359848, \"source\": 45, \"target\": 46, \"weight\": 1, \"y\": 0.04134962494994794, \"pair\": [45, 46], \"edge\": 175}, {\"x\": 0.10041775413522544, \"source\": 45, \"target\": 47, \"weight\": 7, \"y\": 0.002684926951578756, \"pair\": [45, 47], \"edge\": 176}, {\"x\": 0.07117801885355013, \"source\": 45, \"target\": 47, \"weight\": 7, \"y\": 0.018805982704749424, \"pair\": [45, 47], \"edge\": 176}, {\"x\": 0.10041775413522544, \"source\": 45, \"target\": 48, \"weight\": 1, \"y\": 0.002684926951578756, \"pair\": [45, 48], \"edge\": 177}, {\"x\": 0.11705337669328504, \"source\": 45, \"target\": 48, \"weight\": 1, \"y\": 0.03095482957258405, \"pair\": [45, 48], \"edge\": 177}, {\"x\": 0.10041775413522544, \"source\": 45, \"target\": 51, \"weight\": 3, \"y\": 0.002684926951578756, \"pair\": [45, 51], \"edge\": 178}, {\"x\": 0.08879501531574102, \"source\": 45, \"target\": 51, \"weight\": 3, \"y\": 0.02014889049051504, \"pair\": [45, 51], \"edge\": 178}, {\"x\": 0.10041775413522544, \"source\": 45, \"target\": 54, \"weight\": 1, \"y\": 0.002684926951578756, \"pair\": [45, 54], \"edge\": 179}, {\"x\": 0.08276473970865998, \"source\": 45, \"target\": 54, \"weight\": 1, \"y\": -0.03758480137070527, \"pair\": [45, 54], \"edge\": 179}, {\"x\": 0.10041775413522544, \"source\": 45, \"target\": 55, \"weight\": 1, \"y\": 0.002684926951578756, \"pair\": [45, 55], \"edge\": 180}, {\"x\": 0.1455269606594044, \"source\": 45, \"target\": 55, \"weight\": 1, \"y\": 0.07295561769313157, \"pair\": [45, 55], \"edge\": 180}, {\"x\": 0.10041775413522544, \"source\": 45, \"target\": 50, \"weight\": 1, \"y\": 0.002684926951578756, \"pair\": [45, 50], \"edge\": 181}, {\"x\": 0.09034633976307598, \"source\": 45, \"target\": 50, \"weight\": 1, \"y\": 0.05953513091318408, \"pair\": [45, 50], \"edge\": 181}, {\"x\": 0.10041775413522544, \"source\": 45, \"target\": 89, \"weight\": 2, \"y\": 0.002684926951578756, \"pair\": [45, 89], \"edge\": 182}, {\"x\": 0.08419189704703227, \"source\": 45, \"target\": 89, \"weight\": 2, \"y\": -0.02097242429132012, \"pair\": [45, 89], \"edge\": 182}, {\"x\": 0.10041775413522544, \"source\": 45, \"target\": 94, \"weight\": 1, \"y\": 0.002684926951578756, \"pair\": [45, 94], \"edge\": 183}, {\"x\": 0.17623771316630127, \"source\": 45, \"target\": 94, \"weight\": 1, \"y\": -0.04787930076631226, \"pair\": [45, 94], \"edge\": 183}, {\"x\": 0.10041775413522544, \"source\": 45, \"target\": 68, \"weight\": 1, \"y\": 0.002684926951578756, \"pair\": [45, 68], \"edge\": 184}, {\"x\": 0.06273653542802236, \"source\": 45, \"target\": 68, \"weight\": 1, \"y\": -0.07677518063870974, \"pair\": [45, 68], \"edge\": 184}, {\"x\": 0.16007765352359848, \"source\": 46, \"target\": 47, \"weight\": 1, \"y\": 0.04134962494994794, \"pair\": [46, 47], \"edge\": 185}, {\"x\": 0.07117801885355013, \"source\": 46, \"target\": 47, \"weight\": 1, \"y\": 0.018805982704749424, \"pair\": [46, 47], \"edge\": 185}, {\"x\": 0.16007765352359848, \"source\": 46, \"target\": 48, \"weight\": 1, \"y\": 0.04134962494994794, \"pair\": [46, 48], \"edge\": 186}, {\"x\": 0.11705337669328504, \"source\": 46, \"target\": 48, \"weight\": 1, \"y\": 0.03095482957258405, \"pair\": [46, 48], \"edge\": 186}, {\"x\": 0.07117801885355013, \"source\": 47, \"target\": 48, \"weight\": 8, \"y\": 0.018805982704749424, \"pair\": [47, 48], \"edge\": 187}, {\"x\": 0.11705337669328504, \"source\": 47, \"target\": 48, \"weight\": 8, \"y\": 0.03095482957258405, \"pair\": [47, 48], \"edge\": 187}, {\"x\": 0.07117801885355013, \"source\": 47, \"target\": 49, \"weight\": 1, \"y\": 0.018805982704749424, \"pair\": [47, 49], \"edge\": 188}, {\"x\": 0.1177549863041608, \"source\": 47, \"target\": 49, \"weight\": 1, \"y\": 0.10661320320752536, \"pair\": [47, 49], \"edge\": 188}, {\"x\": 0.07117801885355013, \"source\": 47, \"target\": 50, \"weight\": 10, \"y\": 0.018805982704749424, \"pair\": [47, 50], \"edge\": 189}, {\"x\": 0.09034633976307598, \"source\": 47, \"target\": 50, \"weight\": 10, \"y\": 0.05953513091318408, \"pair\": [47, 50], \"edge\": 189}, {\"x\": 0.07117801885355013, \"source\": 47, \"target\": 51, \"weight\": 5, \"y\": 0.018805982704749424, \"pair\": [47, 51], \"edge\": 190}, {\"x\": 0.08879501531574102, \"source\": 47, \"target\": 51, \"weight\": 5, \"y\": 0.02014889049051504, \"pair\": [47, 51], \"edge\": 190}, {\"x\": 0.07117801885355013, \"source\": 47, \"target\": 54, \"weight\": 1, \"y\": 0.018805982704749424, \"pair\": [47, 54], \"edge\": 191}, {\"x\": 0.08276473970865998, \"source\": 47, \"target\": 54, \"weight\": 1, \"y\": -0.03758480137070527, \"pair\": [47, 54], \"edge\": 191}, {\"x\": 0.07117801885355013, \"source\": 47, \"target\": 55, \"weight\": 1, \"y\": 0.018805982704749424, \"pair\": [47, 55], \"edge\": 192}, {\"x\": 0.1455269606594044, \"source\": 47, \"target\": 55, \"weight\": 1, \"y\": 0.07295561769313157, \"pair\": [47, 55], \"edge\": 192}, {\"x\": 0.07117801885355013, \"source\": 47, \"target\": 68, \"weight\": 1, \"y\": 0.018805982704749424, \"pair\": [47, 68], \"edge\": 193}, {\"x\": 0.06273653542802236, \"source\": 47, \"target\": 68, \"weight\": 1, \"y\": -0.07677518063870974, \"pair\": [47, 68], \"edge\": 193}, {\"x\": 0.07117801885355013, \"source\": 47, \"target\": 89, \"weight\": 4, \"y\": 0.018805982704749424, \"pair\": [47, 89], \"edge\": 194}, {\"x\": 0.08419189704703227, \"source\": 47, \"target\": 89, \"weight\": 4, \"y\": -0.02097242429132012, \"pair\": [47, 89], \"edge\": 194}, {\"x\": 0.07117801885355013, \"source\": 47, \"target\": 90, \"weight\": 2, \"y\": 0.018805982704749424, \"pair\": [47, 90], \"edge\": 195}, {\"x\": 0.0785023990127484, \"source\": 47, \"target\": 90, \"weight\": 2, \"y\": 0.09633915188874138, \"pair\": [47, 90], \"edge\": 195}, {\"x\": 0.07117801885355013, \"source\": 47, \"target\": 93, \"weight\": 1, \"y\": 0.018805982704749424, \"pair\": [47, 93], \"edge\": 196}, {\"x\": 0.1254586362084501, \"source\": 47, \"target\": 93, \"weight\": 1, \"y\": -0.011845745356646708, \"pair\": [47, 93], \"edge\": 196}, {\"x\": 0.11705337669328504, \"source\": 48, \"target\": 49, \"weight\": 1, \"y\": 0.03095482957258405, \"pair\": [48, 49], \"edge\": 197}, {\"x\": 0.1177549863041608, \"source\": 48, \"target\": 49, \"weight\": 1, \"y\": 0.10661320320752536, \"pair\": [48, 49], \"edge\": 197}, {\"x\": 0.11705337669328504, \"source\": 48, \"target\": 50, \"weight\": 1, \"y\": 0.03095482957258405, \"pair\": [48, 50], \"edge\": 198}, {\"x\": 0.09034633976307598, \"source\": 48, \"target\": 50, \"weight\": 1, \"y\": 0.05953513091318408, \"pair\": [48, 50], \"edge\": 198}, {\"x\": 0.11705337669328504, \"source\": 48, \"target\": 51, \"weight\": 3, \"y\": 0.03095482957258405, \"pair\": [48, 51], \"edge\": 199}, {\"x\": 0.08879501531574102, \"source\": 48, \"target\": 51, \"weight\": 3, \"y\": 0.02014889049051504, \"pair\": [48, 51], \"edge\": 199}, {\"x\": 0.11705337669328504, \"source\": 48, \"target\": 93, \"weight\": 3, \"y\": 0.03095482957258405, \"pair\": [48, 93], \"edge\": 200}, {\"x\": 0.1254586362084501, \"source\": 48, \"target\": 93, \"weight\": 3, \"y\": -0.011845745356646708, \"pair\": [48, 93], \"edge\": 200}, {\"x\": 0.11705337669328504, \"source\": 48, \"target\": 89, \"weight\": 1, \"y\": 0.03095482957258405, \"pair\": [48, 89], \"edge\": 201}, {\"x\": 0.08419189704703227, \"source\": 48, \"target\": 89, \"weight\": 1, \"y\": -0.02097242429132012, \"pair\": [48, 89], \"edge\": 201}, {\"x\": 0.11705337669328504, \"source\": 48, \"target\": 95, \"weight\": 1, \"y\": 0.03095482957258405, \"pair\": [48, 95], \"edge\": 202}, {\"x\": 0.17700967089288305, \"source\": 48, \"target\": 95, \"weight\": 1, \"y\": -0.01295652547890839, \"pair\": [48, 95], \"edge\": 202}, {\"x\": 0.11705337669328504, \"source\": 48, \"target\": 68, \"weight\": 1, \"y\": 0.03095482957258405, \"pair\": [48, 68], \"edge\": 203}, {\"x\": 0.06273653542802236, \"source\": 48, \"target\": 68, \"weight\": 1, \"y\": -0.07677518063870974, \"pair\": [48, 68], \"edge\": 203}, {\"x\": 0.1177549863041608, \"source\": 49, \"target\": 50, \"weight\": 1, \"y\": 0.10661320320752536, \"pair\": [49, 50], \"edge\": 204}, {\"x\": 0.09034633976307598, \"source\": 49, \"target\": 50, \"weight\": 1, \"y\": 0.05953513091318408, \"pair\": [49, 50], \"edge\": 204}, {\"x\": 0.09034633976307598, \"source\": 50, \"target\": 51, \"weight\": 1, \"y\": 0.05953513091318408, \"pair\": [50, 51], \"edge\": 205}, {\"x\": 0.08879501531574102, \"source\": 50, \"target\": 51, \"weight\": 1, \"y\": 0.02014889049051504, \"pair\": [50, 51], \"edge\": 205}, {\"x\": 0.09034633976307598, \"source\": 50, \"target\": 89, \"weight\": 1, \"y\": 0.05953513091318408, \"pair\": [50, 89], \"edge\": 206}, {\"x\": 0.08419189704703227, \"source\": 50, \"target\": 89, \"weight\": 1, \"y\": -0.02097242429132012, \"pair\": [50, 89], \"edge\": 206}, {\"x\": 0.09034633976307598, \"source\": 50, \"target\": 90, \"weight\": 2, \"y\": 0.05953513091318408, \"pair\": [50, 90], \"edge\": 207}, {\"x\": 0.0785023990127484, \"source\": 50, \"target\": 90, \"weight\": 2, \"y\": 0.09633915188874138, \"pair\": [50, 90], \"edge\": 207}, {\"x\": 0.08879501531574102, \"source\": 51, \"target\": 52, \"weight\": 1, \"y\": 0.02014889049051504, \"pair\": [51, 52], \"edge\": 208}, {\"x\": 0.2073729378023853, \"source\": 51, \"target\": 52, \"weight\": 1, \"y\": 0.13949084769516717, \"pair\": [51, 52], \"edge\": 208}, {\"x\": 0.08879501531574102, \"source\": 51, \"target\": 53, \"weight\": 1, \"y\": 0.02014889049051504, \"pair\": [51, 53], \"edge\": 209}, {\"x\": 0.18597917465257235, \"source\": 51, \"target\": 53, \"weight\": 1, \"y\": 0.10536674925772639, \"pair\": [51, 53], \"edge\": 209}, {\"x\": 0.08879501531574102, \"source\": 51, \"target\": 54, \"weight\": 1, \"y\": 0.02014889049051504, \"pair\": [51, 54], \"edge\": 210}, {\"x\": 0.08276473970865998, \"source\": 51, \"target\": 54, \"weight\": 1, \"y\": -0.03758480137070527, \"pair\": [51, 54], \"edge\": 210}, {\"x\": 0.08879501531574102, \"source\": 51, \"target\": 89, \"weight\": 4, \"y\": 0.02014889049051504, \"pair\": [51, 89], \"edge\": 211}, {\"x\": 0.08419189704703227, \"source\": 51, \"target\": 89, \"weight\": 4, \"y\": -0.02097242429132012, \"pair\": [51, 89], \"edge\": 211}, {\"x\": 0.2073729378023853, \"source\": 52, \"target\": 53, \"weight\": 1, \"y\": 0.13949084769516717, \"pair\": [52, 53], \"edge\": 212}, {\"x\": 0.18597917465257235, \"source\": 52, \"target\": 53, \"weight\": 1, \"y\": 0.10536674925772639, \"pair\": [52, 53], \"edge\": 212}, {\"x\": 0.2073729378023853, \"source\": 52, \"target\": 96, \"weight\": 1, \"y\": 0.13949084769516717, \"pair\": [52, 96], \"edge\": 213}, {\"x\": 0.2908951112779772, \"source\": 52, \"target\": 96, \"weight\": 1, \"y\": 0.220120363919095, \"pair\": [52, 96], \"edge\": 213}, {\"x\": 0.08276473970865998, \"source\": 54, \"target\": 68, \"weight\": 1, \"y\": -0.03758480137070527, \"pair\": [54, 68], \"edge\": 214}, {\"x\": 0.06273653542802236, \"source\": 54, \"target\": 68, \"weight\": 1, \"y\": -0.07677518063870974, \"pair\": [54, 68], \"edge\": 214}, {\"x\": 0.08276473970865998, \"source\": 54, \"target\": 70, \"weight\": 2, \"y\": -0.03758480137070527, \"pair\": [54, 70], \"edge\": 215}, {\"x\": 0.06874944094237588, \"source\": 54, \"target\": 70, \"weight\": 2, \"y\": -0.047239773073776005, \"pair\": [54, 70], \"edge\": 215}, {\"x\": 1.0, \"source\": 56, \"target\": 57, \"weight\": 1, \"y\": -0.31654287432724065, \"pair\": [56, 57], \"edge\": 216}, {\"x\": 0.9833935072773411, \"source\": 56, \"target\": 57, \"weight\": 1, \"y\": -0.3535942544516167, \"pair\": [56, 57], \"edge\": 216}, {\"x\": -0.18492941415722705, \"source\": 58, \"target\": 59, \"weight\": 1, \"y\": -0.17625837164480054, \"pair\": [58, 59], \"edge\": 217}, {\"x\": -0.1640947579319588, \"source\": 58, \"target\": 59, \"weight\": 1, \"y\": -0.188092295651261, \"pair\": [58, 59], \"edge\": 217}, {\"x\": -0.11966546773701857, \"source\": 61, \"target\": 62, \"weight\": 1, \"y\": -0.011954973820502089, \"pair\": [61, 62], \"edge\": 218}, {\"x\": -0.23599260788076, \"source\": 61, \"target\": 62, \"weight\": 1, \"y\": -0.006507229823581309, \"pair\": [61, 62], \"edge\": 218}, {\"x\": -0.11966546773701857, \"source\": 61, \"target\": 66, \"weight\": 1, \"y\": -0.011954973820502089, \"pair\": [61, 66], \"edge\": 219}, {\"x\": -0.23897966887493483, \"source\": 61, \"target\": 66, \"weight\": 1, \"y\": -0.0389667629338048, \"pair\": [61, 66], \"edge\": 219}, {\"x\": -0.056039092246434385, \"source\": 63, \"target\": 64, \"weight\": 3, \"y\": -0.14026048849165834, \"pair\": [63, 64], \"edge\": 220}, {\"x\": -0.030641572558633177, \"source\": 63, \"target\": 64, \"weight\": 3, \"y\": -0.07765524168918447, \"pair\": [63, 64], \"edge\": 220}, {\"x\": -0.056039092246434385, \"source\": 63, \"target\": 72, \"weight\": 2, \"y\": -0.14026048849165834, \"pair\": [63, 72], \"edge\": 221}, {\"x\": -0.054223668595532834, \"source\": 63, \"target\": 72, \"weight\": 2, \"y\": -0.21785962815549784, \"pair\": [63, 72], \"edge\": 221}, {\"x\": -0.030641572558633177, \"source\": 64, \"target\": 68, \"weight\": 1, \"y\": -0.07765524168918447, \"pair\": [64, 68], \"edge\": 222}, {\"x\": 0.06273653542802236, \"source\": 64, \"target\": 68, \"weight\": 1, \"y\": -0.07677518063870974, \"pair\": [64, 68], \"edge\": 222}, {\"x\": 0.06273653542802236, \"source\": 68, \"target\": 69, \"weight\": 1, \"y\": -0.07677518063870974, \"pair\": [68, 69], \"edge\": 223}, {\"x\": 0.15534502953258011, \"source\": 68, \"target\": 69, \"weight\": 1, \"y\": -0.1597877755529839, \"pair\": [68, 69], \"edge\": 223}, {\"x\": 0.06273653542802236, \"source\": 68, \"target\": 70, \"weight\": 4, \"y\": -0.07677518063870974, \"pair\": [68, 70], \"edge\": 224}, {\"x\": 0.06874944094237588, \"source\": 68, \"target\": 70, \"weight\": 4, \"y\": -0.047239773073776005, \"pair\": [68, 70], \"edge\": 224}, {\"x\": 0.06273653542802236, \"source\": 68, \"target\": 79, \"weight\": 1, \"y\": -0.07677518063870974, \"pair\": [68, 79], \"edge\": 225}, {\"x\": -0.037003951618323636, \"source\": 68, \"target\": 79, \"weight\": 1, \"y\": -0.28178414571159477, \"pair\": [68, 79], \"edge\": 225}, {\"x\": 0.06273653542802236, \"source\": 68, \"target\": 83, \"weight\": 1, \"y\": -0.07677518063870974, \"pair\": [68, 83], \"edge\": 226}, {\"x\": 0.025430608516896446, \"source\": 68, \"target\": 83, \"weight\": 1, \"y\": -0.20583124487500434, \"pair\": [68, 83], \"edge\": 226}, {\"x\": 0.06273653542802236, \"source\": 68, \"target\": 89, \"weight\": 7, \"y\": -0.07677518063870974, \"pair\": [68, 89], \"edge\": 227}, {\"x\": 0.08419189704703227, \"source\": 68, \"target\": 89, \"weight\": 7, \"y\": -0.02097242429132012, \"pair\": [68, 89], \"edge\": 227}, {\"x\": 0.06273653542802236, \"source\": 68, \"target\": 97, \"weight\": 1, \"y\": -0.07677518063870974, \"pair\": [68, 97], \"edge\": 228}, {\"x\": 0.09474699958454538, \"source\": 68, \"target\": 97, \"weight\": 1, \"y\": -0.09247765010836596, \"pair\": [68, 97], \"edge\": 228}, {\"x\": 0.06874944094237588, \"source\": 70, \"target\": 89, \"weight\": 3, \"y\": -0.047239773073776005, \"pair\": [70, 89], \"edge\": 229}, {\"x\": 0.08419189704703227, \"source\": 70, \"target\": 89, \"weight\": 3, \"y\": -0.02097242429132012, \"pair\": [70, 89], \"edge\": 229}, {\"x\": 0.06874944094237588, \"source\": 70, \"target\": 93, \"weight\": 2, \"y\": -0.047239773073776005, \"pair\": [70, 93], \"edge\": 230}, {\"x\": 0.1254586362084501, \"source\": 70, \"target\": 93, \"weight\": 2, \"y\": -0.011845745356646708, \"pair\": [70, 93], \"edge\": 230}, {\"x\": 0.06874944094237588, \"source\": 70, \"target\": 97, \"weight\": 1, \"y\": -0.047239773073776005, \"pair\": [70, 97], \"edge\": 231}, {\"x\": 0.09474699958454538, \"source\": 70, \"target\": 97, \"weight\": 1, \"y\": -0.09247765010836596, \"pair\": [70, 97], \"edge\": 231}, {\"x\": 0.9688799672612421, \"source\": 73, \"target\": 74, \"weight\": 1, \"y\": -0.0667152065748477, \"pair\": [73, 74], \"edge\": 232}, {\"x\": 0.9840418611844868, \"source\": 73, \"target\": 74, \"weight\": 1, \"y\": -0.10420484012538193, \"pair\": [73, 74], \"edge\": 232}, {\"x\": 0.007469284917281426, \"source\": 75, \"target\": 76, \"weight\": 1, \"y\": -0.17092042012771036, \"pair\": [75, 76], \"edge\": 233}, {\"x\": -0.02174063302018561, \"source\": 75, \"target\": 76, \"weight\": 1, \"y\": -0.17943032558075114, \"pair\": [75, 76], \"edge\": 233}, {\"x\": 0.007469284917281426, \"source\": 75, \"target\": 77, \"weight\": 1, \"y\": -0.17092042012771036, \"pair\": [75, 77], \"edge\": 234}, {\"x\": 0.005126176174485453, \"source\": 75, \"target\": 77, \"weight\": 1, \"y\": -0.2228619899787118, \"pair\": [75, 77], \"edge\": 234}, {\"x\": -0.02174063302018561, \"source\": 76, \"target\": 77, \"weight\": 2, \"y\": -0.17943032558075114, \"pair\": [76, 77], \"edge\": 235}, {\"x\": 0.005126176174485453, \"source\": 76, \"target\": 77, \"weight\": 2, \"y\": -0.2228619899787118, \"pair\": [76, 77], \"edge\": 235}, {\"x\": -0.02174063302018561, \"source\": 76, \"target\": 78, \"weight\": 1, \"y\": -0.17943032558075114, \"pair\": [76, 78], \"edge\": 236}, {\"x\": 0.02163463999097515, \"source\": 76, \"target\": 78, \"weight\": 1, \"y\": -0.25544958595372547, \"pair\": [76, 78], \"edge\": 236}, {\"x\": -0.02174063302018561, \"source\": 76, \"target\": 98, \"weight\": 1, \"y\": -0.17943032558075114, \"pair\": [76, 98], \"edge\": 237}, {\"x\": -0.011975080761383637, \"source\": 76, \"target\": 98, \"weight\": 1, \"y\": -0.2612569049501908, \"pair\": [76, 98], \"edge\": 237}, {\"x\": -0.037003951618323636, \"source\": 79, \"target\": 80, \"weight\": 1, \"y\": -0.28178414571159477, \"pair\": [79, 80], \"edge\": 238}, {\"x\": -0.11081179691516838, \"source\": 79, \"target\": 80, \"weight\": 1, \"y\": -0.37656216448342167, \"pair\": [79, 80], \"edge\": 238}, {\"x\": -0.037003951618323636, \"source\": 79, \"target\": 81, \"weight\": 1, \"y\": -0.28178414571159477, \"pair\": [79, 81], \"edge\": 239}, {\"x\": -0.07398284031010639, \"source\": 79, \"target\": 81, \"weight\": 1, \"y\": -0.39399204015933625, \"pair\": [79, 81], \"edge\": 239}, {\"x\": -0.037003951618323636, \"source\": 79, \"target\": 82, \"weight\": 1, \"y\": -0.28178414571159477, \"pair\": [79, 82], \"edge\": 240}, {\"x\": -0.09901437769863956, \"source\": 79, \"target\": 82, \"weight\": 1, \"y\": -0.3964516343588997, \"pair\": [79, 82], \"edge\": 240}, {\"x\": -0.037003951618323636, \"source\": 79, \"target\": 83, \"weight\": 1, \"y\": -0.28178414571159477, \"pair\": [79, 83], \"edge\": 241}, {\"x\": 0.025430608516896446, \"source\": 79, \"target\": 83, \"weight\": 1, \"y\": -0.20583124487500434, \"pair\": [79, 83], \"edge\": 241}, {\"x\": -0.11081179691516838, \"source\": 80, \"target\": 81, \"weight\": 1, \"y\": -0.37656216448342167, \"pair\": [80, 81], \"edge\": 242}, {\"x\": -0.07398284031010639, \"source\": 80, \"target\": 81, \"weight\": 1, \"y\": -0.39399204015933625, \"pair\": [80, 81], \"edge\": 242}, {\"x\": -0.11081179691516838, \"source\": 80, \"target\": 82, \"weight\": 1, \"y\": -0.37656216448342167, \"pair\": [80, 82], \"edge\": 243}, {\"x\": -0.09901437769863956, \"source\": 80, \"target\": 82, \"weight\": 1, \"y\": -0.3964516343588997, \"pair\": [80, 82], \"edge\": 243}, {\"x\": -0.07398284031010639, \"source\": 81, \"target\": 82, \"weight\": 1, \"y\": -0.39399204015933625, \"pair\": [81, 82], \"edge\": 244}, {\"x\": -0.09901437769863956, \"source\": 81, \"target\": 82, \"weight\": 1, \"y\": -0.3964516343588997, \"pair\": [81, 82], \"edge\": 244}, {\"x\": -0.3748038713496283, \"source\": 84, \"target\": 85, \"weight\": 1, \"y\": 0.135461209452502, \"pair\": [84, 85], \"edge\": 245}, {\"x\": -0.33966858175826775, \"source\": 84, \"target\": 85, \"weight\": 1, \"y\": 0.10297130105798322, \"pair\": [84, 85], \"edge\": 245}, {\"x\": -0.3748038713496283, \"source\": 84, \"target\": 86, \"weight\": 1, \"y\": 0.135461209452502, \"pair\": [84, 86], \"edge\": 246}, {\"x\": -0.3893719061692324, \"source\": 84, \"target\": 86, \"weight\": 1, \"y\": 0.11779598919048392, \"pair\": [84, 86], \"edge\": 246}, {\"x\": -0.3748038713496283, \"source\": 84, \"target\": 87, \"weight\": 1, \"y\": 0.135461209452502, \"pair\": [84, 87], \"edge\": 247}, {\"x\": -0.3904170530999228, \"source\": 84, \"target\": 87, \"weight\": 1, \"y\": 0.09492560497673808, \"pair\": [84, 87], \"edge\": 247}, {\"x\": -0.33966858175826775, \"source\": 85, \"target\": 86, \"weight\": 1, \"y\": 0.10297130105798322, \"pair\": [85, 86], \"edge\": 248}, {\"x\": -0.3893719061692324, \"source\": 85, \"target\": 86, \"weight\": 1, \"y\": 0.11779598919048392, \"pair\": [85, 86], \"edge\": 248}, {\"x\": -0.33966858175826775, \"source\": 85, \"target\": 87, \"weight\": 1, \"y\": 0.10297130105798322, \"pair\": [85, 87], \"edge\": 249}, {\"x\": -0.3904170530999228, \"source\": 85, \"target\": 87, \"weight\": 1, \"y\": 0.09492560497673808, \"pair\": [85, 87], \"edge\": 249}, {\"x\": -0.3893719061692324, \"source\": 86, \"target\": 87, \"weight\": 1, \"y\": 0.11779598919048392, \"pair\": [86, 87], \"edge\": 250}, {\"x\": -0.3904170530999228, \"source\": 86, \"target\": 87, \"weight\": 1, \"y\": 0.09492560497673808, \"pair\": [86, 87], \"edge\": 250}, {\"x\": 0.08419189704703227, \"source\": 89, \"target\": 93, \"weight\": 2, \"y\": -0.02097242429132012, \"pair\": [89, 93], \"edge\": 251}, {\"x\": 0.1254586362084501, \"source\": 89, \"target\": 93, \"weight\": 2, \"y\": -0.011845745356646708, \"pair\": [89, 93], \"edge\": 251}, {\"x\": 0.08419189704703227, \"source\": 89, \"target\": 94, \"weight\": 1, \"y\": -0.02097242429132012, \"pair\": [89, 94], \"edge\": 252}, {\"x\": 0.17623771316630127, \"source\": 89, \"target\": 94, \"weight\": 1, \"y\": -0.04787930076631226, \"pair\": [89, 94], \"edge\": 252}, {\"x\": 0.08419189704703227, \"source\": 89, \"target\": 95, \"weight\": 1, \"y\": -0.02097242429132012, \"pair\": [89, 95], \"edge\": 253}, {\"x\": 0.17700967089288305, \"source\": 89, \"target\": 95, \"weight\": 1, \"y\": -0.01295652547890839, \"pair\": [89, 95], \"edge\": 253}, {\"x\": 0.29405420144270583, \"source\": 91, \"target\": 92, \"weight\": 1, \"y\": 0.03906983535776226, \"pair\": [91, 92], \"edge\": 254}, {\"x\": 0.2953894253991343, \"source\": 91, \"target\": 92, \"weight\": 1, \"y\": 0.005390318607781721, \"pair\": [91, 92], \"edge\": 254}, {\"x\": 0.1254586362084501, \"source\": 93, \"target\": 95, \"weight\": 1, \"y\": -0.011845745356646708, \"pair\": [93, 95], \"edge\": 255}, {\"x\": 0.17700967089288305, \"source\": 93, \"target\": 95, \"weight\": 1, \"y\": -0.01295652547890839, \"pair\": [93, 95], \"edge\": 255}], \"data-321a68357ea3610409b01fd605cd8fd1\": [{\"x\": 0.7742874486799499, \"y\": 0.5118736526538851, \"id\": 0.0}, {\"x\": 0.8275919628216338, \"y\": 0.5652124335182597, \"id\": 1.0}, {\"x\": 0.831945869665898, \"y\": 0.5319223294473572, \"id\": 2.0}, {\"x\": -0.2713646677481388, \"y\": 0.16813658099006523, \"id\": 3.0}, {\"x\": -0.2902234045391699, \"y\": 0.21010482716733897, \"id\": 4.0}, {\"x\": -0.25016631544945583, \"y\": 0.14104479809456952, \"id\": 5.0}, {\"x\": -0.23749646566634144, \"y\": 0.14468595974833082, \"id\": 6.0}, {\"x\": -0.24841023810360466, \"y\": 0.22314649054010852, \"id\": 7.0}, {\"x\": -0.27607783527041946, \"y\": 0.2262309599729506, \"id\": 8.0}, {\"x\": -0.26271436173988244, \"y\": 0.21084895828993597, \"id\": 9.0}, {\"x\": -0.3434189120122944, \"y\": 0.0708219335108565, \"id\": 10.0}, {\"x\": -0.25462573199231797, \"y\": 0.06931654778861604, \"id\": 11.0}, {\"x\": -0.27470517992732346, \"y\": 0.06938321781282342, \"id\": 12.0}, {\"x\": -0.313332756284518, \"y\": 0.08915317997595999, \"id\": 13.0}, {\"x\": -0.30426009642291757, \"y\": 0.10291503297841009, \"id\": 14.0}, {\"x\": -0.31397478751285507, \"y\": 0.07150056265632794, \"id\": 15.0}, {\"x\": -0.5809357508719802, \"y\": 0.861922036965538, \"id\": 16.0}, {\"x\": -0.5413522544552076, \"y\": 0.79657033546486, \"id\": 17.0}, {\"x\": -0.1211203561774911, \"y\": -0.0747314451859159, \"id\": 18.0}, {\"x\": -0.2414026991695374, \"y\": -0.1098668430474719, \"id\": 19.0}, {\"x\": -0.22345641289191176, \"y\": -0.1337582341921364, \"id\": 20.0}, {\"x\": -0.2143547239351559, \"y\": 0.003791796648513245, \"id\": 21.0}, {\"x\": -0.0481799605476668, \"y\": -0.16819239953321818, \"id\": 22.0}, {\"x\": -0.15823874374769129, \"y\": -0.10858330989437703, \"id\": 23.0}, {\"x\": -0.23532760622592114, \"y\": -0.21459847371502708, \"id\": 24.0}, {\"x\": -0.18008173051450233, \"y\": 0.005015555628553944, \"id\": 25.0}, {\"x\": -0.08374405782833477, \"y\": 0.032132162872611586, \"id\": 26.0}, {\"x\": 0.015155338771232518, \"y\": -0.0237949812938176, \"id\": 27.0}, {\"x\": -0.22745410678189415, \"y\": 0.09591285457938409, \"id\": 28.0}, {\"x\": 0.003499004701826287, \"y\": -0.0710581432720969, \"id\": 29.0}, {\"x\": 0.04224440922701651, \"y\": -0.16153126726475023, \"id\": 30.0}, {\"x\": -0.09694618546715744, \"y\": -0.1674817286605503, \"id\": 31.0}, {\"x\": -0.35638156860897063, \"y\": -0.15275275651976816, \"id\": 32.0}, {\"x\": -0.3179738240402524, \"y\": -0.15371013649137227, \"id\": 33.0}, {\"x\": -0.18520660198138159, \"y\": -0.06276800852037102, \"id\": 34.0}, {\"x\": -0.2044802321985295, \"y\": -0.07765320517369118, \"id\": 35.0}, {\"x\": 0.1824583313460389, \"y\": 0.009782518718294362, \"id\": 36.0}, {\"x\": 0.34056236700902254, \"y\": 0.009326450386316241, \"id\": 37.0}, {\"x\": 0.11163817201568127, \"y\": -0.041736627061624114, \"id\": 38.0}, {\"x\": 0.13372815220552836, \"y\": 0.1605162712739612, \"id\": 39.0}, {\"x\": 0.19236976386643478, \"y\": 0.2663563059221274, \"id\": 40.0}, {\"x\": 0.16922811540721625, \"y\": 0.2618595044608058, \"id\": 41.0}, {\"x\": 0.15465578426260826, \"y\": 0.2797908640906716, \"id\": 42.0}, {\"x\": -0.0032126685304665905, \"y\": -0.01971618392164536, \"id\": 43.0}, {\"x\": 0.014177116932700448, \"y\": 0.045118703412298324, \"id\": 44.0}, {\"x\": 0.10041775413522544, \"y\": 0.002684926951578756, \"id\": 45.0}, {\"x\": 0.16007765352359848, \"y\": 0.04134962494994794, \"id\": 46.0}, {\"x\": 0.07117801885355013, \"y\": 0.018805982704749424, \"id\": 47.0}, {\"x\": 0.11705337669328504, \"y\": 0.03095482957258405, \"id\": 48.0}, {\"x\": 0.1177549863041608, \"y\": 0.10661320320752536, \"id\": 49.0}, {\"x\": 0.09034633976307598, \"y\": 0.05953513091318408, \"id\": 50.0}, {\"x\": 0.08879501531574102, \"y\": 0.02014889049051504, \"id\": 51.0}, {\"x\": 0.2073729378023853, \"y\": 0.13949084769516717, \"id\": 52.0}, {\"x\": 0.18597917465257235, \"y\": 0.10536674925772639, \"id\": 53.0}, {\"x\": 0.08276473970865998, \"y\": -0.03758480137070527, \"id\": 54.0}, {\"x\": 0.1455269606594044, \"y\": 0.07295561769313157, \"id\": 55.0}, {\"x\": 1.0, \"y\": -0.31654287432724065, \"id\": 56.0}, {\"x\": 0.9833935072773411, \"y\": -0.3535942544516167, \"id\": 57.0}, {\"x\": -0.18492941415722705, \"y\": -0.17625837164480054, \"id\": 58.0}, {\"x\": -0.1640947579319588, \"y\": -0.188092295651261, \"id\": 59.0}, {\"x\": -0.11891552607506636, \"y\": -0.18305219276498932, \"id\": 60.0}, {\"x\": -0.11966546773701857, \"y\": -0.011954973820502089, \"id\": 61.0}, {\"x\": -0.23599260788076, \"y\": -0.006507229823581309, \"id\": 62.0}, {\"x\": -0.056039092246434385, \"y\": -0.14026048849165834, \"id\": 63.0}, {\"x\": -0.030641572558633177, \"y\": -0.07765524168918447, \"id\": 64.0}, {\"x\": -0.20405387216101264, \"y\": -0.1562100301487121, \"id\": 65.0}, {\"x\": -0.23897966887493483, \"y\": -0.0389667629338048, \"id\": 66.0}, {\"x\": 0.44511627014077465, \"y\": 0.012483678992233512, \"id\": 67.0}, {\"x\": 0.06273653542802236, \"y\": -0.07677518063870974, \"id\": 68.0}, {\"x\": 0.15534502953258011, \"y\": -0.1597877755529839, \"id\": 69.0}, {\"x\": 0.06874944094237588, \"y\": -0.047239773073776005, \"id\": 70.0}, {\"x\": -0.0669225814703642, \"y\": -0.08646780054287956, \"id\": 71.0}, {\"x\": -0.054223668595532834, \"y\": -0.21785962815549784, \"id\": 72.0}, {\"x\": 0.9688799672612421, \"y\": -0.0667152065748477, \"id\": 73.0}, {\"x\": 0.9840418611844868, \"y\": -0.10420484012538193, \"id\": 74.0}, {\"x\": 0.007469284917281426, \"y\": -0.17092042012771036, \"id\": 75.0}, {\"x\": -0.02174063302018561, \"y\": -0.17943032558075114, \"id\": 76.0}, {\"x\": 0.005126176174485453, \"y\": -0.2228619899787118, \"id\": 77.0}, {\"x\": 0.02163463999097515, \"y\": -0.25544958595372547, \"id\": 78.0}, {\"x\": -0.037003951618323636, \"y\": -0.28178414571159477, \"id\": 79.0}, {\"x\": -0.11081179691516838, \"y\": -0.37656216448342167, \"id\": 80.0}, {\"x\": -0.07398284031010639, \"y\": -0.39399204015933625, \"id\": 81.0}, {\"x\": -0.09901437769863956, \"y\": -0.3964516343588997, \"id\": 82.0}, {\"x\": 0.025430608516896446, \"y\": -0.20583124487500434, \"id\": 83.0}, {\"x\": -0.3748038713496283, \"y\": 0.135461209452502, \"id\": 84.0}, {\"x\": -0.33966858175826775, \"y\": 0.10297130105798322, \"id\": 85.0}, {\"x\": -0.3893719061692324, \"y\": 0.11779598919048392, \"id\": 86.0}, {\"x\": -0.3904170530999228, \"y\": 0.09492560497673808, \"id\": 87.0}, {\"x\": -0.3392136300652572, \"y\": -0.18622094080637322, \"id\": 88.0}, {\"x\": 0.08419189704703227, \"y\": -0.02097242429132012, \"id\": 89.0}, {\"x\": 0.0785023990127484, \"y\": 0.09633915188874138, \"id\": 90.0}, {\"x\": 0.29405420144270583, \"y\": 0.03906983535776226, \"id\": 91.0}, {\"x\": 0.2953894253991343, \"y\": 0.005390318607781721, \"id\": 92.0}, {\"x\": 0.1254586362084501, \"y\": -0.011845745356646708, \"id\": 93.0}, {\"x\": 0.17623771316630127, \"y\": -0.04787930076631226, \"id\": 94.0}, {\"x\": 0.17700967089288305, \"y\": -0.01295652547890839, \"id\": 95.0}, {\"x\": 0.2908951112779772, \"y\": 0.220120363919095, \"id\": 96.0}, {\"x\": 0.09474699958454538, \"y\": -0.09247765010836596, \"id\": 97.0}, {\"x\": -0.011975080761383637, \"y\": -0.2612569049501908, \"id\": 98.0}, {\"x\": -0.013738548332712283, \"y\": -0.3030810853390118, \"id\": 99.0}, {\"x\": -0.16839743229162535, \"y\": -0.049218488592912786, \"id\": 100.0}]}};\n",
      "    var embedOpt = {\"mode\": \"vega-lite\"};\n",
      "\n",
      "    function showError(el, error){\n",
      "        el.innerHTML = ('<div class=\"error\" style=\"color:red;\">'\n",
      "                        + '<p>JavaScript Error: ' + error.message + '</p>'\n",
      "                        + \"<p>This usually means there's a typo in your chart specification. \"\n",
      "                        + \"See the javascript console for the full traceback.</p>\"\n",
      "                        + '</div>');\n",
      "        throw error;\n",
      "    }\n",
      "    const el = document.getElementById('vis');\n",
      "    vegaEmbed(\"#vis\", spec, embedOpt)\n",
      "      .catch(error => showError(el, error));\n",
      "  })(vegaEmbed);\n",
      "\n",
      "</script>\n",
      "\n",
      "We can now try to discribe a bit further the network (number of nodes, edges, degree...):\n",
      "\n",
      "```python\n",
      "c_degree = nx.degree_centrality(G)\n",
      "c_degree = list(c_degree.values())\n",
      "c = list(greedy_modularity_communities(G))\n",
      "\n",
      "nb_nodes = len(list(G.nodes()))\n",
      "nb_edges = len(list(G.edges()))\n",
      "degrees = list(dict(G.degree()).values())\n",
      "avg_degree = np.mean(degrees)\n",
      "median_degree = np.median(degrees)\n",
      "max_degree = np.max(degrees)        \n",
      "min_degree = np.min(degrees) \n",
      "avg_centrality = np.mean(c_degree) \n",
      "min_centrality = np.min(c_degree) \n",
      "max_centrality = np.max(c_degree) \n",
      "median_centrality = np.median(c_degree) \n",
      "\n",
      "print(\"Number of nodes: \", nb_nodes)\n",
      "print(\"Number of edges: \", nb_edges)\n",
      "print(\"Average degree: \", avg_degree)\n",
      "print(\"Median degree: \", median_degree)\n",
      "print(\"Max degree: \", max_degree)\n",
      "print(\"Min degree: \", min_degree)\n",
      "print(\"Average centrality: \", avg_centrality)\n",
      "print(\"Median centrality: \", median_centrality)\n",
      "print(\"Max centrality: \", max_centrality)\n",
      "print(\"Min centrality: \", min_centrality)\n",
      "```\n",
      "\n",
      "It heads the following:\n",
      "\n",
      "```bash\n",
      "Number of nodes:  101\n",
      "Number of edges:  256\n",
      "Average degree:  5.069306930693069\n",
      "Median degree:  3.0\n",
      "Max degree:  24\n",
      "Min degree:  1\n",
      "Average centrality:  0.050693069306930696\n",
      "Median centrality:  0.03\n",
      "Max centrality:  0.24\n",
      "Min centrality:  0.01\n",
      "```\n",
      "\n",
      "On average, each person is connected to 5 other (which is quite high). There seems to be a least one person with a large centrality since this person has 24 edges.\n",
      "\n",
      "We can now look at the distribution of the degrees:\n",
      "\n",
      "```python\n",
      "hist = alt.Chart(pd.DataFrame(degrees, columns=['Degrees'])).mark_bar().encode(\n",
      "    alt.X(\"Degrees:Q\", bin=alt.Bin(maxbins=25)),\n",
      "    y='count()').properties(title=\"Degree Distribution\", height=300, width=500)\n",
      "\n",
      "hist\n",
      "```\n",
      "\n",
      "<div id=\"vis2\"></div>\n",
      "<script>\n",
      "  (function(vegaEmbed) {\n",
      "    var spec2 = {\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-c127aeaedd28c891faa99b0a0c5d0f14\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"bin\": {\"maxbins\": 25}, \"field\": \"Degrees\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"count\"}}, \"height\": 300, \"title\": \"Degree Distribution\", \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-c127aeaedd28c891faa99b0a0c5d0f14\": [{\"Degrees\": 2}, {\"Degrees\": 2}, {\"Degrees\": 2}, {\"Degrees\": 8}, {\"Degrees\": 6}, {\"Degrees\": 10}, {\"Degrees\": 10}, {\"Degrees\": 6}, {\"Degrees\": 6}, {\"Degrees\": 6}, {\"Degrees\": 5}, {\"Degrees\": 12}, {\"Degrees\": 16}, {\"Degrees\": 6}, {\"Degrees\": 6}, {\"Degrees\": 6}, {\"Degrees\": 1}, {\"Degrees\": 1}, {\"Degrees\": 24}, {\"Degrees\": 9}, {\"Degrees\": 2}, {\"Degrees\": 2}, {\"Degrees\": 14}, {\"Degrees\": 8}, {\"Degrees\": 1}, {\"Degrees\": 13}, {\"Degrees\": 2}, {\"Degrees\": 16}, {\"Degrees\": 5}, {\"Degrees\": 13}, {\"Degrees\": 2}, {\"Degrees\": 2}, {\"Degrees\": 2}, {\"Degrees\": 3}, {\"Degrees\": 6}, {\"Degrees\": 5}, {\"Degrees\": 8}, {\"Degrees\": 2}, {\"Degrees\": 3}, {\"Degrees\": 7}, {\"Degrees\": 3}, {\"Degrees\": 3}, {\"Degrees\": 3}, {\"Degrees\": 9}, {\"Degrees\": 3}, {\"Degrees\": 12}, {\"Degrees\": 4}, {\"Degrees\": 19}, {\"Degrees\": 12}, {\"Degrees\": 4}, {\"Degrees\": 8}, {\"Degrees\": 11}, {\"Degrees\": 3}, {\"Degrees\": 2}, {\"Degrees\": 6}, {\"Degrees\": 2}, {\"Degrees\": 1}, {\"Degrees\": 1}, {\"Degrees\": 2}, {\"Degrees\": 2}, {\"Degrees\": 2}, {\"Degrees\": 4}, {\"Degrees\": 1}, {\"Degrees\": 4}, {\"Degrees\": 6}, {\"Degrees\": 1}, {\"Degrees\": 1}, {\"Degrees\": 1}, {\"Degrees\": 15}, {\"Degrees\": 1}, {\"Degrees\": 6}, {\"Degrees\": 2}, {\"Degrees\": 2}, {\"Degrees\": 1}, {\"Degrees\": 1}, {\"Degrees\": 4}, {\"Degrees\": 7}, {\"Degrees\": 3}, {\"Degrees\": 2}, {\"Degrees\": 5}, {\"Degrees\": 3}, {\"Degrees\": 3}, {\"Degrees\": 3}, {\"Degrees\": 2}, {\"Degrees\": 4}, {\"Degrees\": 8}, {\"Degrees\": 4}, {\"Degrees\": 4}, {\"Degrees\": 2}, {\"Degrees\": 12}, {\"Degrees\": 2}, {\"Degrees\": 2}, {\"Degrees\": 2}, {\"Degrees\": 6}, {\"Degrees\": 2}, {\"Degrees\": 3}, {\"Degrees\": 1}, {\"Degrees\": 3}, {\"Degrees\": 2}, {\"Degrees\": 1}, {\"Degrees\": 4}]}};\n",
      "    var embedOpt2 = {\"mode\": \"vega-lite\"};\n",
      "\n",
      "    function showError(el2, error2){\n",
      "        el2.innerHTML = ('<div class=\"error\" style=\"color:red;\">'\n",
      "                        + '<p>JavaScript Error: ' + error2.message + '</p>'\n",
      "                        + \"<p>This usually means there's a typo in your chart specification. \"\n",
      "                        + \"See the javascript console for the full traceback.</p>\"\n",
      "                        + '</div>');\n",
      "        throw error2;\n",
      "    }\n",
      "    const el2 = document.getElementById('vis2');\n",
      "    vegaEmbed(\"#vis2\", spec2, embedOpt2)\n",
      "      .catch(error => showError(el2, error));\n",
      "  })(vegaEmbed);\n",
      "\n",
      "</script>\n",
      "\n",
      "It is much closer to a Barabasi-Albert model distribution, meaning that the network has strong signs of preferential attachment.\n",
      "\n",
      "The two datasets have 47 nodes in common, so we can plot them together:\n",
      "\n",
      "```python\n",
      "file = open('Datasets/Montagna_phonecalls_edgelist.csv', 'r')\n",
      "\n",
      "for row in file:\n",
      "    r = row.split()\n",
      "    n1, n2, w = int(r[0]), int(r[1]), int(r[2])\n",
      "    G.add_node(n1)\n",
      "    G.add_node(n2)\n",
      "    G.nodes[n1]['id'] = n1\n",
      "    G.nodes[n2]['id'] = n2\n",
      "    G.add_edge(n1, n2)\n",
      "    G.edges[(n1,n2)]['weight']=w\n",
      "    G.edges[(n1,n2)]['method']='phonecalls'\n",
      "\n",
      "pos = nx.spring_layout(G)\n",
      "\n",
      "chart = nxa.draw_networkx(\n",
      "    G=G,\n",
      "    pos=pos,\n",
      "    width='weight:N'\n",
      ").properties(title=\"Sicilian Mafia all meetings\", height=500, width=500\n",
      ").interactive()\n",
      "\n",
      "chart\n",
      "```\n",
      "\n",
      "<div id=\"vis3\"></div>\n",
      "<script>\n",
      "  (function(vegaEmbed) {\n",
      "    var spec3 = {\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"domain\": false, \"grid\": false, \"labels\": false, \"ticks\": false}}, \"layer\": [{\"data\": {\"name\": \"data-613d3142b26280774cf6d8cd663387ff\"}, \"mark\": {\"type\": \"line\", \"color\": \"black\", \"opacity\": 1}, \"encoding\": {\"detail\": {\"type\": \"quantitative\", \"field\": \"edge\"}, \"size\": {\"type\": \"nominal\", \"field\": \"weight\", \"legend\": null}, \"x\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"\"}, \"field\": \"x\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"\"}, \"field\": \"y\"}}, \"selection\": {\"selector011\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}}, {\"data\": {\"name\": \"data-9afb9796b7d186c4d9c70efa3f76c953\"}, \"mark\": {\"type\": \"point\", \"fill\": \"red\", \"opacity\": 1, \"size\": 300}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"x\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"y\"}}}], \"height\": 500, \"title\": \"Sicilian Mafia all meetings\", \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-613d3142b26280774cf6d8cd663387ff\": [{\"x\": -0.6469108023227067, \"source\": 0, \"method\": \"meetings\", \"target\": 1, \"weight\": 1, \"y\": 0.4935455961948123, \"pair\": [0, 1], \"edge\": 0}, {\"x\": -0.6258949217935961, \"source\": 0, \"method\": \"meetings\", \"target\": 1, \"weight\": 1, \"y\": 0.5048322656819353, \"pair\": [0, 1], \"edge\": 0}, {\"x\": -0.6469108023227067, \"source\": 0, \"method\": \"meetings\", \"target\": 2, \"weight\": 1, \"y\": 0.4935455961948123, \"pair\": [0, 2], \"edge\": 1}, {\"x\": -0.6704455901428831, \"source\": 0, \"method\": \"meetings\", \"target\": 2, \"weight\": 1, \"y\": 0.5293828017384508, \"pair\": [0, 2], \"edge\": 1}, {\"x\": -0.6258949217935961, \"source\": 1, \"method\": \"meetings\", \"target\": 2, \"weight\": 1, \"y\": 0.5048322656819353, \"pair\": [1, 2], \"edge\": 2}, {\"x\": -0.6704455901428831, \"source\": 1, \"method\": \"meetings\", \"target\": 2, \"weight\": 1, \"y\": 0.5293828017384508, \"pair\": [1, 2], \"edge\": 2}, {\"x\": -0.13719554016384364, \"source\": 3, \"method\": \"meetings\", \"target\": 4, \"weight\": 1, \"y\": -0.28877228822605044, \"pair\": [3, 4], \"edge\": 3}, {\"x\": -0.14106817729117918, \"source\": 3, \"method\": \"meetings\", \"target\": 4, \"weight\": 1, \"y\": -0.32711971026099, \"pair\": [3, 4], \"edge\": 3}, {\"x\": -0.13719554016384364, \"source\": 3, \"method\": \"meetings\", \"target\": 5, \"weight\": 1, \"y\": -0.28877228822605044, \"pair\": [3, 5], \"edge\": 4}, {\"x\": -0.1131678875451056, \"source\": 3, \"method\": \"meetings\", \"target\": 5, \"weight\": 1, \"y\": -0.2618534715182959, \"pair\": [3, 5], \"edge\": 4}, {\"x\": -0.13719554016384364, \"source\": 3, \"method\": \"meetings\", \"target\": 6, \"weight\": 1, \"y\": -0.28877228822605044, \"pair\": [3, 6], \"edge\": 5}, {\"x\": -0.12205939130269741, \"source\": 3, \"method\": \"meetings\", \"target\": 6, \"weight\": 1, \"y\": -0.2593127809814257, \"pair\": [3, 6], \"edge\": 5}, {\"x\": -0.13719554016384364, \"source\": 3, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": -0.28877228822605044, \"pair\": [3, 7], \"edge\": 6}, {\"x\": -0.12736245884392888, \"source\": 3, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": -0.32455179918832044, \"pair\": [3, 7], \"edge\": 6}, {\"x\": -0.13719554016384364, \"source\": 3, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": -0.28877228822605044, \"pair\": [3, 8], \"edge\": 7}, {\"x\": -0.15916258710208359, \"source\": 3, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": -0.3180250558711278, \"pair\": [3, 8], \"edge\": 7}, {\"x\": -0.13719554016384364, \"source\": 3, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": -0.28877228822605044, \"pair\": [3, 9], \"edge\": 8}, {\"x\": -0.1466580043966531, \"source\": 3, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": -0.3166044178140925, \"pair\": [3, 9], \"edge\": 8}, {\"x\": -0.13719554016384364, \"source\": 3, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": -0.28877228822605044, \"pair\": [3, 11], \"edge\": 9}, {\"x\": -0.10013535944768197, \"source\": 3, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": -0.22784700669661914, \"pair\": [3, 11], \"edge\": 9}, {\"x\": -0.13719554016384364, \"source\": 3, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": -0.28877228822605044, \"pair\": [3, 12], \"edge\": 10}, {\"x\": -0.12905166604739432, \"source\": 3, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": -0.2071446291709666, \"pair\": [3, 12], \"edge\": 10}, {\"x\": -0.14106817729117918, \"source\": 4, \"method\": \"meetings\", \"target\": 5, \"weight\": 1, \"y\": -0.32711971026099, \"pair\": [4, 5], \"edge\": 11}, {\"x\": -0.1131678875451056, \"source\": 4, \"method\": \"meetings\", \"target\": 5, \"weight\": 1, \"y\": -0.2618534715182959, \"pair\": [4, 5], \"edge\": 11}, {\"x\": -0.14106817729117918, \"source\": 4, \"method\": \"meetings\", \"target\": 6, \"weight\": 1, \"y\": -0.32711971026099, \"pair\": [4, 6], \"edge\": 12}, {\"x\": -0.12205939130269741, \"source\": 4, \"method\": \"meetings\", \"target\": 6, \"weight\": 1, \"y\": -0.2593127809814257, \"pair\": [4, 6], \"edge\": 12}, {\"x\": -0.14106817729117918, \"source\": 4, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": -0.32711971026099, \"pair\": [4, 7], \"edge\": 13}, {\"x\": -0.12736245884392888, \"source\": 4, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": -0.32455179918832044, \"pair\": [4, 7], \"edge\": 13}, {\"x\": -0.14106817729117918, \"source\": 4, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": -0.32711971026099, \"pair\": [4, 8], \"edge\": 14}, {\"x\": -0.15916258710208359, \"source\": 4, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": -0.3180250558711278, \"pair\": [4, 8], \"edge\": 14}, {\"x\": -0.14106817729117918, \"source\": 4, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": -0.32711971026099, \"pair\": [4, 9], \"edge\": 15}, {\"x\": -0.1466580043966531, \"source\": 4, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": -0.3166044178140925, \"pair\": [4, 9], \"edge\": 15}, {\"x\": -0.1131678875451056, \"source\": 5, \"method\": \"meetings\", \"target\": 6, \"weight\": 1, \"y\": -0.2618534715182959, \"pair\": [5, 6], \"edge\": 16}, {\"x\": -0.12205939130269741, \"source\": 5, \"method\": \"meetings\", \"target\": 6, \"weight\": 1, \"y\": -0.2593127809814257, \"pair\": [5, 6], \"edge\": 16}, {\"x\": -0.1131678875451056, \"source\": 5, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": -0.2618534715182959, \"pair\": [5, 7], \"edge\": 17}, {\"x\": -0.12736245884392888, \"source\": 5, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": -0.32455179918832044, \"pair\": [5, 7], \"edge\": 17}, {\"x\": -0.1131678875451056, \"source\": 5, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": -0.2618534715182959, \"pair\": [5, 8], \"edge\": 18}, {\"x\": -0.15916258710208359, \"source\": 5, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": -0.3180250558711278, \"pair\": [5, 8], \"edge\": 18}, {\"x\": -0.1131678875451056, \"source\": 5, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": -0.2618534715182959, \"pair\": [5, 9], \"edge\": 19}, {\"x\": -0.1466580043966531, \"source\": 5, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": -0.3166044178140925, \"pair\": [5, 9], \"edge\": 19}, {\"x\": -0.1131678875451056, \"source\": 5, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": -0.2618534715182959, \"pair\": [5, 11], \"edge\": 20}, {\"x\": -0.10013535944768197, \"source\": 5, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": -0.22784700669661914, \"pair\": [5, 11], \"edge\": 20}, {\"x\": -0.1131678875451056, \"source\": 5, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": -0.2618534715182959, \"pair\": [5, 12], \"edge\": 21}, {\"x\": -0.12905166604739432, \"source\": 5, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": -0.2071446291709666, \"pair\": [5, 12], \"edge\": 21}, {\"x\": -0.1131678875451056, \"source\": 5, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": -0.2618534715182959, \"pair\": [5, 28], \"edge\": 22}, {\"x\": -0.09992051427352441, \"source\": 5, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": -0.21558223756616396, \"pair\": [5, 28], \"edge\": 22}, {\"x\": -0.1131678875451056, \"source\": 5, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": -0.2618534715182959, \"pair\": [5, 25], \"edge\": 23}, {\"x\": -0.0675123612720282, \"source\": 5, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": -0.1365942017604369, \"pair\": [5, 25], \"edge\": 23}, {\"x\": -0.12205939130269741, \"source\": 6, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": -0.2593127809814257, \"pair\": [6, 7], \"edge\": 24}, {\"x\": -0.12736245884392888, \"source\": 6, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": -0.32455179918832044, \"pair\": [6, 7], \"edge\": 24}, {\"x\": -0.12205939130269741, \"source\": 6, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": -0.2593127809814257, \"pair\": [6, 8], \"edge\": 25}, {\"x\": -0.15916258710208359, \"source\": 6, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": -0.3180250558711278, \"pair\": [6, 8], \"edge\": 25}, {\"x\": -0.12205939130269741, \"source\": 6, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": -0.2593127809814257, \"pair\": [6, 9], \"edge\": 26}, {\"x\": -0.1466580043966531, \"source\": 6, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": -0.3166044178140925, \"pair\": [6, 9], \"edge\": 26}, {\"x\": -0.12205939130269741, \"source\": 6, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": -0.2593127809814257, \"pair\": [6, 11], \"edge\": 27}, {\"x\": -0.10013535944768197, \"source\": 6, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": -0.22784700669661914, \"pair\": [6, 11], \"edge\": 27}, {\"x\": -0.12205939130269741, \"source\": 6, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": -0.2593127809814257, \"pair\": [6, 12], \"edge\": 28}, {\"x\": -0.12905166604739432, \"source\": 6, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": -0.2071446291709666, \"pair\": [6, 12], \"edge\": 28}, {\"x\": -0.12205939130269741, \"source\": 6, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": -0.2593127809814257, \"pair\": [6, 28], \"edge\": 29}, {\"x\": -0.09992051427352441, \"source\": 6, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": -0.21558223756616396, \"pair\": [6, 28], \"edge\": 29}, {\"x\": -0.12205939130269741, \"source\": 6, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": -0.2593127809814257, \"pair\": [6, 25], \"edge\": 30}, {\"x\": -0.0675123612720282, \"source\": 6, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": -0.1365942017604369, \"pair\": [6, 25], \"edge\": 30}, {\"x\": -0.12736245884392888, \"source\": 7, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": -0.32455179918832044, \"pair\": [7, 8], \"edge\": 31}, {\"x\": -0.15916258710208359, \"source\": 7, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": -0.3180250558711278, \"pair\": [7, 8], \"edge\": 31}, {\"x\": -0.12736245884392888, \"source\": 7, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": -0.32455179918832044, \"pair\": [7, 9], \"edge\": 32}, {\"x\": -0.1466580043966531, \"source\": 7, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": -0.3166044178140925, \"pair\": [7, 9], \"edge\": 32}, {\"x\": -0.15916258710208359, \"source\": 8, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": -0.3180250558711278, \"pair\": [8, 9], \"edge\": 33}, {\"x\": -0.1466580043966531, \"source\": 8, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": -0.3166044178140925, \"pair\": [8, 9], \"edge\": 33}, {\"x\": -0.11569665465029463, \"source\": 10, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": -0.27507341347162906, \"pair\": [10, 11], \"edge\": 34}, {\"x\": -0.10013535944768197, \"source\": 10, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": -0.22784700669661914, \"pair\": [10, 11], \"edge\": 34}, {\"x\": -0.11569665465029463, \"source\": 10, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": -0.27507341347162906, \"pair\": [10, 12], \"edge\": 35}, {\"x\": -0.12905166604739432, \"source\": 10, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": -0.2071446291709666, \"pair\": [10, 12], \"edge\": 35}, {\"x\": -0.11569665465029463, \"source\": 10, \"method\": \"meetings\", \"target\": 13, \"weight\": 1, \"y\": -0.27507341347162906, \"pair\": [10, 13], \"edge\": 36}, {\"x\": -0.14271756231597862, \"source\": 10, \"method\": \"meetings\", \"target\": 13, \"weight\": 1, \"y\": -0.2508039210394611, \"pair\": [10, 13], \"edge\": 36}, {\"x\": -0.11569665465029463, \"source\": 10, \"method\": \"meetings\", \"target\": 14, \"weight\": 1, \"y\": -0.27507341347162906, \"pair\": [10, 14], \"edge\": 37}, {\"x\": -0.13741879538112697, \"source\": 10, \"method\": \"meetings\", \"target\": 14, \"weight\": 1, \"y\": -0.27478263231154304, \"pair\": [10, 14], \"edge\": 37}, {\"x\": -0.11569665465029463, \"source\": 10, \"method\": \"meetings\", \"target\": 15, \"weight\": 1, \"y\": -0.27507341347162906, \"pair\": [10, 15], \"edge\": 38}, {\"x\": -0.13445332259574821, \"source\": 10, \"method\": \"meetings\", \"target\": 15, \"weight\": 1, \"y\": -0.24644340276353865, \"pair\": [10, 15], \"edge\": 38}, {\"x\": -0.10013535944768197, \"source\": 11, \"method\": \"phonecalls\", \"target\": 12, \"weight\": 1, \"y\": -0.22784700669661914, \"pair\": [11, 12], \"edge\": 39}, {\"x\": -0.12905166604739432, \"source\": 11, \"method\": \"phonecalls\", \"target\": 12, \"weight\": 1, \"y\": -0.2071446291709666, \"pair\": [11, 12], \"edge\": 39}, {\"x\": -0.10013535944768197, \"source\": 11, \"method\": \"meetings\", \"target\": 13, \"weight\": 2, \"y\": -0.22784700669661914, \"pair\": [11, 13], \"edge\": 40}, {\"x\": -0.14271756231597862, \"source\": 11, \"method\": \"meetings\", \"target\": 13, \"weight\": 2, \"y\": -0.2508039210394611, \"pair\": [11, 13], \"edge\": 40}, {\"x\": -0.10013535944768197, \"source\": 11, \"method\": \"meetings\", \"target\": 14, \"weight\": 2, \"y\": -0.22784700669661914, \"pair\": [11, 14], \"edge\": 41}, {\"x\": -0.13741879538112697, \"source\": 11, \"method\": \"meetings\", \"target\": 14, \"weight\": 2, \"y\": -0.27478263231154304, \"pair\": [11, 14], \"edge\": 41}, {\"x\": -0.10013535944768197, \"source\": 11, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": -0.22784700669661914, \"pair\": [11, 15], \"edge\": 42}, {\"x\": -0.13445332259574821, \"source\": 11, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": -0.24644340276353865, \"pair\": [11, 15], \"edge\": 42}, {\"x\": -0.10013535944768197, \"source\": 11, \"method\": \"meetings\", \"target\": 18, \"weight\": 1, \"y\": -0.22784700669661914, \"pair\": [11, 18], \"edge\": 43}, {\"x\": -0.05818896841716495, \"source\": 11, \"method\": \"meetings\", \"target\": 18, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [11, 18], \"edge\": 43}, {\"x\": -0.10013535944768197, \"source\": 11, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": -0.22784700669661914, \"pair\": [11, 25], \"edge\": 44}, {\"x\": -0.0675123612720282, \"source\": 11, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": -0.1365942017604369, \"pair\": [11, 25], \"edge\": 44}, {\"x\": -0.10013535944768197, \"source\": 11, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": -0.22784700669661914, \"pair\": [11, 28], \"edge\": 45}, {\"x\": -0.09992051427352441, \"source\": 11, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": -0.21558223756616396, \"pair\": [11, 28], \"edge\": 45}, {\"x\": -0.10013535944768197, \"source\": 11, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": -0.22784700669661914, \"pair\": [11, 85], \"edge\": 46}, {\"x\": -0.16613248928451432, \"source\": 11, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": -0.2494687678551724, \"pair\": [11, 85], \"edge\": 46}, {\"x\": -0.10013535944768197, \"source\": 11, \"method\": \"phonecalls\", \"target\": 145, \"weight\": 2, \"y\": -0.22784700669661914, \"pair\": [11, 145], \"edge\": 47}, {\"x\": -0.06893170463044125, \"source\": 11, \"method\": \"phonecalls\", \"target\": 145, \"weight\": 2, \"y\": -0.2844255270446289, \"pair\": [11, 145], \"edge\": 47}, {\"x\": -0.10013535944768197, \"source\": 11, \"method\": \"phonecalls\", \"target\": 144, \"weight\": 1, \"y\": -0.22784700669661914, \"pair\": [11, 144], \"edge\": 48}, {\"x\": -0.07985989343518832, \"source\": 11, \"method\": \"phonecalls\", \"target\": 144, \"weight\": 1, \"y\": -0.3246025864534094, \"pair\": [11, 144], \"edge\": 48}, {\"x\": -0.10013535944768197, \"source\": 11, \"method\": \"phonecalls\", \"target\": 143, \"weight\": 2, \"y\": -0.22784700669661914, \"pair\": [11, 143], \"edge\": 49}, {\"x\": -0.08504197444015457, \"source\": 11, \"method\": \"phonecalls\", \"target\": 143, \"weight\": 2, \"y\": -0.2963860817703671, \"pair\": [11, 143], \"edge\": 49}, {\"x\": -0.12905166604739432, \"source\": 12, \"method\": \"meetings\", \"target\": 13, \"weight\": 2, \"y\": -0.2071446291709666, \"pair\": [12, 13], \"edge\": 50}, {\"x\": -0.14271756231597862, \"source\": 12, \"method\": \"meetings\", \"target\": 13, \"weight\": 2, \"y\": -0.2508039210394611, \"pair\": [12, 13], \"edge\": 50}, {\"x\": -0.12905166604739432, \"source\": 12, \"method\": \"meetings\", \"target\": 14, \"weight\": 2, \"y\": -0.2071446291709666, \"pair\": [12, 14], \"edge\": 51}, {\"x\": -0.13741879538112697, \"source\": 12, \"method\": \"meetings\", \"target\": 14, \"weight\": 2, \"y\": -0.27478263231154304, \"pair\": [12, 14], \"edge\": 51}, {\"x\": -0.12905166604739432, \"source\": 12, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": -0.2071446291709666, \"pair\": [12, 15], \"edge\": 52}, {\"x\": -0.13445332259574821, \"source\": 12, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": -0.24644340276353865, \"pair\": [12, 15], \"edge\": 52}, {\"x\": -0.12905166604739432, \"source\": 12, \"method\": \"meetings\", \"target\": 18, \"weight\": 1, \"y\": -0.2071446291709666, \"pair\": [12, 18], \"edge\": 53}, {\"x\": -0.05818896841716495, \"source\": 12, \"method\": \"meetings\", \"target\": 18, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [12, 18], \"edge\": 53}, {\"x\": -0.12905166604739432, \"source\": 12, \"method\": \"meetings\", \"target\": 21, \"weight\": 1, \"y\": -0.2071446291709666, \"pair\": [12, 21], \"edge\": 54}, {\"x\": -0.10166508329327305, \"source\": 12, \"method\": \"meetings\", \"target\": 21, \"weight\": 1, \"y\": -0.13260281294659096, \"pair\": [12, 21], \"edge\": 54}, {\"x\": -0.12905166604739432, \"source\": 12, \"method\": \"meetings\", \"target\": 25, \"weight\": 2, \"y\": -0.2071446291709666, \"pair\": [12, 25], \"edge\": 55}, {\"x\": -0.0675123612720282, \"source\": 12, \"method\": \"meetings\", \"target\": 25, \"weight\": 2, \"y\": -0.1365942017604369, \"pair\": [12, 25], \"edge\": 55}, {\"x\": -0.12905166604739432, \"source\": 12, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": -0.2071446291709666, \"pair\": [12, 28], \"edge\": 56}, {\"x\": -0.09992051427352441, \"source\": 12, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": -0.21558223756616396, \"pair\": [12, 28], \"edge\": 56}, {\"x\": -0.12905166604739432, \"source\": 12, \"method\": \"meetings\", \"target\": 84, \"weight\": 1, \"y\": -0.2071446291709666, \"pair\": [12, 84], \"edge\": 57}, {\"x\": -0.19745425247945897, \"source\": 12, \"method\": \"meetings\", \"target\": 84, \"weight\": 1, \"y\": -0.27120405127703506, \"pair\": [12, 84], \"edge\": 57}, {\"x\": -0.12905166604739432, \"source\": 12, \"method\": \"meetings\", \"target\": 85, \"weight\": 2, \"y\": -0.2071446291709666, \"pair\": [12, 85], \"edge\": 58}, {\"x\": -0.16613248928451432, \"source\": 12, \"method\": \"meetings\", \"target\": 85, \"weight\": 2, \"y\": -0.2494687678551724, \"pair\": [12, 85], \"edge\": 58}, {\"x\": -0.12905166604739432, \"source\": 12, \"method\": \"meetings\", \"target\": 86, \"weight\": 1, \"y\": -0.2071446291709666, \"pair\": [12, 86], \"edge\": 59}, {\"x\": -0.20574340160449894, \"source\": 12, \"method\": \"meetings\", \"target\": 86, \"weight\": 1, \"y\": -0.24650064929612356, \"pair\": [12, 86], \"edge\": 59}, {\"x\": -0.12905166604739432, \"source\": 12, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": -0.2071446291709666, \"pair\": [12, 87], \"edge\": 60}, {\"x\": -0.20733371983185353, \"source\": 12, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": -0.26079972581200556, \"pair\": [12, 87], \"edge\": 60}, {\"x\": -0.14271756231597862, \"source\": 13, \"method\": \"meetings\", \"target\": 14, \"weight\": 2, \"y\": -0.2508039210394611, \"pair\": [13, 14], \"edge\": 61}, {\"x\": -0.13741879538112697, \"source\": 13, \"method\": \"meetings\", \"target\": 14, \"weight\": 2, \"y\": -0.27478263231154304, \"pair\": [13, 14], \"edge\": 61}, {\"x\": -0.14271756231597862, \"source\": 13, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": -0.2508039210394611, \"pair\": [13, 15], \"edge\": 62}, {\"x\": -0.13445332259574821, \"source\": 13, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": -0.24644340276353865, \"pair\": [13, 15], \"edge\": 62}, {\"x\": -0.14271756231597862, \"source\": 13, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": -0.2508039210394611, \"pair\": [13, 85], \"edge\": 63}, {\"x\": -0.16613248928451432, \"source\": 13, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": -0.2494687678551724, \"pair\": [13, 85], \"edge\": 63}, {\"x\": -0.13741879538112697, \"source\": 14, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": -0.27478263231154304, \"pair\": [14, 15], \"edge\": 64}, {\"x\": -0.13445332259574821, \"source\": 14, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": -0.24644340276353865, \"pair\": [14, 15], \"edge\": 64}, {\"x\": -0.13741879538112697, \"source\": 14, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": -0.27478263231154304, \"pair\": [14, 85], \"edge\": 65}, {\"x\": -0.16613248928451432, \"source\": 14, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": -0.2494687678551724, \"pair\": [14, 85], \"edge\": 65}, {\"x\": -0.13741879538112697, \"source\": 14, \"method\": \"phonecalls\", \"target\": 101, \"weight\": 2, \"y\": -0.27478263231154304, \"pair\": [14, 101], \"edge\": 66}, {\"x\": -0.14916093623352256, \"source\": 14, \"method\": \"phonecalls\", \"target\": 101, \"weight\": 2, \"y\": -0.35072097679131975, \"pair\": [14, 101], \"edge\": 66}, {\"x\": -0.13445332259574821, \"source\": 15, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": -0.24644340276353865, \"pair\": [15, 85], \"edge\": 67}, {\"x\": -0.16613248928451432, \"source\": 15, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": -0.2494687678551724, \"pair\": [15, 85], \"edge\": 67}, {\"x\": -0.9578758462413073, \"source\": 16, \"method\": \"meetings\", \"target\": 17, \"weight\": 1, \"y\": 0.17770291444097472, \"pair\": [16, 17], \"edge\": 68}, {\"x\": -0.9286109055297189, \"source\": 16, \"method\": \"meetings\", \"target\": 17, \"weight\": 1, \"y\": 0.180672255115451, \"pair\": [16, 17], \"edge\": 68}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 19, \"weight\": 6, \"y\": -0.024414181320765033, \"pair\": [18, 19], \"edge\": 69}, {\"x\": -0.07183262544818632, \"source\": 18, \"method\": \"phonecalls\", \"target\": 19, \"weight\": 6, \"y\": -0.042498896918285775, \"pair\": [18, 19], \"edge\": 69}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"meetings\", \"target\": 20, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 20], \"edge\": 70}, {\"x\": -0.12112646250555749, \"source\": 18, \"method\": \"meetings\", \"target\": 20, \"weight\": 1, \"y\": -0.03214720104080549, \"pair\": [18, 20], \"edge\": 70}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 21, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 21], \"edge\": 71}, {\"x\": -0.10166508329327305, \"source\": 18, \"method\": \"phonecalls\", \"target\": 21, \"weight\": 1, \"y\": -0.13260281294659096, \"pair\": [18, 21], \"edge\": 71}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"meetings\", \"target\": 22, \"weight\": 2, \"y\": -0.024414181320765033, \"pair\": [18, 22], \"edge\": 72}, {\"x\": -0.06840055701831564, \"source\": 18, \"method\": \"meetings\", \"target\": 22, \"weight\": 2, \"y\": 0.02407701603622507, \"pair\": [18, 22], \"edge\": 72}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"meetings\", \"target\": 31, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 31], \"edge\": 73}, {\"x\": -0.10917566139158077, \"source\": 18, \"method\": \"meetings\", \"target\": 31, \"weight\": 1, \"y\": 0.03036180577307121, \"pair\": [18, 31], \"edge\": 73}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 34, \"weight\": 3, \"y\": -0.024414181320765033, \"pair\": [18, 34], \"edge\": 74}, {\"x\": -0.06419144111152837, \"source\": 18, \"method\": \"phonecalls\", \"target\": 34, \"weight\": 3, \"y\": -0.08288517507788387, \"pair\": [18, 34], \"edge\": 74}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 35, \"weight\": 5, \"y\": -0.024414181320765033, \"pair\": [18, 35], \"edge\": 75}, {\"x\": -0.06148592987207431, \"source\": 18, \"method\": \"phonecalls\", \"target\": 35, \"weight\": 5, \"y\": -0.06535239641622947, \"pair\": [18, 35], \"edge\": 75}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"meetings\", \"target\": 23, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 23], \"edge\": 76}, {\"x\": -0.09202740679831034, \"source\": 18, \"method\": \"meetings\", \"target\": 23, \"weight\": 1, \"y\": -0.06923147365647722, \"pair\": [18, 23], \"edge\": 76}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 25, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 25], \"edge\": 77}, {\"x\": -0.0675123612720282, \"source\": 18, \"method\": \"phonecalls\", \"target\": 25, \"weight\": 1, \"y\": -0.1365942017604369, \"pair\": [18, 25], \"edge\": 77}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 58, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 58], \"edge\": 78}, {\"x\": -0.129566842289825, \"source\": 18, \"method\": \"phonecalls\", \"target\": 58, \"weight\": 1, \"y\": 0.0828576593062837, \"pair\": [18, 58], \"edge\": 78}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"meetings\", \"target\": 59, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 59], \"edge\": 79}, {\"x\": -0.1359303859231084, \"source\": 18, \"method\": \"meetings\", \"target\": 59, \"weight\": 1, \"y\": 0.045187628118315865, \"pair\": [18, 59], \"edge\": 79}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"meetings\", \"target\": 60, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 60], \"edge\": 80}, {\"x\": -0.11526915040445217, \"source\": 18, \"method\": \"meetings\", \"target\": 60, \"weight\": 1, \"y\": 0.011303709862621584, \"pair\": [18, 60], \"edge\": 80}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"meetings\", \"target\": 63, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 63], \"edge\": 81}, {\"x\": -0.044333366833317324, \"source\": 18, \"method\": \"meetings\", \"target\": 63, \"weight\": 1, \"y\": 0.03645810553430277, \"pair\": [18, 63], \"edge\": 81}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"meetings\", \"target\": 64, \"weight\": 2, \"y\": -0.024414181320765033, \"pair\": [18, 64], \"edge\": 82}, {\"x\": 0.0015726656641651463, \"source\": 18, \"method\": \"meetings\", \"target\": 64, \"weight\": 2, \"y\": 0.042423991886305794, \"pair\": [18, 64], \"edge\": 82}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 29, \"weight\": 7, \"y\": -0.024414181320765033, \"pair\": [18, 29], \"edge\": 83}, {\"x\": 0.009827164577304493, \"source\": 18, \"method\": \"phonecalls\", \"target\": 29, \"weight\": 7, \"y\": 0.011969150957854575, \"pair\": [18, 29], \"edge\": 83}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"meetings\", \"target\": 65, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 65], \"edge\": 84}, {\"x\": -0.1615845006331375, \"source\": 18, \"method\": \"meetings\", \"target\": 65, \"weight\": 1, \"y\": -0.06451997678060535, \"pair\": [18, 65], \"edge\": 84}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"meetings\", \"target\": 43, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 43], \"edge\": 85}, {\"x\": 0.056427457504084594, \"source\": 18, \"method\": \"meetings\", \"target\": 43, \"weight\": 1, \"y\": 0.023052111387909423, \"pair\": [18, 43], \"edge\": 85}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"meetings\", \"target\": 27, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 27], \"edge\": 86}, {\"x\": 0.07327337847372146, \"source\": 18, \"method\": \"meetings\", \"target\": 27, \"weight\": 1, \"y\": -0.02274334124103196, \"pair\": [18, 27], \"edge\": 86}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 47], \"edge\": 87}, {\"x\": 0.12454620975849745, \"source\": 18, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": 0.004060318457434848, \"pair\": [18, 47], \"edge\": 87}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 71, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 71], \"edge\": 88}, {\"x\": -0.018676323437857613, \"source\": 18, \"method\": \"phonecalls\", \"target\": 71, \"weight\": 1, \"y\": -0.027416187164782193, \"pair\": [18, 71], \"edge\": 88}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"meetings\", \"target\": 76, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 76], \"edge\": 89}, {\"x\": -0.06624873531690086, \"source\": 18, \"method\": \"meetings\", \"target\": 76, \"weight\": 1, \"y\": 0.05558734678423388, \"pair\": [18, 76], \"edge\": 89}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 100], \"edge\": 90}, {\"x\": -0.07196025040825678, \"source\": 18, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": -0.10303828596702301, \"pair\": [18, 100], \"edge\": 90}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 102, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 102], \"edge\": 91}, {\"x\": -0.1734239553872992, \"source\": 18, \"method\": \"phonecalls\", \"target\": 102, \"weight\": 1, \"y\": -0.02449448699782483, \"pair\": [18, 102], \"edge\": 91}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 32, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 32], \"edge\": 92}, {\"x\": -0.11217308042859002, \"source\": 18, \"method\": \"phonecalls\", \"target\": 32, \"weight\": 1, \"y\": -0.052709121764239864, \"pair\": [18, 32], \"edge\": 92}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 103, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 103], \"edge\": 93}, {\"x\": -0.14266031547280894, \"source\": 18, \"method\": \"phonecalls\", \"target\": 103, \"weight\": 1, \"y\": 0.017505601280870555, \"pair\": [18, 103], \"edge\": 93}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 104, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 104], \"edge\": 94}, {\"x\": -0.1651022035957404, \"source\": 18, \"method\": \"phonecalls\", \"target\": 104, \"weight\": 1, \"y\": -0.04497884255293872, \"pair\": [18, 104], \"edge\": 94}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 61, \"weight\": 2, \"y\": -0.024414181320765033, \"pair\": [18, 61], \"edge\": 95}, {\"x\": 0.0765058900082385, \"source\": 18, \"method\": \"phonecalls\", \"target\": 61, \"weight\": 2, \"y\": 0.10550349655861112, \"pair\": [18, 61], \"edge\": 95}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 33, \"weight\": 5, \"y\": -0.024414181320765033, \"pair\": [18, 33], \"edge\": 96}, {\"x\": -0.09023568341000139, \"source\": 18, \"method\": \"phonecalls\", \"target\": 33, \"weight\": 5, \"y\": -0.03912003645737478, \"pair\": [18, 33], \"edge\": 96}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 108, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 108], \"edge\": 97}, {\"x\": -0.15995067451222547, \"source\": 18, \"method\": \"phonecalls\", \"target\": 108, \"weight\": 1, \"y\": 0.010765874210307754, \"pair\": [18, 108], \"edge\": 97}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 127, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 127], \"edge\": 98}, {\"x\": -0.1704206846060183, \"source\": 18, \"method\": \"phonecalls\", \"target\": 127, \"weight\": 1, \"y\": -0.00339958445457216, \"pair\": [18, 127], \"edge\": 98}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 66, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 66], \"edge\": 99}, {\"x\": 0.02562639712079193, \"source\": 18, \"method\": \"phonecalls\", \"target\": 66, \"weight\": 1, \"y\": 0.07139918504611162, \"pair\": [18, 66], \"edge\": 99}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 132, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 132], \"edge\": 100}, {\"x\": -0.1473046573488152, \"source\": 18, \"method\": \"phonecalls\", \"target\": 132, \"weight\": 1, \"y\": -0.010111306146297653, \"pair\": [18, 132], \"edge\": 100}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 125, \"weight\": 2, \"y\": -0.024414181320765033, \"pair\": [18, 125], \"edge\": 101}, {\"x\": 0.017044446665228825, \"source\": 18, \"method\": \"phonecalls\", \"target\": 125, \"weight\": 2, \"y\": -0.06892376678482033, \"pair\": [18, 125], \"edge\": 101}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 140, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 140], \"edge\": 102}, {\"x\": -0.12447358796066915, \"source\": 18, \"method\": \"phonecalls\", \"target\": 140, \"weight\": 1, \"y\": -0.09450098486430009, \"pair\": [18, 140], \"edge\": 102}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 141, \"weight\": 4, \"y\": -0.024414181320765033, \"pair\": [18, 141], \"edge\": 103}, {\"x\": -0.08643426121979292, \"source\": 18, \"method\": \"phonecalls\", \"target\": 141, \"weight\": 4, \"y\": -0.01344136938846853, \"pair\": [18, 141], \"edge\": 103}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 146, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 146], \"edge\": 104}, {\"x\": -0.15130321331834784, \"source\": 18, \"method\": \"phonecalls\", \"target\": 146, \"weight\": 1, \"y\": -0.031167044152272526, \"pair\": [18, 146], \"edge\": 104}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 153, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 153], \"edge\": 105}, {\"x\": -0.138648966183581, \"source\": 18, \"method\": \"phonecalls\", \"target\": 153, \"weight\": 1, \"y\": -0.08021061797451869, \"pair\": [18, 153], \"edge\": 105}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 99, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 99], \"edge\": 106}, {\"x\": -0.10299342519647912, \"source\": 18, \"method\": \"phonecalls\", \"target\": 99, \"weight\": 1, \"y\": -0.07985723383561413, \"pair\": [18, 99], \"edge\": 106}, {\"x\": -0.05818896841716495, \"source\": 18, \"method\": \"phonecalls\", \"target\": 110, \"weight\": 1, \"y\": -0.024414181320765033, \"pair\": [18, 110], \"edge\": 107}, {\"x\": -0.16271620928766461, \"source\": 18, \"method\": \"phonecalls\", \"target\": 110, \"weight\": 1, \"y\": 0.049341122201079037, \"pair\": [18, 110], \"edge\": 107}, {\"x\": -0.07183262544818632, \"source\": 19, \"method\": \"meetings\", \"target\": 20, \"weight\": 1, \"y\": -0.042498896918285775, \"pair\": [19, 20], \"edge\": 108}, {\"x\": -0.12112646250555749, \"source\": 19, \"method\": \"meetings\", \"target\": 20, \"weight\": 1, \"y\": -0.03214720104080549, \"pair\": [19, 20], \"edge\": 108}, {\"x\": -0.07183262544818632, \"source\": 19, \"method\": \"meetings\", \"target\": 32, \"weight\": 1, \"y\": -0.042498896918285775, \"pair\": [19, 32], \"edge\": 109}, {\"x\": -0.11217308042859002, \"source\": 19, \"method\": \"meetings\", \"target\": 32, \"weight\": 1, \"y\": -0.052709121764239864, \"pair\": [19, 32], \"edge\": 109}, {\"x\": -0.07183262544818632, \"source\": 19, \"method\": \"meetings\", \"target\": 33, \"weight\": 2, \"y\": -0.042498896918285775, \"pair\": [19, 33], \"edge\": 110}, {\"x\": -0.09023568341000139, \"source\": 19, \"method\": \"meetings\", \"target\": 33, \"weight\": 2, \"y\": -0.03912003645737478, \"pair\": [19, 33], \"edge\": 110}, {\"x\": -0.07183262544818632, \"source\": 19, \"method\": \"meetings\", \"target\": 34, \"weight\": 1, \"y\": -0.042498896918285775, \"pair\": [19, 34], \"edge\": 111}, {\"x\": -0.06419144111152837, \"source\": 19, \"method\": \"meetings\", \"target\": 34, \"weight\": 1, \"y\": -0.08288517507788387, \"pair\": [19, 34], \"edge\": 111}, {\"x\": -0.07183262544818632, \"source\": 19, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": -0.042498896918285775, \"pair\": [19, 35], \"edge\": 112}, {\"x\": -0.06148592987207431, \"source\": 19, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": -0.06535239641622947, \"pair\": [19, 35], \"edge\": 112}, {\"x\": -0.07183262544818632, \"source\": 19, \"method\": \"meetings\", \"target\": 23, \"weight\": 1, \"y\": -0.042498896918285775, \"pair\": [19, 23], \"edge\": 113}, {\"x\": -0.09202740679831034, \"source\": 19, \"method\": \"meetings\", \"target\": 23, \"weight\": 1, \"y\": -0.06923147365647722, \"pair\": [19, 23], \"edge\": 113}, {\"x\": -0.07183262544818632, \"source\": 19, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": -0.042498896918285775, \"pair\": [19, 25], \"edge\": 114}, {\"x\": -0.0675123612720282, \"source\": 19, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": -0.1365942017604369, \"pair\": [19, 25], \"edge\": 114}, {\"x\": -0.07183262544818632, \"source\": 19, \"method\": \"meetings\", \"target\": 88, \"weight\": 1, \"y\": -0.042498896918285775, \"pair\": [19, 88], \"edge\": 115}, {\"x\": -0.14325802404642687, \"source\": 19, \"method\": \"meetings\", \"target\": 88, \"weight\": 1, \"y\": -0.06367572716925764, \"pair\": [19, 88], \"edge\": 115}, {\"x\": -0.07183262544818632, \"source\": 19, \"method\": \"phonecalls\", \"target\": 22, \"weight\": 1, \"y\": -0.042498896918285775, \"pair\": [19, 22], \"edge\": 116}, {\"x\": -0.06840055701831564, \"source\": 19, \"method\": \"phonecalls\", \"target\": 22, \"weight\": 1, \"y\": 0.02407701603622507, \"pair\": [19, 22], \"edge\": 116}, {\"x\": -0.07183262544818632, \"source\": 19, \"method\": \"phonecalls\", \"target\": 142, \"weight\": 1, \"y\": -0.042498896918285775, \"pair\": [19, 142], \"edge\": 117}, {\"x\": 0.002387980116398114, \"source\": 19, \"method\": \"phonecalls\", \"target\": 142, \"weight\": 1, \"y\": 0.031050859678300714, \"pair\": [19, 142], \"edge\": 117}, {\"x\": -0.06840055701831564, \"source\": 22, \"method\": \"meetings\", \"target\": 23, \"weight\": 2, \"y\": 0.02407701603622507, \"pair\": [22, 23], \"edge\": 118}, {\"x\": -0.09202740679831034, \"source\": 22, \"method\": \"meetings\", \"target\": 23, \"weight\": 2, \"y\": -0.06923147365647722, \"pair\": [22, 23], \"edge\": 118}, {\"x\": -0.06840055701831564, \"source\": 22, \"method\": \"meetings\", \"target\": 29, \"weight\": 2, \"y\": 0.02407701603622507, \"pair\": [22, 29], \"edge\": 119}, {\"x\": 0.009827164577304493, \"source\": 22, \"method\": \"meetings\", \"target\": 29, \"weight\": 2, \"y\": 0.011969150957854575, \"pair\": [22, 29], \"edge\": 119}, {\"x\": -0.06840055701831564, \"source\": 22, \"method\": \"meetings\", \"target\": 30, \"weight\": 1, \"y\": 0.02407701603622507, \"pair\": [22, 30], \"edge\": 120}, {\"x\": -0.03190853169300275, \"source\": 22, \"method\": \"meetings\", \"target\": 30, \"weight\": 1, \"y\": 0.06299391075788366, \"pair\": [22, 30], \"edge\": 120}, {\"x\": -0.06840055701831564, \"source\": 22, \"method\": \"phonecalls\", \"target\": 31, \"weight\": 1, \"y\": 0.02407701603622507, \"pair\": [22, 31], \"edge\": 121}, {\"x\": -0.10917566139158077, \"source\": 22, \"method\": \"phonecalls\", \"target\": 31, \"weight\": 1, \"y\": 0.03036180577307121, \"pair\": [22, 31], \"edge\": 121}, {\"x\": -0.06840055701831564, \"source\": 22, \"method\": \"meetings\", \"target\": 60, \"weight\": 1, \"y\": 0.02407701603622507, \"pair\": [22, 60], \"edge\": 122}, {\"x\": -0.11526915040445217, \"source\": 22, \"method\": \"meetings\", \"target\": 60, \"weight\": 1, \"y\": 0.011303709862621584, \"pair\": [22, 60], \"edge\": 122}, {\"x\": -0.06840055701831564, \"source\": 22, \"method\": \"meetings\", \"target\": 63, \"weight\": 2, \"y\": 0.02407701603622507, \"pair\": [22, 63], \"edge\": 123}, {\"x\": -0.044333366833317324, \"source\": 22, \"method\": \"meetings\", \"target\": 63, \"weight\": 2, \"y\": 0.03645810553430277, \"pair\": [22, 63], \"edge\": 123}, {\"x\": -0.06840055701831564, \"source\": 22, \"method\": \"meetings\", \"target\": 72, \"weight\": 1, \"y\": 0.02407701603622507, \"pair\": [22, 72], \"edge\": 124}, {\"x\": -0.09015529937777945, \"source\": 22, \"method\": \"meetings\", \"target\": 72, \"weight\": 1, \"y\": 0.06859928174123174, \"pair\": [22, 72], \"edge\": 124}, {\"x\": -0.06840055701831564, \"source\": 22, \"method\": \"meetings\", \"target\": 76, \"weight\": 2, \"y\": 0.02407701603622507, \"pair\": [22, 76], \"edge\": 125}, {\"x\": -0.06624873531690086, \"source\": 22, \"method\": \"meetings\", \"target\": 76, \"weight\": 2, \"y\": 0.05558734678423388, \"pair\": [22, 76], \"edge\": 125}, {\"x\": -0.06840055701831564, \"source\": 22, \"method\": \"meetings\", \"target\": 77, \"weight\": 2, \"y\": 0.02407701603622507, \"pair\": [22, 77], \"edge\": 126}, {\"x\": -0.07022785790207993, \"source\": 22, \"method\": \"meetings\", \"target\": 77, \"weight\": 2, \"y\": 0.09344730750644979, \"pair\": [22, 77], \"edge\": 126}, {\"x\": -0.06840055701831564, \"source\": 22, \"method\": \"meetings\", \"target\": 75, \"weight\": 2, \"y\": 0.02407701603622507, \"pair\": [22, 75], \"edge\": 127}, {\"x\": -0.04568987856672662, \"source\": 22, \"method\": \"meetings\", \"target\": 75, \"weight\": 2, \"y\": 0.10916287074655957, \"pair\": [22, 75], \"edge\": 127}, {\"x\": -0.06840055701831564, \"source\": 22, \"method\": \"meetings\", \"target\": 78, \"weight\": 1, \"y\": 0.02407701603622507, \"pair\": [22, 78], \"edge\": 128}, {\"x\": -0.11391220001392693, \"source\": 22, \"method\": \"meetings\", \"target\": 78, \"weight\": 1, \"y\": 0.09289604964041172, \"pair\": [22, 78], \"edge\": 128}, {\"x\": -0.06840055701831564, \"source\": 22, \"method\": \"meetings\", \"target\": 98, \"weight\": 1, \"y\": 0.02407701603622507, \"pair\": [22, 98], \"edge\": 129}, {\"x\": -0.047198362494428214, \"source\": 22, \"method\": \"meetings\", \"target\": 98, \"weight\": 1, \"y\": 0.06363298454542243, \"pair\": [22, 98], \"edge\": 129}, {\"x\": -0.06840055701831564, \"source\": 22, \"method\": \"meetings\", \"target\": 99, \"weight\": 1, \"y\": 0.02407701603622507, \"pair\": [22, 99], \"edge\": 130}, {\"x\": -0.10299342519647912, \"source\": 22, \"method\": \"meetings\", \"target\": 99, \"weight\": 1, \"y\": -0.07985723383561413, \"pair\": [22, 99], \"edge\": 130}, {\"x\": -0.09202740679831034, \"source\": 23, \"method\": \"meetings\", \"target\": 24, \"weight\": 1, \"y\": -0.06923147365647722, \"pair\": [23, 24], \"edge\": 131}, {\"x\": -0.18997259716385778, \"source\": 23, \"method\": \"meetings\", \"target\": 24, \"weight\": 1, \"y\": -0.11066245210923148, \"pair\": [23, 24], \"edge\": 131}, {\"x\": -0.09202740679831034, \"source\": 23, \"method\": \"meetings\", \"target\": 34, \"weight\": 1, \"y\": -0.06923147365647722, \"pair\": [23, 34], \"edge\": 132}, {\"x\": -0.06419144111152837, \"source\": 23, \"method\": \"meetings\", \"target\": 34, \"weight\": 1, \"y\": -0.08288517507788387, \"pair\": [23, 34], \"edge\": 132}, {\"x\": -0.09202740679831034, \"source\": 23, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": -0.06923147365647722, \"pair\": [23, 35], \"edge\": 133}, {\"x\": -0.06148592987207431, \"source\": 23, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": -0.06535239641622947, \"pair\": [23, 35], \"edge\": 133}, {\"x\": -0.09202740679831034, \"source\": 23, \"method\": \"meetings\", \"target\": 25, \"weight\": 2, \"y\": -0.06923147365647722, \"pair\": [23, 25], \"edge\": 134}, {\"x\": -0.0675123612720282, \"source\": 23, \"method\": \"meetings\", \"target\": 25, \"weight\": 2, \"y\": -0.1365942017604369, \"pair\": [23, 25], \"edge\": 134}, {\"x\": -0.09202740679831034, \"source\": 23, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": -0.06923147365647722, \"pair\": [23, 100], \"edge\": 135}, {\"x\": -0.07196025040825678, \"source\": 23, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": -0.10303828596702301, \"pair\": [23, 100], \"edge\": 135}, {\"x\": -0.09202740679831034, \"source\": 23, \"method\": \"phonecalls\", \"target\": 99, \"weight\": 5, \"y\": -0.06923147365647722, \"pair\": [23, 99], \"edge\": 136}, {\"x\": -0.10299342519647912, \"source\": 23, \"method\": \"phonecalls\", \"target\": 99, \"weight\": 5, \"y\": -0.07985723383561413, \"pair\": [23, 99], \"edge\": 136}, {\"x\": -0.0675123612720282, \"source\": 25, \"method\": \"meetings\", \"target\": 26, \"weight\": 1, \"y\": -0.1365942017604369, \"pair\": [25, 26], \"edge\": 137}, {\"x\": 0.01555844066934661, \"source\": 25, \"method\": \"meetings\", \"target\": 26, \"weight\": 1, \"y\": -0.11196722037829362, \"pair\": [25, 26], \"edge\": 137}, {\"x\": -0.0675123612720282, \"source\": 25, \"method\": \"meetings\", \"target\": 27, \"weight\": 1, \"y\": -0.1365942017604369, \"pair\": [25, 27], \"edge\": 138}, {\"x\": 0.07327337847372146, \"source\": 25, \"method\": \"meetings\", \"target\": 27, \"weight\": 1, \"y\": -0.02274334124103196, \"pair\": [25, 27], \"edge\": 138}, {\"x\": -0.0675123612720282, \"source\": 25, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": -0.1365942017604369, \"pair\": [25, 28], \"edge\": 139}, {\"x\": -0.09992051427352441, \"source\": 25, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": -0.21558223756616396, \"pair\": [25, 28], \"edge\": 139}, {\"x\": -0.0675123612720282, \"source\": 25, \"method\": \"meetings\", \"target\": 34, \"weight\": 2, \"y\": -0.1365942017604369, \"pair\": [25, 34], \"edge\": 140}, {\"x\": -0.06419144111152837, \"source\": 25, \"method\": \"meetings\", \"target\": 34, \"weight\": 2, \"y\": -0.08288517507788387, \"pair\": [25, 34], \"edge\": 140}, {\"x\": -0.0675123612720282, \"source\": 25, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": -0.1365942017604369, \"pair\": [25, 35], \"edge\": 141}, {\"x\": -0.06148592987207431, \"source\": 25, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": -0.06535239641622947, \"pair\": [25, 35], \"edge\": 141}, {\"x\": -0.0675123612720282, \"source\": 25, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": -0.1365942017604369, \"pair\": [25, 100], \"edge\": 142}, {\"x\": -0.07196025040825678, \"source\": 25, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": -0.10303828596702301, \"pair\": [25, 100], \"edge\": 142}, {\"x\": 0.01555844066934661, \"source\": 26, \"method\": \"meetings\", \"target\": 27, \"weight\": 1, \"y\": -0.11196722037829362, \"pair\": [26, 27], \"edge\": 143}, {\"x\": 0.07327337847372146, \"source\": 26, \"method\": \"meetings\", \"target\": 27, \"weight\": 1, \"y\": -0.02274334124103196, \"pair\": [26, 27], \"edge\": 143}, {\"x\": 0.07327337847372146, \"source\": 27, \"method\": \"meetings\", \"target\": 38, \"weight\": 1, \"y\": -0.02274334124103196, \"pair\": [27, 38], \"edge\": 144}, {\"x\": 0.13632901182577112, \"source\": 27, \"method\": \"meetings\", \"target\": 38, \"weight\": 1, \"y\": -0.0375193104803328, \"pair\": [27, 38], \"edge\": 144}, {\"x\": 0.07327337847372146, \"source\": 27, \"method\": \"meetings\", \"target\": 45, \"weight\": 3, \"y\": -0.02274334124103196, \"pair\": [27, 45], \"edge\": 145}, {\"x\": 0.14221237687469263, \"source\": 27, \"method\": \"meetings\", \"target\": 45, \"weight\": 3, \"y\": 0.00591156559326728, \"pair\": [27, 45], \"edge\": 145}, {\"x\": 0.07327337847372146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 1, \"y\": -0.02274334124103196, \"pair\": [27, 51], \"edge\": 146}, {\"x\": 0.12670173276621177, \"source\": 27, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 1, \"y\": -0.02940877817730907, \"pair\": [27, 51], \"edge\": 146}, {\"x\": 0.07327337847372146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 43, \"weight\": 2, \"y\": -0.02274334124103196, \"pair\": [27, 43], \"edge\": 147}, {\"x\": 0.056427457504084594, \"source\": 27, \"method\": \"phonecalls\", \"target\": 43, \"weight\": 2, \"y\": 0.023052111387909423, \"pair\": [27, 43], \"edge\": 147}, {\"x\": 0.07327337847372146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 3, \"y\": -0.02274334124103196, \"pair\": [27, 47], \"edge\": 148}, {\"x\": 0.12454620975849745, \"source\": 27, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 3, \"y\": 0.004060318457434848, \"pair\": [27, 47], \"edge\": 148}, {\"x\": 0.07327337847372146, \"source\": 27, \"method\": \"meetings\", \"target\": 29, \"weight\": 3, \"y\": -0.02274334124103196, \"pair\": [27, 29], \"edge\": 149}, {\"x\": 0.009827164577304493, \"source\": 27, \"method\": \"meetings\", \"target\": 29, \"weight\": 3, \"y\": 0.011969150957854575, \"pair\": [27, 29], \"edge\": 149}, {\"x\": 0.07327337847372146, \"source\": 27, \"method\": \"meetings\", \"target\": 64, \"weight\": 1, \"y\": -0.02274334124103196, \"pair\": [27, 64], \"edge\": 150}, {\"x\": 0.0015726656641651463, \"source\": 27, \"method\": \"meetings\", \"target\": 64, \"weight\": 1, \"y\": 0.042423991886305794, \"pair\": [27, 64], \"edge\": 150}, {\"x\": 0.07327337847372146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": -0.02274334124103196, \"pair\": [27, 68], \"edge\": 151}, {\"x\": 0.12930838529187758, \"source\": 27, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": 0.04930294572899866, \"pair\": [27, 68], \"edge\": 151}, {\"x\": 0.07327337847372146, \"source\": 27, \"method\": \"meetings\", \"target\": 44, \"weight\": 1, \"y\": -0.02274334124103196, \"pair\": [27, 44], \"edge\": 152}, {\"x\": 0.08281810895160242, \"source\": 27, \"method\": \"meetings\", \"target\": 44, \"weight\": 1, \"y\": -0.012893146467164888, \"pair\": [27, 44], \"edge\": 152}, {\"x\": 0.07327337847372146, \"source\": 27, \"method\": \"meetings\", \"target\": 61, \"weight\": 1, \"y\": -0.02274334124103196, \"pair\": [27, 61], \"edge\": 153}, {\"x\": 0.0765058900082385, \"source\": 27, \"method\": \"meetings\", \"target\": 61, \"weight\": 1, \"y\": 0.10550349655861112, \"pair\": [27, 61], \"edge\": 153}, {\"x\": 0.07327337847372146, \"source\": 27, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": -0.02274334124103196, \"pair\": [27, 89], \"edge\": 154}, {\"x\": 0.1306705348653082, \"source\": 27, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": -0.0008491090228239166, \"pair\": [27, 89], \"edge\": 154}, {\"x\": 0.07327337847372146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 70, \"weight\": 1, \"y\": -0.02274334124103196, \"pair\": [27, 70], \"edge\": 155}, {\"x\": 0.11520085475572568, \"source\": 27, \"method\": \"phonecalls\", \"target\": 70, \"weight\": 1, \"y\": 0.027593125185657312, \"pair\": [27, 70], \"edge\": 155}, {\"x\": 0.07327337847372146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 97, \"weight\": 1, \"y\": -0.02274334124103196, \"pair\": [27, 97], \"edge\": 156}, {\"x\": 0.1052822345107166, \"source\": 27, \"method\": \"phonecalls\", \"target\": 97, \"weight\": 1, \"y\": 0.0013827111019374546, \"pair\": [27, 97], \"edge\": 156}, {\"x\": 0.07327337847372146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 119, \"weight\": 1, \"y\": -0.02274334124103196, \"pair\": [27, 119], \"edge\": 157}, {\"x\": 0.10204379289386956, \"source\": 27, \"method\": \"phonecalls\", \"target\": 119, \"weight\": 1, \"y\": -0.11461680183343823, \"pair\": [27, 119], \"edge\": 157}, {\"x\": 0.07327337847372146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 126, \"weight\": 1, \"y\": -0.02274334124103196, \"pair\": [27, 126], \"edge\": 158}, {\"x\": 0.08055317585968884, \"source\": 27, \"method\": \"phonecalls\", \"target\": 126, \"weight\": 1, \"y\": -0.11887342653247886, \"pair\": [27, 126], \"edge\": 158}, {\"x\": 0.07327337847372146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 136, \"weight\": 1, \"y\": -0.02274334124103196, \"pair\": [27, 136], \"edge\": 159}, {\"x\": 0.08415385728694366, \"source\": 27, \"method\": \"phonecalls\", \"target\": 136, \"weight\": 1, \"y\": -0.10002749813791104, \"pair\": [27, 136], \"edge\": 159}, {\"x\": 0.07327337847372146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 138, \"weight\": 1, \"y\": -0.02274334124103196, \"pair\": [27, 138], \"edge\": 160}, {\"x\": 0.12003295515418039, \"source\": 27, \"method\": \"phonecalls\", \"target\": 138, \"weight\": 1, \"y\": -0.11216833071760567, \"pair\": [27, 138], \"edge\": 160}, {\"x\": 0.07327337847372146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 36, \"weight\": 1, \"y\": -0.02274334124103196, \"pair\": [27, 36], \"edge\": 161}, {\"x\": 0.1711484866389356, \"source\": 27, \"method\": \"phonecalls\", \"target\": 36, \"weight\": 1, \"y\": 0.047483080270855134, \"pair\": [27, 36], \"edge\": 161}, {\"x\": 0.009827164577304493, \"source\": 29, \"method\": \"meetings\", \"target\": 30, \"weight\": 1, \"y\": 0.011969150957854575, \"pair\": [29, 30], \"edge\": 162}, {\"x\": -0.03190853169300275, \"source\": 29, \"method\": \"meetings\", \"target\": 30, \"weight\": 1, \"y\": 0.06299391075788366, \"pair\": [29, 30], \"edge\": 162}, {\"x\": 0.009827164577304493, \"source\": 29, \"method\": \"phonecalls\", \"target\": 64, \"weight\": 1, \"y\": 0.011969150957854575, \"pair\": [29, 64], \"edge\": 163}, {\"x\": 0.0015726656641651463, \"source\": 29, \"method\": \"phonecalls\", \"target\": 64, \"weight\": 1, \"y\": 0.042423991886305794, \"pair\": [29, 64], \"edge\": 163}, {\"x\": 0.009827164577304493, \"source\": 29, \"method\": \"phonecalls\", \"target\": 54, \"weight\": 1, \"y\": 0.011969150957854575, \"pair\": [29, 54], \"edge\": 164}, {\"x\": 0.09471192141615767, \"source\": 29, \"method\": \"phonecalls\", \"target\": 54, \"weight\": 1, \"y\": 0.01767593882590723, \"pair\": [29, 54], \"edge\": 164}, {\"x\": 0.009827164577304493, \"source\": 29, \"method\": \"phonecalls\", \"target\": 43, \"weight\": 4, \"y\": 0.011969150957854575, \"pair\": [29, 43], \"edge\": 165}, {\"x\": 0.056427457504084594, \"source\": 29, \"method\": \"phonecalls\", \"target\": 43, \"weight\": 4, \"y\": 0.023052111387909423, \"pair\": [29, 43], \"edge\": 165}, {\"x\": 0.009827164577304493, \"source\": 29, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": 0.011969150957854575, \"pair\": [29, 68], \"edge\": 166}, {\"x\": 0.12930838529187758, \"source\": 29, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": 0.04930294572899866, \"pair\": [29, 68], \"edge\": 166}, {\"x\": 0.009827164577304493, \"source\": 29, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 1, \"y\": 0.011969150957854575, \"pair\": [29, 47], \"edge\": 167}, {\"x\": 0.12454620975849745, \"source\": 29, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 1, \"y\": 0.004060318457434848, \"pair\": [29, 47], \"edge\": 167}, {\"x\": 0.009827164577304493, \"source\": 29, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 1, \"y\": 0.011969150957854575, \"pair\": [29, 51], \"edge\": 168}, {\"x\": 0.12670173276621177, \"source\": 29, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 1, \"y\": -0.02940877817730907, \"pair\": [29, 51], \"edge\": 168}, {\"x\": 0.009827164577304493, \"source\": 29, \"method\": \"meetings\", \"target\": 71, \"weight\": 1, \"y\": 0.011969150957854575, \"pair\": [29, 71], \"edge\": 169}, {\"x\": -0.018676323437857613, \"source\": 29, \"method\": \"meetings\", \"target\": 71, \"weight\": 1, \"y\": -0.027416187164782193, \"pair\": [29, 71], \"edge\": 169}, {\"x\": 0.009827164577304493, \"source\": 29, \"method\": \"meetings\", \"target\": 75, \"weight\": 1, \"y\": 0.011969150957854575, \"pair\": [29, 75], \"edge\": 170}, {\"x\": -0.04568987856672662, \"source\": 29, \"method\": \"meetings\", \"target\": 75, \"weight\": 1, \"y\": 0.10916287074655957, \"pair\": [29, 75], \"edge\": 170}, {\"x\": 0.009827164577304493, \"source\": 29, \"method\": \"meetings\", \"target\": 76, \"weight\": 1, \"y\": 0.011969150957854575, \"pair\": [29, 76], \"edge\": 171}, {\"x\": -0.06624873531690086, \"source\": 29, \"method\": \"meetings\", \"target\": 76, \"weight\": 1, \"y\": 0.05558734678423388, \"pair\": [29, 76], \"edge\": 171}, {\"x\": 0.009827164577304493, \"source\": 29, \"method\": \"phonecalls\", \"target\": 63, \"weight\": 1, \"y\": 0.011969150957854575, \"pair\": [29, 63], \"edge\": 172}, {\"x\": -0.044333366833317324, \"source\": 29, \"method\": \"phonecalls\", \"target\": 63, \"weight\": 1, \"y\": 0.03645810553430277, \"pair\": [29, 63], \"edge\": 172}, {\"x\": 0.009827164577304493, \"source\": 29, \"method\": \"phonecalls\", \"target\": 98, \"weight\": 1, \"y\": 0.011969150957854575, \"pair\": [29, 98], \"edge\": 173}, {\"x\": -0.047198362494428214, \"source\": 29, \"method\": \"phonecalls\", \"target\": 98, \"weight\": 1, \"y\": 0.06363298454542243, \"pair\": [29, 98], \"edge\": 173}, {\"x\": 0.009827164577304493, \"source\": 29, \"method\": \"phonecalls\", \"target\": 139, \"weight\": 1, \"y\": 0.011969150957854575, \"pair\": [29, 139], \"edge\": 174}, {\"x\": 0.015899068650726215, \"source\": 29, \"method\": \"phonecalls\", \"target\": 139, \"weight\": 1, \"y\": 0.06488966703901473, \"pair\": [29, 139], \"edge\": 174}, {\"x\": -0.11217308042859002, \"source\": 32, \"method\": \"meetings\", \"target\": 33, \"weight\": 1, \"y\": -0.052709121764239864, \"pair\": [32, 33], \"edge\": 175}, {\"x\": -0.09023568341000139, \"source\": 32, \"method\": \"meetings\", \"target\": 33, \"weight\": 1, \"y\": -0.03912003645737478, \"pair\": [32, 33], \"edge\": 175}, {\"x\": -0.09023568341000139, \"source\": 33, \"method\": \"meetings\", \"target\": 88, \"weight\": 1, \"y\": -0.03912003645737478, \"pair\": [33, 88], \"edge\": 176}, {\"x\": -0.14325802404642687, \"source\": 33, \"method\": \"meetings\", \"target\": 88, \"weight\": 1, \"y\": -0.06367572716925764, \"pair\": [33, 88], \"edge\": 176}, {\"x\": -0.06419144111152837, \"source\": 34, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": -0.08288517507788387, \"pair\": [34, 35], \"edge\": 177}, {\"x\": -0.06148592987207431, \"source\": 34, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": -0.06535239641622947, \"pair\": [34, 35], \"edge\": 177}, {\"x\": -0.06419144111152837, \"source\": 34, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": -0.08288517507788387, \"pair\": [34, 100], \"edge\": 178}, {\"x\": -0.07196025040825678, \"source\": 34, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": -0.10303828596702301, \"pair\": [34, 100], \"edge\": 178}, {\"x\": 0.1711484866389356, \"source\": 36, \"method\": \"meetings\", \"target\": 37, \"weight\": 1, \"y\": 0.047483080270855134, \"pair\": [36, 37], \"edge\": 179}, {\"x\": 0.21396573515508657, \"source\": 36, \"method\": \"meetings\", \"target\": 37, \"weight\": 1, \"y\": 0.10606993993951315, \"pair\": [36, 37], \"edge\": 179}, {\"x\": 0.1711484866389356, \"source\": 36, \"method\": \"meetings\", \"target\": 45, \"weight\": 2, \"y\": 0.047483080270855134, \"pair\": [36, 45], \"edge\": 180}, {\"x\": 0.14221237687469263, \"source\": 36, \"method\": \"meetings\", \"target\": 45, \"weight\": 2, \"y\": 0.00591156559326728, \"pair\": [36, 45], \"edge\": 180}, {\"x\": 0.1711484866389356, \"source\": 36, \"method\": \"meetings\", \"target\": 46, \"weight\": 1, \"y\": 0.047483080270855134, \"pair\": [36, 46], \"edge\": 181}, {\"x\": 0.19303499834114946, \"source\": 36, \"method\": \"meetings\", \"target\": 46, \"weight\": 1, \"y\": 0.015416290846902747, \"pair\": [36, 46], \"edge\": 181}, {\"x\": 0.1711484866389356, \"source\": 36, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 2, \"y\": 0.047483080270855134, \"pair\": [36, 47], \"edge\": 182}, {\"x\": 0.12454620975849745, \"source\": 36, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 2, \"y\": 0.004060318457434848, \"pair\": [36, 47], \"edge\": 182}, {\"x\": 0.1711484866389356, \"source\": 36, \"method\": \"meetings\", \"target\": 48, \"weight\": 2, \"y\": 0.047483080270855134, \"pair\": [36, 48], \"edge\": 183}, {\"x\": 0.16226555310120305, \"source\": 36, \"method\": \"meetings\", \"target\": 48, \"weight\": 2, \"y\": -0.005004194335036632, \"pair\": [36, 48], \"edge\": 183}, {\"x\": 0.1711484866389356, \"source\": 36, \"method\": \"meetings\", \"target\": 91, \"weight\": 1, \"y\": 0.047483080270855134, \"pair\": [36, 91], \"edge\": 184}, {\"x\": 0.27682614708816616, \"source\": 36, \"method\": \"meetings\", \"target\": 91, \"weight\": 1, \"y\": 0.04143079610206784, \"pair\": [36, 91], \"edge\": 184}, {\"x\": 0.1711484866389356, \"source\": 36, \"method\": \"meetings\", \"target\": 92, \"weight\": 1, \"y\": 0.047483080270855134, \"pair\": [36, 92], \"edge\": 185}, {\"x\": 0.27652528725392855, \"source\": 36, \"method\": \"meetings\", \"target\": 92, \"weight\": 1, \"y\": 0.05921266905758577, \"pair\": [36, 92], \"edge\": 185}, {\"x\": 0.1711484866389356, \"source\": 36, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": 0.047483080270855134, \"pair\": [36, 68], \"edge\": 186}, {\"x\": 0.12930838529187758, \"source\": 36, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": 0.04930294572899866, \"pair\": [36, 68], \"edge\": 186}, {\"x\": 0.1711484866389356, \"source\": 36, \"method\": \"phonecalls\", \"target\": 148, \"weight\": 1, \"y\": 0.047483080270855134, \"pair\": [36, 148], \"edge\": 187}, {\"x\": 0.27166775912051255, \"source\": 36, \"method\": \"phonecalls\", \"target\": 148, \"weight\": 1, \"y\": 0.08037294871408766, \"pair\": [36, 148], \"edge\": 187}, {\"x\": 0.1711484866389356, \"source\": 36, \"method\": \"phonecalls\", \"target\": 61, \"weight\": 2, \"y\": 0.047483080270855134, \"pair\": [36, 61], \"edge\": 188}, {\"x\": 0.0765058900082385, \"source\": 36, \"method\": \"phonecalls\", \"target\": 61, \"weight\": 2, \"y\": 0.10550349655861112, \"pair\": [36, 61], \"edge\": 188}, {\"x\": 0.21396573515508657, \"source\": 37, \"method\": \"meetings\", \"target\": 67, \"weight\": 1, \"y\": 0.10606993993951315, \"pair\": [37, 67], \"edge\": 189}, {\"x\": 0.12512946736741998, \"source\": 37, \"method\": \"meetings\", \"target\": 67, \"weight\": 1, \"y\": 0.12420284401595356, \"pair\": [37, 67], \"edge\": 189}, {\"x\": 0.13632901182577112, \"source\": 38, \"method\": \"meetings\", \"target\": 93, \"weight\": 2, \"y\": -0.0375193104803328, \"pair\": [38, 93], \"edge\": 190}, {\"x\": 0.1493938975418612, \"source\": 38, \"method\": \"meetings\", \"target\": 93, \"weight\": 2, \"y\": -0.013586736562355435, \"pair\": [38, 93], \"edge\": 190}, {\"x\": 0.13632901182577112, \"source\": 38, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": -0.0375193104803328, \"pair\": [38, 89], \"edge\": 191}, {\"x\": 0.1306705348653082, \"source\": 38, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": -0.0008491090228239166, \"pair\": [38, 89], \"edge\": 191}, {\"x\": 0.22942764596132692, \"source\": 39, \"method\": \"meetings\", \"target\": 40, \"weight\": 1, \"y\": -0.04293102853456445, \"pair\": [39, 40], \"edge\": 192}, {\"x\": 0.2593039083738925, \"source\": 39, \"method\": \"meetings\", \"target\": 40, \"weight\": 1, \"y\": -0.0418438258327181, \"pair\": [39, 40], \"edge\": 192}, {\"x\": 0.22942764596132692, \"source\": 39, \"method\": \"meetings\", \"target\": 41, \"weight\": 1, \"y\": -0.04293102853456445, \"pair\": [39, 41], \"edge\": 193}, {\"x\": 0.3054043272464784, \"source\": 39, \"method\": \"meetings\", \"target\": 41, \"weight\": 1, \"y\": -0.07497173532596402, \"pair\": [39, 41], \"edge\": 193}, {\"x\": 0.22942764596132692, \"source\": 39, \"method\": \"meetings\", \"target\": 42, \"weight\": 1, \"y\": -0.04293102853456445, \"pair\": [39, 42], \"edge\": 194}, {\"x\": 0.3125494004778727, \"source\": 39, \"method\": \"meetings\", \"target\": 42, \"weight\": 1, \"y\": -0.05834126928511468, \"pair\": [39, 42], \"edge\": 194}, {\"x\": 0.22942764596132692, \"source\": 39, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": -0.04293102853456445, \"pair\": [39, 47], \"edge\": 195}, {\"x\": 0.12454620975849745, \"source\": 39, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": 0.004060318457434848, \"pair\": [39, 47], \"edge\": 195}, {\"x\": 0.22942764596132692, \"source\": 39, \"method\": \"meetings\", \"target\": 49, \"weight\": 1, \"y\": -0.04293102853456445, \"pair\": [39, 49], \"edge\": 196}, {\"x\": 0.1719680111471714, \"source\": 39, \"method\": \"meetings\", \"target\": 49, \"weight\": 1, \"y\": -0.0607803710553539, \"pair\": [39, 49], \"edge\": 196}, {\"x\": 0.22942764596132692, \"source\": 39, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": -0.04293102853456445, \"pair\": [39, 50], \"edge\": 197}, {\"x\": 0.16961919495454755, \"source\": 39, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": -0.026124506098854063, \"pair\": [39, 50], \"edge\": 197}, {\"x\": 0.22942764596132692, \"source\": 39, \"method\": \"meetings\", \"target\": 48, \"weight\": 1, \"y\": -0.04293102853456445, \"pair\": [39, 48], \"edge\": 198}, {\"x\": 0.16226555310120305, \"source\": 39, \"method\": \"meetings\", \"target\": 48, \"weight\": 1, \"y\": -0.005004194335036632, \"pair\": [39, 48], \"edge\": 198}, {\"x\": 0.2593039083738925, \"source\": 40, \"method\": \"meetings\", \"target\": 41, \"weight\": 1, \"y\": -0.0418438258327181, \"pair\": [40, 41], \"edge\": 199}, {\"x\": 0.3054043272464784, \"source\": 40, \"method\": \"meetings\", \"target\": 41, \"weight\": 1, \"y\": -0.07497173532596402, \"pair\": [40, 41], \"edge\": 199}, {\"x\": 0.2593039083738925, \"source\": 40, \"method\": \"meetings\", \"target\": 42, \"weight\": 1, \"y\": -0.0418438258327181, \"pair\": [40, 42], \"edge\": 200}, {\"x\": 0.3125494004778727, \"source\": 40, \"method\": \"meetings\", \"target\": 42, \"weight\": 1, \"y\": -0.05834126928511468, \"pair\": [40, 42], \"edge\": 200}, {\"x\": 0.2593039083738925, \"source\": 40, \"method\": \"phonecalls\", \"target\": 45, \"weight\": 1, \"y\": -0.0418438258327181, \"pair\": [40, 45], \"edge\": 201}, {\"x\": 0.14221237687469263, \"source\": 40, \"method\": \"phonecalls\", \"target\": 45, \"weight\": 1, \"y\": 0.00591156559326728, \"pair\": [40, 45], \"edge\": 201}, {\"x\": 0.3054043272464784, \"source\": 41, \"method\": \"meetings\", \"target\": 42, \"weight\": 1, \"y\": -0.07497173532596402, \"pair\": [41, 42], \"edge\": 202}, {\"x\": 0.3125494004778727, \"source\": 41, \"method\": \"meetings\", \"target\": 42, \"weight\": 1, \"y\": -0.05834126928511468, \"pair\": [41, 42], \"edge\": 202}, {\"x\": 0.056427457504084594, \"source\": 43, \"method\": \"meetings\", \"target\": 44, \"weight\": 1, \"y\": 0.023052111387909423, \"pair\": [43, 44], \"edge\": 203}, {\"x\": 0.08281810895160242, \"source\": 43, \"method\": \"meetings\", \"target\": 44, \"weight\": 1, \"y\": -0.012893146467164888, \"pair\": [43, 44], \"edge\": 203}, {\"x\": 0.056427457504084594, \"source\": 43, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 4, \"y\": 0.023052111387909423, \"pair\": [43, 47], \"edge\": 204}, {\"x\": 0.12454620975849745, \"source\": 43, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 4, \"y\": 0.004060318457434848, \"pair\": [43, 47], \"edge\": 204}, {\"x\": 0.056427457504084594, \"source\": 43, \"method\": \"meetings\", \"target\": 64, \"weight\": 2, \"y\": 0.023052111387909423, \"pair\": [43, 64], \"edge\": 205}, {\"x\": 0.0015726656641651463, \"source\": 43, \"method\": \"meetings\", \"target\": 64, \"weight\": 2, \"y\": 0.042423991886305794, \"pair\": [43, 64], \"edge\": 205}, {\"x\": 0.056427457504084594, \"source\": 43, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": 0.023052111387909423, \"pair\": [43, 68], \"edge\": 206}, {\"x\": 0.12930838529187758, \"source\": 43, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": 0.04930294572899866, \"pair\": [43, 68], \"edge\": 206}, {\"x\": 0.056427457504084594, \"source\": 43, \"method\": \"meetings\", \"target\": 51, \"weight\": 1, \"y\": 0.023052111387909423, \"pair\": [43, 51], \"edge\": 207}, {\"x\": 0.12670173276621177, \"source\": 43, \"method\": \"meetings\", \"target\": 51, \"weight\": 1, \"y\": -0.02940877817730907, \"pair\": [43, 51], \"edge\": 207}, {\"x\": 0.056427457504084594, \"source\": 43, \"method\": \"meetings\", \"target\": 61, \"weight\": 1, \"y\": 0.023052111387909423, \"pair\": [43, 61], \"edge\": 208}, {\"x\": 0.0765058900082385, \"source\": 43, \"method\": \"meetings\", \"target\": 61, \"weight\": 1, \"y\": 0.10550349655861112, \"pair\": [43, 61], \"edge\": 208}, {\"x\": 0.056427457504084594, \"source\": 43, \"method\": \"phonecalls\", \"target\": 120, \"weight\": 1, \"y\": 0.023052111387909423, \"pair\": [43, 120], \"edge\": 209}, {\"x\": 0.04272606552864228, \"source\": 43, \"method\": \"phonecalls\", \"target\": 120, \"weight\": 1, \"y\": 0.11118288494544035, \"pair\": [43, 120], \"edge\": 209}, {\"x\": 0.056427457504084594, \"source\": 43, \"method\": \"phonecalls\", \"target\": 139, \"weight\": 1, \"y\": 0.023052111387909423, \"pair\": [43, 139], \"edge\": 210}, {\"x\": 0.015899068650726215, \"source\": 43, \"method\": \"phonecalls\", \"target\": 139, \"weight\": 1, \"y\": 0.06488966703901473, \"pair\": [43, 139], \"edge\": 210}, {\"x\": 0.08281810895160242, \"source\": 44, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": -0.012893146467164888, \"pair\": [44, 47], \"edge\": 211}, {\"x\": 0.12454620975849745, \"source\": 44, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": 0.004060318457434848, \"pair\": [44, 47], \"edge\": 211}, {\"x\": 0.14221237687469263, \"source\": 45, \"method\": \"meetings\", \"target\": 46, \"weight\": 1, \"y\": 0.00591156559326728, \"pair\": [45, 46], \"edge\": 212}, {\"x\": 0.19303499834114946, \"source\": 45, \"method\": \"meetings\", \"target\": 46, \"weight\": 1, \"y\": 0.015416290846902747, \"pair\": [45, 46], \"edge\": 212}, {\"x\": 0.14221237687469263, \"source\": 45, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 1, \"y\": 0.00591156559326728, \"pair\": [45, 47], \"edge\": 213}, {\"x\": 0.12454620975849745, \"source\": 45, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 1, \"y\": 0.004060318457434848, \"pair\": [45, 47], \"edge\": 213}, {\"x\": 0.14221237687469263, \"source\": 45, \"method\": \"meetings\", \"target\": 48, \"weight\": 1, \"y\": 0.00591156559326728, \"pair\": [45, 48], \"edge\": 214}, {\"x\": 0.16226555310120305, \"source\": 45, \"method\": \"meetings\", \"target\": 48, \"weight\": 1, \"y\": -0.005004194335036632, \"pair\": [45, 48], \"edge\": 214}, {\"x\": 0.14221237687469263, \"source\": 45, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 1, \"y\": 0.00591156559326728, \"pair\": [45, 51], \"edge\": 215}, {\"x\": 0.12670173276621177, \"source\": 45, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 1, \"y\": -0.02940877817730907, \"pair\": [45, 51], \"edge\": 215}, {\"x\": 0.14221237687469263, \"source\": 45, \"method\": \"phonecalls\", \"target\": 54, \"weight\": 1, \"y\": 0.00591156559326728, \"pair\": [45, 54], \"edge\": 216}, {\"x\": 0.09471192141615767, \"source\": 45, \"method\": \"phonecalls\", \"target\": 54, \"weight\": 1, \"y\": 0.01767593882590723, \"pair\": [45, 54], \"edge\": 216}, {\"x\": 0.14221237687469263, \"source\": 45, \"method\": \"meetings\", \"target\": 55, \"weight\": 1, \"y\": 0.00591156559326728, \"pair\": [45, 55], \"edge\": 217}, {\"x\": 0.20330986542290788, \"source\": 45, \"method\": \"meetings\", \"target\": 55, \"weight\": 1, \"y\": 0.026610690376777588, \"pair\": [45, 55], \"edge\": 217}, {\"x\": 0.14221237687469263, \"source\": 45, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": 0.00591156559326728, \"pair\": [45, 50], \"edge\": 218}, {\"x\": 0.16961919495454755, \"source\": 45, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": -0.026124506098854063, \"pair\": [45, 50], \"edge\": 218}, {\"x\": 0.14221237687469263, \"source\": 45, \"method\": \"meetings\", \"target\": 89, \"weight\": 2, \"y\": 0.00591156559326728, \"pair\": [45, 89], \"edge\": 219}, {\"x\": 0.1306705348653082, \"source\": 45, \"method\": \"meetings\", \"target\": 89, \"weight\": 2, \"y\": -0.0008491090228239166, \"pair\": [45, 89], \"edge\": 219}, {\"x\": 0.14221237687469263, \"source\": 45, \"method\": \"meetings\", \"target\": 94, \"weight\": 1, \"y\": 0.00591156559326728, \"pair\": [45, 94], \"edge\": 220}, {\"x\": 0.21030166093408367, \"source\": 45, \"method\": \"meetings\", \"target\": 94, \"weight\": 1, \"y\": -0.014619795101153854, \"pair\": [45, 94], \"edge\": 220}, {\"x\": 0.14221237687469263, \"source\": 45, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": 0.00591156559326728, \"pair\": [45, 68], \"edge\": 221}, {\"x\": 0.12930838529187758, \"source\": 45, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": 0.04930294572899866, \"pair\": [45, 68], \"edge\": 221}, {\"x\": 0.14221237687469263, \"source\": 45, \"method\": \"phonecalls\", \"target\": 69, \"weight\": 2, \"y\": 0.00591156559326728, \"pair\": [45, 69], \"edge\": 222}, {\"x\": 0.06499654685147466, \"source\": 45, \"method\": \"phonecalls\", \"target\": 69, \"weight\": 2, \"y\": 0.05076504933341431, \"pair\": [45, 69], \"edge\": 222}, {\"x\": 0.19303499834114946, \"source\": 46, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": 0.015416290846902747, \"pair\": [46, 47], \"edge\": 223}, {\"x\": 0.12454620975849745, \"source\": 46, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": 0.004060318457434848, \"pair\": [46, 47], \"edge\": 223}, {\"x\": 0.19303499834114946, \"source\": 46, \"method\": \"meetings\", \"target\": 48, \"weight\": 1, \"y\": 0.015416290846902747, \"pair\": [46, 48], \"edge\": 224}, {\"x\": 0.16226555310120305, \"source\": 46, \"method\": \"meetings\", \"target\": 48, \"weight\": 1, \"y\": -0.005004194335036632, \"pair\": [46, 48], \"edge\": 224}, {\"x\": 0.12454620975849745, \"source\": 47, \"method\": \"phonecalls\", \"target\": 48, \"weight\": 4, \"y\": 0.004060318457434848, \"pair\": [47, 48], \"edge\": 225}, {\"x\": 0.16226555310120305, \"source\": 47, \"method\": \"phonecalls\", \"target\": 48, \"weight\": 4, \"y\": -0.005004194335036632, \"pair\": [47, 48], \"edge\": 225}, {\"x\": 0.12454620975849745, \"source\": 47, \"method\": \"phonecalls\", \"target\": 49, \"weight\": 1, \"y\": 0.004060318457434848, \"pair\": [47, 49], \"edge\": 226}, {\"x\": 0.1719680111471714, \"source\": 47, \"method\": \"phonecalls\", \"target\": 49, \"weight\": 1, \"y\": -0.0607803710553539, \"pair\": [47, 49], \"edge\": 226}, {\"x\": 0.12454620975849745, \"source\": 47, \"method\": \"phonecalls\", \"target\": 50, \"weight\": 2, \"y\": 0.004060318457434848, \"pair\": [47, 50], \"edge\": 227}, {\"x\": 0.16961919495454755, \"source\": 47, \"method\": \"phonecalls\", \"target\": 50, \"weight\": 2, \"y\": -0.026124506098854063, \"pair\": [47, 50], \"edge\": 227}, {\"x\": 0.12454620975849745, \"source\": 47, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 3, \"y\": 0.004060318457434848, \"pair\": [47, 51], \"edge\": 228}, {\"x\": 0.12670173276621177, \"source\": 47, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 3, \"y\": -0.02940877817730907, \"pair\": [47, 51], \"edge\": 228}, {\"x\": 0.12454620975849745, \"source\": 47, \"method\": \"phonecalls\", \"target\": 54, \"weight\": 4, \"y\": 0.004060318457434848, \"pair\": [47, 54], \"edge\": 229}, {\"x\": 0.09471192141615767, \"source\": 47, \"method\": \"phonecalls\", \"target\": 54, \"weight\": 4, \"y\": 0.01767593882590723, \"pair\": [47, 54], \"edge\": 229}, {\"x\": 0.12454620975849745, \"source\": 47, \"method\": \"meetings\", \"target\": 55, \"weight\": 1, \"y\": 0.004060318457434848, \"pair\": [47, 55], \"edge\": 230}, {\"x\": 0.20330986542290788, \"source\": 47, \"method\": \"meetings\", \"target\": 55, \"weight\": 1, \"y\": 0.026610690376777588, \"pair\": [47, 55], \"edge\": 230}, {\"x\": 0.12454620975849745, \"source\": 47, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": 0.004060318457434848, \"pair\": [47, 68], \"edge\": 231}, {\"x\": 0.12930838529187758, \"source\": 47, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": 0.04930294572899866, \"pair\": [47, 68], \"edge\": 231}, {\"x\": 0.12454620975849745, \"source\": 47, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": 0.004060318457434848, \"pair\": [47, 89], \"edge\": 232}, {\"x\": 0.1306705348653082, \"source\": 47, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": -0.0008491090228239166, \"pair\": [47, 89], \"edge\": 232}, {\"x\": 0.12454620975849745, \"source\": 47, \"method\": \"meetings\", \"target\": 90, \"weight\": 2, \"y\": 0.004060318457434848, \"pair\": [47, 90], \"edge\": 233}, {\"x\": 0.1857404129361362, \"source\": 47, \"method\": \"meetings\", \"target\": 90, \"weight\": 2, \"y\": -0.008847597765420238, \"pair\": [47, 90], \"edge\": 233}, {\"x\": 0.12454620975849745, \"source\": 47, \"method\": \"meetings\", \"target\": 93, \"weight\": 1, \"y\": 0.004060318457434848, \"pair\": [47, 93], \"edge\": 234}, {\"x\": 0.1493938975418612, \"source\": 47, \"method\": \"meetings\", \"target\": 93, \"weight\": 1, \"y\": -0.013586736562355435, \"pair\": [47, 93], \"edge\": 234}, {\"x\": 0.12454620975849745, \"source\": 47, \"method\": \"phonecalls\", \"target\": 121, \"weight\": 1, \"y\": 0.004060318457434848, \"pair\": [47, 121], \"edge\": 235}, {\"x\": 0.13142848912300195, \"source\": 47, \"method\": \"phonecalls\", \"target\": 121, \"weight\": 1, \"y\": 0.08943493175815392, \"pair\": [47, 121], \"edge\": 235}, {\"x\": 0.12454620975849745, \"source\": 47, \"method\": \"phonecalls\", \"target\": 122, \"weight\": 1, \"y\": 0.004060318457434848, \"pair\": [47, 122], \"edge\": 236}, {\"x\": 0.10497348057817454, \"source\": 47, \"method\": \"phonecalls\", \"target\": 122, \"weight\": 1, \"y\": 0.07798816626957444, \"pair\": [47, 122], \"edge\": 236}, {\"x\": 0.12454620975849745, \"source\": 47, \"method\": \"phonecalls\", \"target\": 123, \"weight\": 1, \"y\": 0.004060318457434848, \"pair\": [47, 123], \"edge\": 237}, {\"x\": 0.18403301252359253, \"source\": 47, \"method\": \"phonecalls\", \"target\": 123, \"weight\": 1, \"y\": 0.0692229163868061, \"pair\": [47, 123], \"edge\": 237}, {\"x\": 0.12454620975849745, \"source\": 47, \"method\": \"phonecalls\", \"target\": 96, \"weight\": 1, \"y\": 0.004060318457434848, \"pair\": [47, 96], \"edge\": 238}, {\"x\": 0.20403074353407796, \"source\": 47, \"method\": \"phonecalls\", \"target\": 96, \"weight\": 1, \"y\": -0.07332898085776841, \"pair\": [47, 96], \"edge\": 238}, {\"x\": 0.12454620975849745, \"source\": 47, \"method\": \"phonecalls\", \"target\": 135, \"weight\": 1, \"y\": 0.004060318457434848, \"pair\": [47, 135], \"edge\": 239}, {\"x\": 0.23188607153781185, \"source\": 47, \"method\": \"phonecalls\", \"target\": 135, \"weight\": 1, \"y\": 0.005330328540771689, \"pair\": [47, 135], \"edge\": 239}, {\"x\": 0.12454620975849745, \"source\": 47, \"method\": \"phonecalls\", \"target\": 95, \"weight\": 1, \"y\": 0.004060318457434848, \"pair\": [47, 95], \"edge\": 240}, {\"x\": 0.18208660433602217, \"source\": 47, \"method\": \"phonecalls\", \"target\": 95, \"weight\": 1, \"y\": -0.03142957811841357, \"pair\": [47, 95], \"edge\": 240}, {\"x\": 0.12454620975849745, \"source\": 47, \"method\": \"phonecalls\", \"target\": 147, \"weight\": 1, \"y\": 0.004060318457434848, \"pair\": [47, 147], \"edge\": 241}, {\"x\": 0.1583056974309385, \"source\": 47, \"method\": \"phonecalls\", \"target\": 147, \"weight\": 1, \"y\": 0.08054303027166111, \"pair\": [47, 147], \"edge\": 241}, {\"x\": 0.12454620975849745, \"source\": 47, \"method\": \"phonecalls\", \"target\": 56, \"weight\": 1, \"y\": 0.004060318457434848, \"pair\": [47, 56], \"edge\": 242}, {\"x\": 0.12947180502423694, \"source\": 47, \"method\": \"phonecalls\", \"target\": 56, \"weight\": 1, \"y\": -0.09240815551107316, \"pair\": [47, 56], \"edge\": 242}, {\"x\": 0.12454620975849745, \"source\": 47, \"method\": \"phonecalls\", \"target\": 97, \"weight\": 1, \"y\": 0.004060318457434848, \"pair\": [47, 97], \"edge\": 243}, {\"x\": 0.1052822345107166, \"source\": 47, \"method\": \"phonecalls\", \"target\": 97, \"weight\": 1, \"y\": 0.0013827111019374546, \"pair\": [47, 97], \"edge\": 243}, {\"x\": 0.12454620975849745, \"source\": 47, \"method\": \"phonecalls\", \"target\": 151, \"weight\": 4, \"y\": 0.004060318457434848, \"pair\": [47, 151], \"edge\": 244}, {\"x\": 0.15506769838740594, \"source\": 47, \"method\": \"phonecalls\", \"target\": 151, \"weight\": 4, \"y\": 0.028092475139554388, \"pair\": [47, 151], \"edge\": 244}, {\"x\": 0.16226555310120305, \"source\": 48, \"method\": \"meetings\", \"target\": 49, \"weight\": 1, \"y\": -0.005004194335036632, \"pair\": [48, 49], \"edge\": 245}, {\"x\": 0.1719680111471714, \"source\": 48, \"method\": \"meetings\", \"target\": 49, \"weight\": 1, \"y\": -0.0607803710553539, \"pair\": [48, 49], \"edge\": 245}, {\"x\": 0.16226555310120305, \"source\": 48, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": -0.005004194335036632, \"pair\": [48, 50], \"edge\": 246}, {\"x\": 0.16961919495454755, \"source\": 48, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": -0.026124506098854063, \"pair\": [48, 50], \"edge\": 246}, {\"x\": 0.16226555310120305, \"source\": 48, \"method\": \"meetings\", \"target\": 51, \"weight\": 3, \"y\": -0.005004194335036632, \"pair\": [48, 51], \"edge\": 247}, {\"x\": 0.12670173276621177, \"source\": 48, \"method\": \"meetings\", \"target\": 51, \"weight\": 3, \"y\": -0.02940877817730907, \"pair\": [48, 51], \"edge\": 247}, {\"x\": 0.16226555310120305, \"source\": 48, \"method\": \"meetings\", \"target\": 93, \"weight\": 3, \"y\": -0.005004194335036632, \"pair\": [48, 93], \"edge\": 248}, {\"x\": 0.1493938975418612, \"source\": 48, \"method\": \"meetings\", \"target\": 93, \"weight\": 3, \"y\": -0.013586736562355435, \"pair\": [48, 93], \"edge\": 248}, {\"x\": 0.16226555310120305, \"source\": 48, \"method\": \"meetings\", \"target\": 89, \"weight\": 1, \"y\": -0.005004194335036632, \"pair\": [48, 89], \"edge\": 249}, {\"x\": 0.1306705348653082, \"source\": 48, \"method\": \"meetings\", \"target\": 89, \"weight\": 1, \"y\": -0.0008491090228239166, \"pair\": [48, 89], \"edge\": 249}, {\"x\": 0.16226555310120305, \"source\": 48, \"method\": \"meetings\", \"target\": 95, \"weight\": 1, \"y\": -0.005004194335036632, \"pair\": [48, 95], \"edge\": 250}, {\"x\": 0.18208660433602217, \"source\": 48, \"method\": \"meetings\", \"target\": 95, \"weight\": 1, \"y\": -0.03142957811841357, \"pair\": [48, 95], \"edge\": 250}, {\"x\": 0.16226555310120305, \"source\": 48, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": -0.005004194335036632, \"pair\": [48, 68], \"edge\": 251}, {\"x\": 0.12930838529187758, \"source\": 48, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": 0.04930294572899866, \"pair\": [48, 68], \"edge\": 251}, {\"x\": 0.1719680111471714, \"source\": 49, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": -0.0607803710553539, \"pair\": [49, 50], \"edge\": 252}, {\"x\": 0.16961919495454755, \"source\": 49, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": -0.026124506098854063, \"pair\": [49, 50], \"edge\": 252}, {\"x\": 0.1719680111471714, \"source\": 49, \"method\": \"phonecalls\", \"target\": 56, \"weight\": 2, \"y\": -0.0607803710553539, \"pair\": [49, 56], \"edge\": 253}, {\"x\": 0.12947180502423694, \"source\": 49, \"method\": \"phonecalls\", \"target\": 56, \"weight\": 2, \"y\": -0.09240815551107316, \"pair\": [49, 56], \"edge\": 253}, {\"x\": 0.16961919495454755, \"source\": 50, \"method\": \"meetings\", \"target\": 51, \"weight\": 1, \"y\": -0.026124506098854063, \"pair\": [50, 51], \"edge\": 254}, {\"x\": 0.12670173276621177, \"source\": 50, \"method\": \"meetings\", \"target\": 51, \"weight\": 1, \"y\": -0.02940877817730907, \"pair\": [50, 51], \"edge\": 254}, {\"x\": 0.16961919495454755, \"source\": 50, \"method\": \"meetings\", \"target\": 89, \"weight\": 1, \"y\": -0.026124506098854063, \"pair\": [50, 89], \"edge\": 255}, {\"x\": 0.1306705348653082, \"source\": 50, \"method\": \"meetings\", \"target\": 89, \"weight\": 1, \"y\": -0.0008491090228239166, \"pair\": [50, 89], \"edge\": 255}, {\"x\": 0.16961919495454755, \"source\": 50, \"method\": \"meetings\", \"target\": 90, \"weight\": 2, \"y\": -0.026124506098854063, \"pair\": [50, 90], \"edge\": 256}, {\"x\": 0.1857404129361362, \"source\": 50, \"method\": \"meetings\", \"target\": 90, \"weight\": 2, \"y\": -0.008847597765420238, \"pair\": [50, 90], \"edge\": 256}, {\"x\": 0.12670173276621177, \"source\": 51, \"method\": \"meetings\", \"target\": 52, \"weight\": 1, \"y\": -0.02940877817730907, \"pair\": [51, 52], \"edge\": 257}, {\"x\": 0.22178343143465137, \"source\": 51, \"method\": \"meetings\", \"target\": 52, \"weight\": 1, \"y\": -0.11767160535682882, \"pair\": [51, 52], \"edge\": 257}, {\"x\": 0.12670173276621177, \"source\": 51, \"method\": \"meetings\", \"target\": 53, \"weight\": 1, \"y\": -0.02940877817730907, \"pair\": [51, 53], \"edge\": 258}, {\"x\": 0.19253943463590856, \"source\": 51, \"method\": \"meetings\", \"target\": 53, \"weight\": 1, \"y\": -0.11056354453074833, \"pair\": [51, 53], \"edge\": 258}, {\"x\": 0.12670173276621177, \"source\": 51, \"method\": \"meetings\", \"target\": 54, \"weight\": 1, \"y\": -0.02940877817730907, \"pair\": [51, 54], \"edge\": 259}, {\"x\": 0.09471192141615767, \"source\": 51, \"method\": \"meetings\", \"target\": 54, \"weight\": 1, \"y\": 0.01767593882590723, \"pair\": [51, 54], \"edge\": 259}, {\"x\": 0.12670173276621177, \"source\": 51, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": -0.02940877817730907, \"pair\": [51, 89], \"edge\": 260}, {\"x\": 0.1306705348653082, \"source\": 51, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": -0.0008491090228239166, \"pair\": [51, 89], \"edge\": 260}, {\"x\": 0.22178343143465137, \"source\": 52, \"method\": \"meetings\", \"target\": 53, \"weight\": 1, \"y\": -0.11767160535682882, \"pair\": [52, 53], \"edge\": 261}, {\"x\": 0.19253943463590856, \"source\": 52, \"method\": \"meetings\", \"target\": 53, \"weight\": 1, \"y\": -0.11056354453074833, \"pair\": [52, 53], \"edge\": 261}, {\"x\": 0.22178343143465137, \"source\": 52, \"method\": \"meetings\", \"target\": 96, \"weight\": 1, \"y\": -0.11767160535682882, \"pair\": [52, 96], \"edge\": 262}, {\"x\": 0.20403074353407796, \"source\": 52, \"method\": \"meetings\", \"target\": 96, \"weight\": 1, \"y\": -0.07332898085776841, \"pair\": [52, 96], \"edge\": 262}, {\"x\": 0.22178343143465137, \"source\": 52, \"method\": \"phonecalls\", \"target\": 124, \"weight\": 1, \"y\": -0.11767160535682882, \"pair\": [52, 124], \"edge\": 263}, {\"x\": 0.2888032695397457, \"source\": 52, \"method\": \"phonecalls\", \"target\": 124, \"weight\": 1, \"y\": -0.18207447203761953, \"pair\": [52, 124], \"edge\": 263}, {\"x\": 0.09471192141615767, \"source\": 54, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": 0.01767593882590723, \"pair\": [54, 68], \"edge\": 264}, {\"x\": 0.12930838529187758, \"source\": 54, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": 0.04930294572899866, \"pair\": [54, 68], \"edge\": 264}, {\"x\": 0.09471192141615767, \"source\": 54, \"method\": \"meetings\", \"target\": 70, \"weight\": 2, \"y\": 0.01767593882590723, \"pair\": [54, 70], \"edge\": 265}, {\"x\": 0.11520085475572568, \"source\": 54, \"method\": \"meetings\", \"target\": 70, \"weight\": 2, \"y\": 0.027593125185657312, \"pair\": [54, 70], \"edge\": 265}, {\"x\": 0.09471192141615767, \"source\": 54, \"method\": \"phonecalls\", \"target\": 69, \"weight\": 1, \"y\": 0.01767593882590723, \"pair\": [54, 69], \"edge\": 266}, {\"x\": 0.06499654685147466, \"source\": 54, \"method\": \"phonecalls\", \"target\": 69, \"weight\": 1, \"y\": 0.05076504933341431, \"pair\": [54, 69], \"edge\": 266}, {\"x\": 0.12947180502423694, \"source\": 56, \"method\": \"meetings\", \"target\": 57, \"weight\": 1, \"y\": -0.09240815551107316, \"pair\": [56, 57], \"edge\": 267}, {\"x\": 0.18086811323942048, \"source\": 56, \"method\": \"meetings\", \"target\": 57, \"weight\": 1, \"y\": -0.17629096003982858, \"pair\": [56, 57], \"edge\": 267}, {\"x\": 0.12947180502423694, \"source\": 56, \"method\": \"phonecalls\", \"target\": 125, \"weight\": 1, \"y\": -0.09240815551107316, \"pair\": [56, 125], \"edge\": 268}, {\"x\": 0.017044446665228825, \"source\": 56, \"method\": \"phonecalls\", \"target\": 125, \"weight\": 1, \"y\": -0.06892376678482033, \"pair\": [56, 125], \"edge\": 268}, {\"x\": -0.129566842289825, \"source\": 58, \"method\": \"meetings\", \"target\": 59, \"weight\": 1, \"y\": 0.0828576593062837, \"pair\": [58, 59], \"edge\": 269}, {\"x\": -0.1359303859231084, \"source\": 58, \"method\": \"meetings\", \"target\": 59, \"weight\": 1, \"y\": 0.045187628118315865, \"pair\": [58, 59], \"edge\": 269}, {\"x\": -0.129566842289825, \"source\": 58, \"method\": \"phonecalls\", \"target\": 109, \"weight\": 3, \"y\": 0.0828576593062837, \"pair\": [58, 109], \"edge\": 270}, {\"x\": -0.19677523860794632, \"source\": 58, \"method\": \"phonecalls\", \"target\": 109, \"weight\": 3, \"y\": 0.10794372697380812, \"pair\": [58, 109], \"edge\": 270}, {\"x\": -0.129566842289825, \"source\": 58, \"method\": \"phonecalls\", \"target\": 75, \"weight\": 1, \"y\": 0.0828576593062837, \"pair\": [58, 75], \"edge\": 271}, {\"x\": -0.04568987856672662, \"source\": 58, \"method\": \"phonecalls\", \"target\": 75, \"weight\": 1, \"y\": 0.10916287074655957, \"pair\": [58, 75], \"edge\": 271}, {\"x\": -0.129566842289825, \"source\": 58, \"method\": \"phonecalls\", \"target\": 77, \"weight\": 2, \"y\": 0.0828576593062837, \"pair\": [58, 77], \"edge\": 272}, {\"x\": -0.07022785790207993, \"source\": 58, \"method\": \"phonecalls\", \"target\": 77, \"weight\": 2, \"y\": 0.09344730750644979, \"pair\": [58, 77], \"edge\": 272}, {\"x\": 0.0765058900082385, \"source\": 61, \"method\": \"phonecalls\", \"target\": 62, \"weight\": 1, \"y\": 0.10550349655861112, \"pair\": [61, 62], \"edge\": 273}, {\"x\": 0.15678923199530473, \"source\": 61, \"method\": \"phonecalls\", \"target\": 62, \"weight\": 1, \"y\": 0.17436264133149296, \"pair\": [61, 62], \"edge\": 273}, {\"x\": 0.0765058900082385, \"source\": 61, \"method\": \"phonecalls\", \"target\": 66, \"weight\": 3, \"y\": 0.10550349655861112, \"pair\": [61, 66], \"edge\": 274}, {\"x\": 0.02562639712079193, \"source\": 61, \"method\": \"phonecalls\", \"target\": 66, \"weight\": 3, \"y\": 0.07139918504611162, \"pair\": [61, 66], \"edge\": 274}, {\"x\": 0.0765058900082385, \"source\": 61, \"method\": \"phonecalls\", \"target\": 105, \"weight\": 2, \"y\": 0.10550349655861112, \"pair\": [61, 105], \"edge\": 275}, {\"x\": 0.10560066435872675, \"source\": 61, \"method\": \"phonecalls\", \"target\": 105, \"weight\": 2, \"y\": 0.16536952856455303, \"pair\": [61, 105], \"edge\": 275}, {\"x\": 0.0765058900082385, \"source\": 61, \"method\": \"phonecalls\", \"target\": 117, \"weight\": 2, \"y\": 0.10550349655861112, \"pair\": [61, 117], \"edge\": 276}, {\"x\": 0.06262382413958945, \"source\": 61, \"method\": \"phonecalls\", \"target\": 117, \"weight\": 2, \"y\": 0.17156725535811393, \"pair\": [61, 117], \"edge\": 276}, {\"x\": 0.0765058900082385, \"source\": 61, \"method\": \"phonecalls\", \"target\": 80, \"weight\": 1, \"y\": 0.10550349655861112, \"pair\": [61, 80], \"edge\": 277}, {\"x\": 0.21092301140778313, \"source\": 61, \"method\": \"phonecalls\", \"target\": 80, \"weight\": 1, \"y\": 0.17247496333821036, \"pair\": [61, 80], \"edge\": 277}, {\"x\": 0.0765058900082385, \"source\": 61, \"method\": \"phonecalls\", \"target\": 67, \"weight\": 5, \"y\": 0.10550349655861112, \"pair\": [61, 67], \"edge\": 278}, {\"x\": 0.12512946736741998, \"source\": 61, \"method\": \"phonecalls\", \"target\": 67, \"weight\": 5, \"y\": 0.12420284401595356, \"pair\": [61, 67], \"edge\": 278}, {\"x\": 0.0765058900082385, \"source\": 61, \"method\": \"phonecalls\", \"target\": 128, \"weight\": 2, \"y\": 0.10550349655861112, \"pair\": [61, 128], \"edge\": 279}, {\"x\": 0.08408812786074164, \"source\": 61, \"method\": \"phonecalls\", \"target\": 128, \"weight\": 2, \"y\": 0.17293139206219885, \"pair\": [61, 128], \"edge\": 279}, {\"x\": 0.0765058900082385, \"source\": 61, \"method\": \"phonecalls\", \"target\": 129, \"weight\": 1, \"y\": 0.10550349655861112, \"pair\": [61, 129], \"edge\": 280}, {\"x\": 0.09100677664902258, \"source\": 61, \"method\": \"phonecalls\", \"target\": 129, \"weight\": 1, \"y\": 0.2115144272801116, \"pair\": [61, 129], \"edge\": 280}, {\"x\": 0.0765058900082385, \"source\": 61, \"method\": \"phonecalls\", \"target\": 75, \"weight\": 1, \"y\": 0.10550349655861112, \"pair\": [61, 75], \"edge\": 281}, {\"x\": -0.04568987856672662, \"source\": 61, \"method\": \"phonecalls\", \"target\": 75, \"weight\": 1, \"y\": 0.10916287074655957, \"pair\": [61, 75], \"edge\": 281}, {\"x\": 0.0765058900082385, \"source\": 61, \"method\": \"phonecalls\", \"target\": 130, \"weight\": 1, \"y\": 0.10550349655861112, \"pair\": [61, 130], \"edge\": 282}, {\"x\": 0.0679606948694571, \"source\": 61, \"method\": \"phonecalls\", \"target\": 130, \"weight\": 1, \"y\": 0.2101052502291791, \"pair\": [61, 130], \"edge\": 282}, {\"x\": 0.0765058900082385, \"source\": 61, \"method\": \"phonecalls\", \"target\": 131, \"weight\": 4, \"y\": 0.10550349655861112, \"pair\": [61, 131], \"edge\": 283}, {\"x\": 0.08106895315733699, \"source\": 61, \"method\": \"phonecalls\", \"target\": 131, \"weight\": 4, \"y\": 0.1463743587367659, \"pair\": [61, 131], \"edge\": 283}, {\"x\": 0.0765058900082385, \"source\": 61, \"method\": \"phonecalls\", \"target\": 133, \"weight\": 1, \"y\": 0.10550349655861112, \"pair\": [61, 133], \"edge\": 284}, {\"x\": 0.1299003923607913, \"source\": 61, \"method\": \"phonecalls\", \"target\": 133, \"weight\": 1, \"y\": 0.19707592360624399, \"pair\": [61, 133], \"edge\": 284}, {\"x\": 0.0765058900082385, \"source\": 61, \"method\": \"phonecalls\", \"target\": 134, \"weight\": 1, \"y\": 0.10550349655861112, \"pair\": [61, 134], \"edge\": 285}, {\"x\": 0.11126131161317918, \"source\": 61, \"method\": \"phonecalls\", \"target\": 134, \"weight\": 1, \"y\": 0.20695548075123923, \"pair\": [61, 134], \"edge\": 285}, {\"x\": 0.0765058900082385, \"source\": 61, \"method\": \"phonecalls\", \"target\": 142, \"weight\": 1, \"y\": 0.10550349655861112, \"pair\": [61, 142], \"edge\": 286}, {\"x\": 0.002387980116398114, \"source\": 61, \"method\": \"phonecalls\", \"target\": 142, \"weight\": 1, \"y\": 0.031050859678300714, \"pair\": [61, 142], \"edge\": 286}, {\"x\": 0.0765058900082385, \"source\": 61, \"method\": \"phonecalls\", \"target\": 70, \"weight\": 1, \"y\": 0.10550349655861112, \"pair\": [61, 70], \"edge\": 287}, {\"x\": 0.11520085475572568, \"source\": 61, \"method\": \"phonecalls\", \"target\": 70, \"weight\": 1, \"y\": 0.027593125185657312, \"pair\": [61, 70], \"edge\": 287}, {\"x\": -0.044333366833317324, \"source\": 63, \"method\": \"meetings\", \"target\": 64, \"weight\": 3, \"y\": 0.03645810553430277, \"pair\": [63, 64], \"edge\": 288}, {\"x\": 0.0015726656641651463, \"source\": 63, \"method\": \"meetings\", \"target\": 64, \"weight\": 3, \"y\": 0.042423991886305794, \"pair\": [63, 64], \"edge\": 288}, {\"x\": -0.044333366833317324, \"source\": 63, \"method\": \"meetings\", \"target\": 72, \"weight\": 2, \"y\": 0.03645810553430277, \"pair\": [63, 72], \"edge\": 289}, {\"x\": -0.09015529937777945, \"source\": 63, \"method\": \"meetings\", \"target\": 72, \"weight\": 2, \"y\": 0.06859928174123174, \"pair\": [63, 72], \"edge\": 289}, {\"x\": 0.0015726656641651463, \"source\": 64, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": 0.042423991886305794, \"pair\": [64, 68], \"edge\": 290}, {\"x\": 0.12930838529187758, \"source\": 64, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": 0.04930294572899866, \"pair\": [64, 68], \"edge\": 290}, {\"x\": 0.0015726656641651463, \"source\": 64, \"method\": \"phonecalls\", \"target\": 75, \"weight\": 3, \"y\": 0.042423991886305794, \"pair\": [64, 75], \"edge\": 291}, {\"x\": -0.04568987856672662, \"source\": 64, \"method\": \"phonecalls\", \"target\": 75, \"weight\": 3, \"y\": 0.10916287074655957, \"pair\": [64, 75], \"edge\": 291}, {\"x\": 0.12930838529187758, \"source\": 68, \"method\": \"meetings\", \"target\": 69, \"weight\": 1, \"y\": 0.04930294572899866, \"pair\": [68, 69], \"edge\": 292}, {\"x\": 0.06499654685147466, \"source\": 68, \"method\": \"meetings\", \"target\": 69, \"weight\": 1, \"y\": 0.05076504933341431, \"pair\": [68, 69], \"edge\": 292}, {\"x\": 0.12930838529187758, \"source\": 68, \"method\": \"phonecalls\", \"target\": 70, \"weight\": 3, \"y\": 0.04930294572899866, \"pair\": [68, 70], \"edge\": 293}, {\"x\": 0.11520085475572568, \"source\": 68, \"method\": \"phonecalls\", \"target\": 70, \"weight\": 3, \"y\": 0.027593125185657312, \"pair\": [68, 70], \"edge\": 293}, {\"x\": 0.12930838529187758, \"source\": 68, \"method\": \"meetings\", \"target\": 79, \"weight\": 1, \"y\": 0.04930294572899866, \"pair\": [68, 79], \"edge\": 294}, {\"x\": 0.23428326923554282, \"source\": 68, \"method\": \"meetings\", \"target\": 79, \"weight\": 1, \"y\": 0.1455176576368486, \"pair\": [68, 79], \"edge\": 294}, {\"x\": 0.12930838529187758, \"source\": 68, \"method\": \"meetings\", \"target\": 83, \"weight\": 1, \"y\": 0.04930294572899866, \"pair\": [68, 83], \"edge\": 295}, {\"x\": 0.21837442264953602, \"source\": 68, \"method\": \"meetings\", \"target\": 83, \"weight\": 1, \"y\": 0.11841419403997723, \"pair\": [68, 83], \"edge\": 295}, {\"x\": 0.12930838529187758, \"source\": 68, \"method\": \"meetings\", \"target\": 89, \"weight\": 7, \"y\": 0.04930294572899866, \"pair\": [68, 89], \"edge\": 296}, {\"x\": 0.1306705348653082, \"source\": 68, \"method\": \"meetings\", \"target\": 89, \"weight\": 7, \"y\": -0.0008491090228239166, \"pair\": [68, 89], \"edge\": 296}, {\"x\": 0.12930838529187758, \"source\": 68, \"method\": \"phonecalls\", \"target\": 97, \"weight\": 1, \"y\": 0.04930294572899866, \"pair\": [68, 97], \"edge\": 297}, {\"x\": 0.1052822345107166, \"source\": 68, \"method\": \"phonecalls\", \"target\": 97, \"weight\": 1, \"y\": 0.0013827111019374546, \"pair\": [68, 97], \"edge\": 297}, {\"x\": 0.12930838529187758, \"source\": 68, \"method\": \"phonecalls\", \"target\": 149, \"weight\": 1, \"y\": 0.04930294572899866, \"pair\": [68, 149], \"edge\": 298}, {\"x\": 0.20449236804455034, \"source\": 68, \"method\": \"phonecalls\", \"target\": 149, \"weight\": 1, \"y\": 0.09542362324000947, \"pair\": [68, 149], \"edge\": 298}, {\"x\": 0.12930838529187758, \"source\": 68, \"method\": \"phonecalls\", \"target\": 150, \"weight\": 1, \"y\": 0.04930294572899866, \"pair\": [68, 150], \"edge\": 299}, {\"x\": 0.19413321429240044, \"source\": 68, \"method\": \"phonecalls\", \"target\": 150, \"weight\": 1, \"y\": 0.12347534109222182, \"pair\": [68, 150], \"edge\": 299}, {\"x\": 0.12930838529187758, \"source\": 68, \"method\": \"phonecalls\", \"target\": 151, \"weight\": 1, \"y\": 0.04930294572899866, \"pair\": [68, 151], \"edge\": 300}, {\"x\": 0.15506769838740594, \"source\": 68, \"method\": \"phonecalls\", \"target\": 151, \"weight\": 1, \"y\": 0.028092475139554388, \"pair\": [68, 151], \"edge\": 300}, {\"x\": 0.12930838529187758, \"source\": 68, \"method\": \"phonecalls\", \"target\": 152, \"weight\": 1, \"y\": 0.04930294572899866, \"pair\": [68, 152], \"edge\": 301}, {\"x\": 0.17696292221902368, \"source\": 68, \"method\": \"phonecalls\", \"target\": 152, \"weight\": 1, \"y\": 0.13312066951629062, \"pair\": [68, 152], \"edge\": 301}, {\"x\": 0.06499654685147466, \"source\": 69, \"method\": \"phonecalls\", \"target\": 77, \"weight\": 1, \"y\": 0.05076504933341431, \"pair\": [69, 77], \"edge\": 302}, {\"x\": -0.07022785790207993, \"source\": 69, \"method\": \"phonecalls\", \"target\": 77, \"weight\": 1, \"y\": 0.09344730750644979, \"pair\": [69, 77], \"edge\": 302}, {\"x\": 0.11520085475572568, \"source\": 70, \"method\": \"meetings\", \"target\": 89, \"weight\": 3, \"y\": 0.027593125185657312, \"pair\": [70, 89], \"edge\": 303}, {\"x\": 0.1306705348653082, \"source\": 70, \"method\": \"meetings\", \"target\": 89, \"weight\": 3, \"y\": -0.0008491090228239166, \"pair\": [70, 89], \"edge\": 303}, {\"x\": 0.11520085475572568, \"source\": 70, \"method\": \"meetings\", \"target\": 93, \"weight\": 2, \"y\": 0.027593125185657312, \"pair\": [70, 93], \"edge\": 304}, {\"x\": 0.1493938975418612, \"source\": 70, \"method\": \"meetings\", \"target\": 93, \"weight\": 2, \"y\": -0.013586736562355435, \"pair\": [70, 93], \"edge\": 304}, {\"x\": 0.11520085475572568, \"source\": 70, \"method\": \"meetings\", \"target\": 97, \"weight\": 1, \"y\": 0.027593125185657312, \"pair\": [70, 97], \"edge\": 305}, {\"x\": 0.1052822345107166, \"source\": 70, \"method\": \"meetings\", \"target\": 97, \"weight\": 1, \"y\": 0.0013827111019374546, \"pair\": [70, 97], \"edge\": 305}, {\"x\": -0.17778688265058182, \"source\": 73, \"method\": \"meetings\", \"target\": 74, \"weight\": 1, \"y\": 1.0, \"pair\": [73, 74], \"edge\": 306}, {\"x\": -0.206351037761324, \"source\": 73, \"method\": \"meetings\", \"target\": 74, \"weight\": 1, \"y\": 0.997174134984544, \"pair\": [73, 74], \"edge\": 306}, {\"x\": -0.04568987856672662, \"source\": 75, \"method\": \"meetings\", \"target\": 76, \"weight\": 1, \"y\": 0.10916287074655957, \"pair\": [75, 76], \"edge\": 307}, {\"x\": -0.06624873531690086, \"source\": 75, \"method\": \"meetings\", \"target\": 76, \"weight\": 1, \"y\": 0.05558734678423388, \"pair\": [75, 76], \"edge\": 307}, {\"x\": -0.04568987856672662, \"source\": 75, \"method\": \"meetings\", \"target\": 77, \"weight\": 1, \"y\": 0.10916287074655957, \"pair\": [75, 77], \"edge\": 308}, {\"x\": -0.07022785790207993, \"source\": 75, \"method\": \"meetings\", \"target\": 77, \"weight\": 1, \"y\": 0.09344730750644979, \"pair\": [75, 77], \"edge\": 308}, {\"x\": -0.04568987856672662, \"source\": 75, \"method\": \"phonecalls\", \"target\": 112, \"weight\": 5, \"y\": 0.10916287074655957, \"pair\": [75, 112], \"edge\": 309}, {\"x\": -0.054205115196575435, \"source\": 75, \"method\": \"phonecalls\", \"target\": 112, \"weight\": 5, \"y\": 0.1468004156199474, \"pair\": [75, 112], \"edge\": 309}, {\"x\": -0.04568987856672662, \"source\": 75, \"method\": \"phonecalls\", \"target\": 113, \"weight\": 3, \"y\": 0.10916287074655957, \"pair\": [75, 113], \"edge\": 310}, {\"x\": -0.061448185367512814, \"source\": 75, \"method\": \"phonecalls\", \"target\": 113, \"weight\": 3, \"y\": 0.16556652306142625, \"pair\": [75, 113], \"edge\": 310}, {\"x\": -0.04568987856672662, \"source\": 75, \"method\": \"phonecalls\", \"target\": 114, \"weight\": 2, \"y\": 0.10916287074655957, \"pair\": [75, 114], \"edge\": 311}, {\"x\": -0.085186032602534, \"source\": 75, \"method\": \"phonecalls\", \"target\": 114, \"weight\": 2, \"y\": 0.14983446173937112, \"pair\": [75, 114], \"edge\": 311}, {\"x\": -0.04568987856672662, \"source\": 75, \"method\": \"phonecalls\", \"target\": 115, \"weight\": 1, \"y\": 0.10916287074655957, \"pair\": [75, 115], \"edge\": 312}, {\"x\": -0.09352462804570229, \"source\": 75, \"method\": \"phonecalls\", \"target\": 115, \"weight\": 1, \"y\": 0.1987575701306277, \"pair\": [75, 115], \"edge\": 312}, {\"x\": -0.04568987856672662, \"source\": 75, \"method\": \"phonecalls\", \"target\": 116, \"weight\": 1, \"y\": 0.10916287074655957, \"pair\": [75, 116], \"edge\": 313}, {\"x\": -0.06976366605528862, \"source\": 75, \"method\": \"phonecalls\", \"target\": 116, \"weight\": 1, \"y\": 0.20827218033355427, \"pair\": [75, 116], \"edge\": 313}, {\"x\": -0.06624873531690086, \"source\": 76, \"method\": \"meetings\", \"target\": 77, \"weight\": 2, \"y\": 0.05558734678423388, \"pair\": [76, 77], \"edge\": 314}, {\"x\": -0.07022785790207993, \"source\": 76, \"method\": \"meetings\", \"target\": 77, \"weight\": 2, \"y\": 0.09344730750644979, \"pair\": [76, 77], \"edge\": 314}, {\"x\": -0.06624873531690086, \"source\": 76, \"method\": \"meetings\", \"target\": 78, \"weight\": 1, \"y\": 0.05558734678423388, \"pair\": [76, 78], \"edge\": 315}, {\"x\": -0.11391220001392693, \"source\": 76, \"method\": \"meetings\", \"target\": 78, \"weight\": 1, \"y\": 0.09289604964041172, \"pair\": [76, 78], \"edge\": 315}, {\"x\": -0.06624873531690086, \"source\": 76, \"method\": \"meetings\", \"target\": 98, \"weight\": 1, \"y\": 0.05558734678423388, \"pair\": [76, 98], \"edge\": 316}, {\"x\": -0.047198362494428214, \"source\": 76, \"method\": \"meetings\", \"target\": 98, \"weight\": 1, \"y\": 0.06363298454542243, \"pair\": [76, 98], \"edge\": 316}, {\"x\": -0.07022785790207993, \"source\": 77, \"method\": \"phonecalls\", \"target\": 137, \"weight\": 1, \"y\": 0.09344730750644979, \"pair\": [77, 137], \"edge\": 317}, {\"x\": -0.13928941361168168, \"source\": 77, \"method\": \"phonecalls\", \"target\": 137, \"weight\": 1, \"y\": 0.1694414987429792, \"pair\": [77, 137], \"edge\": 317}, {\"x\": -0.07022785790207993, \"source\": 77, \"method\": \"phonecalls\", \"target\": 114, \"weight\": 1, \"y\": 0.09344730750644979, \"pair\": [77, 114], \"edge\": 318}, {\"x\": -0.085186032602534, \"source\": 77, \"method\": \"phonecalls\", \"target\": 114, \"weight\": 1, \"y\": 0.14983446173937112, \"pair\": [77, 114], \"edge\": 318}, {\"x\": 0.23428326923554282, \"source\": 79, \"method\": \"meetings\", \"target\": 80, \"weight\": 1, \"y\": 0.1455176576368486, \"pair\": [79, 80], \"edge\": 319}, {\"x\": 0.21092301140778313, \"source\": 79, \"method\": \"meetings\", \"target\": 80, \"weight\": 1, \"y\": 0.17247496333821036, \"pair\": [79, 80], \"edge\": 319}, {\"x\": 0.23428326923554282, \"source\": 79, \"method\": \"meetings\", \"target\": 81, \"weight\": 1, \"y\": 0.1455176576368486, \"pair\": [79, 81], \"edge\": 320}, {\"x\": 0.28796666734671983, \"source\": 79, \"method\": \"meetings\", \"target\": 81, \"weight\": 1, \"y\": 0.18747561414313713, \"pair\": [79, 81], \"edge\": 320}, {\"x\": 0.23428326923554282, \"source\": 79, \"method\": \"meetings\", \"target\": 82, \"weight\": 1, \"y\": 0.1455176576368486, \"pair\": [79, 82], \"edge\": 321}, {\"x\": 0.2786842077476909, \"source\": 79, \"method\": \"meetings\", \"target\": 82, \"weight\": 1, \"y\": 0.20180529271738848, \"pair\": [79, 82], \"edge\": 321}, {\"x\": 0.23428326923554282, \"source\": 79, \"method\": \"meetings\", \"target\": 83, \"weight\": 1, \"y\": 0.1455176576368486, \"pair\": [79, 83], \"edge\": 322}, {\"x\": 0.21837442264953602, \"source\": 79, \"method\": \"meetings\", \"target\": 83, \"weight\": 1, \"y\": 0.11841419403997723, \"pair\": [79, 83], \"edge\": 322}, {\"x\": 0.21092301140778313, \"source\": 80, \"method\": \"meetings\", \"target\": 81, \"weight\": 1, \"y\": 0.17247496333821036, \"pair\": [80, 81], \"edge\": 323}, {\"x\": 0.28796666734671983, \"source\": 80, \"method\": \"meetings\", \"target\": 81, \"weight\": 1, \"y\": 0.18747561414313713, \"pair\": [80, 81], \"edge\": 323}, {\"x\": 0.21092301140778313, \"source\": 80, \"method\": \"meetings\", \"target\": 82, \"weight\": 1, \"y\": 0.17247496333821036, \"pair\": [80, 82], \"edge\": 324}, {\"x\": 0.2786842077476909, \"source\": 80, \"method\": \"meetings\", \"target\": 82, \"weight\": 1, \"y\": 0.20180529271738848, \"pair\": [80, 82], \"edge\": 324}, {\"x\": 0.28796666734671983, \"source\": 81, \"method\": \"meetings\", \"target\": 82, \"weight\": 1, \"y\": 0.18747561414313713, \"pair\": [81, 82], \"edge\": 325}, {\"x\": 0.2786842077476909, \"source\": 81, \"method\": \"meetings\", \"target\": 82, \"weight\": 1, \"y\": 0.20180529271738848, \"pair\": [81, 82], \"edge\": 325}, {\"x\": -0.19745425247945897, \"source\": 84, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": -0.27120405127703506, \"pair\": [84, 85], \"edge\": 326}, {\"x\": -0.16613248928451432, \"source\": 84, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": -0.2494687678551724, \"pair\": [84, 85], \"edge\": 326}, {\"x\": -0.19745425247945897, \"source\": 84, \"method\": \"meetings\", \"target\": 86, \"weight\": 1, \"y\": -0.27120405127703506, \"pair\": [84, 86], \"edge\": 327}, {\"x\": -0.20574340160449894, \"source\": 84, \"method\": \"meetings\", \"target\": 86, \"weight\": 1, \"y\": -0.24650064929612356, \"pair\": [84, 86], \"edge\": 327}, {\"x\": -0.19745425247945897, \"source\": 84, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": -0.27120405127703506, \"pair\": [84, 87], \"edge\": 328}, {\"x\": -0.20733371983185353, \"source\": 84, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": -0.26079972581200556, \"pair\": [84, 87], \"edge\": 328}, {\"x\": -0.16613248928451432, \"source\": 85, \"method\": \"meetings\", \"target\": 86, \"weight\": 1, \"y\": -0.2494687678551724, \"pair\": [85, 86], \"edge\": 329}, {\"x\": -0.20574340160449894, \"source\": 85, \"method\": \"meetings\", \"target\": 86, \"weight\": 1, \"y\": -0.24650064929612356, \"pair\": [85, 86], \"edge\": 329}, {\"x\": -0.16613248928451432, \"source\": 85, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": -0.2494687678551724, \"pair\": [85, 87], \"edge\": 330}, {\"x\": -0.20733371983185353, \"source\": 85, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": -0.26079972581200556, \"pair\": [85, 87], \"edge\": 330}, {\"x\": -0.20574340160449894, \"source\": 86, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": -0.24650064929612356, \"pair\": [86, 87], \"edge\": 331}, {\"x\": -0.20733371983185353, \"source\": 86, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": -0.26079972581200556, \"pair\": [86, 87], \"edge\": 331}, {\"x\": 0.1306705348653082, \"source\": 89, \"method\": \"meetings\", \"target\": 93, \"weight\": 2, \"y\": -0.0008491090228239166, \"pair\": [89, 93], \"edge\": 332}, {\"x\": 0.1493938975418612, \"source\": 89, \"method\": \"meetings\", \"target\": 93, \"weight\": 2, \"y\": -0.013586736562355435, \"pair\": [89, 93], \"edge\": 332}, {\"x\": 0.1306705348653082, \"source\": 89, \"method\": \"meetings\", \"target\": 94, \"weight\": 1, \"y\": -0.0008491090228239166, \"pair\": [89, 94], \"edge\": 333}, {\"x\": 0.21030166093408367, \"source\": 89, \"method\": \"meetings\", \"target\": 94, \"weight\": 1, \"y\": -0.014619795101153854, \"pair\": [89, 94], \"edge\": 333}, {\"x\": 0.1306705348653082, \"source\": 89, \"method\": \"meetings\", \"target\": 95, \"weight\": 1, \"y\": -0.0008491090228239166, \"pair\": [89, 95], \"edge\": 334}, {\"x\": 0.18208660433602217, \"source\": 89, \"method\": \"meetings\", \"target\": 95, \"weight\": 1, \"y\": -0.03142957811841357, \"pair\": [89, 95], \"edge\": 334}, {\"x\": 0.27682614708816616, \"source\": 91, \"method\": \"meetings\", \"target\": 92, \"weight\": 1, \"y\": 0.04143079610206784, \"pair\": [91, 92], \"edge\": 335}, {\"x\": 0.27652528725392855, \"source\": 91, \"method\": \"meetings\", \"target\": 92, \"weight\": 1, \"y\": 0.05921266905758577, \"pair\": [91, 92], \"edge\": 335}, {\"x\": 0.1493938975418612, \"source\": 93, \"method\": \"meetings\", \"target\": 95, \"weight\": 1, \"y\": -0.013586736562355435, \"pair\": [93, 95], \"edge\": 336}, {\"x\": 0.18208660433602217, \"source\": 93, \"method\": \"meetings\", \"target\": 95, \"weight\": 1, \"y\": -0.03142957811841357, \"pair\": [93, 95], \"edge\": 336}, {\"x\": -0.10299342519647912, \"source\": 99, \"method\": \"phonecalls\", \"target\": 118, \"weight\": 3, \"y\": -0.07985723383561413, \"pair\": [99, 118], \"edge\": 337}, {\"x\": -0.12476524880731904, \"source\": 99, \"method\": \"phonecalls\", \"target\": 118, \"weight\": 3, \"y\": -0.13320973497727737, \"pair\": [99, 118], \"edge\": 337}, {\"x\": 0.7955450547645584, \"source\": 106, \"method\": \"phonecalls\", \"target\": 107, \"weight\": 1, \"y\": -0.4317829697411659, \"pair\": [106, 107], \"edge\": 338}, {\"x\": 0.7805413345704385, \"source\": 106, \"method\": \"phonecalls\", \"target\": 107, \"weight\": 1, \"y\": -0.40680645160878937, \"pair\": [106, 107], \"edge\": 338}, {\"x\": -0.19677523860794632, \"source\": 109, \"method\": \"phonecalls\", \"target\": 110, \"weight\": 1, \"y\": 0.10794372697380812, \"pair\": [109, 110], \"edge\": 339}, {\"x\": -0.16271620928766461, \"source\": 109, \"method\": \"phonecalls\", \"target\": 110, \"weight\": 1, \"y\": 0.049341122201079037, \"pair\": [109, 110], \"edge\": 339}, {\"x\": -0.19677523860794632, \"source\": 109, \"method\": \"phonecalls\", \"target\": 111, \"weight\": 1, \"y\": 0.10794372697380812, \"pair\": [109, 111], \"edge\": 340}, {\"x\": -0.2775771841167416, \"source\": 109, \"method\": \"phonecalls\", \"target\": 111, \"weight\": 1, \"y\": 0.15202063068278032, \"pair\": [109, 111], \"edge\": 340}, {\"x\": -0.08504197444015457, \"source\": 143, \"method\": \"phonecalls\", \"target\": 144, \"weight\": 1, \"y\": -0.2963860817703671, \"pair\": [143, 144], \"edge\": 341}, {\"x\": -0.07985989343518832, \"source\": 143, \"method\": \"phonecalls\", \"target\": 144, \"weight\": 1, \"y\": -0.3246025864534094, \"pair\": [143, 144], \"edge\": 341}], \"data-9afb9796b7d186c4d9c70efa3f76c953\": [{\"x\": -0.6469108023227067, \"y\": 0.4935455961948123, \"id\": 0.0}, {\"x\": -0.6258949217935961, \"y\": 0.5048322656819353, \"id\": 1.0}, {\"x\": -0.6704455901428831, \"y\": 0.5293828017384508, \"id\": 2.0}, {\"x\": -0.13719554016384364, \"y\": -0.28877228822605044, \"id\": 3.0}, {\"x\": -0.14106817729117918, \"y\": -0.32711971026099, \"id\": 4.0}, {\"x\": -0.1131678875451056, \"y\": -0.2618534715182959, \"id\": 5.0}, {\"x\": -0.12205939130269741, \"y\": -0.2593127809814257, \"id\": 6.0}, {\"x\": -0.12736245884392888, \"y\": -0.32455179918832044, \"id\": 7.0}, {\"x\": -0.15916258710208359, \"y\": -0.3180250558711278, \"id\": 8.0}, {\"x\": -0.1466580043966531, \"y\": -0.3166044178140925, \"id\": 9.0}, {\"x\": -0.11569665465029463, \"y\": -0.27507341347162906, \"id\": 10.0}, {\"x\": -0.10013535944768197, \"y\": -0.22784700669661914, \"id\": 11.0}, {\"x\": -0.12905166604739432, \"y\": -0.2071446291709666, \"id\": 12.0}, {\"x\": -0.14271756231597862, \"y\": -0.2508039210394611, \"id\": 13.0}, {\"x\": -0.13741879538112697, \"y\": -0.27478263231154304, \"id\": 14.0}, {\"x\": -0.13445332259574821, \"y\": -0.24644340276353865, \"id\": 15.0}, {\"x\": -0.9578758462413073, \"y\": 0.17770291444097472, \"id\": 16.0}, {\"x\": -0.9286109055297189, \"y\": 0.180672255115451, \"id\": 17.0}, {\"x\": -0.05818896841716495, \"y\": -0.024414181320765033, \"id\": 18.0}, {\"x\": -0.07183262544818632, \"y\": -0.042498896918285775, \"id\": 19.0}, {\"x\": -0.12112646250555749, \"y\": -0.03214720104080549, \"id\": 20.0}, {\"x\": -0.10166508329327305, \"y\": -0.13260281294659096, \"id\": 21.0}, {\"x\": -0.06840055701831564, \"y\": 0.02407701603622507, \"id\": 22.0}, {\"x\": -0.09202740679831034, \"y\": -0.06923147365647722, \"id\": 23.0}, {\"x\": -0.18997259716385778, \"y\": -0.11066245210923148, \"id\": 24.0}, {\"x\": -0.0675123612720282, \"y\": -0.1365942017604369, \"id\": 25.0}, {\"x\": 0.01555844066934661, \"y\": -0.11196722037829362, \"id\": 26.0}, {\"x\": 0.07327337847372146, \"y\": -0.02274334124103196, \"id\": 27.0}, {\"x\": -0.09992051427352441, \"y\": -0.21558223756616396, \"id\": 28.0}, {\"x\": 0.009827164577304493, \"y\": 0.011969150957854575, \"id\": 29.0}, {\"x\": -0.03190853169300275, \"y\": 0.06299391075788366, \"id\": 30.0}, {\"x\": -0.10917566139158077, \"y\": 0.03036180577307121, \"id\": 31.0}, {\"x\": -0.11217308042859002, \"y\": -0.052709121764239864, \"id\": 32.0}, {\"x\": -0.09023568341000139, \"y\": -0.03912003645737478, \"id\": 33.0}, {\"x\": -0.06419144111152837, \"y\": -0.08288517507788387, \"id\": 34.0}, {\"x\": -0.06148592987207431, \"y\": -0.06535239641622947, \"id\": 35.0}, {\"x\": 0.1711484866389356, \"y\": 0.047483080270855134, \"id\": 36.0}, {\"x\": 0.21396573515508657, \"y\": 0.10606993993951315, \"id\": 37.0}, {\"x\": 0.13632901182577112, \"y\": -0.0375193104803328, \"id\": 38.0}, {\"x\": 0.22942764596132692, \"y\": -0.04293102853456445, \"id\": 39.0}, {\"x\": 0.2593039083738925, \"y\": -0.0418438258327181, \"id\": 40.0}, {\"x\": 0.3054043272464784, \"y\": -0.07497173532596402, \"id\": 41.0}, {\"x\": 0.3125494004778727, \"y\": -0.05834126928511468, \"id\": 42.0}, {\"x\": 0.056427457504084594, \"y\": 0.023052111387909423, \"id\": 43.0}, {\"x\": 0.08281810895160242, \"y\": -0.012893146467164888, \"id\": 44.0}, {\"x\": 0.14221237687469263, \"y\": 0.00591156559326728, \"id\": 45.0}, {\"x\": 0.19303499834114946, \"y\": 0.015416290846902747, \"id\": 46.0}, {\"x\": 0.12454620975849745, \"y\": 0.004060318457434848, \"id\": 47.0}, {\"x\": 0.16226555310120305, \"y\": -0.005004194335036632, \"id\": 48.0}, {\"x\": 0.1719680111471714, \"y\": -0.0607803710553539, \"id\": 49.0}, {\"x\": 0.16961919495454755, \"y\": -0.026124506098854063, \"id\": 50.0}, {\"x\": 0.12670173276621177, \"y\": -0.02940877817730907, \"id\": 51.0}, {\"x\": 0.22178343143465137, \"y\": -0.11767160535682882, \"id\": 52.0}, {\"x\": 0.19253943463590856, \"y\": -0.11056354453074833, \"id\": 53.0}, {\"x\": 0.09471192141615767, \"y\": 0.01767593882590723, \"id\": 54.0}, {\"x\": 0.20330986542290788, \"y\": 0.026610690376777588, \"id\": 55.0}, {\"x\": 0.12947180502423694, \"y\": -0.09240815551107316, \"id\": 56.0}, {\"x\": 0.18086811323942048, \"y\": -0.17629096003982858, \"id\": 57.0}, {\"x\": -0.129566842289825, \"y\": 0.0828576593062837, \"id\": 58.0}, {\"x\": -0.1359303859231084, \"y\": 0.045187628118315865, \"id\": 59.0}, {\"x\": -0.11526915040445217, \"y\": 0.011303709862621584, \"id\": 60.0}, {\"x\": 0.0765058900082385, \"y\": 0.10550349655861112, \"id\": 61.0}, {\"x\": 0.15678923199530473, \"y\": 0.17436264133149296, \"id\": 62.0}, {\"x\": -0.044333366833317324, \"y\": 0.03645810553430277, \"id\": 63.0}, {\"x\": 0.0015726656641651463, \"y\": 0.042423991886305794, \"id\": 64.0}, {\"x\": -0.1615845006331375, \"y\": -0.06451997678060535, \"id\": 65.0}, {\"x\": 0.02562639712079193, \"y\": 0.07139918504611162, \"id\": 66.0}, {\"x\": 0.12512946736741998, \"y\": 0.12420284401595356, \"id\": 67.0}, {\"x\": 0.12930838529187758, \"y\": 0.04930294572899866, \"id\": 68.0}, {\"x\": 0.06499654685147466, \"y\": 0.05076504933341431, \"id\": 69.0}, {\"x\": 0.11520085475572568, \"y\": 0.027593125185657312, \"id\": 70.0}, {\"x\": -0.018676323437857613, \"y\": -0.027416187164782193, \"id\": 71.0}, {\"x\": -0.09015529937777945, \"y\": 0.06859928174123174, \"id\": 72.0}, {\"x\": -0.17778688265058182, \"y\": 1.0, \"id\": 73.0}, {\"x\": -0.206351037761324, \"y\": 0.997174134984544, \"id\": 74.0}, {\"x\": -0.04568987856672662, \"y\": 0.10916287074655957, \"id\": 75.0}, {\"x\": -0.06624873531690086, \"y\": 0.05558734678423388, \"id\": 76.0}, {\"x\": -0.07022785790207993, \"y\": 0.09344730750644979, \"id\": 77.0}, {\"x\": -0.11391220001392693, \"y\": 0.09289604964041172, \"id\": 78.0}, {\"x\": 0.23428326923554282, \"y\": 0.1455176576368486, \"id\": 79.0}, {\"x\": 0.21092301140778313, \"y\": 0.17247496333821036, \"id\": 80.0}, {\"x\": 0.28796666734671983, \"y\": 0.18747561414313713, \"id\": 81.0}, {\"x\": 0.2786842077476909, \"y\": 0.20180529271738848, \"id\": 82.0}, {\"x\": 0.21837442264953602, \"y\": 0.11841419403997723, \"id\": 83.0}, {\"x\": -0.19745425247945897, \"y\": -0.27120405127703506, \"id\": 84.0}, {\"x\": -0.16613248928451432, \"y\": -0.2494687678551724, \"id\": 85.0}, {\"x\": -0.20574340160449894, \"y\": -0.24650064929612356, \"id\": 86.0}, {\"x\": -0.20733371983185353, \"y\": -0.26079972581200556, \"id\": 87.0}, {\"x\": -0.14325802404642687, \"y\": -0.06367572716925764, \"id\": 88.0}, {\"x\": 0.1306705348653082, \"y\": -0.0008491090228239166, \"id\": 89.0}, {\"x\": 0.1857404129361362, \"y\": -0.008847597765420238, \"id\": 90.0}, {\"x\": 0.27682614708816616, \"y\": 0.04143079610206784, \"id\": 91.0}, {\"x\": 0.27652528725392855, \"y\": 0.05921266905758577, \"id\": 92.0}, {\"x\": 0.1493938975418612, \"y\": -0.013586736562355435, \"id\": 93.0}, {\"x\": 0.21030166093408367, \"y\": -0.014619795101153854, \"id\": 94.0}, {\"x\": 0.18208660433602217, \"y\": -0.03142957811841357, \"id\": 95.0}, {\"x\": 0.20403074353407796, \"y\": -0.07332898085776841, \"id\": 96.0}, {\"x\": 0.1052822345107166, \"y\": 0.0013827111019374546, \"id\": 97.0}, {\"x\": -0.047198362494428214, \"y\": 0.06363298454542243, \"id\": 98.0}, {\"x\": -0.10299342519647912, \"y\": -0.07985723383561413, \"id\": 99.0}, {\"x\": -0.07196025040825678, \"y\": -0.10303828596702301, \"id\": 100.0}, {\"x\": -0.14916093623352256, \"y\": -0.35072097679131975, \"id\": 101.0}, {\"x\": -0.1734239553872992, \"y\": -0.02449448699782483, \"id\": 102.0}, {\"x\": -0.14266031547280894, \"y\": 0.017505601280870555, \"id\": 103.0}, {\"x\": -0.1651022035957404, \"y\": -0.04497884255293872, \"id\": 104.0}, {\"x\": 0.10560066435872675, \"y\": 0.16536952856455303, \"id\": 105.0}, {\"x\": 0.7955450547645584, \"y\": -0.4317829697411659, \"id\": 106.0}, {\"x\": 0.7805413345704385, \"y\": -0.40680645160878937, \"id\": 107.0}, {\"x\": -0.15995067451222547, \"y\": 0.010765874210307754, \"id\": 108.0}, {\"x\": -0.19677523860794632, \"y\": 0.10794372697380812, \"id\": 109.0}, {\"x\": -0.16271620928766461, \"y\": 0.049341122201079037, \"id\": 110.0}, {\"x\": -0.2775771841167416, \"y\": 0.15202063068278032, \"id\": 111.0}, {\"x\": -0.054205115196575435, \"y\": 0.1468004156199474, \"id\": 112.0}, {\"x\": -0.061448185367512814, \"y\": 0.16556652306142625, \"id\": 113.0}, {\"x\": -0.085186032602534, \"y\": 0.14983446173937112, \"id\": 114.0}, {\"x\": -0.09352462804570229, \"y\": 0.1987575701306277, \"id\": 115.0}, {\"x\": -0.06976366605528862, \"y\": 0.20827218033355427, \"id\": 116.0}, {\"x\": 0.06262382413958945, \"y\": 0.17156725535811393, \"id\": 117.0}, {\"x\": -0.12476524880731904, \"y\": -0.13320973497727737, \"id\": 118.0}, {\"x\": 0.10204379289386956, \"y\": -0.11461680183343823, \"id\": 119.0}, {\"x\": 0.04272606552864228, \"y\": 0.11118288494544035, \"id\": 120.0}, {\"x\": 0.13142848912300195, \"y\": 0.08943493175815392, \"id\": 121.0}, {\"x\": 0.10497348057817454, \"y\": 0.07798816626957444, \"id\": 122.0}, {\"x\": 0.18403301252359253, \"y\": 0.0692229163868061, \"id\": 123.0}, {\"x\": 0.2888032695397457, \"y\": -0.18207447203761953, \"id\": 124.0}, {\"x\": 0.017044446665228825, \"y\": -0.06892376678482033, \"id\": 125.0}, {\"x\": 0.08055317585968884, \"y\": -0.11887342653247886, \"id\": 126.0}, {\"x\": -0.1704206846060183, \"y\": -0.00339958445457216, \"id\": 127.0}, {\"x\": 0.08408812786074164, \"y\": 0.17293139206219885, \"id\": 128.0}, {\"x\": 0.09100677664902258, \"y\": 0.2115144272801116, \"id\": 129.0}, {\"x\": 0.0679606948694571, \"y\": 0.2101052502291791, \"id\": 130.0}, {\"x\": 0.08106895315733699, \"y\": 0.1463743587367659, \"id\": 131.0}, {\"x\": -0.1473046573488152, \"y\": -0.010111306146297653, \"id\": 132.0}, {\"x\": 0.1299003923607913, \"y\": 0.19707592360624399, \"id\": 133.0}, {\"x\": 0.11126131161317918, \"y\": 0.20695548075123923, \"id\": 134.0}, {\"x\": 0.23188607153781185, \"y\": 0.005330328540771689, \"id\": 135.0}, {\"x\": 0.08415385728694366, \"y\": -0.10002749813791104, \"id\": 136.0}, {\"x\": -0.13928941361168168, \"y\": 0.1694414987429792, \"id\": 137.0}, {\"x\": 0.12003295515418039, \"y\": -0.11216833071760567, \"id\": 138.0}, {\"x\": 0.015899068650726215, \"y\": 0.06488966703901473, \"id\": 139.0}, {\"x\": -0.12447358796066915, \"y\": -0.09450098486430009, \"id\": 140.0}, {\"x\": -0.08643426121979292, \"y\": -0.01344136938846853, \"id\": 141.0}, {\"x\": 0.002387980116398114, \"y\": 0.031050859678300714, \"id\": 142.0}, {\"x\": -0.08504197444015457, \"y\": -0.2963860817703671, \"id\": 143.0}, {\"x\": -0.07985989343518832, \"y\": -0.3246025864534094, \"id\": 144.0}, {\"x\": -0.06893170463044125, \"y\": -0.2844255270446289, \"id\": 145.0}, {\"x\": -0.15130321331834784, \"y\": -0.031167044152272526, \"id\": 146.0}, {\"x\": 0.1583056974309385, \"y\": 0.08054303027166111, \"id\": 147.0}, {\"x\": 0.27166775912051255, \"y\": 0.08037294871408766, \"id\": 148.0}, {\"x\": 0.20449236804455034, \"y\": 0.09542362324000947, \"id\": 149.0}, {\"x\": 0.19413321429240044, \"y\": 0.12347534109222182, \"id\": 150.0}, {\"x\": 0.15506769838740594, \"y\": 0.028092475139554388, \"id\": 151.0}, {\"x\": 0.17696292221902368, \"y\": 0.13312066951629062, \"id\": 152.0}, {\"x\": -0.138648966183581, \"y\": -0.08021061797451869, \"id\": 153.0}]}};\n",
      "    var embedOpt3 = {\"mode\": \"vega-lite\"};\n",
      "\n",
      "    function showError(el3, error){\n",
      "        el3.innerHTML = ('<div class=\"error\" style=\"color:red;\">'\n",
      "                        + '<p>JavaScript Error: ' + error.message + '</p>'\n",
      "                        + \"<p>This usually means there's a typo in your chart specification. \"\n",
      "                        + \"See the javascript console for the full traceback.</p>\"\n",
      "                        + '</div>');\n",
      "        throw error;\n",
      "    }\n",
      "    const el3 = document.getElementById('vis3');\n",
      "    vegaEmbed(\"#vis3\", spec3, embedOpt3)\n",
      "      .catch(error => showError(el3, error));\n",
      "  })(vegaEmbed);\n",
      "\n",
      "</script>\n",
      "\n",
      "Now, we can try to display nodes in a different color if their betweenness centrality is above a certain threshold (here, 0.15). We identify three central nodes, which also look like the main information bottleneck in the graph. I do not have much information on these nodes, but we can easily suppose that these people play a key role in their communities. Zoom in the graph to see more details:\n",
      "\n",
      "```python\n",
      "c_degree = nx.betweenness_centrality(G)\n",
      "c_degree = list(c_degree.values())\n",
      "\n",
      "chart = nxa.draw_networkx(\n",
      "    G=G,\n",
      "    pos=nx.spring_layout(G),\n",
      "    width='weight:N',\n",
      "    node_tooltip=['id','is_central']\n",
      ")\n",
      "\n",
      "edges = chart.layer[0].properties(height=500, width=500)\n",
      "nodes = chart.layer[1].properties(height=500, width=500)\n",
      "\n",
      "nodes = nodes.encode(\n",
      "    fill='is_central:N',\n",
      "    color='is_central:N'\n",
      ").properties(\n",
      "    height=500, width=500, title=\"Centrality\"\n",
      ")\n",
      "\n",
      "central = (edges+nodes).interactive()\n",
      "\n",
      "central\n",
      "```\n",
      "\n",
      "<div id=\"vis4\"></div>\n",
      "<script>\n",
      "  (function(vegaEmbed) {\n",
      "    var spec4 = {\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"data\": {\"name\": \"data-1fe126b007e78444b3277fd4bf52a855\"}, \"mark\": {\"type\": \"line\", \"color\": \"black\", \"opacity\": 1}, \"encoding\": {\"detail\": {\"type\": \"quantitative\", \"field\": \"edge\"}, \"size\": {\"type\": \"nominal\", \"field\": \"weight\", \"legend\": null}, \"x\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"\"}, \"field\": \"x\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"\"}, \"field\": \"y\"}}, \"height\": 500, \"selection\": {\"selector021\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 500}, {\"data\": {\"name\": \"data-42c9cfb11ce5c3379bd7fe466969adf5\"}, \"mark\": {\"type\": \"point\", \"fill\": \"red\", \"opacity\": 1, \"size\": 300}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"is_central\"}, \"fill\": {\"type\": \"nominal\", \"field\": \"is_central\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"id\"}, {\"type\": \"nominal\", \"field\": \"is_central\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"x\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"y\"}}, \"height\": 500, \"title\": \"Centrality\", \"width\": 500}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-1fe126b007e78444b3277fd4bf52a855\": [{\"x\": 0.7201729516614895, \"source\": 0, \"method\": \"meetings\", \"target\": 1, \"weight\": 1, \"y\": -0.5941501489847846, \"pair\": [0, 1], \"edge\": 0}, {\"x\": 0.6962269127915148, \"source\": 0, \"method\": \"meetings\", \"target\": 1, \"weight\": 1, \"y\": -0.5955104671854762, \"pair\": [0, 1], \"edge\": 0}, {\"x\": 0.7201729516614895, \"source\": 0, \"method\": \"meetings\", \"target\": 2, \"weight\": 1, \"y\": -0.5941501489847846, \"pair\": [0, 2], \"edge\": 1}, {\"x\": 0.7379742090422982, \"source\": 0, \"method\": \"meetings\", \"target\": 2, \"weight\": 1, \"y\": -0.6257225236087908, \"pair\": [0, 2], \"edge\": 1}, {\"x\": 0.6962269127915148, \"source\": 1, \"method\": \"meetings\", \"target\": 2, \"weight\": 1, \"y\": -0.5955104671854762, \"pair\": [1, 2], \"edge\": 2}, {\"x\": 0.7379742090422982, \"source\": 1, \"method\": \"meetings\", \"target\": 2, \"weight\": 1, \"y\": -0.6257225236087908, \"pair\": [1, 2], \"edge\": 2}, {\"x\": -0.14963029070486847, \"source\": 3, \"method\": \"meetings\", \"target\": 4, \"weight\": 1, \"y\": 0.22839760684015903, \"pair\": [3, 4], \"edge\": 3}, {\"x\": -0.13388635482463185, \"source\": 3, \"method\": \"meetings\", \"target\": 4, \"weight\": 1, \"y\": 0.26327789596963713, \"pair\": [3, 4], \"edge\": 3}, {\"x\": -0.14963029070486847, \"source\": 3, \"method\": \"meetings\", \"target\": 5, \"weight\": 1, \"y\": 0.22839760684015903, \"pair\": [3, 5], \"edge\": 4}, {\"x\": -0.12775596921398163, \"source\": 3, \"method\": \"meetings\", \"target\": 5, \"weight\": 1, \"y\": 0.19934696076295597, \"pair\": [3, 5], \"edge\": 4}, {\"x\": -0.14963029070486847, \"source\": 3, \"method\": \"meetings\", \"target\": 6, \"weight\": 1, \"y\": 0.22839760684015903, \"pair\": [3, 6], \"edge\": 5}, {\"x\": -0.13521738906463823, \"source\": 3, \"method\": \"meetings\", \"target\": 6, \"weight\": 1, \"y\": 0.1960690938502262, \"pair\": [3, 6], \"edge\": 5}, {\"x\": -0.14963029070486847, \"source\": 3, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": 0.22839760684015903, \"pair\": [3, 7], \"edge\": 6}, {\"x\": -0.15003889180280985, \"source\": 3, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": 0.25674390970692657, \"pair\": [3, 7], \"edge\": 6}, {\"x\": -0.14963029070486847, \"source\": 3, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": 0.22839760684015903, \"pair\": [3, 8], \"edge\": 7}, {\"x\": -0.1628937526808075, \"source\": 3, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": 0.2596325546941219, \"pair\": [3, 8], \"edge\": 7}, {\"x\": -0.14963029070486847, \"source\": 3, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.22839760684015903, \"pair\": [3, 9], \"edge\": 8}, {\"x\": -0.14646704789004872, \"source\": 3, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.2690013976338763, \"pair\": [3, 9], \"edge\": 8}, {\"x\": -0.14963029070486847, \"source\": 3, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": 0.22839760684015903, \"pair\": [3, 11], \"edge\": 9}, {\"x\": -0.16285777662829634, \"source\": 3, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": 0.1563535106528167, \"pair\": [3, 11], \"edge\": 9}, {\"x\": -0.14963029070486847, \"source\": 3, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": 0.22839760684015903, \"pair\": [3, 12], \"edge\": 10}, {\"x\": -0.12556282436714905, \"source\": 3, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": 0.16005918201067507, \"pair\": [3, 12], \"edge\": 10}, {\"x\": -0.13388635482463185, \"source\": 4, \"method\": \"meetings\", \"target\": 5, \"weight\": 1, \"y\": 0.26327789596963713, \"pair\": [4, 5], \"edge\": 11}, {\"x\": -0.12775596921398163, \"source\": 4, \"method\": \"meetings\", \"target\": 5, \"weight\": 1, \"y\": 0.19934696076295597, \"pair\": [4, 5], \"edge\": 11}, {\"x\": -0.13388635482463185, \"source\": 4, \"method\": \"meetings\", \"target\": 6, \"weight\": 1, \"y\": 0.26327789596963713, \"pair\": [4, 6], \"edge\": 12}, {\"x\": -0.13521738906463823, \"source\": 4, \"method\": \"meetings\", \"target\": 6, \"weight\": 1, \"y\": 0.1960690938502262, \"pair\": [4, 6], \"edge\": 12}, {\"x\": -0.13388635482463185, \"source\": 4, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": 0.26327789596963713, \"pair\": [4, 7], \"edge\": 13}, {\"x\": -0.15003889180280985, \"source\": 4, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": 0.25674390970692657, \"pair\": [4, 7], \"edge\": 13}, {\"x\": -0.13388635482463185, \"source\": 4, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": 0.26327789596963713, \"pair\": [4, 8], \"edge\": 14}, {\"x\": -0.1628937526808075, \"source\": 4, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": 0.2596325546941219, \"pair\": [4, 8], \"edge\": 14}, {\"x\": -0.13388635482463185, \"source\": 4, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.26327789596963713, \"pair\": [4, 9], \"edge\": 15}, {\"x\": -0.14646704789004872, \"source\": 4, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.2690013976338763, \"pair\": [4, 9], \"edge\": 15}, {\"x\": -0.12775596921398163, \"source\": 5, \"method\": \"meetings\", \"target\": 6, \"weight\": 1, \"y\": 0.19934696076295597, \"pair\": [5, 6], \"edge\": 16}, {\"x\": -0.13521738906463823, \"source\": 5, \"method\": \"meetings\", \"target\": 6, \"weight\": 1, \"y\": 0.1960690938502262, \"pair\": [5, 6], \"edge\": 16}, {\"x\": -0.12775596921398163, \"source\": 5, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": 0.19934696076295597, \"pair\": [5, 7], \"edge\": 17}, {\"x\": -0.15003889180280985, \"source\": 5, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": 0.25674390970692657, \"pair\": [5, 7], \"edge\": 17}, {\"x\": -0.12775596921398163, \"source\": 5, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": 0.19934696076295597, \"pair\": [5, 8], \"edge\": 18}, {\"x\": -0.1628937526808075, \"source\": 5, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": 0.2596325546941219, \"pair\": [5, 8], \"edge\": 18}, {\"x\": -0.12775596921398163, \"source\": 5, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.19934696076295597, \"pair\": [5, 9], \"edge\": 19}, {\"x\": -0.14646704789004872, \"source\": 5, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.2690013976338763, \"pair\": [5, 9], \"edge\": 19}, {\"x\": -0.12775596921398163, \"source\": 5, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": 0.19934696076295597, \"pair\": [5, 11], \"edge\": 20}, {\"x\": -0.16285777662829634, \"source\": 5, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": 0.1563535106528167, \"pair\": [5, 11], \"edge\": 20}, {\"x\": -0.12775596921398163, \"source\": 5, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": 0.19934696076295597, \"pair\": [5, 12], \"edge\": 21}, {\"x\": -0.12556282436714905, \"source\": 5, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": 0.16005918201067507, \"pair\": [5, 12], \"edge\": 21}, {\"x\": -0.12775596921398163, \"source\": 5, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": 0.19934696076295597, \"pair\": [5, 28], \"edge\": 22}, {\"x\": -0.13263709675378124, \"source\": 5, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": 0.1526276065747802, \"pair\": [5, 28], \"edge\": 22}, {\"x\": -0.12775596921398163, \"source\": 5, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": 0.19934696076295597, \"pair\": [5, 25], \"edge\": 23}, {\"x\": -0.09643094783333603, \"source\": 5, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": 0.0738142784386027, \"pair\": [5, 25], \"edge\": 23}, {\"x\": -0.13521738906463823, \"source\": 6, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": 0.1960690938502262, \"pair\": [6, 7], \"edge\": 24}, {\"x\": -0.15003889180280985, \"source\": 6, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": 0.25674390970692657, \"pair\": [6, 7], \"edge\": 24}, {\"x\": -0.13521738906463823, \"source\": 6, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": 0.1960690938502262, \"pair\": [6, 8], \"edge\": 25}, {\"x\": -0.1628937526808075, \"source\": 6, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": 0.2596325546941219, \"pair\": [6, 8], \"edge\": 25}, {\"x\": -0.13521738906463823, \"source\": 6, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.1960690938502262, \"pair\": [6, 9], \"edge\": 26}, {\"x\": -0.14646704789004872, \"source\": 6, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.2690013976338763, \"pair\": [6, 9], \"edge\": 26}, {\"x\": -0.13521738906463823, \"source\": 6, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": 0.1960690938502262, \"pair\": [6, 11], \"edge\": 27}, {\"x\": -0.16285777662829634, \"source\": 6, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": 0.1563535106528167, \"pair\": [6, 11], \"edge\": 27}, {\"x\": -0.13521738906463823, \"source\": 6, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": 0.1960690938502262, \"pair\": [6, 12], \"edge\": 28}, {\"x\": -0.12556282436714905, \"source\": 6, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": 0.16005918201067507, \"pair\": [6, 12], \"edge\": 28}, {\"x\": -0.13521738906463823, \"source\": 6, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": 0.1960690938502262, \"pair\": [6, 28], \"edge\": 29}, {\"x\": -0.13263709675378124, \"source\": 6, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": 0.1526276065747802, \"pair\": [6, 28], \"edge\": 29}, {\"x\": -0.13521738906463823, \"source\": 6, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": 0.1960690938502262, \"pair\": [6, 25], \"edge\": 30}, {\"x\": -0.09643094783333603, \"source\": 6, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": 0.0738142784386027, \"pair\": [6, 25], \"edge\": 30}, {\"x\": -0.15003889180280985, \"source\": 7, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": 0.25674390970692657, \"pair\": [7, 8], \"edge\": 31}, {\"x\": -0.1628937526808075, \"source\": 7, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": 0.2596325546941219, \"pair\": [7, 8], \"edge\": 31}, {\"x\": -0.15003889180280985, \"source\": 7, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.25674390970692657, \"pair\": [7, 9], \"edge\": 32}, {\"x\": -0.14646704789004872, \"source\": 7, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.2690013976338763, \"pair\": [7, 9], \"edge\": 32}, {\"x\": -0.1628937526808075, \"source\": 8, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.2596325546941219, \"pair\": [8, 9], \"edge\": 33}, {\"x\": -0.14646704789004872, \"source\": 8, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.2690013976338763, \"pair\": [8, 9], \"edge\": 33}, {\"x\": -0.1881614913715324, \"source\": 10, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": 0.20067016895572298, \"pair\": [10, 11], \"edge\": 34}, {\"x\": -0.16285777662829634, \"source\": 10, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": 0.1563535106528167, \"pair\": [10, 11], \"edge\": 34}, {\"x\": -0.1881614913715324, \"source\": 10, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": 0.20067016895572298, \"pair\": [10, 12], \"edge\": 35}, {\"x\": -0.12556282436714905, \"source\": 10, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": 0.16005918201067507, \"pair\": [10, 12], \"edge\": 35}, {\"x\": -0.1881614913715324, \"source\": 10, \"method\": \"meetings\", \"target\": 13, \"weight\": 1, \"y\": 0.20067016895572298, \"pair\": [10, 13], \"edge\": 36}, {\"x\": -0.16864511068212604, \"source\": 10, \"method\": \"meetings\", \"target\": 13, \"weight\": 1, \"y\": 0.19014193414739683, \"pair\": [10, 13], \"edge\": 36}, {\"x\": -0.1881614913715324, \"source\": 10, \"method\": \"meetings\", \"target\": 14, \"weight\": 1, \"y\": 0.20067016895572298, \"pair\": [10, 14], \"edge\": 37}, {\"x\": -0.17289915231534964, \"source\": 10, \"method\": \"meetings\", \"target\": 14, \"weight\": 1, \"y\": 0.2124714170018183, \"pair\": [10, 14], \"edge\": 37}, {\"x\": -0.1881614913715324, \"source\": 10, \"method\": \"meetings\", \"target\": 15, \"weight\": 1, \"y\": 0.20067016895572298, \"pair\": [10, 15], \"edge\": 38}, {\"x\": -0.1592264495980308, \"source\": 10, \"method\": \"meetings\", \"target\": 15, \"weight\": 1, \"y\": 0.19558225301864735, \"pair\": [10, 15], \"edge\": 38}, {\"x\": -0.16285777662829634, \"source\": 11, \"method\": \"phonecalls\", \"target\": 12, \"weight\": 1, \"y\": 0.1563535106528167, \"pair\": [11, 12], \"edge\": 39}, {\"x\": -0.12556282436714905, \"source\": 11, \"method\": \"phonecalls\", \"target\": 12, \"weight\": 1, \"y\": 0.16005918201067507, \"pair\": [11, 12], \"edge\": 39}, {\"x\": -0.16285777662829634, \"source\": 11, \"method\": \"meetings\", \"target\": 13, \"weight\": 2, \"y\": 0.1563535106528167, \"pair\": [11, 13], \"edge\": 40}, {\"x\": -0.16864511068212604, \"source\": 11, \"method\": \"meetings\", \"target\": 13, \"weight\": 2, \"y\": 0.19014193414739683, \"pair\": [11, 13], \"edge\": 40}, {\"x\": -0.16285777662829634, \"source\": 11, \"method\": \"meetings\", \"target\": 14, \"weight\": 2, \"y\": 0.1563535106528167, \"pair\": [11, 14], \"edge\": 41}, {\"x\": -0.17289915231534964, \"source\": 11, \"method\": \"meetings\", \"target\": 14, \"weight\": 2, \"y\": 0.2124714170018183, \"pair\": [11, 14], \"edge\": 41}, {\"x\": -0.16285777662829634, \"source\": 11, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": 0.1563535106528167, \"pair\": [11, 15], \"edge\": 42}, {\"x\": -0.1592264495980308, \"source\": 11, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": 0.19558225301864735, \"pair\": [11, 15], \"edge\": 42}, {\"x\": -0.16285777662829634, \"source\": 11, \"method\": \"meetings\", \"target\": 18, \"weight\": 1, \"y\": 0.1563535106528167, \"pair\": [11, 18], \"edge\": 43}, {\"x\": -0.08453031363724207, \"source\": 11, \"method\": \"meetings\", \"target\": 18, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [11, 18], \"edge\": 43}, {\"x\": -0.16285777662829634, \"source\": 11, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": 0.1563535106528167, \"pair\": [11, 25], \"edge\": 44}, {\"x\": -0.09643094783333603, \"source\": 11, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": 0.0738142784386027, \"pair\": [11, 25], \"edge\": 44}, {\"x\": -0.16285777662829634, \"source\": 11, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": 0.1563535106528167, \"pair\": [11, 28], \"edge\": 45}, {\"x\": -0.13263709675378124, \"source\": 11, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": 0.1526276065747802, \"pair\": [11, 28], \"edge\": 45}, {\"x\": -0.16285777662829634, \"source\": 11, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": 0.1563535106528167, \"pair\": [11, 85], \"edge\": 46}, {\"x\": -0.13357750855656716, \"source\": 11, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": 0.21357498512004602, \"pair\": [11, 85], \"edge\": 46}, {\"x\": -0.16285777662829634, \"source\": 11, \"method\": \"phonecalls\", \"target\": 145, \"weight\": 2, \"y\": 0.1563535106528167, \"pair\": [11, 145], \"edge\": 47}, {\"x\": -0.21650989209795615, \"source\": 11, \"method\": \"phonecalls\", \"target\": 145, \"weight\": 2, \"y\": 0.19878735994399557, \"pair\": [11, 145], \"edge\": 47}, {\"x\": -0.16285777662829634, \"source\": 11, \"method\": \"phonecalls\", \"target\": 144, \"weight\": 1, \"y\": 0.1563535106528167, \"pair\": [11, 144], \"edge\": 48}, {\"x\": -0.24621473408099437, \"source\": 11, \"method\": \"phonecalls\", \"target\": 144, \"weight\": 1, \"y\": 0.20826693839176622, \"pair\": [11, 144], \"edge\": 48}, {\"x\": -0.16285777662829634, \"source\": 11, \"method\": \"phonecalls\", \"target\": 143, \"weight\": 2, \"y\": 0.1563535106528167, \"pair\": [11, 143], \"edge\": 49}, {\"x\": -0.22780490089040933, \"source\": 11, \"method\": \"phonecalls\", \"target\": 143, \"weight\": 2, \"y\": 0.18413310479709935, \"pair\": [11, 143], \"edge\": 49}, {\"x\": -0.12556282436714905, \"source\": 12, \"method\": \"meetings\", \"target\": 13, \"weight\": 2, \"y\": 0.16005918201067507, \"pair\": [12, 13], \"edge\": 50}, {\"x\": -0.16864511068212604, \"source\": 12, \"method\": \"meetings\", \"target\": 13, \"weight\": 2, \"y\": 0.19014193414739683, \"pair\": [12, 13], \"edge\": 50}, {\"x\": -0.12556282436714905, \"source\": 12, \"method\": \"meetings\", \"target\": 14, \"weight\": 2, \"y\": 0.16005918201067507, \"pair\": [12, 14], \"edge\": 51}, {\"x\": -0.17289915231534964, \"source\": 12, \"method\": \"meetings\", \"target\": 14, \"weight\": 2, \"y\": 0.2124714170018183, \"pair\": [12, 14], \"edge\": 51}, {\"x\": -0.12556282436714905, \"source\": 12, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": 0.16005918201067507, \"pair\": [12, 15], \"edge\": 52}, {\"x\": -0.1592264495980308, \"source\": 12, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": 0.19558225301864735, \"pair\": [12, 15], \"edge\": 52}, {\"x\": -0.12556282436714905, \"source\": 12, \"method\": \"meetings\", \"target\": 18, \"weight\": 1, \"y\": 0.16005918201067507, \"pair\": [12, 18], \"edge\": 53}, {\"x\": -0.08453031363724207, \"source\": 12, \"method\": \"meetings\", \"target\": 18, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [12, 18], \"edge\": 53}, {\"x\": -0.12556282436714905, \"source\": 12, \"method\": \"meetings\", \"target\": 21, \"weight\": 1, \"y\": 0.16005918201067507, \"pair\": [12, 21], \"edge\": 54}, {\"x\": -0.10342108981135545, \"source\": 12, \"method\": \"meetings\", \"target\": 21, \"weight\": 1, \"y\": 0.09031950817859964, \"pair\": [12, 21], \"edge\": 54}, {\"x\": -0.12556282436714905, \"source\": 12, \"method\": \"meetings\", \"target\": 25, \"weight\": 2, \"y\": 0.16005918201067507, \"pair\": [12, 25], \"edge\": 55}, {\"x\": -0.09643094783333603, \"source\": 12, \"method\": \"meetings\", \"target\": 25, \"weight\": 2, \"y\": 0.0738142784386027, \"pair\": [12, 25], \"edge\": 55}, {\"x\": -0.12556282436714905, \"source\": 12, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": 0.16005918201067507, \"pair\": [12, 28], \"edge\": 56}, {\"x\": -0.13263709675378124, \"source\": 12, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": 0.1526276065747802, \"pair\": [12, 28], \"edge\": 56}, {\"x\": -0.12556282436714905, \"source\": 12, \"method\": \"meetings\", \"target\": 84, \"weight\": 1, \"y\": 0.16005918201067507, \"pair\": [12, 84], \"edge\": 57}, {\"x\": -0.10073022402935924, \"source\": 12, \"method\": \"meetings\", \"target\": 84, \"weight\": 1, \"y\": 0.24866716443059705, \"pair\": [12, 84], \"edge\": 57}, {\"x\": -0.12556282436714905, \"source\": 12, \"method\": \"meetings\", \"target\": 85, \"weight\": 2, \"y\": 0.16005918201067507, \"pair\": [12, 85], \"edge\": 58}, {\"x\": -0.13357750855656716, \"source\": 12, \"method\": \"meetings\", \"target\": 85, \"weight\": 2, \"y\": 0.21357498512004602, \"pair\": [12, 85], \"edge\": 58}, {\"x\": -0.12556282436714905, \"source\": 12, \"method\": \"meetings\", \"target\": 86, \"weight\": 1, \"y\": 0.16005918201067507, \"pair\": [12, 86], \"edge\": 59}, {\"x\": -0.11628078973510426, \"source\": 12, \"method\": \"meetings\", \"target\": 86, \"weight\": 1, \"y\": 0.24607240222139293, \"pair\": [12, 86], \"edge\": 59}, {\"x\": -0.12556282436714905, \"source\": 12, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": 0.16005918201067507, \"pair\": [12, 87], \"edge\": 60}, {\"x\": -0.0989390875103513, \"source\": 12, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": 0.23457672531171775, \"pair\": [12, 87], \"edge\": 60}, {\"x\": -0.16864511068212604, \"source\": 13, \"method\": \"meetings\", \"target\": 14, \"weight\": 2, \"y\": 0.19014193414739683, \"pair\": [13, 14], \"edge\": 61}, {\"x\": -0.17289915231534964, \"source\": 13, \"method\": \"meetings\", \"target\": 14, \"weight\": 2, \"y\": 0.2124714170018183, \"pair\": [13, 14], \"edge\": 61}, {\"x\": -0.16864511068212604, \"source\": 13, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": 0.19014193414739683, \"pair\": [13, 15], \"edge\": 62}, {\"x\": -0.1592264495980308, \"source\": 13, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": 0.19558225301864735, \"pair\": [13, 15], \"edge\": 62}, {\"x\": -0.16864511068212604, \"source\": 13, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": 0.19014193414739683, \"pair\": [13, 85], \"edge\": 63}, {\"x\": -0.13357750855656716, \"source\": 13, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": 0.21357498512004602, \"pair\": [13, 85], \"edge\": 63}, {\"x\": -0.17289915231534964, \"source\": 14, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": 0.2124714170018183, \"pair\": [14, 15], \"edge\": 64}, {\"x\": -0.1592264495980308, \"source\": 14, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": 0.19558225301864735, \"pair\": [14, 15], \"edge\": 64}, {\"x\": -0.17289915231534964, \"source\": 14, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": 0.2124714170018183, \"pair\": [14, 85], \"edge\": 65}, {\"x\": -0.13357750855656716, \"source\": 14, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": 0.21357498512004602, \"pair\": [14, 85], \"edge\": 65}, {\"x\": -0.17289915231534964, \"source\": 14, \"method\": \"phonecalls\", \"target\": 101, \"weight\": 2, \"y\": 0.2124714170018183, \"pair\": [14, 101], \"edge\": 66}, {\"x\": -0.2093653301248616, \"source\": 14, \"method\": \"phonecalls\", \"target\": 101, \"weight\": 2, \"y\": 0.2718010379232694, \"pair\": [14, 101], \"edge\": 66}, {\"x\": -0.1592264495980308, \"source\": 15, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": 0.19558225301864735, \"pair\": [15, 85], \"edge\": 67}, {\"x\": -0.13357750855656716, \"source\": 15, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": 0.21357498512004602, \"pair\": [15, 85], \"edge\": 67}, {\"x\": -0.08054038557313334, \"source\": 16, \"method\": \"meetings\", \"target\": 17, \"weight\": 1, \"y\": 1.0, \"pair\": [16, 17], \"edge\": 68}, {\"x\": -0.05591659914603527, \"source\": 16, \"method\": \"meetings\", \"target\": 17, \"weight\": 1, \"y\": 0.9865849341603224, \"pair\": [16, 17], \"edge\": 68}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 19, \"weight\": 6, \"y\": -0.02574806163662363, \"pair\": [18, 19], \"edge\": 69}, {\"x\": -0.11459200425919056, \"source\": 18, \"method\": \"phonecalls\", \"target\": 19, \"weight\": 6, \"y\": -0.03306616016918993, \"pair\": [18, 19], \"edge\": 69}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"meetings\", \"target\": 20, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 20], \"edge\": 70}, {\"x\": -0.1260335142497204, \"source\": 18, \"method\": \"meetings\", \"target\": 20, \"weight\": 1, \"y\": -0.07916616634145784, \"pair\": [18, 20], \"edge\": 70}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 21, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 21], \"edge\": 71}, {\"x\": -0.10342108981135545, \"source\": 18, \"method\": \"phonecalls\", \"target\": 21, \"weight\": 1, \"y\": 0.09031950817859964, \"pair\": [18, 21], \"edge\": 71}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"meetings\", \"target\": 22, \"weight\": 2, \"y\": -0.02574806163662363, \"pair\": [18, 22], \"edge\": 72}, {\"x\": -0.09319299341154005, \"source\": 18, \"method\": \"meetings\", \"target\": 22, \"weight\": 2, \"y\": -0.0031457699240462016, \"pair\": [18, 22], \"edge\": 72}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"meetings\", \"target\": 31, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 31], \"edge\": 73}, {\"x\": -0.14227761922441623, \"source\": 18, \"method\": \"meetings\", \"target\": 31, \"weight\": 1, \"y\": -0.038928683932491444, \"pair\": [18, 31], \"edge\": 73}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 34, \"weight\": 3, \"y\": -0.02574806163662363, \"pair\": [18, 34], \"edge\": 74}, {\"x\": -0.11311982103657724, \"source\": 18, \"method\": \"phonecalls\", \"target\": 34, \"weight\": 3, \"y\": 0.015511622301406802, \"pair\": [18, 34], \"edge\": 74}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 35, \"weight\": 5, \"y\": -0.02574806163662363, \"pair\": [18, 35], \"edge\": 75}, {\"x\": -0.11155611366244667, \"source\": 18, \"method\": \"phonecalls\", \"target\": 35, \"weight\": 5, \"y\": -0.0016514281767455977, \"pair\": [18, 35], \"edge\": 75}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"meetings\", \"target\": 23, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 23], \"edge\": 76}, {\"x\": -0.1477490548119252, \"source\": 18, \"method\": \"meetings\", \"target\": 23, \"weight\": 1, \"y\": 0.00631745618334823, \"pair\": [18, 23], \"edge\": 76}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 25, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 25], \"edge\": 77}, {\"x\": -0.09643094783333603, \"source\": 18, \"method\": \"phonecalls\", \"target\": 25, \"weight\": 1, \"y\": 0.0738142784386027, \"pair\": [18, 25], \"edge\": 77}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 58, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 58], \"edge\": 78}, {\"x\": -0.1796553108650634, \"source\": 18, \"method\": \"phonecalls\", \"target\": 58, \"weight\": 1, \"y\": 0.016978230999206884, \"pair\": [18, 58], \"edge\": 78}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"meetings\", \"target\": 59, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 59], \"edge\": 79}, {\"x\": -0.18547205897282934, \"source\": 18, \"method\": \"meetings\", \"target\": 59, \"weight\": 1, \"y\": -0.013622424600998686, \"pair\": [18, 59], \"edge\": 79}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"meetings\", \"target\": 60, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 60], \"edge\": 80}, {\"x\": -0.14672338424001544, \"source\": 18, \"method\": \"meetings\", \"target\": 60, \"weight\": 1, \"y\": -0.021796679546102638, \"pair\": [18, 60], \"edge\": 80}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"meetings\", \"target\": 63, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 63], \"edge\": 81}, {\"x\": -0.05009154238426464, \"source\": 18, \"method\": \"meetings\", \"target\": 63, \"weight\": 1, \"y\": -0.015569815709628052, \"pair\": [18, 63], \"edge\": 81}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"meetings\", \"target\": 64, \"weight\": 2, \"y\": -0.02574806163662363, \"pair\": [18, 64], \"edge\": 82}, {\"x\": -0.01732299304953762, \"source\": 18, \"method\": \"meetings\", \"target\": 64, \"weight\": 2, \"y\": -0.02196627921477448, \"pair\": [18, 64], \"edge\": 82}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 29, \"weight\": 7, \"y\": -0.02574806163662363, \"pair\": [18, 29], \"edge\": 83}, {\"x\": -0.005597575826184528, \"source\": 18, \"method\": \"phonecalls\", \"target\": 29, \"weight\": 7, \"y\": -0.020099682282409877, \"pair\": [18, 29], \"edge\": 83}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"meetings\", \"target\": 65, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 65], \"edge\": 84}, {\"x\": -0.1767110217353999, \"source\": 18, \"method\": \"meetings\", \"target\": 65, \"weight\": 1, \"y\": -0.09254455795546651, \"pair\": [18, 65], \"edge\": 84}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"meetings\", \"target\": 43, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 43], \"edge\": 85}, {\"x\": 0.04246655649142668, \"source\": 18, \"method\": \"meetings\", \"target\": 43, \"weight\": 1, \"y\": -0.03598526594869128, \"pair\": [18, 43], \"edge\": 85}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"meetings\", \"target\": 27, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 27], \"edge\": 86}, {\"x\": 0.055912336966714146, \"source\": 18, \"method\": \"meetings\", \"target\": 27, \"weight\": 1, \"y\": -0.032629368428978275, \"pair\": [18, 27], \"edge\": 86}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 47], \"edge\": 87}, {\"x\": 0.11749951811931673, \"source\": 18, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": -0.011598498427487423, \"pair\": [18, 47], \"edge\": 87}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 71, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 71], \"edge\": 88}, {\"x\": -0.04373218280067268, \"source\": 18, \"method\": \"phonecalls\", \"target\": 71, \"weight\": 1, \"y\": -0.059903077944982355, \"pair\": [18, 71], \"edge\": 88}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"meetings\", \"target\": 76, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 76], \"edge\": 89}, {\"x\": -0.07770937983352315, \"source\": 18, \"method\": \"meetings\", \"target\": 76, \"weight\": 1, \"y\": 0.019481654531596497, \"pair\": [18, 76], \"edge\": 89}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 100], \"edge\": 90}, {\"x\": -0.13250071870320515, \"source\": 18, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": 0.03285556147004832, \"pair\": [18, 100], \"edge\": 90}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 102, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 102], \"edge\": 91}, {\"x\": -0.18597664070364336, \"source\": 18, \"method\": \"phonecalls\", \"target\": 102, \"weight\": 1, \"y\": -0.055781757428610766, \"pair\": [18, 102], \"edge\": 91}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 32, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 32], \"edge\": 92}, {\"x\": -0.14676081859617365, \"source\": 18, \"method\": \"phonecalls\", \"target\": 32, \"weight\": 1, \"y\": -0.06335478178038967, \"pair\": [18, 32], \"edge\": 92}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 103, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 103], \"edge\": 93}, {\"x\": -0.16866691498542963, \"source\": 18, \"method\": \"phonecalls\", \"target\": 103, \"weight\": 1, \"y\": -0.07602303452589629, \"pair\": [18, 103], \"edge\": 93}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 104, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 104], \"edge\": 94}, {\"x\": -0.09707311149752565, \"source\": 18, \"method\": \"phonecalls\", \"target\": 104, \"weight\": 1, \"y\": -0.11825623445327407, \"pair\": [18, 104], \"edge\": 94}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 61, \"weight\": 2, \"y\": -0.02574806163662363, \"pair\": [18, 61], \"edge\": 95}, {\"x\": 0.008929531091213954, \"source\": 18, \"method\": \"phonecalls\", \"target\": 61, \"weight\": 2, \"y\": -0.1261098728482192, \"pair\": [18, 61], \"edge\": 95}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 33, \"weight\": 5, \"y\": -0.02574806163662363, \"pair\": [18, 33], \"edge\": 96}, {\"x\": -0.12186521228775143, \"source\": 18, \"method\": \"phonecalls\", \"target\": 33, \"weight\": 5, \"y\": -0.05185706419850732, \"pair\": [18, 33], \"edge\": 96}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 108, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 108], \"edge\": 97}, {\"x\": -0.16363600156122873, \"source\": 18, \"method\": \"phonecalls\", \"target\": 108, \"weight\": 1, \"y\": -0.10327202378351355, \"pair\": [18, 108], \"edge\": 97}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 127, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 127], \"edge\": 98}, {\"x\": -0.13461837099862692, \"source\": 18, \"method\": \"phonecalls\", \"target\": 127, \"weight\": 1, \"y\": -0.12143208539116787, \"pair\": [18, 127], \"edge\": 98}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 66, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 66], \"edge\": 99}, {\"x\": -0.03408591480188705, \"source\": 18, \"method\": \"phonecalls\", \"target\": 66, \"weight\": 1, \"y\": -0.108595724404945, \"pair\": [18, 66], \"edge\": 99}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 132, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 132], \"edge\": 100}, {\"x\": -0.15220642619621907, \"source\": 18, \"method\": \"phonecalls\", \"target\": 132, \"weight\": 1, \"y\": -0.1164493179488152, \"pair\": [18, 132], \"edge\": 100}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 125, \"weight\": 2, \"y\": -0.02574806163662363, \"pair\": [18, 125], \"edge\": 101}, {\"x\": -0.002312125022450362, \"source\": 18, \"method\": \"phonecalls\", \"target\": 125, \"weight\": 2, \"y\": 0.030999456689846404, \"pair\": [18, 125], \"edge\": 101}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 140, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 140], \"edge\": 102}, {\"x\": -0.14106477858092156, \"source\": 18, \"method\": \"phonecalls\", \"target\": 140, \"weight\": 1, \"y\": -0.09895945110745505, \"pair\": [18, 140], \"edge\": 102}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 141, \"weight\": 4, \"y\": -0.02574806163662363, \"pair\": [18, 141], \"edge\": 103}, {\"x\": -0.08983605313128273, \"source\": 18, \"method\": \"phonecalls\", \"target\": 141, \"weight\": 4, \"y\": -0.06072464345394495, \"pair\": [18, 141], \"edge\": 103}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 146, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 146], \"edge\": 104}, {\"x\": -0.11669434463098263, \"source\": 18, \"method\": \"phonecalls\", \"target\": 146, \"weight\": 1, \"y\": -0.12924045697091557, \"pair\": [18, 146], \"edge\": 104}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 153, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 153], \"edge\": 105}, {\"x\": -0.11527871234312491, \"source\": 18, \"method\": \"phonecalls\", \"target\": 153, \"weight\": 1, \"y\": -0.10653945002619622, \"pair\": [18, 153], \"edge\": 105}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 99, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 99], \"edge\": 106}, {\"x\": -0.17391133013629761, \"source\": 18, \"method\": \"phonecalls\", \"target\": 99, \"weight\": 1, \"y\": -0.021100860151469267, \"pair\": [18, 99], \"edge\": 106}, {\"x\": -0.08453031363724207, \"source\": 18, \"method\": \"phonecalls\", \"target\": 110, \"weight\": 1, \"y\": -0.02574806163662363, \"pair\": [18, 110], \"edge\": 107}, {\"x\": -0.20541758295340326, \"source\": 18, \"method\": \"phonecalls\", \"target\": 110, \"weight\": 1, \"y\": -0.008103905117038245, \"pair\": [18, 110], \"edge\": 107}, {\"x\": -0.11459200425919056, \"source\": 19, \"method\": \"meetings\", \"target\": 20, \"weight\": 1, \"y\": -0.03306616016918993, \"pair\": [19, 20], \"edge\": 108}, {\"x\": -0.1260335142497204, \"source\": 19, \"method\": \"meetings\", \"target\": 20, \"weight\": 1, \"y\": -0.07916616634145784, \"pair\": [19, 20], \"edge\": 108}, {\"x\": -0.11459200425919056, \"source\": 19, \"method\": \"meetings\", \"target\": 32, \"weight\": 1, \"y\": -0.03306616016918993, \"pair\": [19, 32], \"edge\": 109}, {\"x\": -0.14676081859617365, \"source\": 19, \"method\": \"meetings\", \"target\": 32, \"weight\": 1, \"y\": -0.06335478178038967, \"pair\": [19, 32], \"edge\": 109}, {\"x\": -0.11459200425919056, \"source\": 19, \"method\": \"meetings\", \"target\": 33, \"weight\": 2, \"y\": -0.03306616016918993, \"pair\": [19, 33], \"edge\": 110}, {\"x\": -0.12186521228775143, \"source\": 19, \"method\": \"meetings\", \"target\": 33, \"weight\": 2, \"y\": -0.05185706419850732, \"pair\": [19, 33], \"edge\": 110}, {\"x\": -0.11459200425919056, \"source\": 19, \"method\": \"meetings\", \"target\": 34, \"weight\": 1, \"y\": -0.03306616016918993, \"pair\": [19, 34], \"edge\": 111}, {\"x\": -0.11311982103657724, \"source\": 19, \"method\": \"meetings\", \"target\": 34, \"weight\": 1, \"y\": 0.015511622301406802, \"pair\": [19, 34], \"edge\": 111}, {\"x\": -0.11459200425919056, \"source\": 19, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": -0.03306616016918993, \"pair\": [19, 35], \"edge\": 112}, {\"x\": -0.11155611366244667, \"source\": 19, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": -0.0016514281767455977, \"pair\": [19, 35], \"edge\": 112}, {\"x\": -0.11459200425919056, \"source\": 19, \"method\": \"meetings\", \"target\": 23, \"weight\": 1, \"y\": -0.03306616016918993, \"pair\": [19, 23], \"edge\": 113}, {\"x\": -0.1477490548119252, \"source\": 19, \"method\": \"meetings\", \"target\": 23, \"weight\": 1, \"y\": 0.00631745618334823, \"pair\": [19, 23], \"edge\": 113}, {\"x\": -0.11459200425919056, \"source\": 19, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": -0.03306616016918993, \"pair\": [19, 25], \"edge\": 114}, {\"x\": -0.09643094783333603, \"source\": 19, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": 0.0738142784386027, \"pair\": [19, 25], \"edge\": 114}, {\"x\": -0.11459200425919056, \"source\": 19, \"method\": \"meetings\", \"target\": 88, \"weight\": 1, \"y\": -0.03306616016918993, \"pair\": [19, 88], \"edge\": 115}, {\"x\": -0.19095671213933582, \"source\": 19, \"method\": \"meetings\", \"target\": 88, \"weight\": 1, \"y\": -0.07872588351902032, \"pair\": [19, 88], \"edge\": 115}, {\"x\": -0.11459200425919056, \"source\": 19, \"method\": \"phonecalls\", \"target\": 22, \"weight\": 1, \"y\": -0.03306616016918993, \"pair\": [19, 22], \"edge\": 116}, {\"x\": -0.09319299341154005, \"source\": 19, \"method\": \"phonecalls\", \"target\": 22, \"weight\": 1, \"y\": -0.0031457699240462016, \"pair\": [19, 22], \"edge\": 116}, {\"x\": -0.11459200425919056, \"source\": 19, \"method\": \"phonecalls\", \"target\": 142, \"weight\": 1, \"y\": -0.03306616016918993, \"pair\": [19, 142], \"edge\": 117}, {\"x\": -0.06662095968825617, \"source\": 19, \"method\": \"phonecalls\", \"target\": 142, \"weight\": 1, \"y\": -0.11904292944398821, \"pair\": [19, 142], \"edge\": 117}, {\"x\": -0.09319299341154005, \"source\": 22, \"method\": \"meetings\", \"target\": 23, \"weight\": 2, \"y\": -0.0031457699240462016, \"pair\": [22, 23], \"edge\": 118}, {\"x\": -0.1477490548119252, \"source\": 22, \"method\": \"meetings\", \"target\": 23, \"weight\": 2, \"y\": 0.00631745618334823, \"pair\": [22, 23], \"edge\": 118}, {\"x\": -0.09319299341154005, \"source\": 22, \"method\": \"meetings\", \"target\": 29, \"weight\": 2, \"y\": -0.0031457699240462016, \"pair\": [22, 29], \"edge\": 119}, {\"x\": -0.005597575826184528, \"source\": 22, \"method\": \"meetings\", \"target\": 29, \"weight\": 2, \"y\": -0.020099682282409877, \"pair\": [22, 29], \"edge\": 119}, {\"x\": -0.09319299341154005, \"source\": 22, \"method\": \"meetings\", \"target\": 30, \"weight\": 1, \"y\": -0.0031457699240462016, \"pair\": [22, 30], \"edge\": 120}, {\"x\": -0.03667232839148727, \"source\": 22, \"method\": \"meetings\", \"target\": 30, \"weight\": 1, \"y\": 0.03701289263695622, \"pair\": [22, 30], \"edge\": 120}, {\"x\": -0.09319299341154005, \"source\": 22, \"method\": \"phonecalls\", \"target\": 31, \"weight\": 1, \"y\": -0.0031457699240462016, \"pair\": [22, 31], \"edge\": 121}, {\"x\": -0.14227761922441623, \"source\": 22, \"method\": \"phonecalls\", \"target\": 31, \"weight\": 1, \"y\": -0.038928683932491444, \"pair\": [22, 31], \"edge\": 121}, {\"x\": -0.09319299341154005, \"source\": 22, \"method\": \"meetings\", \"target\": 60, \"weight\": 1, \"y\": -0.0031457699240462016, \"pair\": [22, 60], \"edge\": 122}, {\"x\": -0.14672338424001544, \"source\": 22, \"method\": \"meetings\", \"target\": 60, \"weight\": 1, \"y\": -0.021796679546102638, \"pair\": [22, 60], \"edge\": 122}, {\"x\": -0.09319299341154005, \"source\": 22, \"method\": \"meetings\", \"target\": 63, \"weight\": 2, \"y\": -0.0031457699240462016, \"pair\": [22, 63], \"edge\": 123}, {\"x\": -0.05009154238426464, \"source\": 22, \"method\": \"meetings\", \"target\": 63, \"weight\": 2, \"y\": -0.015569815709628052, \"pair\": [22, 63], \"edge\": 123}, {\"x\": -0.09319299341154005, \"source\": 22, \"method\": \"meetings\", \"target\": 72, \"weight\": 1, \"y\": -0.0031457699240462016, \"pair\": [22, 72], \"edge\": 124}, {\"x\": -0.06770812530480919, \"source\": 22, \"method\": \"meetings\", \"target\": 72, \"weight\": 1, \"y\": 0.002490925585405996, \"pair\": [22, 72], \"edge\": 124}, {\"x\": -0.09319299341154005, \"source\": 22, \"method\": \"meetings\", \"target\": 76, \"weight\": 2, \"y\": -0.0031457699240462016, \"pair\": [22, 76], \"edge\": 125}, {\"x\": -0.07770937983352315, \"source\": 22, \"method\": \"meetings\", \"target\": 76, \"weight\": 2, \"y\": 0.019481654531596497, \"pair\": [22, 76], \"edge\": 125}, {\"x\": -0.09319299341154005, \"source\": 22, \"method\": \"meetings\", \"target\": 77, \"weight\": 2, \"y\": -0.0031457699240462016, \"pair\": [22, 77], \"edge\": 126}, {\"x\": -0.10924841249342242, \"source\": 22, \"method\": \"meetings\", \"target\": 77, \"weight\": 2, \"y\": 0.03021006080396897, \"pair\": [22, 77], \"edge\": 126}, {\"x\": -0.09319299341154005, \"source\": 22, \"method\": \"meetings\", \"target\": 75, \"weight\": 2, \"y\": -0.0031457699240462016, \"pair\": [22, 75], \"edge\": 127}, {\"x\": -0.0897053430490711, \"source\": 22, \"method\": \"meetings\", \"target\": 75, \"weight\": 2, \"y\": 0.013101398964902963, \"pair\": [22, 75], \"edge\": 127}, {\"x\": -0.09319299341154005, \"source\": 22, \"method\": \"meetings\", \"target\": 78, \"weight\": 1, \"y\": -0.0031457699240462016, \"pair\": [22, 78], \"edge\": 128}, {\"x\": -0.14706553695587654, \"source\": 22, \"method\": \"meetings\", \"target\": 78, \"weight\": 1, \"y\": 0.032673641859445796, \"pair\": [22, 78], \"edge\": 128}, {\"x\": -0.09319299341154005, \"source\": 22, \"method\": \"meetings\", \"target\": 98, \"weight\": 1, \"y\": -0.0031457699240462016, \"pair\": [22, 98], \"edge\": 129}, {\"x\": -0.0439818325353795, \"source\": 22, \"method\": \"meetings\", \"target\": 98, \"weight\": 1, \"y\": 0.01834824861421199, \"pair\": [22, 98], \"edge\": 129}, {\"x\": -0.09319299341154005, \"source\": 22, \"method\": \"meetings\", \"target\": 99, \"weight\": 1, \"y\": -0.0031457699240462016, \"pair\": [22, 99], \"edge\": 130}, {\"x\": -0.17391133013629761, \"source\": 22, \"method\": \"meetings\", \"target\": 99, \"weight\": 1, \"y\": -0.021100860151469267, \"pair\": [22, 99], \"edge\": 130}, {\"x\": -0.1477490548119252, \"source\": 23, \"method\": \"meetings\", \"target\": 24, \"weight\": 1, \"y\": 0.00631745618334823, \"pair\": [23, 24], \"edge\": 131}, {\"x\": -0.2533555765225973, \"source\": 23, \"method\": \"meetings\", \"target\": 24, \"weight\": 1, \"y\": -0.005999011015023959, \"pair\": [23, 24], \"edge\": 131}, {\"x\": -0.1477490548119252, \"source\": 23, \"method\": \"meetings\", \"target\": 34, \"weight\": 1, \"y\": 0.00631745618334823, \"pair\": [23, 34], \"edge\": 132}, {\"x\": -0.11311982103657724, \"source\": 23, \"method\": \"meetings\", \"target\": 34, \"weight\": 1, \"y\": 0.015511622301406802, \"pair\": [23, 34], \"edge\": 132}, {\"x\": -0.1477490548119252, \"source\": 23, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": 0.00631745618334823, \"pair\": [23, 35], \"edge\": 133}, {\"x\": -0.11155611366244667, \"source\": 23, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": -0.0016514281767455977, \"pair\": [23, 35], \"edge\": 133}, {\"x\": -0.1477490548119252, \"source\": 23, \"method\": \"meetings\", \"target\": 25, \"weight\": 2, \"y\": 0.00631745618334823, \"pair\": [23, 25], \"edge\": 134}, {\"x\": -0.09643094783333603, \"source\": 23, \"method\": \"meetings\", \"target\": 25, \"weight\": 2, \"y\": 0.0738142784386027, \"pair\": [23, 25], \"edge\": 134}, {\"x\": -0.1477490548119252, \"source\": 23, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": 0.00631745618334823, \"pair\": [23, 100], \"edge\": 135}, {\"x\": -0.13250071870320515, \"source\": 23, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": 0.03285556147004832, \"pair\": [23, 100], \"edge\": 135}, {\"x\": -0.1477490548119252, \"source\": 23, \"method\": \"phonecalls\", \"target\": 99, \"weight\": 5, \"y\": 0.00631745618334823, \"pair\": [23, 99], \"edge\": 136}, {\"x\": -0.17391133013629761, \"source\": 23, \"method\": \"phonecalls\", \"target\": 99, \"weight\": 5, \"y\": -0.021100860151469267, \"pair\": [23, 99], \"edge\": 136}, {\"x\": -0.09643094783333603, \"source\": 25, \"method\": \"meetings\", \"target\": 26, \"weight\": 1, \"y\": 0.0738142784386027, \"pair\": [25, 26], \"edge\": 137}, {\"x\": -0.004479903134193742, \"source\": 25, \"method\": \"meetings\", \"target\": 26, \"weight\": 1, \"y\": 0.05290831941589025, \"pair\": [25, 26], \"edge\": 137}, {\"x\": -0.09643094783333603, \"source\": 25, \"method\": \"meetings\", \"target\": 27, \"weight\": 1, \"y\": 0.0738142784386027, \"pair\": [25, 27], \"edge\": 138}, {\"x\": 0.055912336966714146, \"source\": 25, \"method\": \"meetings\", \"target\": 27, \"weight\": 1, \"y\": -0.032629368428978275, \"pair\": [25, 27], \"edge\": 138}, {\"x\": -0.09643094783333603, \"source\": 25, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": 0.0738142784386027, \"pair\": [25, 28], \"edge\": 139}, {\"x\": -0.13263709675378124, \"source\": 25, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": 0.1526276065747802, \"pair\": [25, 28], \"edge\": 139}, {\"x\": -0.09643094783333603, \"source\": 25, \"method\": \"meetings\", \"target\": 34, \"weight\": 2, \"y\": 0.0738142784386027, \"pair\": [25, 34], \"edge\": 140}, {\"x\": -0.11311982103657724, \"source\": 25, \"method\": \"meetings\", \"target\": 34, \"weight\": 2, \"y\": 0.015511622301406802, \"pair\": [25, 34], \"edge\": 140}, {\"x\": -0.09643094783333603, \"source\": 25, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": 0.0738142784386027, \"pair\": [25, 35], \"edge\": 141}, {\"x\": -0.11155611366244667, \"source\": 25, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": -0.0016514281767455977, \"pair\": [25, 35], \"edge\": 141}, {\"x\": -0.09643094783333603, \"source\": 25, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": 0.0738142784386027, \"pair\": [25, 100], \"edge\": 142}, {\"x\": -0.13250071870320515, \"source\": 25, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": 0.03285556147004832, \"pair\": [25, 100], \"edge\": 142}, {\"x\": -0.004479903134193742, \"source\": 26, \"method\": \"meetings\", \"target\": 27, \"weight\": 1, \"y\": 0.05290831941589025, \"pair\": [26, 27], \"edge\": 143}, {\"x\": 0.055912336966714146, \"source\": 26, \"method\": \"meetings\", \"target\": 27, \"weight\": 1, \"y\": -0.032629368428978275, \"pair\": [26, 27], \"edge\": 143}, {\"x\": 0.055912336966714146, \"source\": 27, \"method\": \"meetings\", \"target\": 38, \"weight\": 1, \"y\": -0.032629368428978275, \"pair\": [27, 38], \"edge\": 144}, {\"x\": 0.12962454552411715, \"source\": 27, \"method\": \"meetings\", \"target\": 38, \"weight\": 1, \"y\": -0.05207770612396997, \"pair\": [27, 38], \"edge\": 144}, {\"x\": 0.055912336966714146, \"source\": 27, \"method\": \"meetings\", \"target\": 45, \"weight\": 3, \"y\": -0.032629368428978275, \"pair\": [27, 45], \"edge\": 145}, {\"x\": 0.11050776731570415, \"source\": 27, \"method\": \"meetings\", \"target\": 45, \"weight\": 3, \"y\": 0.001464868686066248, \"pair\": [27, 45], \"edge\": 145}, {\"x\": 0.055912336966714146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 1, \"y\": -0.032629368428978275, \"pair\": [27, 51], \"edge\": 146}, {\"x\": 0.1223460005243755, \"source\": 27, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 1, \"y\": 0.0041269931419576755, \"pair\": [27, 51], \"edge\": 146}, {\"x\": 0.055912336966714146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 43, \"weight\": 2, \"y\": -0.032629368428978275, \"pair\": [27, 43], \"edge\": 147}, {\"x\": 0.04246655649142668, \"source\": 27, \"method\": \"phonecalls\", \"target\": 43, \"weight\": 2, \"y\": -0.03598526594869128, \"pair\": [27, 43], \"edge\": 147}, {\"x\": 0.055912336966714146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 3, \"y\": -0.032629368428978275, \"pair\": [27, 47], \"edge\": 148}, {\"x\": 0.11749951811931673, \"source\": 27, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 3, \"y\": -0.011598498427487423, \"pair\": [27, 47], \"edge\": 148}, {\"x\": 0.055912336966714146, \"source\": 27, \"method\": \"meetings\", \"target\": 29, \"weight\": 3, \"y\": -0.032629368428978275, \"pair\": [27, 29], \"edge\": 149}, {\"x\": -0.005597575826184528, \"source\": 27, \"method\": \"meetings\", \"target\": 29, \"weight\": 3, \"y\": -0.020099682282409877, \"pair\": [27, 29], \"edge\": 149}, {\"x\": 0.055912336966714146, \"source\": 27, \"method\": \"meetings\", \"target\": 64, \"weight\": 1, \"y\": -0.032629368428978275, \"pair\": [27, 64], \"edge\": 150}, {\"x\": -0.01732299304953762, \"source\": 27, \"method\": \"meetings\", \"target\": 64, \"weight\": 1, \"y\": -0.02196627921477448, \"pair\": [27, 64], \"edge\": 150}, {\"x\": 0.055912336966714146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": -0.032629368428978275, \"pair\": [27, 68], \"edge\": 151}, {\"x\": 0.10737577986159841, \"source\": 27, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": -0.06426632231873398, \"pair\": [27, 68], \"edge\": 151}, {\"x\": 0.055912336966714146, \"source\": 27, \"method\": \"meetings\", \"target\": 44, \"weight\": 1, \"y\": -0.032629368428978275, \"pair\": [27, 44], \"edge\": 152}, {\"x\": 0.0791141371992724, \"source\": 27, \"method\": \"meetings\", \"target\": 44, \"weight\": 1, \"y\": -0.044430834196097516, \"pair\": [27, 44], \"edge\": 152}, {\"x\": 0.055912336966714146, \"source\": 27, \"method\": \"meetings\", \"target\": 61, \"weight\": 1, \"y\": -0.032629368428978275, \"pair\": [27, 61], \"edge\": 153}, {\"x\": 0.008929531091213954, \"source\": 27, \"method\": \"meetings\", \"target\": 61, \"weight\": 1, \"y\": -0.1261098728482192, \"pair\": [27, 61], \"edge\": 153}, {\"x\": 0.055912336966714146, \"source\": 27, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": -0.032629368428978275, \"pair\": [27, 89], \"edge\": 154}, {\"x\": 0.11592769336653481, \"source\": 27, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": -0.03187484266859175, \"pair\": [27, 89], \"edge\": 154}, {\"x\": 0.055912336966714146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 70, \"weight\": 1, \"y\": -0.032629368428978275, \"pair\": [27, 70], \"edge\": 155}, {\"x\": 0.09004879020314133, \"source\": 27, \"method\": \"phonecalls\", \"target\": 70, \"weight\": 1, \"y\": -0.06260473269975722, \"pair\": [27, 70], \"edge\": 155}, {\"x\": 0.055912336966714146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 97, \"weight\": 1, \"y\": -0.032629368428978275, \"pair\": [27, 97], \"edge\": 156}, {\"x\": 0.12017909083449298, \"source\": 27, \"method\": \"phonecalls\", \"target\": 97, \"weight\": 1, \"y\": -0.06736377733769908, \"pair\": [27, 97], \"edge\": 156}, {\"x\": 0.055912336966714146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 119, \"weight\": 1, \"y\": -0.032629368428978275, \"pair\": [27, 119], \"edge\": 157}, {\"x\": 0.10097502625023547, \"source\": 27, \"method\": \"phonecalls\", \"target\": 119, \"weight\": 1, \"y\": -0.11146261212634913, \"pair\": [27, 119], \"edge\": 157}, {\"x\": 0.055912336966714146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 126, \"weight\": 1, \"y\": -0.032629368428978275, \"pair\": [27, 126], \"edge\": 158}, {\"x\": 0.11841296989609869, \"source\": 27, \"method\": \"phonecalls\", \"target\": 126, \"weight\": 1, \"y\": -0.11019973799399048, \"pair\": [27, 126], \"edge\": 158}, {\"x\": 0.055912336966714146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 136, \"weight\": 1, \"y\": -0.032629368428978275, \"pair\": [27, 136], \"edge\": 159}, {\"x\": 0.08908871458494991, \"source\": 27, \"method\": \"phonecalls\", \"target\": 136, \"weight\": 1, \"y\": -0.12525489749717797, \"pair\": [27, 136], \"edge\": 159}, {\"x\": 0.055912336966714146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 138, \"weight\": 1, \"y\": -0.032629368428978275, \"pair\": [27, 138], \"edge\": 160}, {\"x\": 0.13185385258684215, \"source\": 27, \"method\": \"phonecalls\", \"target\": 138, \"weight\": 1, \"y\": -0.09571096506382044, \"pair\": [27, 138], \"edge\": 160}, {\"x\": 0.055912336966714146, \"source\": 27, \"method\": \"phonecalls\", \"target\": 36, \"weight\": 1, \"y\": -0.032629368428978275, \"pair\": [27, 36], \"edge\": 161}, {\"x\": 0.08958970502208463, \"source\": 27, \"method\": \"phonecalls\", \"target\": 36, \"weight\": 1, \"y\": -0.028086458233268496, \"pair\": [27, 36], \"edge\": 161}, {\"x\": -0.005597575826184528, \"source\": 29, \"method\": \"meetings\", \"target\": 30, \"weight\": 1, \"y\": -0.020099682282409877, \"pair\": [29, 30], \"edge\": 162}, {\"x\": -0.03667232839148727, \"source\": 29, \"method\": \"meetings\", \"target\": 30, \"weight\": 1, \"y\": 0.03701289263695622, \"pair\": [29, 30], \"edge\": 162}, {\"x\": -0.005597575826184528, \"source\": 29, \"method\": \"phonecalls\", \"target\": 64, \"weight\": 1, \"y\": -0.020099682282409877, \"pair\": [29, 64], \"edge\": 163}, {\"x\": -0.01732299304953762, \"source\": 29, \"method\": \"phonecalls\", \"target\": 64, \"weight\": 1, \"y\": -0.02196627921477448, \"pair\": [29, 64], \"edge\": 163}, {\"x\": -0.005597575826184528, \"source\": 29, \"method\": \"phonecalls\", \"target\": 54, \"weight\": 1, \"y\": -0.020099682282409877, \"pair\": [29, 54], \"edge\": 164}, {\"x\": 0.08250584794499385, \"source\": 29, \"method\": \"phonecalls\", \"target\": 54, \"weight\": 1, \"y\": -0.016717253662116047, \"pair\": [29, 54], \"edge\": 164}, {\"x\": -0.005597575826184528, \"source\": 29, \"method\": \"phonecalls\", \"target\": 43, \"weight\": 4, \"y\": -0.020099682282409877, \"pair\": [29, 43], \"edge\": 165}, {\"x\": 0.04246655649142668, \"source\": 29, \"method\": \"phonecalls\", \"target\": 43, \"weight\": 4, \"y\": -0.03598526594869128, \"pair\": [29, 43], \"edge\": 165}, {\"x\": -0.005597575826184528, \"source\": 29, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": -0.020099682282409877, \"pair\": [29, 68], \"edge\": 166}, {\"x\": 0.10737577986159841, \"source\": 29, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": -0.06426632231873398, \"pair\": [29, 68], \"edge\": 166}, {\"x\": -0.005597575826184528, \"source\": 29, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 1, \"y\": -0.020099682282409877, \"pair\": [29, 47], \"edge\": 167}, {\"x\": 0.11749951811931673, \"source\": 29, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 1, \"y\": -0.011598498427487423, \"pair\": [29, 47], \"edge\": 167}, {\"x\": -0.005597575826184528, \"source\": 29, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 1, \"y\": -0.020099682282409877, \"pair\": [29, 51], \"edge\": 168}, {\"x\": 0.1223460005243755, \"source\": 29, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 1, \"y\": 0.0041269931419576755, \"pair\": [29, 51], \"edge\": 168}, {\"x\": -0.005597575826184528, \"source\": 29, \"method\": \"meetings\", \"target\": 71, \"weight\": 1, \"y\": -0.020099682282409877, \"pair\": [29, 71], \"edge\": 169}, {\"x\": -0.04373218280067268, \"source\": 29, \"method\": \"meetings\", \"target\": 71, \"weight\": 1, \"y\": -0.059903077944982355, \"pair\": [29, 71], \"edge\": 169}, {\"x\": -0.005597575826184528, \"source\": 29, \"method\": \"meetings\", \"target\": 75, \"weight\": 1, \"y\": -0.020099682282409877, \"pair\": [29, 75], \"edge\": 170}, {\"x\": -0.0897053430490711, \"source\": 29, \"method\": \"meetings\", \"target\": 75, \"weight\": 1, \"y\": 0.013101398964902963, \"pair\": [29, 75], \"edge\": 170}, {\"x\": -0.005597575826184528, \"source\": 29, \"method\": \"meetings\", \"target\": 76, \"weight\": 1, \"y\": -0.020099682282409877, \"pair\": [29, 76], \"edge\": 171}, {\"x\": -0.07770937983352315, \"source\": 29, \"method\": \"meetings\", \"target\": 76, \"weight\": 1, \"y\": 0.019481654531596497, \"pair\": [29, 76], \"edge\": 171}, {\"x\": -0.005597575826184528, \"source\": 29, \"method\": \"phonecalls\", \"target\": 63, \"weight\": 1, \"y\": -0.020099682282409877, \"pair\": [29, 63], \"edge\": 172}, {\"x\": -0.05009154238426464, \"source\": 29, \"method\": \"phonecalls\", \"target\": 63, \"weight\": 1, \"y\": -0.015569815709628052, \"pair\": [29, 63], \"edge\": 172}, {\"x\": -0.005597575826184528, \"source\": 29, \"method\": \"phonecalls\", \"target\": 98, \"weight\": 1, \"y\": -0.020099682282409877, \"pair\": [29, 98], \"edge\": 173}, {\"x\": -0.0439818325353795, \"source\": 29, \"method\": \"phonecalls\", \"target\": 98, \"weight\": 1, \"y\": 0.01834824861421199, \"pair\": [29, 98], \"edge\": 173}, {\"x\": -0.005597575826184528, \"source\": 29, \"method\": \"phonecalls\", \"target\": 139, \"weight\": 1, \"y\": -0.020099682282409877, \"pair\": [29, 139], \"edge\": 174}, {\"x\": 0.014907341059817546, \"source\": 29, \"method\": \"phonecalls\", \"target\": 139, \"weight\": 1, \"y\": -0.0661372856193289, \"pair\": [29, 139], \"edge\": 174}, {\"x\": -0.14676081859617365, \"source\": 32, \"method\": \"meetings\", \"target\": 33, \"weight\": 1, \"y\": -0.06335478178038967, \"pair\": [32, 33], \"edge\": 175}, {\"x\": -0.12186521228775143, \"source\": 32, \"method\": \"meetings\", \"target\": 33, \"weight\": 1, \"y\": -0.05185706419850732, \"pair\": [32, 33], \"edge\": 175}, {\"x\": -0.12186521228775143, \"source\": 33, \"method\": \"meetings\", \"target\": 88, \"weight\": 1, \"y\": -0.05185706419850732, \"pair\": [33, 88], \"edge\": 176}, {\"x\": -0.19095671213933582, \"source\": 33, \"method\": \"meetings\", \"target\": 88, \"weight\": 1, \"y\": -0.07872588351902032, \"pair\": [33, 88], \"edge\": 176}, {\"x\": -0.11311982103657724, \"source\": 34, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": 0.015511622301406802, \"pair\": [34, 35], \"edge\": 177}, {\"x\": -0.11155611366244667, \"source\": 34, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": -0.0016514281767455977, \"pair\": [34, 35], \"edge\": 177}, {\"x\": -0.11311982103657724, \"source\": 34, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": 0.015511622301406802, \"pair\": [34, 100], \"edge\": 178}, {\"x\": -0.13250071870320515, \"source\": 34, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": 0.03285556147004832, \"pair\": [34, 100], \"edge\": 178}, {\"x\": 0.08958970502208463, \"source\": 36, \"method\": \"meetings\", \"target\": 37, \"weight\": 1, \"y\": -0.028086458233268496, \"pair\": [36, 37], \"edge\": 179}, {\"x\": 0.07605741696983585, \"source\": 36, \"method\": \"meetings\", \"target\": 37, \"weight\": 1, \"y\": -0.1182608540070831, \"pair\": [36, 37], \"edge\": 179}, {\"x\": 0.08958970502208463, \"source\": 36, \"method\": \"meetings\", \"target\": 45, \"weight\": 2, \"y\": -0.028086458233268496, \"pair\": [36, 45], \"edge\": 180}, {\"x\": 0.11050776731570415, \"source\": 36, \"method\": \"meetings\", \"target\": 45, \"weight\": 2, \"y\": 0.001464868686066248, \"pair\": [36, 45], \"edge\": 180}, {\"x\": 0.08958970502208463, \"source\": 36, \"method\": \"meetings\", \"target\": 46, \"weight\": 1, \"y\": -0.028086458233268496, \"pair\": [36, 46], \"edge\": 181}, {\"x\": 0.1325051404831165, \"source\": 36, \"method\": \"meetings\", \"target\": 46, \"weight\": 1, \"y\": 0.02287698773026062, \"pair\": [36, 46], \"edge\": 181}, {\"x\": 0.08958970502208463, \"source\": 36, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 2, \"y\": -0.028086458233268496, \"pair\": [36, 47], \"edge\": 182}, {\"x\": 0.11749951811931673, \"source\": 36, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 2, \"y\": -0.011598498427487423, \"pair\": [36, 47], \"edge\": 182}, {\"x\": 0.08958970502208463, \"source\": 36, \"method\": \"meetings\", \"target\": 48, \"weight\": 2, \"y\": -0.028086458233268496, \"pair\": [36, 48], \"edge\": 183}, {\"x\": 0.1423891155030037, \"source\": 36, \"method\": \"meetings\", \"target\": 48, \"weight\": 2, \"y\": -0.0039815184705114335, \"pair\": [36, 48], \"edge\": 183}, {\"x\": 0.08958970502208463, \"source\": 36, \"method\": \"meetings\", \"target\": 91, \"weight\": 1, \"y\": -0.028086458233268496, \"pair\": [36, 91], \"edge\": 184}, {\"x\": 0.11951898178666384, \"source\": 36, \"method\": \"meetings\", \"target\": 91, \"weight\": 1, \"y\": 0.059923934361632215, \"pair\": [36, 91], \"edge\": 184}, {\"x\": 0.08958970502208463, \"source\": 36, \"method\": \"meetings\", \"target\": 92, \"weight\": 1, \"y\": -0.028086458233268496, \"pair\": [36, 92], \"edge\": 185}, {\"x\": 0.1353383703416821, \"source\": 36, \"method\": \"meetings\", \"target\": 92, \"weight\": 1, \"y\": 0.05345272568729972, \"pair\": [36, 92], \"edge\": 185}, {\"x\": 0.08958970502208463, \"source\": 36, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": -0.028086458233268496, \"pair\": [36, 68], \"edge\": 186}, {\"x\": 0.10737577986159841, \"source\": 36, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": -0.06426632231873398, \"pair\": [36, 68], \"edge\": 186}, {\"x\": 0.08958970502208463, \"source\": 36, \"method\": \"phonecalls\", \"target\": 148, \"weight\": 1, \"y\": -0.028086458233268496, \"pair\": [36, 148], \"edge\": 187}, {\"x\": 0.09669283504956887, \"source\": 36, \"method\": \"phonecalls\", \"target\": 148, \"weight\": 1, \"y\": 0.05777889811249485, \"pair\": [36, 148], \"edge\": 187}, {\"x\": 0.08958970502208463, \"source\": 36, \"method\": \"phonecalls\", \"target\": 61, \"weight\": 2, \"y\": -0.028086458233268496, \"pair\": [36, 61], \"edge\": 188}, {\"x\": 0.008929531091213954, \"source\": 36, \"method\": \"phonecalls\", \"target\": 61, \"weight\": 2, \"y\": -0.1261098728482192, \"pair\": [36, 61], \"edge\": 188}, {\"x\": 0.07605741696983585, \"source\": 37, \"method\": \"meetings\", \"target\": 67, \"weight\": 1, \"y\": -0.1182608540070831, \"pair\": [37, 67], \"edge\": 189}, {\"x\": 0.032197817905865664, \"source\": 37, \"method\": \"meetings\", \"target\": 67, \"weight\": 1, \"y\": -0.15279710099094182, \"pair\": [37, 67], \"edge\": 189}, {\"x\": 0.12962454552411715, \"source\": 38, \"method\": \"meetings\", \"target\": 93, \"weight\": 2, \"y\": -0.05207770612396997, \"pair\": [38, 93], \"edge\": 190}, {\"x\": 0.14155100232463913, \"source\": 38, \"method\": \"meetings\", \"target\": 93, \"weight\": 2, \"y\": -0.040029604132746956, \"pair\": [38, 93], \"edge\": 190}, {\"x\": 0.12962454552411715, \"source\": 38, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": -0.05207770612396997, \"pair\": [38, 89], \"edge\": 191}, {\"x\": 0.11592769336653481, \"source\": 38, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": -0.03187484266859175, \"pair\": [38, 89], \"edge\": 191}, {\"x\": 0.1946430381184062, \"source\": 39, \"method\": \"meetings\", \"target\": 40, \"weight\": 1, \"y\": 0.0630452098149511, \"pair\": [39, 40], \"edge\": 192}, {\"x\": 0.20701103080268415, \"source\": 39, \"method\": \"meetings\", \"target\": 40, \"weight\": 1, \"y\": 0.08655983028322389, \"pair\": [39, 40], \"edge\": 192}, {\"x\": 0.1946430381184062, \"source\": 39, \"method\": \"meetings\", \"target\": 41, \"weight\": 1, \"y\": 0.0630452098149511, \"pair\": [39, 41], \"edge\": 193}, {\"x\": 0.26125595890324854, \"source\": 39, \"method\": \"meetings\", \"target\": 41, \"weight\": 1, \"y\": 0.11119111466981486, \"pair\": [39, 41], \"edge\": 193}, {\"x\": 0.1946430381184062, \"source\": 39, \"method\": \"meetings\", \"target\": 42, \"weight\": 1, \"y\": 0.0630452098149511, \"pair\": [39, 42], \"edge\": 194}, {\"x\": 0.24978374110962673, \"source\": 39, \"method\": \"meetings\", \"target\": 42, \"weight\": 1, \"y\": 0.1240367928051296, \"pair\": [39, 42], \"edge\": 194}, {\"x\": 0.1946430381184062, \"source\": 39, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": 0.0630452098149511, \"pair\": [39, 47], \"edge\": 195}, {\"x\": 0.11749951811931673, \"source\": 39, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": -0.011598498427487423, \"pair\": [39, 47], \"edge\": 195}, {\"x\": 0.1946430381184062, \"source\": 39, \"method\": \"meetings\", \"target\": 49, \"weight\": 1, \"y\": 0.0630452098149511, \"pair\": [39, 49], \"edge\": 196}, {\"x\": 0.15380578878918935, \"source\": 39, \"method\": \"meetings\", \"target\": 49, \"weight\": 1, \"y\": 0.059143810553784086, \"pair\": [39, 49], \"edge\": 196}, {\"x\": 0.1946430381184062, \"source\": 39, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": 0.0630452098149511, \"pair\": [39, 50], \"edge\": 197}, {\"x\": 0.15414731472842055, \"source\": 39, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": 0.02127617608196343, \"pair\": [39, 50], \"edge\": 197}, {\"x\": 0.1946430381184062, \"source\": 39, \"method\": \"meetings\", \"target\": 48, \"weight\": 1, \"y\": 0.0630452098149511, \"pair\": [39, 48], \"edge\": 198}, {\"x\": 0.1423891155030037, \"source\": 39, \"method\": \"meetings\", \"target\": 48, \"weight\": 1, \"y\": -0.0039815184705114335, \"pair\": [39, 48], \"edge\": 198}, {\"x\": 0.20701103080268415, \"source\": 40, \"method\": \"meetings\", \"target\": 41, \"weight\": 1, \"y\": 0.08655983028322389, \"pair\": [40, 41], \"edge\": 199}, {\"x\": 0.26125595890324854, \"source\": 40, \"method\": \"meetings\", \"target\": 41, \"weight\": 1, \"y\": 0.11119111466981486, \"pair\": [40, 41], \"edge\": 199}, {\"x\": 0.20701103080268415, \"source\": 40, \"method\": \"meetings\", \"target\": 42, \"weight\": 1, \"y\": 0.08655983028322389, \"pair\": [40, 42], \"edge\": 200}, {\"x\": 0.24978374110962673, \"source\": 40, \"method\": \"meetings\", \"target\": 42, \"weight\": 1, \"y\": 0.1240367928051296, \"pair\": [40, 42], \"edge\": 200}, {\"x\": 0.20701103080268415, \"source\": 40, \"method\": \"phonecalls\", \"target\": 45, \"weight\": 1, \"y\": 0.08655983028322389, \"pair\": [40, 45], \"edge\": 201}, {\"x\": 0.11050776731570415, \"source\": 40, \"method\": \"phonecalls\", \"target\": 45, \"weight\": 1, \"y\": 0.001464868686066248, \"pair\": [40, 45], \"edge\": 201}, {\"x\": 0.26125595890324854, \"source\": 41, \"method\": \"meetings\", \"target\": 42, \"weight\": 1, \"y\": 0.11119111466981486, \"pair\": [41, 42], \"edge\": 202}, {\"x\": 0.24978374110962673, \"source\": 41, \"method\": \"meetings\", \"target\": 42, \"weight\": 1, \"y\": 0.1240367928051296, \"pair\": [41, 42], \"edge\": 202}, {\"x\": 0.04246655649142668, \"source\": 43, \"method\": \"meetings\", \"target\": 44, \"weight\": 1, \"y\": -0.03598526594869128, \"pair\": [43, 44], \"edge\": 203}, {\"x\": 0.0791141371992724, \"source\": 43, \"method\": \"meetings\", \"target\": 44, \"weight\": 1, \"y\": -0.044430834196097516, \"pair\": [43, 44], \"edge\": 203}, {\"x\": 0.04246655649142668, \"source\": 43, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 4, \"y\": -0.03598526594869128, \"pair\": [43, 47], \"edge\": 204}, {\"x\": 0.11749951811931673, \"source\": 43, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 4, \"y\": -0.011598498427487423, \"pair\": [43, 47], \"edge\": 204}, {\"x\": 0.04246655649142668, \"source\": 43, \"method\": \"meetings\", \"target\": 64, \"weight\": 2, \"y\": -0.03598526594869128, \"pair\": [43, 64], \"edge\": 205}, {\"x\": -0.01732299304953762, \"source\": 43, \"method\": \"meetings\", \"target\": 64, \"weight\": 2, \"y\": -0.02196627921477448, \"pair\": [43, 64], \"edge\": 205}, {\"x\": 0.04246655649142668, \"source\": 43, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": -0.03598526594869128, \"pair\": [43, 68], \"edge\": 206}, {\"x\": 0.10737577986159841, \"source\": 43, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": -0.06426632231873398, \"pair\": [43, 68], \"edge\": 206}, {\"x\": 0.04246655649142668, \"source\": 43, \"method\": \"meetings\", \"target\": 51, \"weight\": 1, \"y\": -0.03598526594869128, \"pair\": [43, 51], \"edge\": 207}, {\"x\": 0.1223460005243755, \"source\": 43, \"method\": \"meetings\", \"target\": 51, \"weight\": 1, \"y\": 0.0041269931419576755, \"pair\": [43, 51], \"edge\": 207}, {\"x\": 0.04246655649142668, \"source\": 43, \"method\": \"meetings\", \"target\": 61, \"weight\": 1, \"y\": -0.03598526594869128, \"pair\": [43, 61], \"edge\": 208}, {\"x\": 0.008929531091213954, \"source\": 43, \"method\": \"meetings\", \"target\": 61, \"weight\": 1, \"y\": -0.1261098728482192, \"pair\": [43, 61], \"edge\": 208}, {\"x\": 0.04246655649142668, \"source\": 43, \"method\": \"phonecalls\", \"target\": 120, \"weight\": 1, \"y\": -0.03598526594869128, \"pair\": [43, 120], \"edge\": 209}, {\"x\": 0.04833733005421891, \"source\": 43, \"method\": \"phonecalls\", \"target\": 120, \"weight\": 1, \"y\": -0.11013309374693225, \"pair\": [43, 120], \"edge\": 209}, {\"x\": 0.04246655649142668, \"source\": 43, \"method\": \"phonecalls\", \"target\": 139, \"weight\": 1, \"y\": -0.03598526594869128, \"pair\": [43, 139], \"edge\": 210}, {\"x\": 0.014907341059817546, \"source\": 43, \"method\": \"phonecalls\", \"target\": 139, \"weight\": 1, \"y\": -0.0661372856193289, \"pair\": [43, 139], \"edge\": 210}, {\"x\": 0.0791141371992724, \"source\": 44, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": -0.044430834196097516, \"pair\": [44, 47], \"edge\": 211}, {\"x\": 0.11749951811931673, \"source\": 44, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": -0.011598498427487423, \"pair\": [44, 47], \"edge\": 211}, {\"x\": 0.11050776731570415, \"source\": 45, \"method\": \"meetings\", \"target\": 46, \"weight\": 1, \"y\": 0.001464868686066248, \"pair\": [45, 46], \"edge\": 212}, {\"x\": 0.1325051404831165, \"source\": 45, \"method\": \"meetings\", \"target\": 46, \"weight\": 1, \"y\": 0.02287698773026062, \"pair\": [45, 46], \"edge\": 212}, {\"x\": 0.11050776731570415, \"source\": 45, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 1, \"y\": 0.001464868686066248, \"pair\": [45, 47], \"edge\": 213}, {\"x\": 0.11749951811931673, \"source\": 45, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 1, \"y\": -0.011598498427487423, \"pair\": [45, 47], \"edge\": 213}, {\"x\": 0.11050776731570415, \"source\": 45, \"method\": \"meetings\", \"target\": 48, \"weight\": 1, \"y\": 0.001464868686066248, \"pair\": [45, 48], \"edge\": 214}, {\"x\": 0.1423891155030037, \"source\": 45, \"method\": \"meetings\", \"target\": 48, \"weight\": 1, \"y\": -0.0039815184705114335, \"pair\": [45, 48], \"edge\": 214}, {\"x\": 0.11050776731570415, \"source\": 45, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 1, \"y\": 0.001464868686066248, \"pair\": [45, 51], \"edge\": 215}, {\"x\": 0.1223460005243755, \"source\": 45, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 1, \"y\": 0.0041269931419576755, \"pair\": [45, 51], \"edge\": 215}, {\"x\": 0.11050776731570415, \"source\": 45, \"method\": \"phonecalls\", \"target\": 54, \"weight\": 1, \"y\": 0.001464868686066248, \"pair\": [45, 54], \"edge\": 216}, {\"x\": 0.08250584794499385, \"source\": 45, \"method\": \"phonecalls\", \"target\": 54, \"weight\": 1, \"y\": -0.016717253662116047, \"pair\": [45, 54], \"edge\": 216}, {\"x\": 0.11050776731570415, \"source\": 45, \"method\": \"meetings\", \"target\": 55, \"weight\": 1, \"y\": 0.001464868686066248, \"pair\": [45, 55], \"edge\": 217}, {\"x\": 0.17943850592679658, \"source\": 45, \"method\": \"meetings\", \"target\": 55, \"weight\": 1, \"y\": 0.012411974556813058, \"pair\": [45, 55], \"edge\": 217}, {\"x\": 0.11050776731570415, \"source\": 45, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": 0.001464868686066248, \"pair\": [45, 50], \"edge\": 218}, {\"x\": 0.15414731472842055, \"source\": 45, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": 0.02127617608196343, \"pair\": [45, 50], \"edge\": 218}, {\"x\": 0.11050776731570415, \"source\": 45, \"method\": \"meetings\", \"target\": 89, \"weight\": 2, \"y\": 0.001464868686066248, \"pair\": [45, 89], \"edge\": 219}, {\"x\": 0.11592769336653481, \"source\": 45, \"method\": \"meetings\", \"target\": 89, \"weight\": 2, \"y\": -0.03187484266859175, \"pair\": [45, 89], \"edge\": 219}, {\"x\": 0.11050776731570415, \"source\": 45, \"method\": \"meetings\", \"target\": 94, \"weight\": 1, \"y\": 0.001464868686066248, \"pair\": [45, 94], \"edge\": 220}, {\"x\": 0.17999465981916565, \"source\": 45, \"method\": \"meetings\", \"target\": 94, \"weight\": 1, \"y\": -0.0146837332222107, \"pair\": [45, 94], \"edge\": 220}, {\"x\": 0.11050776731570415, \"source\": 45, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": 0.001464868686066248, \"pair\": [45, 68], \"edge\": 221}, {\"x\": 0.10737577986159841, \"source\": 45, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": -0.06426632231873398, \"pair\": [45, 68], \"edge\": 221}, {\"x\": 0.11050776731570415, \"source\": 45, \"method\": \"phonecalls\", \"target\": 69, \"weight\": 2, \"y\": 0.001464868686066248, \"pair\": [45, 69], \"edge\": 222}, {\"x\": 0.03569372879297126, \"source\": 45, \"method\": \"phonecalls\", \"target\": 69, \"weight\": 2, \"y\": 0.002582300981474374, \"pair\": [45, 69], \"edge\": 222}, {\"x\": 0.1325051404831165, \"source\": 46, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": 0.02287698773026062, \"pair\": [46, 47], \"edge\": 223}, {\"x\": 0.11749951811931673, \"source\": 46, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": -0.011598498427487423, \"pair\": [46, 47], \"edge\": 223}, {\"x\": 0.1325051404831165, \"source\": 46, \"method\": \"meetings\", \"target\": 48, \"weight\": 1, \"y\": 0.02287698773026062, \"pair\": [46, 48], \"edge\": 224}, {\"x\": 0.1423891155030037, \"source\": 46, \"method\": \"meetings\", \"target\": 48, \"weight\": 1, \"y\": -0.0039815184705114335, \"pair\": [46, 48], \"edge\": 224}, {\"x\": 0.11749951811931673, \"source\": 47, \"method\": \"phonecalls\", \"target\": 48, \"weight\": 4, \"y\": -0.011598498427487423, \"pair\": [47, 48], \"edge\": 225}, {\"x\": 0.1423891155030037, \"source\": 47, \"method\": \"phonecalls\", \"target\": 48, \"weight\": 4, \"y\": -0.0039815184705114335, \"pair\": [47, 48], \"edge\": 225}, {\"x\": 0.11749951811931673, \"source\": 47, \"method\": \"phonecalls\", \"target\": 49, \"weight\": 1, \"y\": -0.011598498427487423, \"pair\": [47, 49], \"edge\": 226}, {\"x\": 0.15380578878918935, \"source\": 47, \"method\": \"phonecalls\", \"target\": 49, \"weight\": 1, \"y\": 0.059143810553784086, \"pair\": [47, 49], \"edge\": 226}, {\"x\": 0.11749951811931673, \"source\": 47, \"method\": \"phonecalls\", \"target\": 50, \"weight\": 2, \"y\": -0.011598498427487423, \"pair\": [47, 50], \"edge\": 227}, {\"x\": 0.15414731472842055, \"source\": 47, \"method\": \"phonecalls\", \"target\": 50, \"weight\": 2, \"y\": 0.02127617608196343, \"pair\": [47, 50], \"edge\": 227}, {\"x\": 0.11749951811931673, \"source\": 47, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 3, \"y\": -0.011598498427487423, \"pair\": [47, 51], \"edge\": 228}, {\"x\": 0.1223460005243755, \"source\": 47, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 3, \"y\": 0.0041269931419576755, \"pair\": [47, 51], \"edge\": 228}, {\"x\": 0.11749951811931673, \"source\": 47, \"method\": \"phonecalls\", \"target\": 54, \"weight\": 4, \"y\": -0.011598498427487423, \"pair\": [47, 54], \"edge\": 229}, {\"x\": 0.08250584794499385, \"source\": 47, \"method\": \"phonecalls\", \"target\": 54, \"weight\": 4, \"y\": -0.016717253662116047, \"pair\": [47, 54], \"edge\": 229}, {\"x\": 0.11749951811931673, \"source\": 47, \"method\": \"meetings\", \"target\": 55, \"weight\": 1, \"y\": -0.011598498427487423, \"pair\": [47, 55], \"edge\": 230}, {\"x\": 0.17943850592679658, \"source\": 47, \"method\": \"meetings\", \"target\": 55, \"weight\": 1, \"y\": 0.012411974556813058, \"pair\": [47, 55], \"edge\": 230}, {\"x\": 0.11749951811931673, \"source\": 47, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": -0.011598498427487423, \"pair\": [47, 68], \"edge\": 231}, {\"x\": 0.10737577986159841, \"source\": 47, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": -0.06426632231873398, \"pair\": [47, 68], \"edge\": 231}, {\"x\": 0.11749951811931673, \"source\": 47, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": -0.011598498427487423, \"pair\": [47, 89], \"edge\": 232}, {\"x\": 0.11592769336653481, \"source\": 47, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": -0.03187484266859175, \"pair\": [47, 89], \"edge\": 232}, {\"x\": 0.11749951811931673, \"source\": 47, \"method\": \"meetings\", \"target\": 90, \"weight\": 2, \"y\": -0.011598498427487423, \"pair\": [47, 90], \"edge\": 233}, {\"x\": 0.17256286389293873, \"source\": 47, \"method\": \"meetings\", \"target\": 90, \"weight\": 2, \"y\": 0.02901924607337627, \"pair\": [47, 90], \"edge\": 233}, {\"x\": 0.11749951811931673, \"source\": 47, \"method\": \"meetings\", \"target\": 93, \"weight\": 1, \"y\": -0.011598498427487423, \"pair\": [47, 93], \"edge\": 234}, {\"x\": 0.14155100232463913, \"source\": 47, \"method\": \"meetings\", \"target\": 93, \"weight\": 1, \"y\": -0.040029604132746956, \"pair\": [47, 93], \"edge\": 234}, {\"x\": 0.11749951811931673, \"source\": 47, \"method\": \"phonecalls\", \"target\": 121, \"weight\": 1, \"y\": -0.011598498427487423, \"pair\": [47, 121], \"edge\": 235}, {\"x\": 0.21863191993225814, \"source\": 47, \"method\": \"phonecalls\", \"target\": 121, \"weight\": 1, \"y\": -0.03860026076366977, \"pair\": [47, 121], \"edge\": 235}, {\"x\": 0.11749951811931673, \"source\": 47, \"method\": \"phonecalls\", \"target\": 122, \"weight\": 1, \"y\": -0.011598498427487423, \"pair\": [47, 122], \"edge\": 236}, {\"x\": 0.2272261385812622, \"source\": 47, \"method\": \"phonecalls\", \"target\": 122, \"weight\": 1, \"y\": -0.02399474177334827, \"pair\": [47, 122], \"edge\": 236}, {\"x\": 0.11749951811931673, \"source\": 47, \"method\": \"phonecalls\", \"target\": 123, \"weight\": 1, \"y\": -0.011598498427487423, \"pair\": [47, 123], \"edge\": 237}, {\"x\": 0.2151495518779385, \"source\": 47, \"method\": \"phonecalls\", \"target\": 123, \"weight\": 1, \"y\": -0.05639080957143462, \"pair\": [47, 123], \"edge\": 237}, {\"x\": 0.11749951811931673, \"source\": 47, \"method\": \"phonecalls\", \"target\": 96, \"weight\": 1, \"y\": -0.011598498427487423, \"pair\": [47, 96], \"edge\": 238}, {\"x\": 0.22139926809416968, \"source\": 47, \"method\": \"phonecalls\", \"target\": 96, \"weight\": 1, \"y\": 0.027407302466213875, \"pair\": [47, 96], \"edge\": 238}, {\"x\": 0.11749951811931673, \"source\": 47, \"method\": \"phonecalls\", \"target\": 135, \"weight\": 1, \"y\": -0.011598498427487423, \"pair\": [47, 135], \"edge\": 239}, {\"x\": 0.23039900420501516, \"source\": 47, \"method\": \"phonecalls\", \"target\": 135, \"weight\": 1, \"y\": -0.006118738494597925, \"pair\": [47, 135], \"edge\": 239}, {\"x\": 0.11749951811931673, \"source\": 47, \"method\": \"phonecalls\", \"target\": 95, \"weight\": 1, \"y\": -0.011598498427487423, \"pair\": [47, 95], \"edge\": 240}, {\"x\": 0.1731732997528489, \"source\": 47, \"method\": \"phonecalls\", \"target\": 95, \"weight\": 1, \"y\": -0.03710375846875645, \"pair\": [47, 95], \"edge\": 240}, {\"x\": 0.11749951811931673, \"source\": 47, \"method\": \"phonecalls\", \"target\": 147, \"weight\": 1, \"y\": -0.011598498427487423, \"pair\": [47, 147], \"edge\": 241}, {\"x\": 0.21685280985526084, \"source\": 47, \"method\": \"phonecalls\", \"target\": 147, \"weight\": 1, \"y\": 0.005653557592003872, \"pair\": [47, 147], \"edge\": 241}, {\"x\": 0.11749951811931673, \"source\": 47, \"method\": \"phonecalls\", \"target\": 56, \"weight\": 1, \"y\": -0.011598498427487423, \"pair\": [47, 56], \"edge\": 242}, {\"x\": 0.10920063064273162, \"source\": 47, \"method\": \"phonecalls\", \"target\": 56, \"weight\": 1, \"y\": 0.08328397848269081, \"pair\": [47, 56], \"edge\": 242}, {\"x\": 0.11749951811931673, \"source\": 47, \"method\": \"phonecalls\", \"target\": 97, \"weight\": 1, \"y\": -0.011598498427487423, \"pair\": [47, 97], \"edge\": 243}, {\"x\": 0.12017909083449298, \"source\": 47, \"method\": \"phonecalls\", \"target\": 97, \"weight\": 1, \"y\": -0.06736377733769908, \"pair\": [47, 97], \"edge\": 243}, {\"x\": 0.11749951811931673, \"source\": 47, \"method\": \"phonecalls\", \"target\": 151, \"weight\": 4, \"y\": -0.011598498427487423, \"pair\": [47, 151], \"edge\": 244}, {\"x\": 0.15383895921775662, \"source\": 47, \"method\": \"phonecalls\", \"target\": 151, \"weight\": 4, \"y\": -0.029209566637755775, \"pair\": [47, 151], \"edge\": 244}, {\"x\": 0.1423891155030037, \"source\": 48, \"method\": \"meetings\", \"target\": 49, \"weight\": 1, \"y\": -0.0039815184705114335, \"pair\": [48, 49], \"edge\": 245}, {\"x\": 0.15380578878918935, \"source\": 48, \"method\": \"meetings\", \"target\": 49, \"weight\": 1, \"y\": 0.059143810553784086, \"pair\": [48, 49], \"edge\": 245}, {\"x\": 0.1423891155030037, \"source\": 48, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": -0.0039815184705114335, \"pair\": [48, 50], \"edge\": 246}, {\"x\": 0.15414731472842055, \"source\": 48, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": 0.02127617608196343, \"pair\": [48, 50], \"edge\": 246}, {\"x\": 0.1423891155030037, \"source\": 48, \"method\": \"meetings\", \"target\": 51, \"weight\": 3, \"y\": -0.0039815184705114335, \"pair\": [48, 51], \"edge\": 247}, {\"x\": 0.1223460005243755, \"source\": 48, \"method\": \"meetings\", \"target\": 51, \"weight\": 3, \"y\": 0.0041269931419576755, \"pair\": [48, 51], \"edge\": 247}, {\"x\": 0.1423891155030037, \"source\": 48, \"method\": \"meetings\", \"target\": 93, \"weight\": 3, \"y\": -0.0039815184705114335, \"pair\": [48, 93], \"edge\": 248}, {\"x\": 0.14155100232463913, \"source\": 48, \"method\": \"meetings\", \"target\": 93, \"weight\": 3, \"y\": -0.040029604132746956, \"pair\": [48, 93], \"edge\": 248}, {\"x\": 0.1423891155030037, \"source\": 48, \"method\": \"meetings\", \"target\": 89, \"weight\": 1, \"y\": -0.0039815184705114335, \"pair\": [48, 89], \"edge\": 249}, {\"x\": 0.11592769336653481, \"source\": 48, \"method\": \"meetings\", \"target\": 89, \"weight\": 1, \"y\": -0.03187484266859175, \"pair\": [48, 89], \"edge\": 249}, {\"x\": 0.1423891155030037, \"source\": 48, \"method\": \"meetings\", \"target\": 95, \"weight\": 1, \"y\": -0.0039815184705114335, \"pair\": [48, 95], \"edge\": 250}, {\"x\": 0.1731732997528489, \"source\": 48, \"method\": \"meetings\", \"target\": 95, \"weight\": 1, \"y\": -0.03710375846875645, \"pair\": [48, 95], \"edge\": 250}, {\"x\": 0.1423891155030037, \"source\": 48, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": -0.0039815184705114335, \"pair\": [48, 68], \"edge\": 251}, {\"x\": 0.10737577986159841, \"source\": 48, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": -0.06426632231873398, \"pair\": [48, 68], \"edge\": 251}, {\"x\": 0.15380578878918935, \"source\": 49, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": 0.059143810553784086, \"pair\": [49, 50], \"edge\": 252}, {\"x\": 0.15414731472842055, \"source\": 49, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": 0.02127617608196343, \"pair\": [49, 50], \"edge\": 252}, {\"x\": 0.15380578878918935, \"source\": 49, \"method\": \"phonecalls\", \"target\": 56, \"weight\": 2, \"y\": 0.059143810553784086, \"pair\": [49, 56], \"edge\": 253}, {\"x\": 0.10920063064273162, \"source\": 49, \"method\": \"phonecalls\", \"target\": 56, \"weight\": 2, \"y\": 0.08328397848269081, \"pair\": [49, 56], \"edge\": 253}, {\"x\": 0.15414731472842055, \"source\": 50, \"method\": \"meetings\", \"target\": 51, \"weight\": 1, \"y\": 0.02127617608196343, \"pair\": [50, 51], \"edge\": 254}, {\"x\": 0.1223460005243755, \"source\": 50, \"method\": \"meetings\", \"target\": 51, \"weight\": 1, \"y\": 0.0041269931419576755, \"pair\": [50, 51], \"edge\": 254}, {\"x\": 0.15414731472842055, \"source\": 50, \"method\": \"meetings\", \"target\": 89, \"weight\": 1, \"y\": 0.02127617608196343, \"pair\": [50, 89], \"edge\": 255}, {\"x\": 0.11592769336653481, \"source\": 50, \"method\": \"meetings\", \"target\": 89, \"weight\": 1, \"y\": -0.03187484266859175, \"pair\": [50, 89], \"edge\": 255}, {\"x\": 0.15414731472842055, \"source\": 50, \"method\": \"meetings\", \"target\": 90, \"weight\": 2, \"y\": 0.02127617608196343, \"pair\": [50, 90], \"edge\": 256}, {\"x\": 0.17256286389293873, \"source\": 50, \"method\": \"meetings\", \"target\": 90, \"weight\": 2, \"y\": 0.02901924607337627, \"pair\": [50, 90], \"edge\": 256}, {\"x\": 0.1223460005243755, \"source\": 51, \"method\": \"meetings\", \"target\": 52, \"weight\": 1, \"y\": 0.0041269931419576755, \"pair\": [51, 52], \"edge\": 257}, {\"x\": 0.24731935142333197, \"source\": 51, \"method\": \"meetings\", \"target\": 52, \"weight\": 1, \"y\": 0.05142964864855591, \"pair\": [51, 52], \"edge\": 257}, {\"x\": 0.1223460005243755, \"source\": 51, \"method\": \"meetings\", \"target\": 53, \"weight\": 1, \"y\": 0.0041269931419576755, \"pair\": [51, 53], \"edge\": 258}, {\"x\": 0.2173462043761638, \"source\": 51, \"method\": \"meetings\", \"target\": 53, \"weight\": 1, \"y\": 0.05239610831530393, \"pair\": [51, 53], \"edge\": 258}, {\"x\": 0.1223460005243755, \"source\": 51, \"method\": \"meetings\", \"target\": 54, \"weight\": 1, \"y\": 0.0041269931419576755, \"pair\": [51, 54], \"edge\": 259}, {\"x\": 0.08250584794499385, \"source\": 51, \"method\": \"meetings\", \"target\": 54, \"weight\": 1, \"y\": -0.016717253662116047, \"pair\": [51, 54], \"edge\": 259}, {\"x\": 0.1223460005243755, \"source\": 51, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": 0.0041269931419576755, \"pair\": [51, 89], \"edge\": 260}, {\"x\": 0.11592769336653481, \"source\": 51, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": -0.03187484266859175, \"pair\": [51, 89], \"edge\": 260}, {\"x\": 0.24731935142333197, \"source\": 52, \"method\": \"meetings\", \"target\": 53, \"weight\": 1, \"y\": 0.05142964864855591, \"pair\": [52, 53], \"edge\": 261}, {\"x\": 0.2173462043761638, \"source\": 52, \"method\": \"meetings\", \"target\": 53, \"weight\": 1, \"y\": 0.05239610831530393, \"pair\": [52, 53], \"edge\": 261}, {\"x\": 0.24731935142333197, \"source\": 52, \"method\": \"meetings\", \"target\": 96, \"weight\": 1, \"y\": 0.05142964864855591, \"pair\": [52, 96], \"edge\": 262}, {\"x\": 0.22139926809416968, \"source\": 52, \"method\": \"meetings\", \"target\": 96, \"weight\": 1, \"y\": 0.027407302466213875, \"pair\": [52, 96], \"edge\": 262}, {\"x\": 0.24731935142333197, \"source\": 52, \"method\": \"phonecalls\", \"target\": 124, \"weight\": 1, \"y\": 0.05142964864855591, \"pair\": [52, 124], \"edge\": 263}, {\"x\": 0.33403134860418493, \"source\": 52, \"method\": \"phonecalls\", \"target\": 124, \"weight\": 1, \"y\": 0.08407371345095738, \"pair\": [52, 124], \"edge\": 263}, {\"x\": 0.08250584794499385, \"source\": 54, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": -0.016717253662116047, \"pair\": [54, 68], \"edge\": 264}, {\"x\": 0.10737577986159841, \"source\": 54, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": -0.06426632231873398, \"pair\": [54, 68], \"edge\": 264}, {\"x\": 0.08250584794499385, \"source\": 54, \"method\": \"meetings\", \"target\": 70, \"weight\": 2, \"y\": -0.016717253662116047, \"pair\": [54, 70], \"edge\": 265}, {\"x\": 0.09004879020314133, \"source\": 54, \"method\": \"meetings\", \"target\": 70, \"weight\": 2, \"y\": -0.06260473269975722, \"pair\": [54, 70], \"edge\": 265}, {\"x\": 0.08250584794499385, \"source\": 54, \"method\": \"phonecalls\", \"target\": 69, \"weight\": 1, \"y\": -0.016717253662116047, \"pair\": [54, 69], \"edge\": 266}, {\"x\": 0.03569372879297126, \"source\": 54, \"method\": \"phonecalls\", \"target\": 69, \"weight\": 1, \"y\": 0.002582300981474374, \"pair\": [54, 69], \"edge\": 266}, {\"x\": 0.10920063064273162, \"source\": 56, \"method\": \"meetings\", \"target\": 57, \"weight\": 1, \"y\": 0.08328397848269081, \"pair\": [56, 57], \"edge\": 267}, {\"x\": 0.1631681511899705, \"source\": 56, \"method\": \"meetings\", \"target\": 57, \"weight\": 1, \"y\": 0.16430154533525543, \"pair\": [56, 57], \"edge\": 267}, {\"x\": 0.10920063064273162, \"source\": 56, \"method\": \"phonecalls\", \"target\": 125, \"weight\": 1, \"y\": 0.08328397848269081, \"pair\": [56, 125], \"edge\": 268}, {\"x\": -0.002312125022450362, \"source\": 56, \"method\": \"phonecalls\", \"target\": 125, \"weight\": 1, \"y\": 0.030999456689846404, \"pair\": [56, 125], \"edge\": 268}, {\"x\": -0.1796553108650634, \"source\": 58, \"method\": \"meetings\", \"target\": 59, \"weight\": 1, \"y\": 0.016978230999206884, \"pair\": [58, 59], \"edge\": 269}, {\"x\": -0.18547205897282934, \"source\": 58, \"method\": \"meetings\", \"target\": 59, \"weight\": 1, \"y\": -0.013622424600998686, \"pair\": [58, 59], \"edge\": 269}, {\"x\": -0.1796553108650634, \"source\": 58, \"method\": \"phonecalls\", \"target\": 109, \"weight\": 3, \"y\": 0.016978230999206884, \"pair\": [58, 109], \"edge\": 270}, {\"x\": -0.25366058108558853, \"source\": 58, \"method\": \"phonecalls\", \"target\": 109, \"weight\": 3, \"y\": 0.02369475297303116, \"pair\": [58, 109], \"edge\": 270}, {\"x\": -0.1796553108650634, \"source\": 58, \"method\": \"phonecalls\", \"target\": 75, \"weight\": 1, \"y\": 0.016978230999206884, \"pair\": [58, 75], \"edge\": 271}, {\"x\": -0.0897053430490711, \"source\": 58, \"method\": \"phonecalls\", \"target\": 75, \"weight\": 1, \"y\": 0.013101398964902963, \"pair\": [58, 75], \"edge\": 271}, {\"x\": -0.1796553108650634, \"source\": 58, \"method\": \"phonecalls\", \"target\": 77, \"weight\": 2, \"y\": 0.016978230999206884, \"pair\": [58, 77], \"edge\": 272}, {\"x\": -0.10924841249342242, \"source\": 58, \"method\": \"phonecalls\", \"target\": 77, \"weight\": 2, \"y\": 0.03021006080396897, \"pair\": [58, 77], \"edge\": 272}, {\"x\": 0.008929531091213954, \"source\": 61, \"method\": \"phonecalls\", \"target\": 62, \"weight\": 1, \"y\": -0.1261098728482192, \"pair\": [61, 62], \"edge\": 273}, {\"x\": -0.013149831069748494, \"source\": 61, \"method\": \"phonecalls\", \"target\": 62, \"weight\": 1, \"y\": -0.22986853313104777, \"pair\": [61, 62], \"edge\": 273}, {\"x\": 0.008929531091213954, \"source\": 61, \"method\": \"phonecalls\", \"target\": 66, \"weight\": 3, \"y\": -0.1261098728482192, \"pair\": [61, 66], \"edge\": 274}, {\"x\": -0.03408591480188705, \"source\": 61, \"method\": \"phonecalls\", \"target\": 66, \"weight\": 3, \"y\": -0.108595724404945, \"pair\": [61, 66], \"edge\": 274}, {\"x\": 0.008929531091213954, \"source\": 61, \"method\": \"phonecalls\", \"target\": 105, \"weight\": 2, \"y\": -0.1261098728482192, \"pair\": [61, 105], \"edge\": 275}, {\"x\": 0.006097783629992272, \"source\": 61, \"method\": \"phonecalls\", \"target\": 105, \"weight\": 2, \"y\": -0.19206453166182635, \"pair\": [61, 105], \"edge\": 275}, {\"x\": 0.008929531091213954, \"source\": 61, \"method\": \"phonecalls\", \"target\": 117, \"weight\": 2, \"y\": -0.1261098728482192, \"pair\": [61, 117], \"edge\": 276}, {\"x\": 0.028087875198861845, \"source\": 61, \"method\": \"phonecalls\", \"target\": 117, \"weight\": 2, \"y\": -0.18748797918508636, \"pair\": [61, 117], \"edge\": 276}, {\"x\": 0.008929531091213954, \"source\": 61, \"method\": \"phonecalls\", \"target\": 80, \"weight\": 1, \"y\": -0.1261098728482192, \"pair\": [61, 80], \"edge\": 277}, {\"x\": 0.11084323162873101, \"source\": 61, \"method\": \"phonecalls\", \"target\": 80, \"weight\": 1, \"y\": -0.21816891907994934, \"pair\": [61, 80], \"edge\": 277}, {\"x\": 0.008929531091213954, \"source\": 61, \"method\": \"phonecalls\", \"target\": 67, \"weight\": 5, \"y\": -0.1261098728482192, \"pair\": [61, 67], \"edge\": 278}, {\"x\": 0.032197817905865664, \"source\": 61, \"method\": \"phonecalls\", \"target\": 67, \"weight\": 5, \"y\": -0.15279710099094182, \"pair\": [61, 67], \"edge\": 278}, {\"x\": 0.008929531091213954, \"source\": 61, \"method\": \"phonecalls\", \"target\": 128, \"weight\": 2, \"y\": -0.1261098728482192, \"pair\": [61, 128], \"edge\": 279}, {\"x\": -0.01516835921482798, \"source\": 61, \"method\": \"phonecalls\", \"target\": 128, \"weight\": 2, \"y\": -0.1870855576983899, \"pair\": [61, 128], \"edge\": 279}, {\"x\": 0.008929531091213954, \"source\": 61, \"method\": \"phonecalls\", \"target\": 129, \"weight\": 1, \"y\": -0.1261098728482192, \"pair\": [61, 129], \"edge\": 280}, {\"x\": 0.005912527447120714, \"source\": 61, \"method\": \"phonecalls\", \"target\": 129, \"weight\": 1, \"y\": -0.22824657678104457, \"pair\": [61, 129], \"edge\": 280}, {\"x\": 0.008929531091213954, \"source\": 61, \"method\": \"phonecalls\", \"target\": 75, \"weight\": 1, \"y\": -0.1261098728482192, \"pair\": [61, 75], \"edge\": 281}, {\"x\": -0.0897053430490711, \"source\": 61, \"method\": \"phonecalls\", \"target\": 75, \"weight\": 1, \"y\": 0.013101398964902963, \"pair\": [61, 75], \"edge\": 281}, {\"x\": 0.008929531091213954, \"source\": 61, \"method\": \"phonecalls\", \"target\": 130, \"weight\": 1, \"y\": -0.1261098728482192, \"pair\": [61, 130], \"edge\": 282}, {\"x\": 0.043000950221258355, \"source\": 61, \"method\": \"phonecalls\", \"target\": 130, \"weight\": 1, \"y\": -0.22010389322158294, \"pair\": [61, 130], \"edge\": 282}, {\"x\": 0.008929531091213954, \"source\": 61, \"method\": \"phonecalls\", \"target\": 131, \"weight\": 4, \"y\": -0.1261098728482192, \"pair\": [61, 131], \"edge\": 283}, {\"x\": 0.003287992924154501, \"source\": 61, \"method\": \"phonecalls\", \"target\": 131, \"weight\": 4, \"y\": -0.16583314177207673, \"pair\": [61, 131], \"edge\": 283}, {\"x\": 0.008929531091213954, \"source\": 61, \"method\": \"phonecalls\", \"target\": 133, \"weight\": 1, \"y\": -0.1261098728482192, \"pair\": [61, 133], \"edge\": 284}, {\"x\": 0.024215995664546363, \"source\": 61, \"method\": \"phonecalls\", \"target\": 133, \"weight\": 1, \"y\": -0.23035942893596337, \"pair\": [61, 133], \"edge\": 284}, {\"x\": 0.008929531091213954, \"source\": 61, \"method\": \"phonecalls\", \"target\": 134, \"weight\": 1, \"y\": -0.1261098728482192, \"pair\": [61, 134], \"edge\": 285}, {\"x\": -0.03156428448687718, \"source\": 61, \"method\": \"phonecalls\", \"target\": 134, \"weight\": 1, \"y\": -0.22017649037116607, \"pair\": [61, 134], \"edge\": 285}, {\"x\": 0.008929531091213954, \"source\": 61, \"method\": \"phonecalls\", \"target\": 142, \"weight\": 1, \"y\": -0.1261098728482192, \"pair\": [61, 142], \"edge\": 286}, {\"x\": -0.06662095968825617, \"source\": 61, \"method\": \"phonecalls\", \"target\": 142, \"weight\": 1, \"y\": -0.11904292944398821, \"pair\": [61, 142], \"edge\": 286}, {\"x\": 0.008929531091213954, \"source\": 61, \"method\": \"phonecalls\", \"target\": 70, \"weight\": 1, \"y\": -0.1261098728482192, \"pair\": [61, 70], \"edge\": 287}, {\"x\": 0.09004879020314133, \"source\": 61, \"method\": \"phonecalls\", \"target\": 70, \"weight\": 1, \"y\": -0.06260473269975722, \"pair\": [61, 70], \"edge\": 287}, {\"x\": -0.05009154238426464, \"source\": 63, \"method\": \"meetings\", \"target\": 64, \"weight\": 3, \"y\": -0.015569815709628052, \"pair\": [63, 64], \"edge\": 288}, {\"x\": -0.01732299304953762, \"source\": 63, \"method\": \"meetings\", \"target\": 64, \"weight\": 3, \"y\": -0.02196627921477448, \"pair\": [63, 64], \"edge\": 288}, {\"x\": -0.05009154238426464, \"source\": 63, \"method\": \"meetings\", \"target\": 72, \"weight\": 2, \"y\": -0.015569815709628052, \"pair\": [63, 72], \"edge\": 289}, {\"x\": -0.06770812530480919, \"source\": 63, \"method\": \"meetings\", \"target\": 72, \"weight\": 2, \"y\": 0.002490925585405996, \"pair\": [63, 72], \"edge\": 289}, {\"x\": -0.01732299304953762, \"source\": 64, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": -0.02196627921477448, \"pair\": [64, 68], \"edge\": 290}, {\"x\": 0.10737577986159841, \"source\": 64, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": -0.06426632231873398, \"pair\": [64, 68], \"edge\": 290}, {\"x\": -0.01732299304953762, \"source\": 64, \"method\": \"phonecalls\", \"target\": 75, \"weight\": 3, \"y\": -0.02196627921477448, \"pair\": [64, 75], \"edge\": 291}, {\"x\": -0.0897053430490711, \"source\": 64, \"method\": \"phonecalls\", \"target\": 75, \"weight\": 3, \"y\": 0.013101398964902963, \"pair\": [64, 75], \"edge\": 291}, {\"x\": 0.10737577986159841, \"source\": 68, \"method\": \"meetings\", \"target\": 69, \"weight\": 1, \"y\": -0.06426632231873398, \"pair\": [68, 69], \"edge\": 292}, {\"x\": 0.03569372879297126, \"source\": 68, \"method\": \"meetings\", \"target\": 69, \"weight\": 1, \"y\": 0.002582300981474374, \"pair\": [68, 69], \"edge\": 292}, {\"x\": 0.10737577986159841, \"source\": 68, \"method\": \"phonecalls\", \"target\": 70, \"weight\": 3, \"y\": -0.06426632231873398, \"pair\": [68, 70], \"edge\": 293}, {\"x\": 0.09004879020314133, \"source\": 68, \"method\": \"phonecalls\", \"target\": 70, \"weight\": 3, \"y\": -0.06260473269975722, \"pair\": [68, 70], \"edge\": 293}, {\"x\": 0.10737577986159841, \"source\": 68, \"method\": \"meetings\", \"target\": 79, \"weight\": 1, \"y\": -0.06426632231873398, \"pair\": [68, 79], \"edge\": 294}, {\"x\": 0.15312231188390643, \"source\": 68, \"method\": \"meetings\", \"target\": 79, \"weight\": 1, \"y\": -0.19018239522654887, \"pair\": [68, 79], \"edge\": 294}, {\"x\": 0.10737577986159841, \"source\": 68, \"method\": \"meetings\", \"target\": 83, \"weight\": 1, \"y\": -0.06426632231873398, \"pair\": [68, 83], \"edge\": 295}, {\"x\": 0.16453961557794217, \"source\": 68, \"method\": \"meetings\", \"target\": 83, \"weight\": 1, \"y\": -0.15235722730304724, \"pair\": [68, 83], \"edge\": 295}, {\"x\": 0.10737577986159841, \"source\": 68, \"method\": \"meetings\", \"target\": 89, \"weight\": 7, \"y\": -0.06426632231873398, \"pair\": [68, 89], \"edge\": 296}, {\"x\": 0.11592769336653481, \"source\": 68, \"method\": \"meetings\", \"target\": 89, \"weight\": 7, \"y\": -0.03187484266859175, \"pair\": [68, 89], \"edge\": 296}, {\"x\": 0.10737577986159841, \"source\": 68, \"method\": \"phonecalls\", \"target\": 97, \"weight\": 1, \"y\": -0.06426632231873398, \"pair\": [68, 97], \"edge\": 297}, {\"x\": 0.12017909083449298, \"source\": 68, \"method\": \"phonecalls\", \"target\": 97, \"weight\": 1, \"y\": -0.06736377733769908, \"pair\": [68, 97], \"edge\": 297}, {\"x\": 0.10737577986159841, \"source\": 68, \"method\": \"phonecalls\", \"target\": 149, \"weight\": 1, \"y\": -0.06426632231873398, \"pair\": [68, 149], \"edge\": 298}, {\"x\": 0.2061357354486069, \"source\": 68, \"method\": \"phonecalls\", \"target\": 149, \"weight\": 1, \"y\": -0.1091222131228468, \"pair\": [68, 149], \"edge\": 298}, {\"x\": 0.10737577986159841, \"source\": 68, \"method\": \"phonecalls\", \"target\": 150, \"weight\": 1, \"y\": -0.06426632231873398, \"pair\": [68, 150], \"edge\": 299}, {\"x\": 0.18868856955683147, \"source\": 68, \"method\": \"phonecalls\", \"target\": 150, \"weight\": 1, \"y\": -0.11595325870045961, \"pair\": [68, 150], \"edge\": 299}, {\"x\": 0.10737577986159841, \"source\": 68, \"method\": \"phonecalls\", \"target\": 151, \"weight\": 1, \"y\": -0.06426632231873398, \"pair\": [68, 151], \"edge\": 300}, {\"x\": 0.15383895921775662, \"source\": 68, \"method\": \"phonecalls\", \"target\": 151, \"weight\": 1, \"y\": -0.029209566637755775, \"pair\": [68, 151], \"edge\": 300}, {\"x\": 0.10737577986159841, \"source\": 68, \"method\": \"phonecalls\", \"target\": 152, \"weight\": 1, \"y\": -0.06426632231873398, \"pair\": [68, 152], \"edge\": 301}, {\"x\": 0.1860002519038319, \"source\": 68, \"method\": \"phonecalls\", \"target\": 152, \"weight\": 1, \"y\": -0.1343849998675848, \"pair\": [68, 152], \"edge\": 301}, {\"x\": 0.03569372879297126, \"source\": 69, \"method\": \"phonecalls\", \"target\": 77, \"weight\": 1, \"y\": 0.002582300981474374, \"pair\": [69, 77], \"edge\": 302}, {\"x\": -0.10924841249342242, \"source\": 69, \"method\": \"phonecalls\", \"target\": 77, \"weight\": 1, \"y\": 0.03021006080396897, \"pair\": [69, 77], \"edge\": 302}, {\"x\": 0.09004879020314133, \"source\": 70, \"method\": \"meetings\", \"target\": 89, \"weight\": 3, \"y\": -0.06260473269975722, \"pair\": [70, 89], \"edge\": 303}, {\"x\": 0.11592769336653481, \"source\": 70, \"method\": \"meetings\", \"target\": 89, \"weight\": 3, \"y\": -0.03187484266859175, \"pair\": [70, 89], \"edge\": 303}, {\"x\": 0.09004879020314133, \"source\": 70, \"method\": \"meetings\", \"target\": 93, \"weight\": 2, \"y\": -0.06260473269975722, \"pair\": [70, 93], \"edge\": 304}, {\"x\": 0.14155100232463913, \"source\": 70, \"method\": \"meetings\", \"target\": 93, \"weight\": 2, \"y\": -0.040029604132746956, \"pair\": [70, 93], \"edge\": 304}, {\"x\": 0.09004879020314133, \"source\": 70, \"method\": \"meetings\", \"target\": 97, \"weight\": 1, \"y\": -0.06260473269975722, \"pair\": [70, 97], \"edge\": 305}, {\"x\": 0.12017909083449298, \"source\": 70, \"method\": \"meetings\", \"target\": 97, \"weight\": 1, \"y\": -0.06736377733769908, \"pair\": [70, 97], \"edge\": 305}, {\"x\": -0.9558270757225705, \"source\": 73, \"method\": \"meetings\", \"target\": 74, \"weight\": 1, \"y\": 0.21472365469606763, \"pair\": [73, 74], \"edge\": 306}, {\"x\": -0.9869683613500567, \"source\": 73, \"method\": \"meetings\", \"target\": 74, \"weight\": 1, \"y\": 0.22259138842837486, \"pair\": [73, 74], \"edge\": 306}, {\"x\": -0.0897053430490711, \"source\": 75, \"method\": \"meetings\", \"target\": 76, \"weight\": 1, \"y\": 0.013101398964902963, \"pair\": [75, 76], \"edge\": 307}, {\"x\": -0.07770937983352315, \"source\": 75, \"method\": \"meetings\", \"target\": 76, \"weight\": 1, \"y\": 0.019481654531596497, \"pair\": [75, 76], \"edge\": 307}, {\"x\": -0.0897053430490711, \"source\": 75, \"method\": \"meetings\", \"target\": 77, \"weight\": 1, \"y\": 0.013101398964902963, \"pair\": [75, 77], \"edge\": 308}, {\"x\": -0.10924841249342242, \"source\": 75, \"method\": \"meetings\", \"target\": 77, \"weight\": 1, \"y\": 0.03021006080396897, \"pair\": [75, 77], \"edge\": 308}, {\"x\": -0.0897053430490711, \"source\": 75, \"method\": \"phonecalls\", \"target\": 112, \"weight\": 5, \"y\": 0.013101398964902963, \"pair\": [75, 112], \"edge\": 309}, {\"x\": -0.104296114169113, \"source\": 75, \"method\": \"phonecalls\", \"target\": 112, \"weight\": 5, \"y\": 0.04931578703238146, \"pair\": [75, 112], \"edge\": 309}, {\"x\": -0.0897053430490711, \"source\": 75, \"method\": \"phonecalls\", \"target\": 113, \"weight\": 3, \"y\": 0.013101398964902963, \"pair\": [75, 113], \"edge\": 310}, {\"x\": -0.11979762923625004, \"source\": 75, \"method\": \"phonecalls\", \"target\": 113, \"weight\": 3, \"y\": 0.06236631777805221, \"pair\": [75, 113], \"edge\": 310}, {\"x\": -0.0897053430490711, \"source\": 75, \"method\": \"phonecalls\", \"target\": 114, \"weight\": 2, \"y\": 0.013101398964902963, \"pair\": [75, 114], \"edge\": 311}, {\"x\": -0.13722982823872137, \"source\": 75, \"method\": \"phonecalls\", \"target\": 114, \"weight\": 2, \"y\": 0.05592259600663083, \"pair\": [75, 114], \"edge\": 311}, {\"x\": -0.0897053430490711, \"source\": 75, \"method\": \"phonecalls\", \"target\": 115, \"weight\": 1, \"y\": 0.013101398964902963, \"pair\": [75, 115], \"edge\": 312}, {\"x\": -0.16932703643076819, \"source\": 75, \"method\": \"phonecalls\", \"target\": 115, \"weight\": 1, \"y\": 0.07611572011729069, \"pair\": [75, 115], \"edge\": 312}, {\"x\": -0.0897053430490711, \"source\": 75, \"method\": \"phonecalls\", \"target\": 116, \"weight\": 1, \"y\": 0.013101398964902963, \"pair\": [75, 116], \"edge\": 313}, {\"x\": -0.18191984932862076, \"source\": 75, \"method\": \"phonecalls\", \"target\": 116, \"weight\": 1, \"y\": 0.05231042727716676, \"pair\": [75, 116], \"edge\": 313}, {\"x\": -0.07770937983352315, \"source\": 76, \"method\": \"meetings\", \"target\": 77, \"weight\": 2, \"y\": 0.019481654531596497, \"pair\": [76, 77], \"edge\": 314}, {\"x\": -0.10924841249342242, \"source\": 76, \"method\": \"meetings\", \"target\": 77, \"weight\": 2, \"y\": 0.03021006080396897, \"pair\": [76, 77], \"edge\": 314}, {\"x\": -0.07770937983352315, \"source\": 76, \"method\": \"meetings\", \"target\": 78, \"weight\": 1, \"y\": 0.019481654531596497, \"pair\": [76, 78], \"edge\": 315}, {\"x\": -0.14706553695587654, \"source\": 76, \"method\": \"meetings\", \"target\": 78, \"weight\": 1, \"y\": 0.032673641859445796, \"pair\": [76, 78], \"edge\": 315}, {\"x\": -0.07770937983352315, \"source\": 76, \"method\": \"meetings\", \"target\": 98, \"weight\": 1, \"y\": 0.019481654531596497, \"pair\": [76, 98], \"edge\": 316}, {\"x\": -0.0439818325353795, \"source\": 76, \"method\": \"meetings\", \"target\": 98, \"weight\": 1, \"y\": 0.01834824861421199, \"pair\": [76, 98], \"edge\": 316}, {\"x\": -0.10924841249342242, \"source\": 77, \"method\": \"phonecalls\", \"target\": 137, \"weight\": 1, \"y\": 0.03021006080396897, \"pair\": [77, 137], \"edge\": 317}, {\"x\": -0.20892837142090975, \"source\": 77, \"method\": \"phonecalls\", \"target\": 137, \"weight\": 1, \"y\": 0.0661836703634387, \"pair\": [77, 137], \"edge\": 317}, {\"x\": -0.10924841249342242, \"source\": 77, \"method\": \"phonecalls\", \"target\": 114, \"weight\": 1, \"y\": 0.03021006080396897, \"pair\": [77, 114], \"edge\": 318}, {\"x\": -0.13722982823872137, \"source\": 77, \"method\": \"phonecalls\", \"target\": 114, \"weight\": 1, \"y\": 0.05592259600663083, \"pair\": [77, 114], \"edge\": 318}, {\"x\": 0.15312231188390643, \"source\": 79, \"method\": \"meetings\", \"target\": 80, \"weight\": 1, \"y\": -0.19018239522654887, \"pair\": [79, 80], \"edge\": 319}, {\"x\": 0.11084323162873101, \"source\": 79, \"method\": \"meetings\", \"target\": 80, \"weight\": 1, \"y\": -0.21816891907994934, \"pair\": [79, 80], \"edge\": 319}, {\"x\": 0.15312231188390643, \"source\": 79, \"method\": \"meetings\", \"target\": 81, \"weight\": 1, \"y\": -0.19018239522654887, \"pair\": [79, 81], \"edge\": 320}, {\"x\": 0.1614037266336705, \"source\": 79, \"method\": \"meetings\", \"target\": 81, \"weight\": 1, \"y\": -0.26360779927938344, \"pair\": [79, 81], \"edge\": 320}, {\"x\": 0.15312231188390643, \"source\": 79, \"method\": \"meetings\", \"target\": 82, \"weight\": 1, \"y\": -0.19018239522654887, \"pair\": [79, 82], \"edge\": 321}, {\"x\": 0.17482884393265355, \"source\": 79, \"method\": \"meetings\", \"target\": 82, \"weight\": 1, \"y\": -0.25567865892978, \"pair\": [79, 82], \"edge\": 321}, {\"x\": 0.15312231188390643, \"source\": 79, \"method\": \"meetings\", \"target\": 83, \"weight\": 1, \"y\": -0.19018239522654887, \"pair\": [79, 83], \"edge\": 322}, {\"x\": 0.16453961557794217, \"source\": 79, \"method\": \"meetings\", \"target\": 83, \"weight\": 1, \"y\": -0.15235722730304724, \"pair\": [79, 83], \"edge\": 322}, {\"x\": 0.11084323162873101, \"source\": 80, \"method\": \"meetings\", \"target\": 81, \"weight\": 1, \"y\": -0.21816891907994934, \"pair\": [80, 81], \"edge\": 323}, {\"x\": 0.1614037266336705, \"source\": 80, \"method\": \"meetings\", \"target\": 81, \"weight\": 1, \"y\": -0.26360779927938344, \"pair\": [80, 81], \"edge\": 323}, {\"x\": 0.11084323162873101, \"source\": 80, \"method\": \"meetings\", \"target\": 82, \"weight\": 1, \"y\": -0.21816891907994934, \"pair\": [80, 82], \"edge\": 324}, {\"x\": 0.17482884393265355, \"source\": 80, \"method\": \"meetings\", \"target\": 82, \"weight\": 1, \"y\": -0.25567865892978, \"pair\": [80, 82], \"edge\": 324}, {\"x\": 0.1614037266336705, \"source\": 81, \"method\": \"meetings\", \"target\": 82, \"weight\": 1, \"y\": -0.26360779927938344, \"pair\": [81, 82], \"edge\": 325}, {\"x\": 0.17482884393265355, \"source\": 81, \"method\": \"meetings\", \"target\": 82, \"weight\": 1, \"y\": -0.25567865892978, \"pair\": [81, 82], \"edge\": 325}, {\"x\": -0.10073022402935924, \"source\": 84, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": 0.24866716443059705, \"pair\": [84, 85], \"edge\": 326}, {\"x\": -0.13357750855656716, \"source\": 84, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": 0.21357498512004602, \"pair\": [84, 85], \"edge\": 326}, {\"x\": -0.10073022402935924, \"source\": 84, \"method\": \"meetings\", \"target\": 86, \"weight\": 1, \"y\": 0.24866716443059705, \"pair\": [84, 86], \"edge\": 327}, {\"x\": -0.11628078973510426, \"source\": 84, \"method\": \"meetings\", \"target\": 86, \"weight\": 1, \"y\": 0.24607240222139293, \"pair\": [84, 86], \"edge\": 327}, {\"x\": -0.10073022402935924, \"source\": 84, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": 0.24866716443059705, \"pair\": [84, 87], \"edge\": 328}, {\"x\": -0.0989390875103513, \"source\": 84, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": 0.23457672531171775, \"pair\": [84, 87], \"edge\": 328}, {\"x\": -0.13357750855656716, \"source\": 85, \"method\": \"meetings\", \"target\": 86, \"weight\": 1, \"y\": 0.21357498512004602, \"pair\": [85, 86], \"edge\": 329}, {\"x\": -0.11628078973510426, \"source\": 85, \"method\": \"meetings\", \"target\": 86, \"weight\": 1, \"y\": 0.24607240222139293, \"pair\": [85, 86], \"edge\": 329}, {\"x\": -0.13357750855656716, \"source\": 85, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": 0.21357498512004602, \"pair\": [85, 87], \"edge\": 330}, {\"x\": -0.0989390875103513, \"source\": 85, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": 0.23457672531171775, \"pair\": [85, 87], \"edge\": 330}, {\"x\": -0.11628078973510426, \"source\": 86, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": 0.24607240222139293, \"pair\": [86, 87], \"edge\": 331}, {\"x\": -0.0989390875103513, \"source\": 86, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": 0.23457672531171775, \"pair\": [86, 87], \"edge\": 331}, {\"x\": 0.11592769336653481, \"source\": 89, \"method\": \"meetings\", \"target\": 93, \"weight\": 2, \"y\": -0.03187484266859175, \"pair\": [89, 93], \"edge\": 332}, {\"x\": 0.14155100232463913, \"source\": 89, \"method\": \"meetings\", \"target\": 93, \"weight\": 2, \"y\": -0.040029604132746956, \"pair\": [89, 93], \"edge\": 332}, {\"x\": 0.11592769336653481, \"source\": 89, \"method\": \"meetings\", \"target\": 94, \"weight\": 1, \"y\": -0.03187484266859175, \"pair\": [89, 94], \"edge\": 333}, {\"x\": 0.17999465981916565, \"source\": 89, \"method\": \"meetings\", \"target\": 94, \"weight\": 1, \"y\": -0.0146837332222107, \"pair\": [89, 94], \"edge\": 333}, {\"x\": 0.11592769336653481, \"source\": 89, \"method\": \"meetings\", \"target\": 95, \"weight\": 1, \"y\": -0.03187484266859175, \"pair\": [89, 95], \"edge\": 334}, {\"x\": 0.1731732997528489, \"source\": 89, \"method\": \"meetings\", \"target\": 95, \"weight\": 1, \"y\": -0.03710375846875645, \"pair\": [89, 95], \"edge\": 334}, {\"x\": 0.11951898178666384, \"source\": 91, \"method\": \"meetings\", \"target\": 92, \"weight\": 1, \"y\": 0.059923934361632215, \"pair\": [91, 92], \"edge\": 335}, {\"x\": 0.1353383703416821, \"source\": 91, \"method\": \"meetings\", \"target\": 92, \"weight\": 1, \"y\": 0.05345272568729972, \"pair\": [91, 92], \"edge\": 335}, {\"x\": 0.14155100232463913, \"source\": 93, \"method\": \"meetings\", \"target\": 95, \"weight\": 1, \"y\": -0.040029604132746956, \"pair\": [93, 95], \"edge\": 336}, {\"x\": 0.1731732997528489, \"source\": 93, \"method\": \"meetings\", \"target\": 95, \"weight\": 1, \"y\": -0.03710375846875645, \"pair\": [93, 95], \"edge\": 336}, {\"x\": -0.17391133013629761, \"source\": 99, \"method\": \"phonecalls\", \"target\": 118, \"weight\": 3, \"y\": -0.021100860151469267, \"pair\": [99, 118], \"edge\": 337}, {\"x\": -0.23499493928727933, \"source\": 99, \"method\": \"phonecalls\", \"target\": 118, \"weight\": 3, \"y\": -0.039460140276008936, \"pair\": [99, 118], \"edge\": 337}, {\"x\": 0.8898205077766663, \"source\": 106, \"method\": \"phonecalls\", \"target\": 107, \"weight\": 1, \"y\": -0.2636672397649877, \"pair\": [106, 107], \"edge\": 338}, {\"x\": 0.9014495196824311, \"source\": 106, \"method\": \"phonecalls\", \"target\": 107, \"weight\": 1, \"y\": -0.2381199822355443, \"pair\": [106, 107], \"edge\": 338}, {\"x\": -0.25366058108558853, \"source\": 109, \"method\": \"phonecalls\", \"target\": 110, \"weight\": 1, \"y\": 0.02369475297303116, \"pair\": [109, 110], \"edge\": 339}, {\"x\": -0.20541758295340326, \"source\": 109, \"method\": \"phonecalls\", \"target\": 110, \"weight\": 1, \"y\": -0.008103905117038245, \"pair\": [109, 110], \"edge\": 339}, {\"x\": -0.25366058108558853, \"source\": 109, \"method\": \"phonecalls\", \"target\": 111, \"weight\": 1, \"y\": 0.02369475297303116, \"pair\": [109, 111], \"edge\": 340}, {\"x\": -0.3475259768303975, \"source\": 109, \"method\": \"phonecalls\", \"target\": 111, \"weight\": 1, \"y\": 0.03526928665566693, \"pair\": [109, 111], \"edge\": 340}, {\"x\": -0.22780490089040933, \"source\": 143, \"method\": \"phonecalls\", \"target\": 144, \"weight\": 1, \"y\": 0.18413310479709935, \"pair\": [143, 144], \"edge\": 341}, {\"x\": -0.24621473408099437, \"source\": 143, \"method\": \"phonecalls\", \"target\": 144, \"weight\": 1, \"y\": 0.20826693839176622, \"pair\": [143, 144], \"edge\": 341}], \"data-42c9cfb11ce5c3379bd7fe466969adf5\": [{\"x\": 0.7201729516614895, \"y\": -0.5941501489847846, \"id\": 0, \"is_central\": \"no\"}, {\"x\": 0.6962269127915148, \"y\": -0.5955104671854762, \"id\": 1, \"is_central\": \"no\"}, {\"x\": 0.7379742090422982, \"y\": -0.6257225236087908, \"id\": 2, \"is_central\": \"no\"}, {\"x\": -0.14963029070486847, \"y\": 0.22839760684015903, \"id\": 3, \"is_central\": \"no\"}, {\"x\": -0.13388635482463185, \"y\": 0.26327789596963713, \"id\": 4, \"is_central\": \"no\"}, {\"x\": -0.12775596921398163, \"y\": 0.19934696076295597, \"id\": 5, \"is_central\": \"no\"}, {\"x\": -0.13521738906463823, \"y\": 0.1960690938502262, \"id\": 6, \"is_central\": \"no\"}, {\"x\": -0.15003889180280985, \"y\": 0.25674390970692657, \"id\": 7, \"is_central\": \"no\"}, {\"x\": -0.1628937526808075, \"y\": 0.2596325546941219, \"id\": 8, \"is_central\": \"no\"}, {\"x\": -0.14646704789004872, \"y\": 0.2690013976338763, \"id\": 9, \"is_central\": \"no\"}, {\"x\": -0.1881614913715324, \"y\": 0.20067016895572298, \"id\": 10, \"is_central\": \"no\"}, {\"x\": -0.16285777662829634, \"y\": 0.1563535106528167, \"id\": 11, \"is_central\": \"no\"}, {\"x\": -0.12556282436714905, \"y\": 0.16005918201067507, \"id\": 12, \"is_central\": \"no\"}, {\"x\": -0.16864511068212604, \"y\": 0.19014193414739683, \"id\": 13, \"is_central\": \"no\"}, {\"x\": -0.17289915231534964, \"y\": 0.2124714170018183, \"id\": 14, \"is_central\": \"no\"}, {\"x\": -0.1592264495980308, \"y\": 0.19558225301864735, \"id\": 15, \"is_central\": \"no\"}, {\"x\": -0.08054038557313334, \"y\": 1.0, \"id\": 16, \"is_central\": \"no\"}, {\"x\": -0.05591659914603527, \"y\": 0.9865849341603224, \"id\": 17, \"is_central\": \"no\"}, {\"x\": -0.08453031363724207, \"y\": -0.02574806163662363, \"id\": 18, \"is_central\": \"yes\"}, {\"x\": -0.11459200425919056, \"y\": -0.03306616016918993, \"id\": 19, \"is_central\": \"no\"}, {\"x\": -0.1260335142497204, \"y\": -0.07916616634145784, \"id\": 20, \"is_central\": \"no\"}, {\"x\": -0.10342108981135545, \"y\": 0.09031950817859964, \"id\": 21, \"is_central\": \"no\"}, {\"x\": -0.09319299341154005, \"y\": -0.0031457699240462016, \"id\": 22, \"is_central\": \"no\"}, {\"x\": -0.1477490548119252, \"y\": 0.00631745618334823, \"id\": 23, \"is_central\": \"no\"}, {\"x\": -0.2533555765225973, \"y\": -0.005999011015023959, \"id\": 24, \"is_central\": \"no\"}, {\"x\": -0.09643094783333603, \"y\": 0.0738142784386027, \"id\": 25, \"is_central\": \"no\"}, {\"x\": -0.004479903134193742, \"y\": 0.05290831941589025, \"id\": 26, \"is_central\": \"no\"}, {\"x\": 0.055912336966714146, \"y\": -0.032629368428978275, \"id\": 27, \"is_central\": \"no\"}, {\"x\": -0.13263709675378124, \"y\": 0.1526276065747802, \"id\": 28, \"is_central\": \"no\"}, {\"x\": -0.005597575826184528, \"y\": -0.020099682282409877, \"id\": 29, \"is_central\": \"no\"}, {\"x\": -0.03667232839148727, \"y\": 0.03701289263695622, \"id\": 30, \"is_central\": \"no\"}, {\"x\": -0.14227761922441623, \"y\": -0.038928683932491444, \"id\": 31, \"is_central\": \"no\"}, {\"x\": -0.14676081859617365, \"y\": -0.06335478178038967, \"id\": 32, \"is_central\": \"no\"}, {\"x\": -0.12186521228775143, \"y\": -0.05185706419850732, \"id\": 33, \"is_central\": \"no\"}, {\"x\": -0.11311982103657724, \"y\": 0.015511622301406802, \"id\": 34, \"is_central\": \"no\"}, {\"x\": -0.11155611366244667, \"y\": -0.0016514281767455977, \"id\": 35, \"is_central\": \"no\"}, {\"x\": 0.08958970502208463, \"y\": -0.028086458233268496, \"id\": 36, \"is_central\": \"no\"}, {\"x\": 0.07605741696983585, \"y\": -0.1182608540070831, \"id\": 37, \"is_central\": \"no\"}, {\"x\": 0.12962454552411715, \"y\": -0.05207770612396997, \"id\": 38, \"is_central\": \"no\"}, {\"x\": 0.1946430381184062, \"y\": 0.0630452098149511, \"id\": 39, \"is_central\": \"no\"}, {\"x\": 0.20701103080268415, \"y\": 0.08655983028322389, \"id\": 40, \"is_central\": \"no\"}, {\"x\": 0.26125595890324854, \"y\": 0.11119111466981486, \"id\": 41, \"is_central\": \"no\"}, {\"x\": 0.24978374110962673, \"y\": 0.1240367928051296, \"id\": 42, \"is_central\": \"no\"}, {\"x\": 0.04246655649142668, \"y\": -0.03598526594869128, \"id\": 43, \"is_central\": \"no\"}, {\"x\": 0.0791141371992724, \"y\": -0.044430834196097516, \"id\": 44, \"is_central\": \"no\"}, {\"x\": 0.11050776731570415, \"y\": 0.001464868686066248, \"id\": 45, \"is_central\": \"no\"}, {\"x\": 0.1325051404831165, \"y\": 0.02287698773026062, \"id\": 46, \"is_central\": \"no\"}, {\"x\": 0.11749951811931673, \"y\": -0.011598498427487423, \"id\": 47, \"is_central\": \"yes\"}, {\"x\": 0.1423891155030037, \"y\": -0.0039815184705114335, \"id\": 48, \"is_central\": \"no\"}, {\"x\": 0.15380578878918935, \"y\": 0.059143810553784086, \"id\": 49, \"is_central\": \"no\"}, {\"x\": 0.15414731472842055, \"y\": 0.02127617608196343, \"id\": 50, \"is_central\": \"no\"}, {\"x\": 0.1223460005243755, \"y\": 0.0041269931419576755, \"id\": 51, \"is_central\": \"no\"}, {\"x\": 0.24731935142333197, \"y\": 0.05142964864855591, \"id\": 52, \"is_central\": \"no\"}, {\"x\": 0.2173462043761638, \"y\": 0.05239610831530393, \"id\": 53, \"is_central\": \"no\"}, {\"x\": 0.08250584794499385, \"y\": -0.016717253662116047, \"id\": 54, \"is_central\": \"no\"}, {\"x\": 0.17943850592679658, \"y\": 0.012411974556813058, \"id\": 55, \"is_central\": \"no\"}, {\"x\": 0.10920063064273162, \"y\": 0.08328397848269081, \"id\": 56, \"is_central\": \"no\"}, {\"x\": 0.1631681511899705, \"y\": 0.16430154533525543, \"id\": 57, \"is_central\": \"no\"}, {\"x\": -0.1796553108650634, \"y\": 0.016978230999206884, \"id\": 58, \"is_central\": \"no\"}, {\"x\": -0.18547205897282934, \"y\": -0.013622424600998686, \"id\": 59, \"is_central\": \"no\"}, {\"x\": -0.14672338424001544, \"y\": -0.021796679546102638, \"id\": 60, \"is_central\": \"no\"}, {\"x\": 0.008929531091213954, \"y\": -0.1261098728482192, \"id\": 61, \"is_central\": \"yes\"}, {\"x\": -0.013149831069748494, \"y\": -0.22986853313104777, \"id\": 62, \"is_central\": \"no\"}, {\"x\": -0.05009154238426464, \"y\": -0.015569815709628052, \"id\": 63, \"is_central\": \"no\"}, {\"x\": -0.01732299304953762, \"y\": -0.02196627921477448, \"id\": 64, \"is_central\": \"no\"}, {\"x\": -0.1767110217353999, \"y\": -0.09254455795546651, \"id\": 65, \"is_central\": \"no\"}, {\"x\": -0.03408591480188705, \"y\": -0.108595724404945, \"id\": 66, \"is_central\": \"no\"}, {\"x\": 0.032197817905865664, \"y\": -0.15279710099094182, \"id\": 67, \"is_central\": \"no\"}, {\"x\": 0.10737577986159841, \"y\": -0.06426632231873398, \"id\": 68, \"is_central\": \"no\"}, {\"x\": 0.03569372879297126, \"y\": 0.002582300981474374, \"id\": 69, \"is_central\": \"no\"}, {\"x\": 0.09004879020314133, \"y\": -0.06260473269975722, \"id\": 70, \"is_central\": \"no\"}, {\"x\": -0.04373218280067268, \"y\": -0.059903077944982355, \"id\": 71, \"is_central\": \"no\"}, {\"x\": -0.06770812530480919, \"y\": 0.002490925585405996, \"id\": 72, \"is_central\": \"no\"}, {\"x\": -0.9558270757225705, \"y\": 0.21472365469606763, \"id\": 73, \"is_central\": \"no\"}, {\"x\": -0.9869683613500567, \"y\": 0.22259138842837486, \"id\": 74, \"is_central\": \"no\"}, {\"x\": -0.0897053430490711, \"y\": 0.013101398964902963, \"id\": 75, \"is_central\": \"no\"}, {\"x\": -0.07770937983352315, \"y\": 0.019481654531596497, \"id\": 76, \"is_central\": \"no\"}, {\"x\": -0.10924841249342242, \"y\": 0.03021006080396897, \"id\": 77, \"is_central\": \"no\"}, {\"x\": -0.14706553695587654, \"y\": 0.032673641859445796, \"id\": 78, \"is_central\": \"no\"}, {\"x\": 0.15312231188390643, \"y\": -0.19018239522654887, \"id\": 79, \"is_central\": \"no\"}, {\"x\": 0.11084323162873101, \"y\": -0.21816891907994934, \"id\": 80, \"is_central\": \"no\"}, {\"x\": 0.1614037266336705, \"y\": -0.26360779927938344, \"id\": 81, \"is_central\": \"no\"}, {\"x\": 0.17482884393265355, \"y\": -0.25567865892978, \"id\": 82, \"is_central\": \"no\"}, {\"x\": 0.16453961557794217, \"y\": -0.15235722730304724, \"id\": 83, \"is_central\": \"no\"}, {\"x\": -0.10073022402935924, \"y\": 0.24866716443059705, \"id\": 84, \"is_central\": \"no\"}, {\"x\": -0.13357750855656716, \"y\": 0.21357498512004602, \"id\": 85, \"is_central\": \"no\"}, {\"x\": -0.11628078973510426, \"y\": 0.24607240222139293, \"id\": 86, \"is_central\": \"no\"}, {\"x\": -0.0989390875103513, \"y\": 0.23457672531171775, \"id\": 87, \"is_central\": \"no\"}, {\"x\": -0.19095671213933582, \"y\": -0.07872588351902032, \"id\": 88, \"is_central\": \"no\"}, {\"x\": 0.11592769336653481, \"y\": -0.03187484266859175, \"id\": 89, \"is_central\": \"no\"}, {\"x\": 0.17256286389293873, \"y\": 0.02901924607337627, \"id\": 90, \"is_central\": \"no\"}, {\"x\": 0.11951898178666384, \"y\": 0.059923934361632215, \"id\": 91, \"is_central\": \"no\"}, {\"x\": 0.1353383703416821, \"y\": 0.05345272568729972, \"id\": 92, \"is_central\": \"no\"}, {\"x\": 0.14155100232463913, \"y\": -0.040029604132746956, \"id\": 93, \"is_central\": \"no\"}, {\"x\": 0.17999465981916565, \"y\": -0.0146837332222107, \"id\": 94, \"is_central\": \"no\"}, {\"x\": 0.1731732997528489, \"y\": -0.03710375846875645, \"id\": 95, \"is_central\": \"no\"}, {\"x\": 0.22139926809416968, \"y\": 0.027407302466213875, \"id\": 96, \"is_central\": \"no\"}, {\"x\": 0.12017909083449298, \"y\": -0.06736377733769908, \"id\": 97, \"is_central\": \"no\"}, {\"x\": -0.0439818325353795, \"y\": 0.01834824861421199, \"id\": 98, \"is_central\": \"no\"}, {\"x\": -0.17391133013629761, \"y\": -0.021100860151469267, \"id\": 99, \"is_central\": \"no\"}, {\"x\": -0.13250071870320515, \"y\": 0.03285556147004832, \"id\": 100, \"is_central\": \"no\"}, {\"x\": -0.2093653301248616, \"y\": 0.2718010379232694, \"id\": 101, \"is_central\": \"no\"}, {\"x\": -0.18597664070364336, \"y\": -0.055781757428610766, \"id\": 102, \"is_central\": \"no\"}, {\"x\": -0.16866691498542963, \"y\": -0.07602303452589629, \"id\": 103, \"is_central\": \"no\"}, {\"x\": -0.09707311149752565, \"y\": -0.11825623445327407, \"id\": 104, \"is_central\": \"no\"}, {\"x\": 0.006097783629992272, \"y\": -0.19206453166182635, \"id\": 105, \"is_central\": \"no\"}, {\"x\": 0.8898205077766663, \"y\": -0.2636672397649877, \"id\": 106, \"is_central\": \"no\"}, {\"x\": 0.9014495196824311, \"y\": -0.2381199822355443, \"id\": 107, \"is_central\": \"no\"}, {\"x\": -0.16363600156122873, \"y\": -0.10327202378351355, \"id\": 108, \"is_central\": \"no\"}, {\"x\": -0.25366058108558853, \"y\": 0.02369475297303116, \"id\": 109, \"is_central\": \"no\"}, {\"x\": -0.20541758295340326, \"y\": -0.008103905117038245, \"id\": 110, \"is_central\": \"no\"}, {\"x\": -0.3475259768303975, \"y\": 0.03526928665566693, \"id\": 111, \"is_central\": \"no\"}, {\"x\": -0.104296114169113, \"y\": 0.04931578703238146, \"id\": 112, \"is_central\": \"no\"}, {\"x\": -0.11979762923625004, \"y\": 0.06236631777805221, \"id\": 113, \"is_central\": \"no\"}, {\"x\": -0.13722982823872137, \"y\": 0.05592259600663083, \"id\": 114, \"is_central\": \"no\"}, {\"x\": -0.16932703643076819, \"y\": 0.07611572011729069, \"id\": 115, \"is_central\": \"no\"}, {\"x\": -0.18191984932862076, \"y\": 0.05231042727716676, \"id\": 116, \"is_central\": \"no\"}, {\"x\": 0.028087875198861845, \"y\": -0.18748797918508636, \"id\": 117, \"is_central\": \"no\"}, {\"x\": -0.23499493928727933, \"y\": -0.039460140276008936, \"id\": 118, \"is_central\": \"no\"}, {\"x\": 0.10097502625023547, \"y\": -0.11146261212634913, \"id\": 119, \"is_central\": \"no\"}, {\"x\": 0.04833733005421891, \"y\": -0.11013309374693225, \"id\": 120, \"is_central\": \"no\"}, {\"x\": 0.21863191993225814, \"y\": -0.03860026076366977, \"id\": 121, \"is_central\": \"no\"}, {\"x\": 0.2272261385812622, \"y\": -0.02399474177334827, \"id\": 122, \"is_central\": \"no\"}, {\"x\": 0.2151495518779385, \"y\": -0.05639080957143462, \"id\": 123, \"is_central\": \"no\"}, {\"x\": 0.33403134860418493, \"y\": 0.08407371345095738, \"id\": 124, \"is_central\": \"no\"}, {\"x\": -0.002312125022450362, \"y\": 0.030999456689846404, \"id\": 125, \"is_central\": \"no\"}, {\"x\": 0.11841296989609869, \"y\": -0.11019973799399048, \"id\": 126, \"is_central\": \"no\"}, {\"x\": -0.13461837099862692, \"y\": -0.12143208539116787, \"id\": 127, \"is_central\": \"no\"}, {\"x\": -0.01516835921482798, \"y\": -0.1870855576983899, \"id\": 128, \"is_central\": \"no\"}, {\"x\": 0.005912527447120714, \"y\": -0.22824657678104457, \"id\": 129, \"is_central\": \"no\"}, {\"x\": 0.043000950221258355, \"y\": -0.22010389322158294, \"id\": 130, \"is_central\": \"no\"}, {\"x\": 0.003287992924154501, \"y\": -0.16583314177207673, \"id\": 131, \"is_central\": \"no\"}, {\"x\": -0.15220642619621907, \"y\": -0.1164493179488152, \"id\": 132, \"is_central\": \"no\"}, {\"x\": 0.024215995664546363, \"y\": -0.23035942893596337, \"id\": 133, \"is_central\": \"no\"}, {\"x\": -0.03156428448687718, \"y\": -0.22017649037116607, \"id\": 134, \"is_central\": \"no\"}, {\"x\": 0.23039900420501516, \"y\": -0.006118738494597925, \"id\": 135, \"is_central\": \"no\"}, {\"x\": 0.08908871458494991, \"y\": -0.12525489749717797, \"id\": 136, \"is_central\": \"no\"}, {\"x\": -0.20892837142090975, \"y\": 0.0661836703634387, \"id\": 137, \"is_central\": \"no\"}, {\"x\": 0.13185385258684215, \"y\": -0.09571096506382044, \"id\": 138, \"is_central\": \"no\"}, {\"x\": 0.014907341059817546, \"y\": -0.0661372856193289, \"id\": 139, \"is_central\": \"no\"}, {\"x\": -0.14106477858092156, \"y\": -0.09895945110745505, \"id\": 140, \"is_central\": \"no\"}, {\"x\": -0.08983605313128273, \"y\": -0.06072464345394495, \"id\": 141, \"is_central\": \"no\"}, {\"x\": -0.06662095968825617, \"y\": -0.11904292944398821, \"id\": 142, \"is_central\": \"no\"}, {\"x\": -0.22780490089040933, \"y\": 0.18413310479709935, \"id\": 143, \"is_central\": \"no\"}, {\"x\": -0.24621473408099437, \"y\": 0.20826693839176622, \"id\": 144, \"is_central\": \"no\"}, {\"x\": -0.21650989209795615, \"y\": 0.19878735994399557, \"id\": 145, \"is_central\": \"no\"}, {\"x\": -0.11669434463098263, \"y\": -0.12924045697091557, \"id\": 146, \"is_central\": \"no\"}, {\"x\": 0.21685280985526084, \"y\": 0.005653557592003872, \"id\": 147, \"is_central\": \"no\"}, {\"x\": 0.09669283504956887, \"y\": 0.05777889811249485, \"id\": 148, \"is_central\": \"no\"}, {\"x\": 0.2061357354486069, \"y\": -0.1091222131228468, \"id\": 149, \"is_central\": \"no\"}, {\"x\": 0.18868856955683147, \"y\": -0.11595325870045961, \"id\": 150, \"is_central\": \"no\"}, {\"x\": 0.15383895921775662, \"y\": -0.029209566637755775, \"id\": 151, \"is_central\": \"no\"}, {\"x\": 0.1860002519038319, \"y\": -0.1343849998675848, \"id\": 152, \"is_central\": \"no\"}, {\"x\": -0.11527871234312491, \"y\": -0.10653945002619622, \"id\": 153, \"is_central\": \"no\"}]}};\n",
      "    var embedOpt4 = {\"mode\": \"vega-lite\"};\n",
      "\n",
      "    function showError(el4, error){\n",
      "        el4.innerHTML = ('<div class=\"error\" style=\"color:red;\">'\n",
      "                        + '<p>JavaScript Error: ' + error.message + '</p>'\n",
      "                        + \"<p>This usually means there's a typo in your chart specification. \"\n",
      "                        + \"See the javascript console for the full traceback.</p>\"\n",
      "                        + '</div>');\n",
      "        throw error;\n",
      "    }\n",
      "    const el4 = document.getElementById('vis4');\n",
      "    vegaEmbed(\"#vis4\", spec4, embedOpt4)\n",
      "      .catch(error => showError(el4, error));\n",
      "  })(vegaEmbed);\n",
      "\n",
      "</script>\n",
      "\n",
      "We can focus on running a simple community detection algorithm. We use the greedy modularity algorithm from NetworkX:\n",
      "\n",
      "\n",
      "```python\n",
      "c = list(greedy_modularity_communities(G))\n",
      "\n",
      "for n in G.nodes():\n",
      "    if n in list(c[0]):\n",
      "        G.nodes[n]['community'] = '0'\n",
      "    elif n in list(c[1]):\n",
      "        G.nodes[n]['community'] = '1'\n",
      "    elif n in list(c[2]):\n",
      "        G.nodes[n]['community'] = '2'\n",
      "    elif n in list(c[3]):\n",
      "        G.nodes[n]['community'] = '3'\n",
      "    elif n in list(c[4]):\n",
      "        G.nodes[n]['community'] = '4'\n",
      "    elif n in list(c[5]):\n",
      "        G.nodes[n]['community'] = '5'\n",
      "    elif n in list(c[6]):\n",
      "        G.nodes[n]['community'] = '6'\n",
      "    elif n in list(c[7]):\n",
      "        G.nodes[n]['community'] = '7'\n",
      "    elif n in list(c[8]):\n",
      "        G.nodes[n]['community'] = '8'\n",
      "\n",
      "chart = nxa.draw_networkx(\n",
      "    G=G,\n",
      "    pos=nx.spring_layout(G),\n",
      "    width='weight:N',\n",
      "    node_tooltip=['id','community']\n",
      ")\n",
      "\n",
      "edges = chart.layer[0].properties(height=500, width=500)\n",
      "nodes = chart.layer[1].properties(height=500, width=500)\n",
      "\n",
      "nodes = nodes.encode(\n",
      "    fill='community:N',\n",
      "    color='community:N'\n",
      ").properties(\n",
      "    height=500, width=500, title=\"Community\"\n",
      ")\n",
      "\n",
      "community = (edges+nodes).interactive()\n",
      "\n",
      "community\n",
      "```\n",
      "\n",
      "<div id=\"vis5\"></div>\n",
      "<script>\n",
      "  (function(vegaEmbed) {\n",
      "    var spec5 = {\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"data\": {\"name\": \"data-9ffa4fdbb4e746d8598e9c761f47d829\"}, \"mark\": {\"type\": \"line\", \"color\": \"black\", \"opacity\": 1}, \"encoding\": {\"detail\": {\"type\": \"quantitative\", \"field\": \"edge\"}, \"size\": {\"type\": \"nominal\", \"field\": \"weight\", \"legend\": null}, \"x\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"\"}, \"field\": \"x\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"\"}, \"field\": \"y\"}}, \"height\": 500, \"selection\": {\"selector024\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 500}, {\"data\": {\"name\": \"data-34951df27a40796f893d648887092be9\"}, \"mark\": {\"type\": \"point\", \"fill\": \"red\", \"opacity\": 1, \"size\": 300}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"community\"}, \"fill\": {\"type\": \"nominal\", \"field\": \"community\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"id\"}, {\"type\": \"nominal\", \"field\": \"community\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"x\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"y\"}}, \"height\": 500, \"title\": \"Community\", \"width\": 500}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-9ffa4fdbb4e746d8598e9c761f47d829\": [{\"x\": -0.802093013012138, \"source\": 0, \"method\": \"meetings\", \"target\": 1, \"weight\": 1, \"y\": -0.5824842235196528, \"pair\": [0, 1], \"edge\": 0}, {\"x\": -0.7645186779044639, \"source\": 0, \"method\": \"meetings\", \"target\": 1, \"weight\": 1, \"y\": -0.5731788170630375, \"pair\": [0, 1], \"edge\": 0}, {\"x\": -0.802093013012138, \"source\": 0, \"method\": \"meetings\", \"target\": 2, \"weight\": 1, \"y\": -0.5824842235196528, \"pair\": [0, 2], \"edge\": 1}, {\"x\": -0.7648805140133405, \"source\": 0, \"method\": \"meetings\", \"target\": 2, \"weight\": 1, \"y\": -0.5468246324562787, \"pair\": [0, 2], \"edge\": 1}, {\"x\": -0.7645186779044639, \"source\": 1, \"method\": \"meetings\", \"target\": 2, \"weight\": 1, \"y\": -0.5731788170630375, \"pair\": [1, 2], \"edge\": 2}, {\"x\": -0.7648805140133405, \"source\": 1, \"method\": \"meetings\", \"target\": 2, \"weight\": 1, \"y\": -0.5468246324562787, \"pair\": [1, 2], \"edge\": 2}, {\"x\": 0.23860360308586037, \"source\": 3, \"method\": \"meetings\", \"target\": 4, \"weight\": 1, \"y\": 0.08907969795281509, \"pair\": [3, 4], \"edge\": 3}, {\"x\": 0.28231284393317024, \"source\": 3, \"method\": \"meetings\", \"target\": 4, \"weight\": 1, \"y\": 0.06144180346211253, \"pair\": [3, 4], \"edge\": 3}, {\"x\": 0.23860360308586037, \"source\": 3, \"method\": \"meetings\", \"target\": 5, \"weight\": 1, \"y\": 0.08907969795281509, \"pair\": [3, 5], \"edge\": 4}, {\"x\": 0.20924144653777046, \"source\": 3, \"method\": \"meetings\", \"target\": 5, \"weight\": 1, \"y\": 0.07358121775706167, \"pair\": [3, 5], \"edge\": 4}, {\"x\": 0.23860360308586037, \"source\": 3, \"method\": \"meetings\", \"target\": 6, \"weight\": 1, \"y\": 0.08907969795281509, \"pair\": [3, 6], \"edge\": 5}, {\"x\": 0.21058049738425882, \"source\": 3, \"method\": \"meetings\", \"target\": 6, \"weight\": 1, \"y\": 0.06513414641735353, \"pair\": [3, 6], \"edge\": 5}, {\"x\": 0.23860360308586037, \"source\": 3, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": 0.08907969795281509, \"pair\": [3, 7], \"edge\": 6}, {\"x\": 0.2735223829326983, \"source\": 3, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": 0.07164451799613736, \"pair\": [3, 7], \"edge\": 6}, {\"x\": 0.23860360308586037, \"source\": 3, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": 0.08907969795281509, \"pair\": [3, 8], \"edge\": 7}, {\"x\": 0.2858410918363624, \"source\": 3, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": 0.08009334352883232, \"pair\": [3, 8], \"edge\": 7}, {\"x\": 0.23860360308586037, \"source\": 3, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.08907969795281509, \"pair\": [3, 9], \"edge\": 8}, {\"x\": 0.27856298881071917, \"source\": 3, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.09154594050975798, \"pair\": [3, 9], \"edge\": 8}, {\"x\": 0.23860360308586037, \"source\": 3, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": 0.08907969795281509, \"pair\": [3, 11], \"edge\": 9}, {\"x\": 0.1602994927743264, \"source\": 3, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": 0.09918589460426964, \"pair\": [3, 11], \"edge\": 9}, {\"x\": 0.23860360308586037, \"source\": 3, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": 0.08907969795281509, \"pair\": [3, 12], \"edge\": 10}, {\"x\": 0.14340048891091195, \"source\": 3, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": 0.10340329927972684, \"pair\": [3, 12], \"edge\": 10}, {\"x\": 0.28231284393317024, \"source\": 4, \"method\": \"meetings\", \"target\": 5, \"weight\": 1, \"y\": 0.06144180346211253, \"pair\": [4, 5], \"edge\": 11}, {\"x\": 0.20924144653777046, \"source\": 4, \"method\": \"meetings\", \"target\": 5, \"weight\": 1, \"y\": 0.07358121775706167, \"pair\": [4, 5], \"edge\": 11}, {\"x\": 0.28231284393317024, \"source\": 4, \"method\": \"meetings\", \"target\": 6, \"weight\": 1, \"y\": 0.06144180346211253, \"pair\": [4, 6], \"edge\": 12}, {\"x\": 0.21058049738425882, \"source\": 4, \"method\": \"meetings\", \"target\": 6, \"weight\": 1, \"y\": 0.06513414641735353, \"pair\": [4, 6], \"edge\": 12}, {\"x\": 0.28231284393317024, \"source\": 4, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": 0.06144180346211253, \"pair\": [4, 7], \"edge\": 13}, {\"x\": 0.2735223829326983, \"source\": 4, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": 0.07164451799613736, \"pair\": [4, 7], \"edge\": 13}, {\"x\": 0.28231284393317024, \"source\": 4, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": 0.06144180346211253, \"pair\": [4, 8], \"edge\": 14}, {\"x\": 0.2858410918363624, \"source\": 4, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": 0.08009334352883232, \"pair\": [4, 8], \"edge\": 14}, {\"x\": 0.28231284393317024, \"source\": 4, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.06144180346211253, \"pair\": [4, 9], \"edge\": 15}, {\"x\": 0.27856298881071917, \"source\": 4, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.09154594050975798, \"pair\": [4, 9], \"edge\": 15}, {\"x\": 0.20924144653777046, \"source\": 5, \"method\": \"meetings\", \"target\": 6, \"weight\": 1, \"y\": 0.07358121775706167, \"pair\": [5, 6], \"edge\": 16}, {\"x\": 0.21058049738425882, \"source\": 5, \"method\": \"meetings\", \"target\": 6, \"weight\": 1, \"y\": 0.06513414641735353, \"pair\": [5, 6], \"edge\": 16}, {\"x\": 0.20924144653777046, \"source\": 5, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": 0.07358121775706167, \"pair\": [5, 7], \"edge\": 17}, {\"x\": 0.2735223829326983, \"source\": 5, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": 0.07164451799613736, \"pair\": [5, 7], \"edge\": 17}, {\"x\": 0.20924144653777046, \"source\": 5, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": 0.07358121775706167, \"pair\": [5, 8], \"edge\": 18}, {\"x\": 0.2858410918363624, \"source\": 5, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": 0.08009334352883232, \"pair\": [5, 8], \"edge\": 18}, {\"x\": 0.20924144653777046, \"source\": 5, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.07358121775706167, \"pair\": [5, 9], \"edge\": 19}, {\"x\": 0.27856298881071917, \"source\": 5, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.09154594050975798, \"pair\": [5, 9], \"edge\": 19}, {\"x\": 0.20924144653777046, \"source\": 5, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": 0.07358121775706167, \"pair\": [5, 11], \"edge\": 20}, {\"x\": 0.1602994927743264, \"source\": 5, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": 0.09918589460426964, \"pair\": [5, 11], \"edge\": 20}, {\"x\": 0.20924144653777046, \"source\": 5, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": 0.07358121775706167, \"pair\": [5, 12], \"edge\": 21}, {\"x\": 0.14340048891091195, \"source\": 5, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": 0.10340329927972684, \"pair\": [5, 12], \"edge\": 21}, {\"x\": 0.20924144653777046, \"source\": 5, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": 0.07358121775706167, \"pair\": [5, 28], \"edge\": 22}, {\"x\": 0.16785441410916155, \"source\": 5, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": 0.060067062225702864, \"pair\": [5, 28], \"edge\": 22}, {\"x\": 0.20924144653777046, \"source\": 5, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": 0.07358121775706167, \"pair\": [5, 25], \"edge\": 23}, {\"x\": 0.07314904598050838, \"source\": 5, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": 0.018843802477964286, \"pair\": [5, 25], \"edge\": 23}, {\"x\": 0.21058049738425882, \"source\": 6, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": 0.06513414641735353, \"pair\": [6, 7], \"edge\": 24}, {\"x\": 0.2735223829326983, \"source\": 6, \"method\": \"meetings\", \"target\": 7, \"weight\": 1, \"y\": 0.07164451799613736, \"pair\": [6, 7], \"edge\": 24}, {\"x\": 0.21058049738425882, \"source\": 6, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": 0.06513414641735353, \"pair\": [6, 8], \"edge\": 25}, {\"x\": 0.2858410918363624, \"source\": 6, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": 0.08009334352883232, \"pair\": [6, 8], \"edge\": 25}, {\"x\": 0.21058049738425882, \"source\": 6, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.06513414641735353, \"pair\": [6, 9], \"edge\": 26}, {\"x\": 0.27856298881071917, \"source\": 6, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.09154594050975798, \"pair\": [6, 9], \"edge\": 26}, {\"x\": 0.21058049738425882, \"source\": 6, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": 0.06513414641735353, \"pair\": [6, 11], \"edge\": 27}, {\"x\": 0.1602994927743264, \"source\": 6, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": 0.09918589460426964, \"pair\": [6, 11], \"edge\": 27}, {\"x\": 0.21058049738425882, \"source\": 6, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": 0.06513414641735353, \"pair\": [6, 12], \"edge\": 28}, {\"x\": 0.14340048891091195, \"source\": 6, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": 0.10340329927972684, \"pair\": [6, 12], \"edge\": 28}, {\"x\": 0.21058049738425882, \"source\": 6, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": 0.06513414641735353, \"pair\": [6, 28], \"edge\": 29}, {\"x\": 0.16785441410916155, \"source\": 6, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": 0.060067062225702864, \"pair\": [6, 28], \"edge\": 29}, {\"x\": 0.21058049738425882, \"source\": 6, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": 0.06513414641735353, \"pair\": [6, 25], \"edge\": 30}, {\"x\": 0.07314904598050838, \"source\": 6, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": 0.018843802477964286, \"pair\": [6, 25], \"edge\": 30}, {\"x\": 0.2735223829326983, \"source\": 7, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": 0.07164451799613736, \"pair\": [7, 8], \"edge\": 31}, {\"x\": 0.2858410918363624, \"source\": 7, \"method\": \"meetings\", \"target\": 8, \"weight\": 1, \"y\": 0.08009334352883232, \"pair\": [7, 8], \"edge\": 31}, {\"x\": 0.2735223829326983, \"source\": 7, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.07164451799613736, \"pair\": [7, 9], \"edge\": 32}, {\"x\": 0.27856298881071917, \"source\": 7, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.09154594050975798, \"pair\": [7, 9], \"edge\": 32}, {\"x\": 0.2858410918363624, \"source\": 8, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.08009334352883232, \"pair\": [8, 9], \"edge\": 33}, {\"x\": 0.27856298881071917, \"source\": 8, \"method\": \"meetings\", \"target\": 9, \"weight\": 1, \"y\": 0.09154594050975798, \"pair\": [8, 9], \"edge\": 33}, {\"x\": 0.20595742614062706, \"source\": 10, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": 0.14588579669116095, \"pair\": [10, 11], \"edge\": 34}, {\"x\": 0.1602994927743264, \"source\": 10, \"method\": \"meetings\", \"target\": 11, \"weight\": 1, \"y\": 0.09918589460426964, \"pair\": [10, 11], \"edge\": 34}, {\"x\": 0.20595742614062706, \"source\": 10, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": 0.14588579669116095, \"pair\": [10, 12], \"edge\": 35}, {\"x\": 0.14340048891091195, \"source\": 10, \"method\": \"meetings\", \"target\": 12, \"weight\": 1, \"y\": 0.10340329927972684, \"pair\": [10, 12], \"edge\": 35}, {\"x\": 0.20595742614062706, \"source\": 10, \"method\": \"meetings\", \"target\": 13, \"weight\": 1, \"y\": 0.14588579669116095, \"pair\": [10, 13], \"edge\": 36}, {\"x\": 0.18663132449362335, \"source\": 10, \"method\": \"meetings\", \"target\": 13, \"weight\": 1, \"y\": 0.13433733897838734, \"pair\": [10, 13], \"edge\": 36}, {\"x\": 0.20595742614062706, \"source\": 10, \"method\": \"meetings\", \"target\": 14, \"weight\": 1, \"y\": 0.14588579669116095, \"pair\": [10, 14], \"edge\": 37}, {\"x\": 0.18586204868298753, \"source\": 10, \"method\": \"meetings\", \"target\": 14, \"weight\": 1, \"y\": 0.16749211517500212, \"pair\": [10, 14], \"edge\": 37}, {\"x\": 0.20595742614062706, \"source\": 10, \"method\": \"meetings\", \"target\": 15, \"weight\": 1, \"y\": 0.14588579669116095, \"pair\": [10, 15], \"edge\": 38}, {\"x\": 0.17573924264549753, \"source\": 10, \"method\": \"meetings\", \"target\": 15, \"weight\": 1, \"y\": 0.14319322131666967, \"pair\": [10, 15], \"edge\": 38}, {\"x\": 0.1602994927743264, \"source\": 11, \"method\": \"phonecalls\", \"target\": 12, \"weight\": 1, \"y\": 0.09918589460426964, \"pair\": [11, 12], \"edge\": 39}, {\"x\": 0.14340048891091195, \"source\": 11, \"method\": \"phonecalls\", \"target\": 12, \"weight\": 1, \"y\": 0.10340329927972684, \"pair\": [11, 12], \"edge\": 39}, {\"x\": 0.1602994927743264, \"source\": 11, \"method\": \"meetings\", \"target\": 13, \"weight\": 2, \"y\": 0.09918589460426964, \"pair\": [11, 13], \"edge\": 40}, {\"x\": 0.18663132449362335, \"source\": 11, \"method\": \"meetings\", \"target\": 13, \"weight\": 2, \"y\": 0.13433733897838734, \"pair\": [11, 13], \"edge\": 40}, {\"x\": 0.1602994927743264, \"source\": 11, \"method\": \"meetings\", \"target\": 14, \"weight\": 2, \"y\": 0.09918589460426964, \"pair\": [11, 14], \"edge\": 41}, {\"x\": 0.18586204868298753, \"source\": 11, \"method\": \"meetings\", \"target\": 14, \"weight\": 2, \"y\": 0.16749211517500212, \"pair\": [11, 14], \"edge\": 41}, {\"x\": 0.1602994927743264, \"source\": 11, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": 0.09918589460426964, \"pair\": [11, 15], \"edge\": 42}, {\"x\": 0.17573924264549753, \"source\": 11, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": 0.14319322131666967, \"pair\": [11, 15], \"edge\": 42}, {\"x\": 0.1602994927743264, \"source\": 11, \"method\": \"meetings\", \"target\": 18, \"weight\": 1, \"y\": 0.09918589460426964, \"pair\": [11, 18], \"edge\": 43}, {\"x\": -0.05612050668302144, \"source\": 11, \"method\": \"meetings\", \"target\": 18, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [11, 18], \"edge\": 43}, {\"x\": 0.1602994927743264, \"source\": 11, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": 0.09918589460426964, \"pair\": [11, 25], \"edge\": 44}, {\"x\": 0.07314904598050838, \"source\": 11, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": 0.018843802477964286, \"pair\": [11, 25], \"edge\": 44}, {\"x\": 0.1602994927743264, \"source\": 11, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": 0.09918589460426964, \"pair\": [11, 28], \"edge\": 45}, {\"x\": 0.16785441410916155, \"source\": 11, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": 0.060067062225702864, \"pair\": [11, 28], \"edge\": 45}, {\"x\": 0.1602994927743264, \"source\": 11, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": 0.09918589460426964, \"pair\": [11, 85], \"edge\": 46}, {\"x\": 0.19056353716074514, \"source\": 11, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": 0.15642328640415717, \"pair\": [11, 85], \"edge\": 46}, {\"x\": 0.1602994927743264, \"source\": 11, \"method\": \"phonecalls\", \"target\": 145, \"weight\": 2, \"y\": 0.09918589460426964, \"pair\": [11, 145], \"edge\": 47}, {\"x\": 0.22644213244410646, \"source\": 11, \"method\": \"phonecalls\", \"target\": 145, \"weight\": 2, \"y\": 0.13441820924980916, \"pair\": [11, 145], \"edge\": 47}, {\"x\": 0.1602994927743264, \"source\": 11, \"method\": \"phonecalls\", \"target\": 144, \"weight\": 1, \"y\": 0.09918589460426964, \"pair\": [11, 144], \"edge\": 48}, {\"x\": 0.2634363623209933, \"source\": 11, \"method\": \"phonecalls\", \"target\": 144, \"weight\": 1, \"y\": 0.14861794413837914, \"pair\": [11, 144], \"edge\": 48}, {\"x\": 0.1602994927743264, \"source\": 11, \"method\": \"phonecalls\", \"target\": 143, \"weight\": 2, \"y\": 0.09918589460426964, \"pair\": [11, 143], \"edge\": 49}, {\"x\": 0.23861229243734594, \"source\": 11, \"method\": \"phonecalls\", \"target\": 143, \"weight\": 2, \"y\": 0.12480818117104757, \"pair\": [11, 143], \"edge\": 49}, {\"x\": 0.14340048891091195, \"source\": 12, \"method\": \"meetings\", \"target\": 13, \"weight\": 2, \"y\": 0.10340329927972684, \"pair\": [12, 13], \"edge\": 50}, {\"x\": 0.18663132449362335, \"source\": 12, \"method\": \"meetings\", \"target\": 13, \"weight\": 2, \"y\": 0.13433733897838734, \"pair\": [12, 13], \"edge\": 50}, {\"x\": 0.14340048891091195, \"source\": 12, \"method\": \"meetings\", \"target\": 14, \"weight\": 2, \"y\": 0.10340329927972684, \"pair\": [12, 14], \"edge\": 51}, {\"x\": 0.18586204868298753, \"source\": 12, \"method\": \"meetings\", \"target\": 14, \"weight\": 2, \"y\": 0.16749211517500212, \"pair\": [12, 14], \"edge\": 51}, {\"x\": 0.14340048891091195, \"source\": 12, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": 0.10340329927972684, \"pair\": [12, 15], \"edge\": 52}, {\"x\": 0.17573924264549753, \"source\": 12, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": 0.14319322131666967, \"pair\": [12, 15], \"edge\": 52}, {\"x\": 0.14340048891091195, \"source\": 12, \"method\": \"meetings\", \"target\": 18, \"weight\": 1, \"y\": 0.10340329927972684, \"pair\": [12, 18], \"edge\": 53}, {\"x\": -0.05612050668302144, \"source\": 12, \"method\": \"meetings\", \"target\": 18, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [12, 18], \"edge\": 53}, {\"x\": 0.14340048891091195, \"source\": 12, \"method\": \"meetings\", \"target\": 21, \"weight\": 1, \"y\": 0.10340329927972684, \"pair\": [12, 21], \"edge\": 54}, {\"x\": 0.058197338132539224, \"source\": 12, \"method\": \"meetings\", \"target\": 21, \"weight\": 1, \"y\": 0.04054154351798948, \"pair\": [12, 21], \"edge\": 54}, {\"x\": 0.14340048891091195, \"source\": 12, \"method\": \"meetings\", \"target\": 25, \"weight\": 2, \"y\": 0.10340329927972684, \"pair\": [12, 25], \"edge\": 55}, {\"x\": 0.07314904598050838, \"source\": 12, \"method\": \"meetings\", \"target\": 25, \"weight\": 2, \"y\": 0.018843802477964286, \"pair\": [12, 25], \"edge\": 55}, {\"x\": 0.14340048891091195, \"source\": 12, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": 0.10340329927972684, \"pair\": [12, 28], \"edge\": 56}, {\"x\": 0.16785441410916155, \"source\": 12, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": 0.060067062225702864, \"pair\": [12, 28], \"edge\": 56}, {\"x\": 0.14340048891091195, \"source\": 12, \"method\": \"meetings\", \"target\": 84, \"weight\": 1, \"y\": 0.10340329927972684, \"pair\": [12, 84], \"edge\": 57}, {\"x\": 0.21369006215689948, \"source\": 12, \"method\": \"meetings\", \"target\": 84, \"weight\": 1, \"y\": 0.1902963496879098, \"pair\": [12, 84], \"edge\": 57}, {\"x\": 0.14340048891091195, \"source\": 12, \"method\": \"meetings\", \"target\": 85, \"weight\": 2, \"y\": 0.10340329927972684, \"pair\": [12, 85], \"edge\": 58}, {\"x\": 0.19056353716074514, \"source\": 12, \"method\": \"meetings\", \"target\": 85, \"weight\": 2, \"y\": 0.15642328640415717, \"pair\": [12, 85], \"edge\": 58}, {\"x\": 0.14340048891091195, \"source\": 12, \"method\": \"meetings\", \"target\": 86, \"weight\": 1, \"y\": 0.10340329927972684, \"pair\": [12, 86], \"edge\": 59}, {\"x\": 0.1983824078622018, \"source\": 12, \"method\": \"meetings\", \"target\": 86, \"weight\": 1, \"y\": 0.19598317965489814, \"pair\": [12, 86], \"edge\": 59}, {\"x\": 0.14340048891091195, \"source\": 12, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": 0.10340329927972684, \"pair\": [12, 87], \"edge\": 60}, {\"x\": 0.22441207019825102, \"source\": 12, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": 0.17847843085648182, \"pair\": [12, 87], \"edge\": 60}, {\"x\": 0.18663132449362335, \"source\": 13, \"method\": \"meetings\", \"target\": 14, \"weight\": 2, \"y\": 0.13433733897838734, \"pair\": [13, 14], \"edge\": 61}, {\"x\": 0.18586204868298753, \"source\": 13, \"method\": \"meetings\", \"target\": 14, \"weight\": 2, \"y\": 0.16749211517500212, \"pair\": [13, 14], \"edge\": 61}, {\"x\": 0.18663132449362335, \"source\": 13, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": 0.13433733897838734, \"pair\": [13, 15], \"edge\": 62}, {\"x\": 0.17573924264549753, \"source\": 13, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": 0.14319322131666967, \"pair\": [13, 15], \"edge\": 62}, {\"x\": 0.18663132449362335, \"source\": 13, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": 0.13433733897838734, \"pair\": [13, 85], \"edge\": 63}, {\"x\": 0.19056353716074514, \"source\": 13, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": 0.15642328640415717, \"pair\": [13, 85], \"edge\": 63}, {\"x\": 0.18586204868298753, \"source\": 14, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": 0.16749211517500212, \"pair\": [14, 15], \"edge\": 64}, {\"x\": 0.17573924264549753, \"source\": 14, \"method\": \"meetings\", \"target\": 15, \"weight\": 2, \"y\": 0.14319322131666967, \"pair\": [14, 15], \"edge\": 64}, {\"x\": 0.18586204868298753, \"source\": 14, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": 0.16749211517500212, \"pair\": [14, 85], \"edge\": 65}, {\"x\": 0.19056353716074514, \"source\": 14, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": 0.15642328640415717, \"pair\": [14, 85], \"edge\": 65}, {\"x\": 0.18586204868298753, \"source\": 14, \"method\": \"phonecalls\", \"target\": 101, \"weight\": 2, \"y\": 0.16749211517500212, \"pair\": [14, 101], \"edge\": 66}, {\"x\": 0.22787150157572195, \"source\": 14, \"method\": \"phonecalls\", \"target\": 101, \"weight\": 2, \"y\": 0.23794535711803252, \"pair\": [14, 101], \"edge\": 66}, {\"x\": 0.17573924264549753, \"source\": 15, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": 0.14319322131666967, \"pair\": [15, 85], \"edge\": 67}, {\"x\": 0.19056353716074514, \"source\": 15, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": 0.15642328640415717, \"pair\": [15, 85], \"edge\": 67}, {\"x\": -0.07761935479169774, \"source\": 16, \"method\": \"meetings\", \"target\": 17, \"weight\": 1, \"y\": -0.94000716765174, \"pair\": [16, 17], \"edge\": 68}, {\"x\": -0.07401406503186662, \"source\": 16, \"method\": \"meetings\", \"target\": 17, \"weight\": 1, \"y\": -0.8965666719477022, \"pair\": [16, 17], \"edge\": 68}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 19, \"weight\": 6, \"y\": -0.01768164671407569, \"pair\": [18, 19], \"edge\": 69}, {\"x\": -0.0386660772673068, \"source\": 18, \"method\": \"phonecalls\", \"target\": 19, \"weight\": 6, \"y\": -0.05897078740977869, \"pair\": [18, 19], \"edge\": 69}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"meetings\", \"target\": 20, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 20], \"edge\": 70}, {\"x\": -0.06321557290680196, \"source\": 18, \"method\": \"meetings\", \"target\": 20, \"weight\": 1, \"y\": -0.10507750890540686, \"pair\": [18, 20], \"edge\": 70}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 21, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 21], \"edge\": 71}, {\"x\": 0.058197338132539224, \"source\": 18, \"method\": \"phonecalls\", \"target\": 21, \"weight\": 1, \"y\": 0.04054154351798948, \"pair\": [18, 21], \"edge\": 71}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"meetings\", \"target\": 22, \"weight\": 2, \"y\": -0.01768164671407569, \"pair\": [18, 22], \"edge\": 72}, {\"x\": -0.10435783249725597, \"source\": 18, \"method\": \"meetings\", \"target\": 22, \"weight\": 2, \"y\": -0.04194020355486868, \"pair\": [18, 22], \"edge\": 72}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"meetings\", \"target\": 31, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 31], \"edge\": 73}, {\"x\": -0.10927812066240415, \"source\": 18, \"method\": \"meetings\", \"target\": 31, \"weight\": 1, \"y\": -0.08734890286142297, \"pair\": [18, 31], \"edge\": 73}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 34, \"weight\": 3, \"y\": -0.01768164671407569, \"pair\": [18, 34], \"edge\": 74}, {\"x\": 0.0032855872505247415, \"source\": 18, \"method\": \"phonecalls\", \"target\": 34, \"weight\": 3, \"y\": -0.021647749997618267, \"pair\": [18, 34], \"edge\": 74}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 35, \"weight\": 5, \"y\": -0.01768164671407569, \"pair\": [18, 35], \"edge\": 75}, {\"x\": -0.01673381052039805, \"source\": 18, \"method\": \"phonecalls\", \"target\": 35, \"weight\": 5, \"y\": -0.023753091559328095, \"pair\": [18, 35], \"edge\": 75}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"meetings\", \"target\": 23, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 23], \"edge\": 76}, {\"x\": -0.008948305588105252, \"source\": 18, \"method\": \"meetings\", \"target\": 23, \"weight\": 1, \"y\": -0.05504141076884899, \"pair\": [18, 23], \"edge\": 76}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 25, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 25], \"edge\": 77}, {\"x\": 0.07314904598050838, \"source\": 18, \"method\": \"phonecalls\", \"target\": 25, \"weight\": 1, \"y\": 0.018843802477964286, \"pair\": [18, 25], \"edge\": 77}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 58, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 58], \"edge\": 78}, {\"x\": -0.18241099698058447, \"source\": 18, \"method\": \"phonecalls\", \"target\": 58, \"weight\": 1, \"y\": 0.032024336890226164, \"pair\": [18, 58], \"edge\": 78}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"meetings\", \"target\": 59, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 59], \"edge\": 79}, {\"x\": -0.1559017209514067, \"source\": 18, \"method\": \"meetings\", \"target\": 59, \"weight\": 1, \"y\": 0.021592614097687188, \"pair\": [18, 59], \"edge\": 79}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"meetings\", \"target\": 60, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 60], \"edge\": 80}, {\"x\": -0.12520490671870255, \"source\": 18, \"method\": \"meetings\", \"target\": 60, \"weight\": 1, \"y\": -0.07745856899409895, \"pair\": [18, 60], \"edge\": 80}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"meetings\", \"target\": 63, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 63], \"edge\": 81}, {\"x\": -0.11572944960277982, \"source\": 18, \"method\": \"meetings\", \"target\": 63, \"weight\": 1, \"y\": -0.023077593783232188, \"pair\": [18, 63], \"edge\": 81}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"meetings\", \"target\": 64, \"weight\": 2, \"y\": -0.01768164671407569, \"pair\": [18, 64], \"edge\": 82}, {\"x\": -0.08203963210578026, \"source\": 18, \"method\": \"meetings\", \"target\": 64, \"weight\": 2, \"y\": -0.006091601590310482, \"pair\": [18, 64], \"edge\": 82}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 29, \"weight\": 7, \"y\": -0.01768164671407569, \"pair\": [18, 29], \"edge\": 83}, {\"x\": -0.06656634161106342, \"source\": 18, \"method\": \"phonecalls\", \"target\": 29, \"weight\": 7, \"y\": 0.02072802436799425, \"pair\": [18, 29], \"edge\": 83}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"meetings\", \"target\": 65, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 65], \"edge\": 84}, {\"x\": -0.1702165144146252, \"source\": 18, \"method\": \"meetings\", \"target\": 65, \"weight\": 1, \"y\": -0.001404905897743331, \"pair\": [18, 65], \"edge\": 84}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"meetings\", \"target\": 43, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 43], \"edge\": 85}, {\"x\": -0.03881531608244006, \"source\": 18, \"method\": \"meetings\", \"target\": 43, \"weight\": 1, \"y\": 0.04540671878796461, \"pair\": [18, 43], \"edge\": 85}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"meetings\", \"target\": 27, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 27], \"edge\": 86}, {\"x\": -0.012391752374116128, \"source\": 18, \"method\": \"meetings\", \"target\": 27, \"weight\": 1, \"y\": 0.05130248030961548, \"pair\": [18, 27], \"edge\": 86}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 47], \"edge\": 87}, {\"x\": 0.007108527109453355, \"source\": 18, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": 0.11003438822517303, \"pair\": [18, 47], \"edge\": 87}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 71, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 71], \"edge\": 88}, {\"x\": -0.10834854337472752, \"source\": 18, \"method\": \"phonecalls\", \"target\": 71, \"weight\": 1, \"y\": 0.009031121091285323, \"pair\": [18, 71], \"edge\": 88}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"meetings\", \"target\": 76, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 76], \"edge\": 89}, {\"x\": -0.13440995012367013, \"source\": 18, \"method\": \"meetings\", \"target\": 76, \"weight\": 1, \"y\": -0.035573320781739935, \"pair\": [18, 76], \"edge\": 89}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 100], \"edge\": 90}, {\"x\": 0.023332033262655517, \"source\": 18, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": -0.040032722379539636, \"pair\": [18, 100], \"edge\": 90}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 102, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 102], \"edge\": 91}, {\"x\": -0.09926376548922529, \"source\": 18, \"method\": \"phonecalls\", \"target\": 102, \"weight\": 1, \"y\": -0.12170590325524416, \"pair\": [18, 102], \"edge\": 91}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 32, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 32], \"edge\": 92}, {\"x\": -0.07733357836765282, \"source\": 18, \"method\": \"phonecalls\", \"target\": 32, \"weight\": 1, \"y\": -0.09150374006610085, \"pair\": [18, 32], \"edge\": 92}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 103, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 103], \"edge\": 93}, {\"x\": -0.17606769760215724, \"source\": 18, \"method\": \"phonecalls\", \"target\": 103, \"weight\": 1, \"y\": 0.015576909333247556, \"pair\": [18, 103], \"edge\": 93}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 104, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 104], \"edge\": 94}, {\"x\": -0.17409306862461033, \"source\": 18, \"method\": \"phonecalls\", \"target\": 104, \"weight\": 1, \"y\": -0.031377524456192796, \"pair\": [18, 104], \"edge\": 94}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 61, \"weight\": 2, \"y\": -0.01768164671407569, \"pair\": [18, 61], \"edge\": 95}, {\"x\": 0.0048703552552857585, \"source\": 18, \"method\": \"phonecalls\", \"target\": 61, \"weight\": 2, \"y\": -0.11040957072110401, \"pair\": [18, 61], \"edge\": 95}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 33, \"weight\": 5, \"y\": -0.01768164671407569, \"pair\": [18, 33], \"edge\": 96}, {\"x\": -0.06601643841065934, \"source\": 18, \"method\": \"phonecalls\", \"target\": 33, \"weight\": 5, \"y\": -0.0655494401056117, \"pair\": [18, 33], \"edge\": 96}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 108, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 108], \"edge\": 97}, {\"x\": -0.14847023407178417, \"source\": 18, \"method\": \"phonecalls\", \"target\": 108, \"weight\": 1, \"y\": 0.04412350134247153, \"pair\": [18, 108], \"edge\": 97}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 127, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 127], \"edge\": 98}, {\"x\": -0.1536562263982753, \"source\": 18, \"method\": \"phonecalls\", \"target\": 127, \"weight\": 1, \"y\": -0.08360286417691555, \"pair\": [18, 127], \"edge\": 98}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 66, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 66], \"edge\": 99}, {\"x\": -0.022640902960118864, \"source\": 18, \"method\": \"phonecalls\", \"target\": 66, \"weight\": 1, \"y\": -0.11038841407886545, \"pair\": [18, 66], \"edge\": 99}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 132, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 132], \"edge\": 100}, {\"x\": -0.133672498611843, \"source\": 18, \"method\": \"phonecalls\", \"target\": 132, \"weight\": 1, \"y\": -0.11022599588864794, \"pair\": [18, 132], \"edge\": 100}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 125, \"weight\": 2, \"y\": -0.01768164671407569, \"pair\": [18, 125], \"edge\": 101}, {\"x\": 0.012050096233286714, \"source\": 18, \"method\": \"phonecalls\", \"target\": 125, \"weight\": 2, \"y\": 0.012727612759410905, \"pair\": [18, 125], \"edge\": 101}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 140, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 140], \"edge\": 102}, {\"x\": -0.17836078870967892, \"source\": 18, \"method\": \"phonecalls\", \"target\": 140, \"weight\": 1, \"y\": -0.049201881466319465, \"pair\": [18, 140], \"edge\": 102}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 141, \"weight\": 4, \"y\": -0.01768164671407569, \"pair\": [18, 141], \"edge\": 103}, {\"x\": -0.08111005054221812, \"source\": 18, \"method\": \"phonecalls\", \"target\": 141, \"weight\": 4, \"y\": -0.03890183331178787, \"pair\": [18, 141], \"edge\": 103}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 146, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 146], \"edge\": 104}, {\"x\": -0.11752748960995511, \"source\": 18, \"method\": \"phonecalls\", \"target\": 146, \"weight\": 1, \"y\": -0.12158857212955436, \"pair\": [18, 146], \"edge\": 104}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 153, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 153], \"edge\": 105}, {\"x\": -0.1872731340151251, \"source\": 18, \"method\": \"phonecalls\", \"target\": 153, \"weight\": 1, \"y\": -0.010370550444382997, \"pair\": [18, 153], \"edge\": 105}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 99, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 99], \"edge\": 106}, {\"x\": -0.03800130853345794, \"source\": 18, \"method\": \"phonecalls\", \"target\": 99, \"weight\": 1, \"y\": -0.09253436619166476, \"pair\": [18, 99], \"edge\": 106}, {\"x\": -0.05612050668302144, \"source\": 18, \"method\": \"phonecalls\", \"target\": 110, \"weight\": 1, \"y\": -0.01768164671407569, \"pair\": [18, 110], \"edge\": 107}, {\"x\": -0.16554649083433273, \"source\": 18, \"method\": \"phonecalls\", \"target\": 110, \"weight\": 1, \"y\": 0.0613612565078103, \"pair\": [18, 110], \"edge\": 107}, {\"x\": -0.0386660772673068, \"source\": 19, \"method\": \"meetings\", \"target\": 20, \"weight\": 1, \"y\": -0.05897078740977869, \"pair\": [19, 20], \"edge\": 108}, {\"x\": -0.06321557290680196, \"source\": 19, \"method\": \"meetings\", \"target\": 20, \"weight\": 1, \"y\": -0.10507750890540686, \"pair\": [19, 20], \"edge\": 108}, {\"x\": -0.0386660772673068, \"source\": 19, \"method\": \"meetings\", \"target\": 32, \"weight\": 1, \"y\": -0.05897078740977869, \"pair\": [19, 32], \"edge\": 109}, {\"x\": -0.07733357836765282, \"source\": 19, \"method\": \"meetings\", \"target\": 32, \"weight\": 1, \"y\": -0.09150374006610085, \"pair\": [19, 32], \"edge\": 109}, {\"x\": -0.0386660772673068, \"source\": 19, \"method\": \"meetings\", \"target\": 33, \"weight\": 2, \"y\": -0.05897078740977869, \"pair\": [19, 33], \"edge\": 110}, {\"x\": -0.06601643841065934, \"source\": 19, \"method\": \"meetings\", \"target\": 33, \"weight\": 2, \"y\": -0.0655494401056117, \"pair\": [19, 33], \"edge\": 110}, {\"x\": -0.0386660772673068, \"source\": 19, \"method\": \"meetings\", \"target\": 34, \"weight\": 1, \"y\": -0.05897078740977869, \"pair\": [19, 34], \"edge\": 111}, {\"x\": 0.0032855872505247415, \"source\": 19, \"method\": \"meetings\", \"target\": 34, \"weight\": 1, \"y\": -0.021647749997618267, \"pair\": [19, 34], \"edge\": 111}, {\"x\": -0.0386660772673068, \"source\": 19, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": -0.05897078740977869, \"pair\": [19, 35], \"edge\": 112}, {\"x\": -0.01673381052039805, \"source\": 19, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": -0.023753091559328095, \"pair\": [19, 35], \"edge\": 112}, {\"x\": -0.0386660772673068, \"source\": 19, \"method\": \"meetings\", \"target\": 23, \"weight\": 1, \"y\": -0.05897078740977869, \"pair\": [19, 23], \"edge\": 113}, {\"x\": -0.008948305588105252, \"source\": 19, \"method\": \"meetings\", \"target\": 23, \"weight\": 1, \"y\": -0.05504141076884899, \"pair\": [19, 23], \"edge\": 113}, {\"x\": -0.0386660772673068, \"source\": 19, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": -0.05897078740977869, \"pair\": [19, 25], \"edge\": 114}, {\"x\": 0.07314904598050838, \"source\": 19, \"method\": \"meetings\", \"target\": 25, \"weight\": 1, \"y\": 0.018843802477964286, \"pair\": [19, 25], \"edge\": 114}, {\"x\": -0.0386660772673068, \"source\": 19, \"method\": \"meetings\", \"target\": 88, \"weight\": 1, \"y\": -0.05897078740977869, \"pair\": [19, 88], \"edge\": 115}, {\"x\": -0.07809806699308035, \"source\": 19, \"method\": \"meetings\", \"target\": 88, \"weight\": 1, \"y\": -0.14026940956002884, \"pair\": [19, 88], \"edge\": 115}, {\"x\": -0.0386660772673068, \"source\": 19, \"method\": \"phonecalls\", \"target\": 22, \"weight\": 1, \"y\": -0.05897078740977869, \"pair\": [19, 22], \"edge\": 116}, {\"x\": -0.10435783249725597, \"source\": 19, \"method\": \"phonecalls\", \"target\": 22, \"weight\": 1, \"y\": -0.04194020355486868, \"pair\": [19, 22], \"edge\": 116}, {\"x\": -0.0386660772673068, \"source\": 19, \"method\": \"phonecalls\", \"target\": 142, \"weight\": 1, \"y\": -0.05897078740977869, \"pair\": [19, 142], \"edge\": 117}, {\"x\": -0.04413909119047113, \"source\": 19, \"method\": \"phonecalls\", \"target\": 142, \"weight\": 1, \"y\": -0.1607756139326318, \"pair\": [19, 142], \"edge\": 117}, {\"x\": -0.10435783249725597, \"source\": 22, \"method\": \"meetings\", \"target\": 23, \"weight\": 2, \"y\": -0.04194020355486868, \"pair\": [22, 23], \"edge\": 118}, {\"x\": -0.008948305588105252, \"source\": 22, \"method\": \"meetings\", \"target\": 23, \"weight\": 2, \"y\": -0.05504141076884899, \"pair\": [22, 23], \"edge\": 118}, {\"x\": -0.10435783249725597, \"source\": 22, \"method\": \"meetings\", \"target\": 29, \"weight\": 2, \"y\": -0.04194020355486868, \"pair\": [22, 29], \"edge\": 119}, {\"x\": -0.06656634161106342, \"source\": 22, \"method\": \"meetings\", \"target\": 29, \"weight\": 2, \"y\": 0.02072802436799425, \"pair\": [22, 29], \"edge\": 119}, {\"x\": -0.10435783249725597, \"source\": 22, \"method\": \"meetings\", \"target\": 30, \"weight\": 1, \"y\": -0.04194020355486868, \"pair\": [22, 30], \"edge\": 120}, {\"x\": -0.13184619813604853, \"source\": 22, \"method\": \"meetings\", \"target\": 30, \"weight\": 1, \"y\": 0.0239530046935114, \"pair\": [22, 30], \"edge\": 120}, {\"x\": -0.10435783249725597, \"source\": 22, \"method\": \"phonecalls\", \"target\": 31, \"weight\": 1, \"y\": -0.04194020355486868, \"pair\": [22, 31], \"edge\": 121}, {\"x\": -0.10927812066240415, \"source\": 22, \"method\": \"phonecalls\", \"target\": 31, \"weight\": 1, \"y\": -0.08734890286142297, \"pair\": [22, 31], \"edge\": 121}, {\"x\": -0.10435783249725597, \"source\": 22, \"method\": \"meetings\", \"target\": 60, \"weight\": 1, \"y\": -0.04194020355486868, \"pair\": [22, 60], \"edge\": 122}, {\"x\": -0.12520490671870255, \"source\": 22, \"method\": \"meetings\", \"target\": 60, \"weight\": 1, \"y\": -0.07745856899409895, \"pair\": [22, 60], \"edge\": 122}, {\"x\": -0.10435783249725597, \"source\": 22, \"method\": \"meetings\", \"target\": 63, \"weight\": 2, \"y\": -0.04194020355486868, \"pair\": [22, 63], \"edge\": 123}, {\"x\": -0.11572944960277982, \"source\": 22, \"method\": \"meetings\", \"target\": 63, \"weight\": 2, \"y\": -0.023077593783232188, \"pair\": [22, 63], \"edge\": 123}, {\"x\": -0.10435783249725597, \"source\": 22, \"method\": \"meetings\", \"target\": 72, \"weight\": 1, \"y\": -0.04194020355486868, \"pair\": [22, 72], \"edge\": 124}, {\"x\": -0.1685060948873108, \"source\": 22, \"method\": \"meetings\", \"target\": 72, \"weight\": 1, \"y\": -0.05601789641213182, \"pair\": [22, 72], \"edge\": 124}, {\"x\": -0.10435783249725597, \"source\": 22, \"method\": \"meetings\", \"target\": 76, \"weight\": 2, \"y\": -0.04194020355486868, \"pair\": [22, 76], \"edge\": 125}, {\"x\": -0.13440995012367013, \"source\": 22, \"method\": \"meetings\", \"target\": 76, \"weight\": 2, \"y\": -0.035573320781739935, \"pair\": [22, 76], \"edge\": 125}, {\"x\": -0.10435783249725597, \"source\": 22, \"method\": \"meetings\", \"target\": 77, \"weight\": 2, \"y\": -0.04194020355486868, \"pair\": [22, 77], \"edge\": 126}, {\"x\": -0.1674280211177842, \"source\": 22, \"method\": \"meetings\", \"target\": 77, \"weight\": 2, \"y\": -0.012273254905121884, \"pair\": [22, 77], \"edge\": 126}, {\"x\": -0.10435783249725597, \"source\": 22, \"method\": \"meetings\", \"target\": 75, \"weight\": 2, \"y\": -0.04194020355486868, \"pair\": [22, 75], \"edge\": 127}, {\"x\": -0.15725665594995872, \"source\": 22, \"method\": \"meetings\", \"target\": 75, \"weight\": 2, \"y\": -0.06183290734652462, \"pair\": [22, 75], \"edge\": 127}, {\"x\": -0.10435783249725597, \"source\": 22, \"method\": \"meetings\", \"target\": 78, \"weight\": 1, \"y\": -0.04194020355486868, \"pair\": [22, 78], \"edge\": 128}, {\"x\": -0.1796591076086675, \"source\": 22, \"method\": \"meetings\", \"target\": 78, \"weight\": 1, \"y\": -0.10029407843374245, \"pair\": [22, 78], \"edge\": 128}, {\"x\": -0.10435783249725597, \"source\": 22, \"method\": \"meetings\", \"target\": 98, \"weight\": 1, \"y\": -0.04194020355486868, \"pair\": [22, 98], \"edge\": 129}, {\"x\": -0.1373101562119631, \"source\": 22, \"method\": \"meetings\", \"target\": 98, \"weight\": 1, \"y\": -0.005371636152092576, \"pair\": [22, 98], \"edge\": 129}, {\"x\": -0.10435783249725597, \"source\": 22, \"method\": \"meetings\", \"target\": 99, \"weight\": 1, \"y\": -0.04194020355486868, \"pair\": [22, 99], \"edge\": 130}, {\"x\": -0.03800130853345794, \"source\": 22, \"method\": \"meetings\", \"target\": 99, \"weight\": 1, \"y\": -0.09253436619166476, \"pair\": [22, 99], \"edge\": 130}, {\"x\": -0.008948305588105252, \"source\": 23, \"method\": \"meetings\", \"target\": 24, \"weight\": 1, \"y\": -0.05504141076884899, \"pair\": [23, 24], \"edge\": 131}, {\"x\": 0.05520866651359483, \"source\": 23, \"method\": \"meetings\", \"target\": 24, \"weight\": 1, \"y\": -0.14337579861081434, \"pair\": [23, 24], \"edge\": 131}, {\"x\": -0.008948305588105252, \"source\": 23, \"method\": \"meetings\", \"target\": 34, \"weight\": 1, \"y\": -0.05504141076884899, \"pair\": [23, 34], \"edge\": 132}, {\"x\": 0.0032855872505247415, \"source\": 23, \"method\": \"meetings\", \"target\": 34, \"weight\": 1, \"y\": -0.021647749997618267, \"pair\": [23, 34], \"edge\": 132}, {\"x\": -0.008948305588105252, \"source\": 23, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": -0.05504141076884899, \"pair\": [23, 35], \"edge\": 133}, {\"x\": -0.01673381052039805, \"source\": 23, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": -0.023753091559328095, \"pair\": [23, 35], \"edge\": 133}, {\"x\": -0.008948305588105252, \"source\": 23, \"method\": \"meetings\", \"target\": 25, \"weight\": 2, \"y\": -0.05504141076884899, \"pair\": [23, 25], \"edge\": 134}, {\"x\": 0.07314904598050838, \"source\": 23, \"method\": \"meetings\", \"target\": 25, \"weight\": 2, \"y\": 0.018843802477964286, \"pair\": [23, 25], \"edge\": 134}, {\"x\": -0.008948305588105252, \"source\": 23, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": -0.05504141076884899, \"pair\": [23, 100], \"edge\": 135}, {\"x\": 0.023332033262655517, \"source\": 23, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": -0.040032722379539636, \"pair\": [23, 100], \"edge\": 135}, {\"x\": -0.008948305588105252, \"source\": 23, \"method\": \"phonecalls\", \"target\": 99, \"weight\": 5, \"y\": -0.05504141076884899, \"pair\": [23, 99], \"edge\": 136}, {\"x\": -0.03800130853345794, \"source\": 23, \"method\": \"phonecalls\", \"target\": 99, \"weight\": 5, \"y\": -0.09253436619166476, \"pair\": [23, 99], \"edge\": 136}, {\"x\": 0.07314904598050838, \"source\": 25, \"method\": \"meetings\", \"target\": 26, \"weight\": 1, \"y\": 0.018843802477964286, \"pair\": [25, 26], \"edge\": 137}, {\"x\": 0.08126935347307138, \"source\": 25, \"method\": \"meetings\", \"target\": 26, \"weight\": 1, \"y\": 0.046609146158610254, \"pair\": [25, 26], \"edge\": 137}, {\"x\": 0.07314904598050838, \"source\": 25, \"method\": \"meetings\", \"target\": 27, \"weight\": 1, \"y\": 0.018843802477964286, \"pair\": [25, 27], \"edge\": 138}, {\"x\": -0.012391752374116128, \"source\": 25, \"method\": \"meetings\", \"target\": 27, \"weight\": 1, \"y\": 0.05130248030961548, \"pair\": [25, 27], \"edge\": 138}, {\"x\": 0.07314904598050838, \"source\": 25, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": 0.018843802477964286, \"pair\": [25, 28], \"edge\": 139}, {\"x\": 0.16785441410916155, \"source\": 25, \"method\": \"meetings\", \"target\": 28, \"weight\": 1, \"y\": 0.060067062225702864, \"pair\": [25, 28], \"edge\": 139}, {\"x\": 0.07314904598050838, \"source\": 25, \"method\": \"meetings\", \"target\": 34, \"weight\": 2, \"y\": 0.018843802477964286, \"pair\": [25, 34], \"edge\": 140}, {\"x\": 0.0032855872505247415, \"source\": 25, \"method\": \"meetings\", \"target\": 34, \"weight\": 2, \"y\": -0.021647749997618267, \"pair\": [25, 34], \"edge\": 140}, {\"x\": 0.07314904598050838, \"source\": 25, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": 0.018843802477964286, \"pair\": [25, 35], \"edge\": 141}, {\"x\": -0.01673381052039805, \"source\": 25, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": -0.023753091559328095, \"pair\": [25, 35], \"edge\": 141}, {\"x\": 0.07314904598050838, \"source\": 25, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": 0.018843802477964286, \"pair\": [25, 100], \"edge\": 142}, {\"x\": 0.023332033262655517, \"source\": 25, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": -0.040032722379539636, \"pair\": [25, 100], \"edge\": 142}, {\"x\": 0.08126935347307138, \"source\": 26, \"method\": \"meetings\", \"target\": 27, \"weight\": 1, \"y\": 0.046609146158610254, \"pair\": [26, 27], \"edge\": 143}, {\"x\": -0.012391752374116128, \"source\": 26, \"method\": \"meetings\", \"target\": 27, \"weight\": 1, \"y\": 0.05130248030961548, \"pair\": [26, 27], \"edge\": 143}, {\"x\": -0.012391752374116128, \"source\": 27, \"method\": \"meetings\", \"target\": 38, \"weight\": 1, \"y\": 0.05130248030961548, \"pair\": [27, 38], \"edge\": 144}, {\"x\": -0.008018244466440517, \"source\": 27, \"method\": \"meetings\", \"target\": 38, \"weight\": 1, \"y\": 0.10072445639961984, \"pair\": [27, 38], \"edge\": 144}, {\"x\": -0.012391752374116128, \"source\": 27, \"method\": \"meetings\", \"target\": 45, \"weight\": 3, \"y\": 0.05130248030961548, \"pair\": [27, 45], \"edge\": 145}, {\"x\": 0.014345250393342512, \"source\": 27, \"method\": \"meetings\", \"target\": 45, \"weight\": 3, \"y\": 0.09666990489701383, \"pair\": [27, 45], \"edge\": 145}, {\"x\": -0.012391752374116128, \"source\": 27, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 1, \"y\": 0.05130248030961548, \"pair\": [27, 51], \"edge\": 146}, {\"x\": -0.022636151635812406, \"source\": 27, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 1, \"y\": 0.12274076369864406, \"pair\": [27, 51], \"edge\": 146}, {\"x\": -0.012391752374116128, \"source\": 27, \"method\": \"phonecalls\", \"target\": 43, \"weight\": 2, \"y\": 0.05130248030961548, \"pair\": [27, 43], \"edge\": 147}, {\"x\": -0.03881531608244006, \"source\": 27, \"method\": \"phonecalls\", \"target\": 43, \"weight\": 2, \"y\": 0.04540671878796461, \"pair\": [27, 43], \"edge\": 147}, {\"x\": -0.012391752374116128, \"source\": 27, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 3, \"y\": 0.05130248030961548, \"pair\": [27, 47], \"edge\": 148}, {\"x\": 0.007108527109453355, \"source\": 27, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 3, \"y\": 0.11003438822517303, \"pair\": [27, 47], \"edge\": 148}, {\"x\": -0.012391752374116128, \"source\": 27, \"method\": \"meetings\", \"target\": 29, \"weight\": 3, \"y\": 0.05130248030961548, \"pair\": [27, 29], \"edge\": 149}, {\"x\": -0.06656634161106342, \"source\": 27, \"method\": \"meetings\", \"target\": 29, \"weight\": 3, \"y\": 0.02072802436799425, \"pair\": [27, 29], \"edge\": 149}, {\"x\": -0.012391752374116128, \"source\": 27, \"method\": \"meetings\", \"target\": 64, \"weight\": 1, \"y\": 0.05130248030961548, \"pair\": [27, 64], \"edge\": 150}, {\"x\": -0.08203963210578026, \"source\": 27, \"method\": \"meetings\", \"target\": 64, \"weight\": 1, \"y\": -0.006091601590310482, \"pair\": [27, 64], \"edge\": 150}, {\"x\": -0.012391752374116128, \"source\": 27, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": 0.05130248030961548, \"pair\": [27, 68], \"edge\": 151}, {\"x\": 0.03471850621848811, \"source\": 27, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": 0.027941958673130613, \"pair\": [27, 68], \"edge\": 151}, {\"x\": -0.012391752374116128, \"source\": 27, \"method\": \"meetings\", \"target\": 44, \"weight\": 1, \"y\": 0.05130248030961548, \"pair\": [27, 44], \"edge\": 152}, {\"x\": -0.04924921034069172, \"source\": 27, \"method\": \"meetings\", \"target\": 44, \"weight\": 1, \"y\": 0.102421470202217, \"pair\": [27, 44], \"edge\": 152}, {\"x\": -0.012391752374116128, \"source\": 27, \"method\": \"meetings\", \"target\": 61, \"weight\": 1, \"y\": 0.05130248030961548, \"pair\": [27, 61], \"edge\": 153}, {\"x\": 0.0048703552552857585, \"source\": 27, \"method\": \"meetings\", \"target\": 61, \"weight\": 1, \"y\": -0.11040957072110401, \"pair\": [27, 61], \"edge\": 153}, {\"x\": -0.012391752374116128, \"source\": 27, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": 0.05130248030961548, \"pair\": [27, 89], \"edge\": 154}, {\"x\": 0.00986576584186687, \"source\": 27, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": 0.08031211042643539, \"pair\": [27, 89], \"edge\": 154}, {\"x\": -0.012391752374116128, \"source\": 27, \"method\": \"phonecalls\", \"target\": 70, \"weight\": 1, \"y\": 0.05130248030961548, \"pair\": [27, 70], \"edge\": 155}, {\"x\": 0.011819622080482078, \"source\": 27, \"method\": \"phonecalls\", \"target\": 70, \"weight\": 1, \"y\": 0.029445565255388533, \"pair\": [27, 70], \"edge\": 155}, {\"x\": -0.012391752374116128, \"source\": 27, \"method\": \"phonecalls\", \"target\": 97, \"weight\": 1, \"y\": 0.05130248030961548, \"pair\": [27, 97], \"edge\": 156}, {\"x\": 0.03141336674461058, \"source\": 27, \"method\": \"phonecalls\", \"target\": 97, \"weight\": 1, \"y\": 0.062183868103891184, \"pair\": [27, 97], \"edge\": 156}, {\"x\": -0.012391752374116128, \"source\": 27, \"method\": \"phonecalls\", \"target\": 119, \"weight\": 1, \"y\": 0.05130248030961548, \"pair\": [27, 119], \"edge\": 157}, {\"x\": -0.0915877212667124, \"source\": 27, \"method\": \"phonecalls\", \"target\": 119, \"weight\": 1, \"y\": 0.11701018058264209, \"pair\": [27, 119], \"edge\": 157}, {\"x\": -0.012391752374116128, \"source\": 27, \"method\": \"phonecalls\", \"target\": 126, \"weight\": 1, \"y\": 0.05130248030961548, \"pair\": [27, 126], \"edge\": 158}, {\"x\": -0.08725800601701894, \"source\": 27, \"method\": \"phonecalls\", \"target\": 126, \"weight\": 1, \"y\": 0.13847821207887648, \"pair\": [27, 126], \"edge\": 158}, {\"x\": -0.012391752374116128, \"source\": 27, \"method\": \"phonecalls\", \"target\": 136, \"weight\": 1, \"y\": 0.05130248030961548, \"pair\": [27, 136], \"edge\": 159}, {\"x\": 0.05032606131256557, \"source\": 27, \"method\": \"phonecalls\", \"target\": 136, \"weight\": 1, \"y\": -0.015961203093154453, \"pair\": [27, 136], \"edge\": 159}, {\"x\": -0.012391752374116128, \"source\": 27, \"method\": \"phonecalls\", \"target\": 138, \"weight\": 1, \"y\": 0.05130248030961548, \"pair\": [27, 138], \"edge\": 160}, {\"x\": -0.06727209544701435, \"source\": 27, \"method\": \"phonecalls\", \"target\": 138, \"weight\": 1, \"y\": 0.140436626954799, \"pair\": [27, 138], \"edge\": 160}, {\"x\": -0.012391752374116128, \"source\": 27, \"method\": \"phonecalls\", \"target\": 36, \"weight\": 1, \"y\": 0.05130248030961548, \"pair\": [27, 36], \"edge\": 161}, {\"x\": 0.05734925339610061, \"source\": 27, \"method\": \"phonecalls\", \"target\": 36, \"weight\": 1, \"y\": 0.010343123664195639, \"pair\": [27, 36], \"edge\": 161}, {\"x\": -0.06656634161106342, \"source\": 29, \"method\": \"meetings\", \"target\": 30, \"weight\": 1, \"y\": 0.02072802436799425, \"pair\": [29, 30], \"edge\": 162}, {\"x\": -0.13184619813604853, \"source\": 29, \"method\": \"meetings\", \"target\": 30, \"weight\": 1, \"y\": 0.0239530046935114, \"pair\": [29, 30], \"edge\": 162}, {\"x\": -0.06656634161106342, \"source\": 29, \"method\": \"phonecalls\", \"target\": 64, \"weight\": 1, \"y\": 0.02072802436799425, \"pair\": [29, 64], \"edge\": 163}, {\"x\": -0.08203963210578026, \"source\": 29, \"method\": \"phonecalls\", \"target\": 64, \"weight\": 1, \"y\": -0.006091601590310482, \"pair\": [29, 64], \"edge\": 163}, {\"x\": -0.06656634161106342, \"source\": 29, \"method\": \"phonecalls\", \"target\": 54, \"weight\": 1, \"y\": 0.02072802436799425, \"pair\": [29, 54], \"edge\": 164}, {\"x\": -0.012499975810208244, \"source\": 29, \"method\": \"phonecalls\", \"target\": 54, \"weight\": 1, \"y\": 0.0743061072952657, \"pair\": [29, 54], \"edge\": 164}, {\"x\": -0.06656634161106342, \"source\": 29, \"method\": \"phonecalls\", \"target\": 43, \"weight\": 4, \"y\": 0.02072802436799425, \"pair\": [29, 43], \"edge\": 165}, {\"x\": -0.03881531608244006, \"source\": 29, \"method\": \"phonecalls\", \"target\": 43, \"weight\": 4, \"y\": 0.04540671878796461, \"pair\": [29, 43], \"edge\": 165}, {\"x\": -0.06656634161106342, \"source\": 29, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": 0.02072802436799425, \"pair\": [29, 68], \"edge\": 166}, {\"x\": 0.03471850621848811, \"source\": 29, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": 0.027941958673130613, \"pair\": [29, 68], \"edge\": 166}, {\"x\": -0.06656634161106342, \"source\": 29, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 1, \"y\": 0.02072802436799425, \"pair\": [29, 47], \"edge\": 167}, {\"x\": 0.007108527109453355, \"source\": 29, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 1, \"y\": 0.11003438822517303, \"pair\": [29, 47], \"edge\": 167}, {\"x\": -0.06656634161106342, \"source\": 29, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 1, \"y\": 0.02072802436799425, \"pair\": [29, 51], \"edge\": 168}, {\"x\": -0.022636151635812406, \"source\": 29, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 1, \"y\": 0.12274076369864406, \"pair\": [29, 51], \"edge\": 168}, {\"x\": -0.06656634161106342, \"source\": 29, \"method\": \"meetings\", \"target\": 71, \"weight\": 1, \"y\": 0.02072802436799425, \"pair\": [29, 71], \"edge\": 169}, {\"x\": -0.10834854337472752, \"source\": 29, \"method\": \"meetings\", \"target\": 71, \"weight\": 1, \"y\": 0.009031121091285323, \"pair\": [29, 71], \"edge\": 169}, {\"x\": -0.06656634161106342, \"source\": 29, \"method\": \"meetings\", \"target\": 75, \"weight\": 1, \"y\": 0.02072802436799425, \"pair\": [29, 75], \"edge\": 170}, {\"x\": -0.15725665594995872, \"source\": 29, \"method\": \"meetings\", \"target\": 75, \"weight\": 1, \"y\": -0.06183290734652462, \"pair\": [29, 75], \"edge\": 170}, {\"x\": -0.06656634161106342, \"source\": 29, \"method\": \"meetings\", \"target\": 76, \"weight\": 1, \"y\": 0.02072802436799425, \"pair\": [29, 76], \"edge\": 171}, {\"x\": -0.13440995012367013, \"source\": 29, \"method\": \"meetings\", \"target\": 76, \"weight\": 1, \"y\": -0.035573320781739935, \"pair\": [29, 76], \"edge\": 171}, {\"x\": -0.06656634161106342, \"source\": 29, \"method\": \"phonecalls\", \"target\": 63, \"weight\": 1, \"y\": 0.02072802436799425, \"pair\": [29, 63], \"edge\": 172}, {\"x\": -0.11572944960277982, \"source\": 29, \"method\": \"phonecalls\", \"target\": 63, \"weight\": 1, \"y\": -0.023077593783232188, \"pair\": [29, 63], \"edge\": 172}, {\"x\": -0.06656634161106342, \"source\": 29, \"method\": \"phonecalls\", \"target\": 98, \"weight\": 1, \"y\": 0.02072802436799425, \"pair\": [29, 98], \"edge\": 173}, {\"x\": -0.1373101562119631, \"source\": 29, \"method\": \"phonecalls\", \"target\": 98, \"weight\": 1, \"y\": -0.005371636152092576, \"pair\": [29, 98], \"edge\": 173}, {\"x\": -0.06656634161106342, \"source\": 29, \"method\": \"phonecalls\", \"target\": 139, \"weight\": 1, \"y\": 0.02072802436799425, \"pair\": [29, 139], \"edge\": 174}, {\"x\": -0.10667639502577143, \"source\": 29, \"method\": \"phonecalls\", \"target\": 139, \"weight\": 1, \"y\": 0.07638231999526468, \"pair\": [29, 139], \"edge\": 174}, {\"x\": -0.07733357836765282, \"source\": 32, \"method\": \"meetings\", \"target\": 33, \"weight\": 1, \"y\": -0.09150374006610085, \"pair\": [32, 33], \"edge\": 175}, {\"x\": -0.06601643841065934, \"source\": 32, \"method\": \"meetings\", \"target\": 33, \"weight\": 1, \"y\": -0.0655494401056117, \"pair\": [32, 33], \"edge\": 175}, {\"x\": -0.06601643841065934, \"source\": 33, \"method\": \"meetings\", \"target\": 88, \"weight\": 1, \"y\": -0.0655494401056117, \"pair\": [33, 88], \"edge\": 176}, {\"x\": -0.07809806699308035, \"source\": 33, \"method\": \"meetings\", \"target\": 88, \"weight\": 1, \"y\": -0.14026940956002884, \"pair\": [33, 88], \"edge\": 176}, {\"x\": 0.0032855872505247415, \"source\": 34, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": -0.021647749997618267, \"pair\": [34, 35], \"edge\": 177}, {\"x\": -0.01673381052039805, \"source\": 34, \"method\": \"meetings\", \"target\": 35, \"weight\": 1, \"y\": -0.023753091559328095, \"pair\": [34, 35], \"edge\": 177}, {\"x\": 0.0032855872505247415, \"source\": 34, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": -0.021647749997618267, \"pair\": [34, 100], \"edge\": 178}, {\"x\": 0.023332033262655517, \"source\": 34, \"method\": \"meetings\", \"target\": 100, \"weight\": 1, \"y\": -0.040032722379539636, \"pair\": [34, 100], \"edge\": 178}, {\"x\": 0.05734925339610061, \"source\": 36, \"method\": \"meetings\", \"target\": 37, \"weight\": 1, \"y\": 0.010343123664195639, \"pair\": [36, 37], \"edge\": 179}, {\"x\": 0.09071083209925236, \"source\": 36, \"method\": \"meetings\", \"target\": 37, \"weight\": 1, \"y\": -0.08992370202512684, \"pair\": [36, 37], \"edge\": 179}, {\"x\": 0.05734925339610061, \"source\": 36, \"method\": \"meetings\", \"target\": 45, \"weight\": 2, \"y\": 0.010343123664195639, \"pair\": [36, 45], \"edge\": 180}, {\"x\": 0.014345250393342512, \"source\": 36, \"method\": \"meetings\", \"target\": 45, \"weight\": 2, \"y\": 0.09666990489701383, \"pair\": [36, 45], \"edge\": 180}, {\"x\": 0.05734925339610061, \"source\": 36, \"method\": \"meetings\", \"target\": 46, \"weight\": 1, \"y\": 0.010343123664195639, \"pair\": [36, 46], \"edge\": 181}, {\"x\": 0.06539720733271068, \"source\": 36, \"method\": \"meetings\", \"target\": 46, \"weight\": 1, \"y\": 0.08609400131444722, \"pair\": [36, 46], \"edge\": 181}, {\"x\": 0.05734925339610061, \"source\": 36, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 2, \"y\": 0.010343123664195639, \"pair\": [36, 47], \"edge\": 182}, {\"x\": 0.007108527109453355, \"source\": 36, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 2, \"y\": 0.11003438822517303, \"pair\": [36, 47], \"edge\": 182}, {\"x\": 0.05734925339610061, \"source\": 36, \"method\": \"meetings\", \"target\": 48, \"weight\": 2, \"y\": 0.010343123664195639, \"pair\": [36, 48], \"edge\": 183}, {\"x\": 0.03284354606986613, \"source\": 36, \"method\": \"meetings\", \"target\": 48, \"weight\": 2, \"y\": 0.10465010441704334, \"pair\": [36, 48], \"edge\": 183}, {\"x\": 0.05734925339610061, \"source\": 36, \"method\": \"meetings\", \"target\": 91, \"weight\": 1, \"y\": 0.010343123664195639, \"pair\": [36, 91], \"edge\": 184}, {\"x\": 0.16440400899713334, \"source\": 36, \"method\": \"meetings\", \"target\": 91, \"weight\": 1, \"y\": -0.024083091381361547, \"pair\": [36, 91], \"edge\": 184}, {\"x\": 0.05734925339610061, \"source\": 36, \"method\": \"meetings\", \"target\": 92, \"weight\": 1, \"y\": 0.010343123664195639, \"pair\": [36, 92], \"edge\": 185}, {\"x\": 0.15937579571068833, \"source\": 36, \"method\": \"meetings\", \"target\": 92, \"weight\": 1, \"y\": -0.04195568206221384, \"pair\": [36, 92], \"edge\": 185}, {\"x\": 0.05734925339610061, \"source\": 36, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": 0.010343123664195639, \"pair\": [36, 68], \"edge\": 186}, {\"x\": 0.03471850621848811, \"source\": 36, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": 0.027941958673130613, \"pair\": [36, 68], \"edge\": 186}, {\"x\": 0.05734925339610061, \"source\": 36, \"method\": \"phonecalls\", \"target\": 148, \"weight\": 1, \"y\": 0.010343123664195639, \"pair\": [36, 148], \"edge\": 187}, {\"x\": 0.1464062231275505, \"source\": 36, \"method\": \"phonecalls\", \"target\": 148, \"weight\": 1, \"y\": -0.059838364010903235, \"pair\": [36, 148], \"edge\": 187}, {\"x\": 0.05734925339610061, \"source\": 36, \"method\": \"phonecalls\", \"target\": 61, \"weight\": 2, \"y\": 0.010343123664195639, \"pair\": [36, 61], \"edge\": 188}, {\"x\": 0.0048703552552857585, \"source\": 36, \"method\": \"phonecalls\", \"target\": 61, \"weight\": 2, \"y\": -0.11040957072110401, \"pair\": [36, 61], \"edge\": 188}, {\"x\": 0.09071083209925236, \"source\": 37, \"method\": \"meetings\", \"target\": 67, \"weight\": 1, \"y\": -0.08992370202512684, \"pair\": [37, 67], \"edge\": 189}, {\"x\": 0.042361526319305065, \"source\": 37, \"method\": \"meetings\", \"target\": 67, \"weight\": 1, \"y\": -0.13333806121176184, \"pair\": [37, 67], \"edge\": 189}, {\"x\": -0.008018244466440517, \"source\": 38, \"method\": \"meetings\", \"target\": 93, \"weight\": 2, \"y\": 0.10072445639961984, \"pair\": [38, 93], \"edge\": 190}, {\"x\": 0.027239467393576043, \"source\": 38, \"method\": \"meetings\", \"target\": 93, \"weight\": 2, \"y\": 0.08938023744814323, \"pair\": [38, 93], \"edge\": 190}, {\"x\": -0.008018244466440517, \"source\": 38, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": 0.10072445639961984, \"pair\": [38, 89], \"edge\": 191}, {\"x\": 0.00986576584186687, \"source\": 38, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": 0.08031211042643539, \"pair\": [38, 89], \"edge\": 191}, {\"x\": 0.04309348428219574, \"source\": 39, \"method\": \"meetings\", \"target\": 40, \"weight\": 1, \"y\": 0.2053099652866237, \"pair\": [39, 40], \"edge\": 192}, {\"x\": 0.03993349451976024, \"source\": 39, \"method\": \"meetings\", \"target\": 40, \"weight\": 1, \"y\": 0.2382904291101742, \"pair\": [39, 40], \"edge\": 192}, {\"x\": 0.04309348428219574, \"source\": 39, \"method\": \"meetings\", \"target\": 41, \"weight\": 1, \"y\": 0.2053099652866237, \"pair\": [39, 41], \"edge\": 193}, {\"x\": 0.060834102272328154, \"source\": 39, \"method\": \"meetings\", \"target\": 41, \"weight\": 1, \"y\": 0.2963564369834344, \"pair\": [39, 41], \"edge\": 193}, {\"x\": 0.04309348428219574, \"source\": 39, \"method\": \"meetings\", \"target\": 42, \"weight\": 1, \"y\": 0.2053099652866237, \"pair\": [39, 42], \"edge\": 194}, {\"x\": 0.04211946342367626, \"source\": 39, \"method\": \"meetings\", \"target\": 42, \"weight\": 1, \"y\": 0.2993487255455769, \"pair\": [39, 42], \"edge\": 194}, {\"x\": 0.04309348428219574, \"source\": 39, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": 0.2053099652866237, \"pair\": [39, 47], \"edge\": 195}, {\"x\": 0.007108527109453355, \"source\": 39, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": 0.11003438822517303, \"pair\": [39, 47], \"edge\": 195}, {\"x\": 0.04309348428219574, \"source\": 39, \"method\": \"meetings\", \"target\": 49, \"weight\": 1, \"y\": 0.2053099652866237, \"pair\": [39, 49], \"edge\": 196}, {\"x\": 0.06934001875360078, \"source\": 39, \"method\": \"meetings\", \"target\": 49, \"weight\": 1, \"y\": 0.14488308920109902, \"pair\": [39, 49], \"edge\": 196}, {\"x\": 0.04309348428219574, \"source\": 39, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": 0.2053099652866237, \"pair\": [39, 50], \"edge\": 197}, {\"x\": 0.01908768168133293, \"source\": 39, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": 0.14941301779317726, \"pair\": [39, 50], \"edge\": 197}, {\"x\": 0.04309348428219574, \"source\": 39, \"method\": \"meetings\", \"target\": 48, \"weight\": 1, \"y\": 0.2053099652866237, \"pair\": [39, 48], \"edge\": 198}, {\"x\": 0.03284354606986613, \"source\": 39, \"method\": \"meetings\", \"target\": 48, \"weight\": 1, \"y\": 0.10465010441704334, \"pair\": [39, 48], \"edge\": 198}, {\"x\": 0.03993349451976024, \"source\": 40, \"method\": \"meetings\", \"target\": 41, \"weight\": 1, \"y\": 0.2382904291101742, \"pair\": [40, 41], \"edge\": 199}, {\"x\": 0.060834102272328154, \"source\": 40, \"method\": \"meetings\", \"target\": 41, \"weight\": 1, \"y\": 0.2963564369834344, \"pair\": [40, 41], \"edge\": 199}, {\"x\": 0.03993349451976024, \"source\": 40, \"method\": \"meetings\", \"target\": 42, \"weight\": 1, \"y\": 0.2382904291101742, \"pair\": [40, 42], \"edge\": 200}, {\"x\": 0.04211946342367626, \"source\": 40, \"method\": \"meetings\", \"target\": 42, \"weight\": 1, \"y\": 0.2993487255455769, \"pair\": [40, 42], \"edge\": 200}, {\"x\": 0.03993349451976024, \"source\": 40, \"method\": \"phonecalls\", \"target\": 45, \"weight\": 1, \"y\": 0.2382904291101742, \"pair\": [40, 45], \"edge\": 201}, {\"x\": 0.014345250393342512, \"source\": 40, \"method\": \"phonecalls\", \"target\": 45, \"weight\": 1, \"y\": 0.09666990489701383, \"pair\": [40, 45], \"edge\": 201}, {\"x\": 0.060834102272328154, \"source\": 41, \"method\": \"meetings\", \"target\": 42, \"weight\": 1, \"y\": 0.2963564369834344, \"pair\": [41, 42], \"edge\": 202}, {\"x\": 0.04211946342367626, \"source\": 41, \"method\": \"meetings\", \"target\": 42, \"weight\": 1, \"y\": 0.2993487255455769, \"pair\": [41, 42], \"edge\": 202}, {\"x\": -0.03881531608244006, \"source\": 43, \"method\": \"meetings\", \"target\": 44, \"weight\": 1, \"y\": 0.04540671878796461, \"pair\": [43, 44], \"edge\": 203}, {\"x\": -0.04924921034069172, \"source\": 43, \"method\": \"meetings\", \"target\": 44, \"weight\": 1, \"y\": 0.102421470202217, \"pair\": [43, 44], \"edge\": 203}, {\"x\": -0.03881531608244006, \"source\": 43, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 4, \"y\": 0.04540671878796461, \"pair\": [43, 47], \"edge\": 204}, {\"x\": 0.007108527109453355, \"source\": 43, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 4, \"y\": 0.11003438822517303, \"pair\": [43, 47], \"edge\": 204}, {\"x\": -0.03881531608244006, \"source\": 43, \"method\": \"meetings\", \"target\": 64, \"weight\": 2, \"y\": 0.04540671878796461, \"pair\": [43, 64], \"edge\": 205}, {\"x\": -0.08203963210578026, \"source\": 43, \"method\": \"meetings\", \"target\": 64, \"weight\": 2, \"y\": -0.006091601590310482, \"pair\": [43, 64], \"edge\": 205}, {\"x\": -0.03881531608244006, \"source\": 43, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": 0.04540671878796461, \"pair\": [43, 68], \"edge\": 206}, {\"x\": 0.03471850621848811, \"source\": 43, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": 0.027941958673130613, \"pair\": [43, 68], \"edge\": 206}, {\"x\": -0.03881531608244006, \"source\": 43, \"method\": \"meetings\", \"target\": 51, \"weight\": 1, \"y\": 0.04540671878796461, \"pair\": [43, 51], \"edge\": 207}, {\"x\": -0.022636151635812406, \"source\": 43, \"method\": \"meetings\", \"target\": 51, \"weight\": 1, \"y\": 0.12274076369864406, \"pair\": [43, 51], \"edge\": 207}, {\"x\": -0.03881531608244006, \"source\": 43, \"method\": \"meetings\", \"target\": 61, \"weight\": 1, \"y\": 0.04540671878796461, \"pair\": [43, 61], \"edge\": 208}, {\"x\": 0.0048703552552857585, \"source\": 43, \"method\": \"meetings\", \"target\": 61, \"weight\": 1, \"y\": -0.11040957072110401, \"pair\": [43, 61], \"edge\": 208}, {\"x\": -0.03881531608244006, \"source\": 43, \"method\": \"phonecalls\", \"target\": 120, \"weight\": 1, \"y\": 0.04540671878796461, \"pair\": [43, 120], \"edge\": 209}, {\"x\": -0.1275593951692764, \"source\": 43, \"method\": \"phonecalls\", \"target\": 120, \"weight\": 1, \"y\": 0.11875604489352103, \"pair\": [43, 120], \"edge\": 209}, {\"x\": -0.03881531608244006, \"source\": 43, \"method\": \"phonecalls\", \"target\": 139, \"weight\": 1, \"y\": 0.04540671878796461, \"pair\": [43, 139], \"edge\": 210}, {\"x\": -0.10667639502577143, \"source\": 43, \"method\": \"phonecalls\", \"target\": 139, \"weight\": 1, \"y\": 0.07638231999526468, \"pair\": [43, 139], \"edge\": 210}, {\"x\": -0.04924921034069172, \"source\": 44, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": 0.102421470202217, \"pair\": [44, 47], \"edge\": 211}, {\"x\": 0.007108527109453355, \"source\": 44, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": 0.11003438822517303, \"pair\": [44, 47], \"edge\": 211}, {\"x\": 0.014345250393342512, \"source\": 45, \"method\": \"meetings\", \"target\": 46, \"weight\": 1, \"y\": 0.09666990489701383, \"pair\": [45, 46], \"edge\": 212}, {\"x\": 0.06539720733271068, \"source\": 45, \"method\": \"meetings\", \"target\": 46, \"weight\": 1, \"y\": 0.08609400131444722, \"pair\": [45, 46], \"edge\": 212}, {\"x\": 0.014345250393342512, \"source\": 45, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 1, \"y\": 0.09666990489701383, \"pair\": [45, 47], \"edge\": 213}, {\"x\": 0.007108527109453355, \"source\": 45, \"method\": \"phonecalls\", \"target\": 47, \"weight\": 1, \"y\": 0.11003438822517303, \"pair\": [45, 47], \"edge\": 213}, {\"x\": 0.014345250393342512, \"source\": 45, \"method\": \"meetings\", \"target\": 48, \"weight\": 1, \"y\": 0.09666990489701383, \"pair\": [45, 48], \"edge\": 214}, {\"x\": 0.03284354606986613, \"source\": 45, \"method\": \"meetings\", \"target\": 48, \"weight\": 1, \"y\": 0.10465010441704334, \"pair\": [45, 48], \"edge\": 214}, {\"x\": 0.014345250393342512, \"source\": 45, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 1, \"y\": 0.09666990489701383, \"pair\": [45, 51], \"edge\": 215}, {\"x\": -0.022636151635812406, \"source\": 45, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 1, \"y\": 0.12274076369864406, \"pair\": [45, 51], \"edge\": 215}, {\"x\": 0.014345250393342512, \"source\": 45, \"method\": \"phonecalls\", \"target\": 54, \"weight\": 1, \"y\": 0.09666990489701383, \"pair\": [45, 54], \"edge\": 216}, {\"x\": -0.012499975810208244, \"source\": 45, \"method\": \"phonecalls\", \"target\": 54, \"weight\": 1, \"y\": 0.0743061072952657, \"pair\": [45, 54], \"edge\": 216}, {\"x\": 0.014345250393342512, \"source\": 45, \"method\": \"meetings\", \"target\": 55, \"weight\": 1, \"y\": 0.09666990489701383, \"pair\": [45, 55], \"edge\": 217}, {\"x\": 0.05452234251722276, \"source\": 45, \"method\": \"meetings\", \"target\": 55, \"weight\": 1, \"y\": 0.160441589610005, \"pair\": [45, 55], \"edge\": 217}, {\"x\": 0.014345250393342512, \"source\": 45, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": 0.09666990489701383, \"pair\": [45, 50], \"edge\": 218}, {\"x\": 0.01908768168133293, \"source\": 45, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": 0.14941301779317726, \"pair\": [45, 50], \"edge\": 218}, {\"x\": 0.014345250393342512, \"source\": 45, \"method\": \"meetings\", \"target\": 89, \"weight\": 2, \"y\": 0.09666990489701383, \"pair\": [45, 89], \"edge\": 219}, {\"x\": 0.00986576584186687, \"source\": 45, \"method\": \"meetings\", \"target\": 89, \"weight\": 2, \"y\": 0.08031211042643539, \"pair\": [45, 89], \"edge\": 219}, {\"x\": 0.014345250393342512, \"source\": 45, \"method\": \"meetings\", \"target\": 94, \"weight\": 1, \"y\": 0.09666990489701383, \"pair\": [45, 94], \"edge\": 220}, {\"x\": -0.014229000506406199, \"source\": 45, \"method\": \"meetings\", \"target\": 94, \"weight\": 1, \"y\": 0.15649185613013583, \"pair\": [45, 94], \"edge\": 220}, {\"x\": 0.014345250393342512, \"source\": 45, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": 0.09666990489701383, \"pair\": [45, 68], \"edge\": 221}, {\"x\": 0.03471850621848811, \"source\": 45, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": 0.027941958673130613, \"pair\": [45, 68], \"edge\": 221}, {\"x\": 0.014345250393342512, \"source\": 45, \"method\": \"phonecalls\", \"target\": 69, \"weight\": 2, \"y\": 0.09666990489701383, \"pair\": [45, 69], \"edge\": 222}, {\"x\": -0.05404415963374475, \"source\": 45, \"method\": \"phonecalls\", \"target\": 69, \"weight\": 2, \"y\": 0.0555737666564064, \"pair\": [45, 69], \"edge\": 222}, {\"x\": 0.06539720733271068, \"source\": 46, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": 0.08609400131444722, \"pair\": [46, 47], \"edge\": 223}, {\"x\": 0.007108527109453355, \"source\": 46, \"method\": \"meetings\", \"target\": 47, \"weight\": 1, \"y\": 0.11003438822517303, \"pair\": [46, 47], \"edge\": 223}, {\"x\": 0.06539720733271068, \"source\": 46, \"method\": \"meetings\", \"target\": 48, \"weight\": 1, \"y\": 0.08609400131444722, \"pair\": [46, 48], \"edge\": 224}, {\"x\": 0.03284354606986613, \"source\": 46, \"method\": \"meetings\", \"target\": 48, \"weight\": 1, \"y\": 0.10465010441704334, \"pair\": [46, 48], \"edge\": 224}, {\"x\": 0.007108527109453355, \"source\": 47, \"method\": \"phonecalls\", \"target\": 48, \"weight\": 4, \"y\": 0.11003438822517303, \"pair\": [47, 48], \"edge\": 225}, {\"x\": 0.03284354606986613, \"source\": 47, \"method\": \"phonecalls\", \"target\": 48, \"weight\": 4, \"y\": 0.10465010441704334, \"pair\": [47, 48], \"edge\": 225}, {\"x\": 0.007108527109453355, \"source\": 47, \"method\": \"phonecalls\", \"target\": 49, \"weight\": 1, \"y\": 0.11003438822517303, \"pair\": [47, 49], \"edge\": 226}, {\"x\": 0.06934001875360078, \"source\": 47, \"method\": \"phonecalls\", \"target\": 49, \"weight\": 1, \"y\": 0.14488308920109902, \"pair\": [47, 49], \"edge\": 226}, {\"x\": 0.007108527109453355, \"source\": 47, \"method\": \"phonecalls\", \"target\": 50, \"weight\": 2, \"y\": 0.11003438822517303, \"pair\": [47, 50], \"edge\": 227}, {\"x\": 0.01908768168133293, \"source\": 47, \"method\": \"phonecalls\", \"target\": 50, \"weight\": 2, \"y\": 0.14941301779317726, \"pair\": [47, 50], \"edge\": 227}, {\"x\": 0.007108527109453355, \"source\": 47, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 3, \"y\": 0.11003438822517303, \"pair\": [47, 51], \"edge\": 228}, {\"x\": -0.022636151635812406, \"source\": 47, \"method\": \"phonecalls\", \"target\": 51, \"weight\": 3, \"y\": 0.12274076369864406, \"pair\": [47, 51], \"edge\": 228}, {\"x\": 0.007108527109453355, \"source\": 47, \"method\": \"phonecalls\", \"target\": 54, \"weight\": 4, \"y\": 0.11003438822517303, \"pair\": [47, 54], \"edge\": 229}, {\"x\": -0.012499975810208244, \"source\": 47, \"method\": \"phonecalls\", \"target\": 54, \"weight\": 4, \"y\": 0.0743061072952657, \"pair\": [47, 54], \"edge\": 229}, {\"x\": 0.007108527109453355, \"source\": 47, \"method\": \"meetings\", \"target\": 55, \"weight\": 1, \"y\": 0.11003438822517303, \"pair\": [47, 55], \"edge\": 230}, {\"x\": 0.05452234251722276, \"source\": 47, \"method\": \"meetings\", \"target\": 55, \"weight\": 1, \"y\": 0.160441589610005, \"pair\": [47, 55], \"edge\": 230}, {\"x\": 0.007108527109453355, \"source\": 47, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": 0.11003438822517303, \"pair\": [47, 68], \"edge\": 231}, {\"x\": 0.03471850621848811, \"source\": 47, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": 0.027941958673130613, \"pair\": [47, 68], \"edge\": 231}, {\"x\": 0.007108527109453355, \"source\": 47, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": 0.11003438822517303, \"pair\": [47, 89], \"edge\": 232}, {\"x\": 0.00986576584186687, \"source\": 47, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": 0.08031211042643539, \"pair\": [47, 89], \"edge\": 232}, {\"x\": 0.007108527109453355, \"source\": 47, \"method\": \"meetings\", \"target\": 90, \"weight\": 2, \"y\": 0.11003438822517303, \"pair\": [47, 90], \"edge\": 233}, {\"x\": 0.005731416596121094, \"source\": 47, \"method\": \"meetings\", \"target\": 90, \"weight\": 2, \"y\": 0.17761324046325702, \"pair\": [47, 90], \"edge\": 233}, {\"x\": 0.007108527109453355, \"source\": 47, \"method\": \"meetings\", \"target\": 93, \"weight\": 1, \"y\": 0.11003438822517303, \"pair\": [47, 93], \"edge\": 234}, {\"x\": 0.027239467393576043, \"source\": 47, \"method\": \"meetings\", \"target\": 93, \"weight\": 1, \"y\": 0.08938023744814323, \"pair\": [47, 93], \"edge\": 234}, {\"x\": 0.007108527109453355, \"source\": 47, \"method\": \"phonecalls\", \"target\": 121, \"weight\": 1, \"y\": 0.11003438822517303, \"pair\": [47, 121], \"edge\": 235}, {\"x\": 0.06311721348021446, \"source\": 47, \"method\": \"phonecalls\", \"target\": 121, \"weight\": 1, \"y\": 0.20757439495810204, \"pair\": [47, 121], \"edge\": 235}, {\"x\": 0.007108527109453355, \"source\": 47, \"method\": \"phonecalls\", \"target\": 122, \"weight\": 1, \"y\": 0.11003438822517303, \"pair\": [47, 122], \"edge\": 236}, {\"x\": -0.01484911107013809, \"source\": 47, \"method\": \"phonecalls\", \"target\": 122, \"weight\": 1, \"y\": 0.22357741515042764, \"pair\": [47, 122], \"edge\": 236}, {\"x\": 0.007108527109453355, \"source\": 47, \"method\": \"phonecalls\", \"target\": 123, \"weight\": 1, \"y\": 0.11003438822517303, \"pair\": [47, 123], \"edge\": 237}, {\"x\": 0.006382036204762352, \"source\": 47, \"method\": \"phonecalls\", \"target\": 123, \"weight\": 1, \"y\": 0.22084645561869615, \"pair\": [47, 123], \"edge\": 237}, {\"x\": 0.007108527109453355, \"source\": 47, \"method\": \"phonecalls\", \"target\": 96, \"weight\": 1, \"y\": 0.11003438822517303, \"pair\": [47, 96], \"edge\": 238}, {\"x\": -0.05061792141094029, \"source\": 47, \"method\": \"phonecalls\", \"target\": 96, \"weight\": 1, \"y\": 0.21777227600247498, \"pair\": [47, 96], \"edge\": 238}, {\"x\": 0.007108527109453355, \"source\": 47, \"method\": \"phonecalls\", \"target\": 135, \"weight\": 1, \"y\": 0.11003438822517303, \"pair\": [47, 135], \"edge\": 239}, {\"x\": 0.03428521848756629, \"source\": 47, \"method\": \"phonecalls\", \"target\": 135, \"weight\": 1, \"y\": 0.21961471482378736, \"pair\": [47, 135], \"edge\": 239}, {\"x\": 0.007108527109453355, \"source\": 47, \"method\": \"phonecalls\", \"target\": 95, \"weight\": 1, \"y\": 0.11003438822517303, \"pair\": [47, 95], \"edge\": 240}, {\"x\": 0.03684315176773381, \"source\": 47, \"method\": \"phonecalls\", \"target\": 95, \"weight\": 1, \"y\": 0.13846405716536056, \"pair\": [47, 95], \"edge\": 240}, {\"x\": 0.007108527109453355, \"source\": 47, \"method\": \"phonecalls\", \"target\": 147, \"weight\": 1, \"y\": 0.11003438822517303, \"pair\": [47, 147], \"edge\": 241}, {\"x\": -0.03145493629479878, \"source\": 47, \"method\": \"phonecalls\", \"target\": 147, \"weight\": 1, \"y\": 0.20903569392972257, \"pair\": [47, 147], \"edge\": 241}, {\"x\": 0.007108527109453355, \"source\": 47, \"method\": \"phonecalls\", \"target\": 56, \"weight\": 1, \"y\": 0.11003438822517303, \"pair\": [47, 56], \"edge\": 242}, {\"x\": 0.09412333274359878, \"source\": 47, \"method\": \"phonecalls\", \"target\": 56, \"weight\": 1, \"y\": 0.08676444262522966, \"pair\": [47, 56], \"edge\": 242}, {\"x\": 0.007108527109453355, \"source\": 47, \"method\": \"phonecalls\", \"target\": 97, \"weight\": 1, \"y\": 0.11003438822517303, \"pair\": [47, 97], \"edge\": 243}, {\"x\": 0.03141336674461058, \"source\": 47, \"method\": \"phonecalls\", \"target\": 97, \"weight\": 1, \"y\": 0.062183868103891184, \"pair\": [47, 97], \"edge\": 243}, {\"x\": 0.007108527109453355, \"source\": 47, \"method\": \"phonecalls\", \"target\": 151, \"weight\": 4, \"y\": 0.11003438822517303, \"pair\": [47, 151], \"edge\": 244}, {\"x\": 0.04684013351762619, \"source\": 47, \"method\": \"phonecalls\", \"target\": 151, \"weight\": 4, \"y\": 0.11062064395722294, \"pair\": [47, 151], \"edge\": 244}, {\"x\": 0.03284354606986613, \"source\": 48, \"method\": \"meetings\", \"target\": 49, \"weight\": 1, \"y\": 0.10465010441704334, \"pair\": [48, 49], \"edge\": 245}, {\"x\": 0.06934001875360078, \"source\": 48, \"method\": \"meetings\", \"target\": 49, \"weight\": 1, \"y\": 0.14488308920109902, \"pair\": [48, 49], \"edge\": 245}, {\"x\": 0.03284354606986613, \"source\": 48, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": 0.10465010441704334, \"pair\": [48, 50], \"edge\": 246}, {\"x\": 0.01908768168133293, \"source\": 48, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": 0.14941301779317726, \"pair\": [48, 50], \"edge\": 246}, {\"x\": 0.03284354606986613, \"source\": 48, \"method\": \"meetings\", \"target\": 51, \"weight\": 3, \"y\": 0.10465010441704334, \"pair\": [48, 51], \"edge\": 247}, {\"x\": -0.022636151635812406, \"source\": 48, \"method\": \"meetings\", \"target\": 51, \"weight\": 3, \"y\": 0.12274076369864406, \"pair\": [48, 51], \"edge\": 247}, {\"x\": 0.03284354606986613, \"source\": 48, \"method\": \"meetings\", \"target\": 93, \"weight\": 3, \"y\": 0.10465010441704334, \"pair\": [48, 93], \"edge\": 248}, {\"x\": 0.027239467393576043, \"source\": 48, \"method\": \"meetings\", \"target\": 93, \"weight\": 3, \"y\": 0.08938023744814323, \"pair\": [48, 93], \"edge\": 248}, {\"x\": 0.03284354606986613, \"source\": 48, \"method\": \"meetings\", \"target\": 89, \"weight\": 1, \"y\": 0.10465010441704334, \"pair\": [48, 89], \"edge\": 249}, {\"x\": 0.00986576584186687, \"source\": 48, \"method\": \"meetings\", \"target\": 89, \"weight\": 1, \"y\": 0.08031211042643539, \"pair\": [48, 89], \"edge\": 249}, {\"x\": 0.03284354606986613, \"source\": 48, \"method\": \"meetings\", \"target\": 95, \"weight\": 1, \"y\": 0.10465010441704334, \"pair\": [48, 95], \"edge\": 250}, {\"x\": 0.03684315176773381, \"source\": 48, \"method\": \"meetings\", \"target\": 95, \"weight\": 1, \"y\": 0.13846405716536056, \"pair\": [48, 95], \"edge\": 250}, {\"x\": 0.03284354606986613, \"source\": 48, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": 0.10465010441704334, \"pair\": [48, 68], \"edge\": 251}, {\"x\": 0.03471850621848811, \"source\": 48, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": 0.027941958673130613, \"pair\": [48, 68], \"edge\": 251}, {\"x\": 0.06934001875360078, \"source\": 49, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": 0.14488308920109902, \"pair\": [49, 50], \"edge\": 252}, {\"x\": 0.01908768168133293, \"source\": 49, \"method\": \"meetings\", \"target\": 50, \"weight\": 1, \"y\": 0.14941301779317726, \"pair\": [49, 50], \"edge\": 252}, {\"x\": 0.06934001875360078, \"source\": 49, \"method\": \"phonecalls\", \"target\": 56, \"weight\": 2, \"y\": 0.14488308920109902, \"pair\": [49, 56], \"edge\": 253}, {\"x\": 0.09412333274359878, \"source\": 49, \"method\": \"phonecalls\", \"target\": 56, \"weight\": 2, \"y\": 0.08676444262522966, \"pair\": [49, 56], \"edge\": 253}, {\"x\": 0.01908768168133293, \"source\": 50, \"method\": \"meetings\", \"target\": 51, \"weight\": 1, \"y\": 0.14941301779317726, \"pair\": [50, 51], \"edge\": 254}, {\"x\": -0.022636151635812406, \"source\": 50, \"method\": \"meetings\", \"target\": 51, \"weight\": 1, \"y\": 0.12274076369864406, \"pair\": [50, 51], \"edge\": 254}, {\"x\": 0.01908768168133293, \"source\": 50, \"method\": \"meetings\", \"target\": 89, \"weight\": 1, \"y\": 0.14941301779317726, \"pair\": [50, 89], \"edge\": 255}, {\"x\": 0.00986576584186687, \"source\": 50, \"method\": \"meetings\", \"target\": 89, \"weight\": 1, \"y\": 0.08031211042643539, \"pair\": [50, 89], \"edge\": 255}, {\"x\": 0.01908768168133293, \"source\": 50, \"method\": \"meetings\", \"target\": 90, \"weight\": 2, \"y\": 0.14941301779317726, \"pair\": [50, 90], \"edge\": 256}, {\"x\": 0.005731416596121094, \"source\": 50, \"method\": \"meetings\", \"target\": 90, \"weight\": 2, \"y\": 0.17761324046325702, \"pair\": [50, 90], \"edge\": 256}, {\"x\": -0.022636151635812406, \"source\": 51, \"method\": \"meetings\", \"target\": 52, \"weight\": 1, \"y\": 0.12274076369864406, \"pair\": [51, 52], \"edge\": 257}, {\"x\": -0.07810978486531617, \"source\": 51, \"method\": \"meetings\", \"target\": 52, \"weight\": 1, \"y\": 0.2544256558594014, \"pair\": [51, 52], \"edge\": 257}, {\"x\": -0.022636151635812406, \"source\": 51, \"method\": \"meetings\", \"target\": 53, \"weight\": 1, \"y\": 0.12274076369864406, \"pair\": [51, 53], \"edge\": 258}, {\"x\": -0.08308573848455948, \"source\": 51, \"method\": \"meetings\", \"target\": 53, \"weight\": 1, \"y\": 0.2214746864725049, \"pair\": [51, 53], \"edge\": 258}, {\"x\": -0.022636151635812406, \"source\": 51, \"method\": \"meetings\", \"target\": 54, \"weight\": 1, \"y\": 0.12274076369864406, \"pair\": [51, 54], \"edge\": 259}, {\"x\": -0.012499975810208244, \"source\": 51, \"method\": \"meetings\", \"target\": 54, \"weight\": 1, \"y\": 0.0743061072952657, \"pair\": [51, 54], \"edge\": 259}, {\"x\": -0.022636151635812406, \"source\": 51, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": 0.12274076369864406, \"pair\": [51, 89], \"edge\": 260}, {\"x\": 0.00986576584186687, \"source\": 51, \"method\": \"meetings\", \"target\": 89, \"weight\": 4, \"y\": 0.08031211042643539, \"pair\": [51, 89], \"edge\": 260}, {\"x\": -0.07810978486531617, \"source\": 52, \"method\": \"meetings\", \"target\": 53, \"weight\": 1, \"y\": 0.2544256558594014, \"pair\": [52, 53], \"edge\": 261}, {\"x\": -0.08308573848455948, \"source\": 52, \"method\": \"meetings\", \"target\": 53, \"weight\": 1, \"y\": 0.2214746864725049, \"pair\": [52, 53], \"edge\": 261}, {\"x\": -0.07810978486531617, \"source\": 52, \"method\": \"meetings\", \"target\": 96, \"weight\": 1, \"y\": 0.2544256558594014, \"pair\": [52, 96], \"edge\": 262}, {\"x\": -0.05061792141094029, \"source\": 52, \"method\": \"meetings\", \"target\": 96, \"weight\": 1, \"y\": 0.21777227600247498, \"pair\": [52, 96], \"edge\": 262}, {\"x\": -0.07810978486531617, \"source\": 52, \"method\": \"phonecalls\", \"target\": 124, \"weight\": 1, \"y\": 0.2544256558594014, \"pair\": [52, 124], \"edge\": 263}, {\"x\": -0.11589057331876794, \"source\": 52, \"method\": \"phonecalls\", \"target\": 124, \"weight\": 1, \"y\": 0.3497996671693867, \"pair\": [52, 124], \"edge\": 263}, {\"x\": -0.012499975810208244, \"source\": 54, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": 0.0743061072952657, \"pair\": [54, 68], \"edge\": 264}, {\"x\": 0.03471850621848811, \"source\": 54, \"method\": \"phonecalls\", \"target\": 68, \"weight\": 1, \"y\": 0.027941958673130613, \"pair\": [54, 68], \"edge\": 264}, {\"x\": -0.012499975810208244, \"source\": 54, \"method\": \"meetings\", \"target\": 70, \"weight\": 2, \"y\": 0.0743061072952657, \"pair\": [54, 70], \"edge\": 265}, {\"x\": 0.011819622080482078, \"source\": 54, \"method\": \"meetings\", \"target\": 70, \"weight\": 2, \"y\": 0.029445565255388533, \"pair\": [54, 70], \"edge\": 265}, {\"x\": -0.012499975810208244, \"source\": 54, \"method\": \"phonecalls\", \"target\": 69, \"weight\": 1, \"y\": 0.0743061072952657, \"pair\": [54, 69], \"edge\": 266}, {\"x\": -0.05404415963374475, \"source\": 54, \"method\": \"phonecalls\", \"target\": 69, \"weight\": 1, \"y\": 0.0555737666564064, \"pair\": [54, 69], \"edge\": 266}, {\"x\": 0.09412333274359878, \"source\": 56, \"method\": \"meetings\", \"target\": 57, \"weight\": 1, \"y\": 0.08676444262522966, \"pair\": [56, 57], \"edge\": 267}, {\"x\": 0.19750003762175775, \"source\": 56, \"method\": \"meetings\", \"target\": 57, \"weight\": 1, \"y\": 0.04815107032506278, \"pair\": [56, 57], \"edge\": 267}, {\"x\": 0.09412333274359878, \"source\": 56, \"method\": \"phonecalls\", \"target\": 125, \"weight\": 1, \"y\": 0.08676444262522966, \"pair\": [56, 125], \"edge\": 268}, {\"x\": 0.012050096233286714, \"source\": 56, \"method\": \"phonecalls\", \"target\": 125, \"weight\": 1, \"y\": 0.012727612759410905, \"pair\": [56, 125], \"edge\": 268}, {\"x\": -0.18241099698058447, \"source\": 58, \"method\": \"meetings\", \"target\": 59, \"weight\": 1, \"y\": 0.032024336890226164, \"pair\": [58, 59], \"edge\": 269}, {\"x\": -0.1559017209514067, \"source\": 58, \"method\": \"meetings\", \"target\": 59, \"weight\": 1, \"y\": 0.021592614097687188, \"pair\": [58, 59], \"edge\": 269}, {\"x\": -0.18241099698058447, \"source\": 58, \"method\": \"phonecalls\", \"target\": 109, \"weight\": 3, \"y\": 0.032024336890226164, \"pair\": [58, 109], \"edge\": 270}, {\"x\": -0.21748147572590495, \"source\": 58, \"method\": \"phonecalls\", \"target\": 109, \"weight\": 3, \"y\": 0.09593019893154699, \"pair\": [58, 109], \"edge\": 270}, {\"x\": -0.18241099698058447, \"source\": 58, \"method\": \"phonecalls\", \"target\": 75, \"weight\": 1, \"y\": 0.032024336890226164, \"pair\": [58, 75], \"edge\": 271}, {\"x\": -0.15725665594995872, \"source\": 58, \"method\": \"phonecalls\", \"target\": 75, \"weight\": 1, \"y\": -0.06183290734652462, \"pair\": [58, 75], \"edge\": 271}, {\"x\": -0.18241099698058447, \"source\": 58, \"method\": \"phonecalls\", \"target\": 77, \"weight\": 2, \"y\": 0.032024336890226164, \"pair\": [58, 77], \"edge\": 272}, {\"x\": -0.1674280211177842, \"source\": 58, \"method\": \"phonecalls\", \"target\": 77, \"weight\": 2, \"y\": -0.012273254905121884, \"pair\": [58, 77], \"edge\": 272}, {\"x\": 0.0048703552552857585, \"source\": 61, \"method\": \"phonecalls\", \"target\": 62, \"weight\": 1, \"y\": -0.11040957072110401, \"pair\": [61, 62], \"edge\": 273}, {\"x\": 0.060586395520634266, \"source\": 61, \"method\": \"phonecalls\", \"target\": 62, \"weight\": 1, \"y\": -0.2107327427687022, \"pair\": [61, 62], \"edge\": 273}, {\"x\": 0.0048703552552857585, \"source\": 61, \"method\": \"phonecalls\", \"target\": 66, \"weight\": 3, \"y\": -0.11040957072110401, \"pair\": [61, 66], \"edge\": 274}, {\"x\": -0.022640902960118864, \"source\": 61, \"method\": \"phonecalls\", \"target\": 66, \"weight\": 3, \"y\": -0.11038841407886545, \"pair\": [61, 66], \"edge\": 274}, {\"x\": 0.0048703552552857585, \"source\": 61, \"method\": \"phonecalls\", \"target\": 105, \"weight\": 2, \"y\": -0.11040957072110401, \"pair\": [61, 105], \"edge\": 275}, {\"x\": 0.039903417854644616, \"source\": 61, \"method\": \"phonecalls\", \"target\": 105, \"weight\": 2, \"y\": -0.17873055666970827, \"pair\": [61, 105], \"edge\": 275}, {\"x\": 0.0048703552552857585, \"source\": 61, \"method\": \"phonecalls\", \"target\": 117, \"weight\": 2, \"y\": -0.11040957072110401, \"pair\": [61, 117], \"edge\": 276}, {\"x\": -0.00444784078162752, \"source\": 61, \"method\": \"phonecalls\", \"target\": 117, \"weight\": 2, \"y\": -0.18765976946808133, \"pair\": [61, 117], \"edge\": 276}, {\"x\": 0.0048703552552857585, \"source\": 61, \"method\": \"phonecalls\", \"target\": 80, \"weight\": 1, \"y\": -0.11040957072110401, \"pair\": [61, 80], \"edge\": 277}, {\"x\": 0.11968141191543366, \"source\": 61, \"method\": \"phonecalls\", \"target\": 80, \"weight\": 1, \"y\": -0.17155214344222655, \"pair\": [61, 80], \"edge\": 277}, {\"x\": 0.0048703552552857585, \"source\": 61, \"method\": \"phonecalls\", \"target\": 67, \"weight\": 5, \"y\": -0.11040957072110401, \"pair\": [61, 67], \"edge\": 278}, {\"x\": 0.042361526319305065, \"source\": 61, \"method\": \"phonecalls\", \"target\": 67, \"weight\": 5, \"y\": -0.13333806121176184, \"pair\": [61, 67], \"edge\": 278}, {\"x\": 0.0048703552552857585, \"source\": 61, \"method\": \"phonecalls\", \"target\": 128, \"weight\": 2, \"y\": -0.11040957072110401, \"pair\": [61, 128], \"edge\": 279}, {\"x\": 0.01793498624792611, \"source\": 61, \"method\": \"phonecalls\", \"target\": 128, \"weight\": 2, \"y\": -0.1857666690597249, \"pair\": [61, 128], \"edge\": 279}, {\"x\": 0.0048703552552857585, \"source\": 61, \"method\": \"phonecalls\", \"target\": 129, \"weight\": 1, \"y\": -0.11040957072110401, \"pair\": [61, 129], \"edge\": 280}, {\"x\": -0.0013334346347747073, \"source\": 61, \"method\": \"phonecalls\", \"target\": 129, \"weight\": 1, \"y\": -0.229445870323091, \"pair\": [61, 129], \"edge\": 280}, {\"x\": 0.0048703552552857585, \"source\": 61, \"method\": \"phonecalls\", \"target\": 75, \"weight\": 1, \"y\": -0.11040957072110401, \"pair\": [61, 75], \"edge\": 281}, {\"x\": -0.15725665594995872, \"source\": 61, \"method\": \"phonecalls\", \"target\": 75, \"weight\": 1, \"y\": -0.06183290734652462, \"pair\": [61, 75], \"edge\": 281}, {\"x\": 0.0048703552552857585, \"source\": 61, \"method\": \"phonecalls\", \"target\": 130, \"weight\": 1, \"y\": -0.11040957072110401, \"pair\": [61, 130], \"edge\": 282}, {\"x\": 0.019788470465907235, \"source\": 61, \"method\": \"phonecalls\", \"target\": 130, \"weight\": 1, \"y\": -0.22757867641728233, \"pair\": [61, 130], \"edge\": 282}, {\"x\": 0.0048703552552857585, \"source\": 61, \"method\": \"phonecalls\", \"target\": 131, \"weight\": 4, \"y\": -0.11040957072110401, \"pair\": [61, 131], \"edge\": 283}, {\"x\": 0.016315752571887842, \"source\": 61, \"method\": \"phonecalls\", \"target\": 131, \"weight\": 4, \"y\": -0.15829498963549138, \"pair\": [61, 131], \"edge\": 283}, {\"x\": 0.0048703552552857585, \"source\": 61, \"method\": \"phonecalls\", \"target\": 133, \"weight\": 1, \"y\": -0.11040957072110401, \"pair\": [61, 133], \"edge\": 284}, {\"x\": -0.023772059143059947, \"source\": 61, \"method\": \"phonecalls\", \"target\": 133, \"weight\": 1, \"y\": -0.22515184985888426, \"pair\": [61, 133], \"edge\": 284}, {\"x\": 0.0048703552552857585, \"source\": 61, \"method\": \"phonecalls\", \"target\": 134, \"weight\": 1, \"y\": -0.11040957072110401, \"pair\": [61, 134], \"edge\": 285}, {\"x\": 0.040932892363857123, \"source\": 61, \"method\": \"phonecalls\", \"target\": 134, \"weight\": 1, \"y\": -0.22412387161229547, \"pair\": [61, 134], \"edge\": 285}, {\"x\": 0.0048703552552857585, \"source\": 61, \"method\": \"phonecalls\", \"target\": 142, \"weight\": 1, \"y\": -0.11040957072110401, \"pair\": [61, 142], \"edge\": 286}, {\"x\": -0.04413909119047113, \"source\": 61, \"method\": \"phonecalls\", \"target\": 142, \"weight\": 1, \"y\": -0.1607756139326318, \"pair\": [61, 142], \"edge\": 286}, {\"x\": 0.0048703552552857585, \"source\": 61, \"method\": \"phonecalls\", \"target\": 70, \"weight\": 1, \"y\": -0.11040957072110401, \"pair\": [61, 70], \"edge\": 287}, {\"x\": 0.011819622080482078, \"source\": 61, \"method\": \"phonecalls\", \"target\": 70, \"weight\": 1, \"y\": 0.029445565255388533, \"pair\": [61, 70], \"edge\": 287}, {\"x\": -0.11572944960277982, \"source\": 63, \"method\": \"meetings\", \"target\": 64, \"weight\": 3, \"y\": -0.023077593783232188, \"pair\": [63, 64], \"edge\": 288}, {\"x\": -0.08203963210578026, \"source\": 63, \"method\": \"meetings\", \"target\": 64, \"weight\": 3, \"y\": -0.006091601590310482, \"pair\": [63, 64], \"edge\": 288}, {\"x\": -0.11572944960277982, \"source\": 63, \"method\": \"meetings\", \"target\": 72, \"weight\": 2, \"y\": -0.023077593783232188, \"pair\": [63, 72], \"edge\": 289}, {\"x\": -0.1685060948873108, \"source\": 63, \"method\": \"meetings\", \"target\": 72, \"weight\": 2, \"y\": -0.05601789641213182, \"pair\": [63, 72], \"edge\": 289}, {\"x\": -0.08203963210578026, \"source\": 64, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": -0.006091601590310482, \"pair\": [64, 68], \"edge\": 290}, {\"x\": 0.03471850621848811, \"source\": 64, \"method\": \"meetings\", \"target\": 68, \"weight\": 1, \"y\": 0.027941958673130613, \"pair\": [64, 68], \"edge\": 290}, {\"x\": -0.08203963210578026, \"source\": 64, \"method\": \"phonecalls\", \"target\": 75, \"weight\": 3, \"y\": -0.006091601590310482, \"pair\": [64, 75], \"edge\": 291}, {\"x\": -0.15725665594995872, \"source\": 64, \"method\": \"phonecalls\", \"target\": 75, \"weight\": 3, \"y\": -0.06183290734652462, \"pair\": [64, 75], \"edge\": 291}, {\"x\": 0.03471850621848811, \"source\": 68, \"method\": \"meetings\", \"target\": 69, \"weight\": 1, \"y\": 0.027941958673130613, \"pair\": [68, 69], \"edge\": 292}, {\"x\": -0.05404415963374475, \"source\": 68, \"method\": \"meetings\", \"target\": 69, \"weight\": 1, \"y\": 0.0555737666564064, \"pair\": [68, 69], \"edge\": 292}, {\"x\": 0.03471850621848811, \"source\": 68, \"method\": \"phonecalls\", \"target\": 70, \"weight\": 3, \"y\": 0.027941958673130613, \"pair\": [68, 70], \"edge\": 293}, {\"x\": 0.011819622080482078, \"source\": 68, \"method\": \"phonecalls\", \"target\": 70, \"weight\": 3, \"y\": 0.029445565255388533, \"pair\": [68, 70], \"edge\": 293}, {\"x\": 0.03471850621848811, \"source\": 68, \"method\": \"meetings\", \"target\": 79, \"weight\": 1, \"y\": 0.027941958673130613, \"pair\": [68, 79], \"edge\": 294}, {\"x\": 0.12897107520534695, \"source\": 68, \"method\": \"meetings\", \"target\": 79, \"weight\": 1, \"y\": -0.1105890148315894, \"pair\": [68, 79], \"edge\": 294}, {\"x\": 0.03471850621848811, \"source\": 68, \"method\": \"meetings\", \"target\": 83, \"weight\": 1, \"y\": 0.027941958673130613, \"pair\": [68, 83], \"edge\": 295}, {\"x\": 0.10847349281750544, \"source\": 68, \"method\": \"meetings\", \"target\": 83, \"weight\": 1, \"y\": -0.057042196238195696, \"pair\": [68, 83], \"edge\": 295}, {\"x\": 0.03471850621848811, \"source\": 68, \"method\": \"meetings\", \"target\": 89, \"weight\": 7, \"y\": 0.027941958673130613, \"pair\": [68, 89], \"edge\": 296}, {\"x\": 0.00986576584186687, \"source\": 68, \"method\": \"meetings\", \"target\": 89, \"weight\": 7, \"y\": 0.08031211042643539, \"pair\": [68, 89], \"edge\": 296}, {\"x\": 0.03471850621848811, \"source\": 68, \"method\": \"phonecalls\", \"target\": 97, \"weight\": 1, \"y\": 0.027941958673130613, \"pair\": [68, 97], \"edge\": 297}, {\"x\": 0.03141336674461058, \"source\": 68, \"method\": \"phonecalls\", \"target\": 97, \"weight\": 1, \"y\": 0.062183868103891184, \"pair\": [68, 97], \"edge\": 297}, {\"x\": 0.03471850621848811, \"source\": 68, \"method\": \"phonecalls\", \"target\": 149, \"weight\": 1, \"y\": 0.027941958673130613, \"pair\": [68, 149], \"edge\": 298}, {\"x\": 0.11508266807083972, \"source\": 68, \"method\": \"phonecalls\", \"target\": 149, \"weight\": 1, \"y\": -0.02696447573610774, \"pair\": [68, 149], \"edge\": 298}, {\"x\": 0.03471850621848811, \"source\": 68, \"method\": \"phonecalls\", \"target\": 150, \"weight\": 1, \"y\": 0.027941958673130613, \"pair\": [68, 150], \"edge\": 299}, {\"x\": 0.13262657088278995, \"source\": 68, \"method\": \"phonecalls\", \"target\": 150, \"weight\": 1, \"y\": 0.015813517971194884, \"pair\": [68, 150], \"edge\": 299}, {\"x\": 0.03471850621848811, \"source\": 68, \"method\": \"phonecalls\", \"target\": 151, \"weight\": 1, \"y\": 0.027941958673130613, \"pair\": [68, 151], \"edge\": 300}, {\"x\": 0.04684013351762619, \"source\": 68, \"method\": \"phonecalls\", \"target\": 151, \"weight\": 1, \"y\": 0.11062064395722294, \"pair\": [68, 151], \"edge\": 300}, {\"x\": 0.03471850621848811, \"source\": 68, \"method\": \"phonecalls\", \"target\": 152, \"weight\": 1, \"y\": 0.027941958673130613, \"pair\": [68, 152], \"edge\": 301}, {\"x\": 0.12894303743924734, \"source\": 68, \"method\": \"phonecalls\", \"target\": 152, \"weight\": 1, \"y\": -0.006738005547372787, \"pair\": [68, 152], \"edge\": 301}, {\"x\": -0.05404415963374475, \"source\": 69, \"method\": \"phonecalls\", \"target\": 77, \"weight\": 1, \"y\": 0.0555737666564064, \"pair\": [69, 77], \"edge\": 302}, {\"x\": -0.1674280211177842, \"source\": 69, \"method\": \"phonecalls\", \"target\": 77, \"weight\": 1, \"y\": -0.012273254905121884, \"pair\": [69, 77], \"edge\": 302}, {\"x\": 0.011819622080482078, \"source\": 70, \"method\": \"meetings\", \"target\": 89, \"weight\": 3, \"y\": 0.029445565255388533, \"pair\": [70, 89], \"edge\": 303}, {\"x\": 0.00986576584186687, \"source\": 70, \"method\": \"meetings\", \"target\": 89, \"weight\": 3, \"y\": 0.08031211042643539, \"pair\": [70, 89], \"edge\": 303}, {\"x\": 0.011819622080482078, \"source\": 70, \"method\": \"meetings\", \"target\": 93, \"weight\": 2, \"y\": 0.029445565255388533, \"pair\": [70, 93], \"edge\": 304}, {\"x\": 0.027239467393576043, \"source\": 70, \"method\": \"meetings\", \"target\": 93, \"weight\": 2, \"y\": 0.08938023744814323, \"pair\": [70, 93], \"edge\": 304}, {\"x\": 0.011819622080482078, \"source\": 70, \"method\": \"meetings\", \"target\": 97, \"weight\": 1, \"y\": 0.029445565255388533, \"pair\": [70, 97], \"edge\": 305}, {\"x\": 0.03141336674461058, \"source\": 70, \"method\": \"meetings\", \"target\": 97, \"weight\": 1, \"y\": 0.062183868103891184, \"pair\": [70, 97], \"edge\": 305}, {\"x\": 0.5115868445874617, \"source\": 73, \"method\": \"meetings\", \"target\": 74, \"weight\": 1, \"y\": 0.9261996655449791, \"pair\": [73, 74], \"edge\": 306}, {\"x\": 0.4883291644532202, \"source\": 73, \"method\": \"meetings\", \"target\": 74, \"weight\": 1, \"y\": 0.8842431905126642, \"pair\": [73, 74], \"edge\": 306}, {\"x\": -0.15725665594995872, \"source\": 75, \"method\": \"meetings\", \"target\": 76, \"weight\": 1, \"y\": -0.06183290734652462, \"pair\": [75, 76], \"edge\": 307}, {\"x\": -0.13440995012367013, \"source\": 75, \"method\": \"meetings\", \"target\": 76, \"weight\": 1, \"y\": -0.035573320781739935, \"pair\": [75, 76], \"edge\": 307}, {\"x\": -0.15725665594995872, \"source\": 75, \"method\": \"meetings\", \"target\": 77, \"weight\": 1, \"y\": -0.06183290734652462, \"pair\": [75, 77], \"edge\": 308}, {\"x\": -0.1674280211177842, \"source\": 75, \"method\": \"meetings\", \"target\": 77, \"weight\": 1, \"y\": -0.012273254905121884, \"pair\": [75, 77], \"edge\": 308}, {\"x\": -0.15725665594995872, \"source\": 75, \"method\": \"phonecalls\", \"target\": 112, \"weight\": 5, \"y\": -0.06183290734652462, \"pair\": [75, 112], \"edge\": 309}, {\"x\": -0.20270155781575283, \"source\": 75, \"method\": \"phonecalls\", \"target\": 112, \"weight\": 5, \"y\": -0.08330058587597301, \"pair\": [75, 112], \"edge\": 309}, {\"x\": -0.15725665594995872, \"source\": 75, \"method\": \"phonecalls\", \"target\": 113, \"weight\": 3, \"y\": -0.06183290734652462, \"pair\": [75, 113], \"edge\": 310}, {\"x\": -0.21525766957998949, \"source\": 75, \"method\": \"phonecalls\", \"target\": 113, \"weight\": 3, \"y\": -0.10051157056248572, \"pair\": [75, 113], \"edge\": 310}, {\"x\": -0.15725665594995872, \"source\": 75, \"method\": \"phonecalls\", \"target\": 114, \"weight\": 2, \"y\": -0.06183290734652462, \"pair\": [75, 114], \"edge\": 311}, {\"x\": -0.2276345604322558, \"source\": 75, \"method\": \"phonecalls\", \"target\": 114, \"weight\": 2, \"y\": -0.056471172319069785, \"pair\": [75, 114], \"edge\": 311}, {\"x\": -0.15725665594995872, \"source\": 75, \"method\": \"phonecalls\", \"target\": 115, \"weight\": 1, \"y\": -0.06183290734652462, \"pair\": [75, 115], \"edge\": 312}, {\"x\": -0.2507342385511679, \"source\": 75, \"method\": \"phonecalls\", \"target\": 115, \"weight\": 1, \"y\": -0.13008608621181492, \"pair\": [75, 115], \"edge\": 312}, {\"x\": -0.15725665594995872, \"source\": 75, \"method\": \"phonecalls\", \"target\": 116, \"weight\": 1, \"y\": -0.06183290734652462, \"pair\": [75, 116], \"edge\": 313}, {\"x\": -0.2657705605074746, \"source\": 75, \"method\": \"phonecalls\", \"target\": 116, \"weight\": 1, \"y\": -0.10237814066108729, \"pair\": [75, 116], \"edge\": 313}, {\"x\": -0.13440995012367013, \"source\": 76, \"method\": \"meetings\", \"target\": 77, \"weight\": 2, \"y\": -0.035573320781739935, \"pair\": [76, 77], \"edge\": 314}, {\"x\": -0.1674280211177842, \"source\": 76, \"method\": \"meetings\", \"target\": 77, \"weight\": 2, \"y\": -0.012273254905121884, \"pair\": [76, 77], \"edge\": 314}, {\"x\": -0.13440995012367013, \"source\": 76, \"method\": \"meetings\", \"target\": 78, \"weight\": 1, \"y\": -0.035573320781739935, \"pair\": [76, 78], \"edge\": 315}, {\"x\": -0.1796591076086675, \"source\": 76, \"method\": \"meetings\", \"target\": 78, \"weight\": 1, \"y\": -0.10029407843374245, \"pair\": [76, 78], \"edge\": 315}, {\"x\": -0.13440995012367013, \"source\": 76, \"method\": \"meetings\", \"target\": 98, \"weight\": 1, \"y\": -0.035573320781739935, \"pair\": [76, 98], \"edge\": 316}, {\"x\": -0.1373101562119631, \"source\": 76, \"method\": \"meetings\", \"target\": 98, \"weight\": 1, \"y\": -0.005371636152092576, \"pair\": [76, 98], \"edge\": 316}, {\"x\": -0.1674280211177842, \"source\": 77, \"method\": \"phonecalls\", \"target\": 137, \"weight\": 1, \"y\": -0.012273254905121884, \"pair\": [77, 137], \"edge\": 317}, {\"x\": -0.27997890602541675, \"source\": 77, \"method\": \"phonecalls\", \"target\": 137, \"weight\": 1, \"y\": -0.011526531863413061, \"pair\": [77, 137], \"edge\": 317}, {\"x\": -0.1674280211177842, \"source\": 77, \"method\": \"phonecalls\", \"target\": 114, \"weight\": 1, \"y\": -0.012273254905121884, \"pair\": [77, 114], \"edge\": 318}, {\"x\": -0.2276345604322558, \"source\": 77, \"method\": \"phonecalls\", \"target\": 114, \"weight\": 1, \"y\": -0.056471172319069785, \"pair\": [77, 114], \"edge\": 318}, {\"x\": 0.12897107520534695, \"source\": 79, \"method\": \"meetings\", \"target\": 80, \"weight\": 1, \"y\": -0.1105890148315894, \"pair\": [79, 80], \"edge\": 319}, {\"x\": 0.11968141191543366, \"source\": 79, \"method\": \"meetings\", \"target\": 80, \"weight\": 1, \"y\": -0.17155214344222655, \"pair\": [79, 80], \"edge\": 319}, {\"x\": 0.12897107520534695, \"source\": 79, \"method\": \"meetings\", \"target\": 81, \"weight\": 1, \"y\": -0.1105890148315894, \"pair\": [79, 81], \"edge\": 320}, {\"x\": 0.17195002667354403, \"source\": 79, \"method\": \"meetings\", \"target\": 81, \"weight\": 1, \"y\": -0.19676009520547832, \"pair\": [79, 81], \"edge\": 320}, {\"x\": 0.12897107520534695, \"source\": 79, \"method\": \"meetings\", \"target\": 82, \"weight\": 1, \"y\": -0.1105890148315894, \"pair\": [79, 82], \"edge\": 321}, {\"x\": 0.18326963790906847, \"source\": 79, \"method\": \"meetings\", \"target\": 82, \"weight\": 1, \"y\": -0.18375314690383282, \"pair\": [79, 82], \"edge\": 321}, {\"x\": 0.12897107520534695, \"source\": 79, \"method\": \"meetings\", \"target\": 83, \"weight\": 1, \"y\": -0.1105890148315894, \"pair\": [79, 83], \"edge\": 322}, {\"x\": 0.10847349281750544, \"source\": 79, \"method\": \"meetings\", \"target\": 83, \"weight\": 1, \"y\": -0.057042196238195696, \"pair\": [79, 83], \"edge\": 322}, {\"x\": 0.11968141191543366, \"source\": 80, \"method\": \"meetings\", \"target\": 81, \"weight\": 1, \"y\": -0.17155214344222655, \"pair\": [80, 81], \"edge\": 323}, {\"x\": 0.17195002667354403, \"source\": 80, \"method\": \"meetings\", \"target\": 81, \"weight\": 1, \"y\": -0.19676009520547832, \"pair\": [80, 81], \"edge\": 323}, {\"x\": 0.11968141191543366, \"source\": 80, \"method\": \"meetings\", \"target\": 82, \"weight\": 1, \"y\": -0.17155214344222655, \"pair\": [80, 82], \"edge\": 324}, {\"x\": 0.18326963790906847, \"source\": 80, \"method\": \"meetings\", \"target\": 82, \"weight\": 1, \"y\": -0.18375314690383282, \"pair\": [80, 82], \"edge\": 324}, {\"x\": 0.17195002667354403, \"source\": 81, \"method\": \"meetings\", \"target\": 82, \"weight\": 1, \"y\": -0.19676009520547832, \"pair\": [81, 82], \"edge\": 325}, {\"x\": 0.18326963790906847, \"source\": 81, \"method\": \"meetings\", \"target\": 82, \"weight\": 1, \"y\": -0.18375314690383282, \"pair\": [81, 82], \"edge\": 325}, {\"x\": 0.21369006215689948, \"source\": 84, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": 0.1902963496879098, \"pair\": [84, 85], \"edge\": 326}, {\"x\": 0.19056353716074514, \"source\": 84, \"method\": \"meetings\", \"target\": 85, \"weight\": 1, \"y\": 0.15642328640415717, \"pair\": [84, 85], \"edge\": 326}, {\"x\": 0.21369006215689948, \"source\": 84, \"method\": \"meetings\", \"target\": 86, \"weight\": 1, \"y\": 0.1902963496879098, \"pair\": [84, 86], \"edge\": 327}, {\"x\": 0.1983824078622018, \"source\": 84, \"method\": \"meetings\", \"target\": 86, \"weight\": 1, \"y\": 0.19598317965489814, \"pair\": [84, 86], \"edge\": 327}, {\"x\": 0.21369006215689948, \"source\": 84, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": 0.1902963496879098, \"pair\": [84, 87], \"edge\": 328}, {\"x\": 0.22441207019825102, \"source\": 84, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": 0.17847843085648182, \"pair\": [84, 87], \"edge\": 328}, {\"x\": 0.19056353716074514, \"source\": 85, \"method\": \"meetings\", \"target\": 86, \"weight\": 1, \"y\": 0.15642328640415717, \"pair\": [85, 86], \"edge\": 329}, {\"x\": 0.1983824078622018, \"source\": 85, \"method\": \"meetings\", \"target\": 86, \"weight\": 1, \"y\": 0.19598317965489814, \"pair\": [85, 86], \"edge\": 329}, {\"x\": 0.19056353716074514, \"source\": 85, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": 0.15642328640415717, \"pair\": [85, 87], \"edge\": 330}, {\"x\": 0.22441207019825102, \"source\": 85, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": 0.17847843085648182, \"pair\": [85, 87], \"edge\": 330}, {\"x\": 0.1983824078622018, \"source\": 86, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": 0.19598317965489814, \"pair\": [86, 87], \"edge\": 331}, {\"x\": 0.22441207019825102, \"source\": 86, \"method\": \"meetings\", \"target\": 87, \"weight\": 1, \"y\": 0.17847843085648182, \"pair\": [86, 87], \"edge\": 331}, {\"x\": 0.00986576584186687, \"source\": 89, \"method\": \"meetings\", \"target\": 93, \"weight\": 2, \"y\": 0.08031211042643539, \"pair\": [89, 93], \"edge\": 332}, {\"x\": 0.027239467393576043, \"source\": 89, \"method\": \"meetings\", \"target\": 93, \"weight\": 2, \"y\": 0.08938023744814323, \"pair\": [89, 93], \"edge\": 332}, {\"x\": 0.00986576584186687, \"source\": 89, \"method\": \"meetings\", \"target\": 94, \"weight\": 1, \"y\": 0.08031211042643539, \"pair\": [89, 94], \"edge\": 333}, {\"x\": -0.014229000506406199, \"source\": 89, \"method\": \"meetings\", \"target\": 94, \"weight\": 1, \"y\": 0.15649185613013583, \"pair\": [89, 94], \"edge\": 333}, {\"x\": 0.00986576584186687, \"source\": 89, \"method\": \"meetings\", \"target\": 95, \"weight\": 1, \"y\": 0.08031211042643539, \"pair\": [89, 95], \"edge\": 334}, {\"x\": 0.03684315176773381, \"source\": 89, \"method\": \"meetings\", \"target\": 95, \"weight\": 1, \"y\": 0.13846405716536056, \"pair\": [89, 95], \"edge\": 334}, {\"x\": 0.16440400899713334, \"source\": 91, \"method\": \"meetings\", \"target\": 92, \"weight\": 1, \"y\": -0.024083091381361547, \"pair\": [91, 92], \"edge\": 335}, {\"x\": 0.15937579571068833, \"source\": 91, \"method\": \"meetings\", \"target\": 92, \"weight\": 1, \"y\": -0.04195568206221384, \"pair\": [91, 92], \"edge\": 335}, {\"x\": 0.027239467393576043, \"source\": 93, \"method\": \"meetings\", \"target\": 95, \"weight\": 1, \"y\": 0.08938023744814323, \"pair\": [93, 95], \"edge\": 336}, {\"x\": 0.03684315176773381, \"source\": 93, \"method\": \"meetings\", \"target\": 95, \"weight\": 1, \"y\": 0.13846405716536056, \"pair\": [93, 95], \"edge\": 336}, {\"x\": -0.03800130853345794, \"source\": 99, \"method\": \"phonecalls\", \"target\": 118, \"weight\": 3, \"y\": -0.09253436619166476, \"pair\": [99, 118], \"edge\": 337}, {\"x\": -0.03900210541452407, \"source\": 99, \"method\": \"phonecalls\", \"target\": 118, \"weight\": 3, \"y\": -0.1499724806300101, \"pair\": [99, 118], \"edge\": 337}, {\"x\": 0.3782975408220712, \"source\": 106, \"method\": \"phonecalls\", \"target\": 107, \"weight\": 1, \"y\": -0.9585953960927783, \"pair\": [106, 107], \"edge\": 338}, {\"x\": 0.3960108209369903, \"source\": 106, \"method\": \"phonecalls\", \"target\": 107, \"weight\": 1, \"y\": -1.0, \"pair\": [106, 107], \"edge\": 338}, {\"x\": -0.21748147572590495, \"source\": 109, \"method\": \"phonecalls\", \"target\": 110, \"weight\": 1, \"y\": 0.09593019893154699, \"pair\": [109, 110], \"edge\": 339}, {\"x\": -0.16554649083433273, \"source\": 109, \"method\": \"phonecalls\", \"target\": 110, \"weight\": 1, \"y\": 0.0613612565078103, \"pair\": [109, 110], \"edge\": 339}, {\"x\": -0.21748147572590495, \"source\": 109, \"method\": \"phonecalls\", \"target\": 111, \"weight\": 1, \"y\": 0.09593019893154699, \"pair\": [109, 111], \"edge\": 340}, {\"x\": -0.21839355981992586, \"source\": 109, \"method\": \"phonecalls\", \"target\": 111, \"weight\": 1, \"y\": 0.19133920636625884, \"pair\": [109, 111], \"edge\": 340}, {\"x\": 0.23861229243734594, \"source\": 143, \"method\": \"phonecalls\", \"target\": 144, \"weight\": 1, \"y\": 0.12480818117104757, \"pair\": [143, 144], \"edge\": 341}, {\"x\": 0.2634363623209933, \"source\": 143, \"method\": \"phonecalls\", \"target\": 144, \"weight\": 1, \"y\": 0.14861794413837914, \"pair\": [143, 144], \"edge\": 341}], \"data-34951df27a40796f893d648887092be9\": [{\"x\": -0.802093013012138, \"id\": 0, \"community\": \"4\", \"y\": -0.5824842235196528, \"is_central\": \"no\"}, {\"x\": -0.7645186779044639, \"id\": 1, \"community\": \"4\", \"y\": -0.5731788170630375, \"is_central\": \"no\"}, {\"x\": -0.7648805140133405, \"id\": 2, \"community\": \"4\", \"y\": -0.5468246324562787, \"is_central\": \"no\"}, {\"x\": 0.23860360308586037, \"id\": 3, \"community\": \"2\", \"y\": 0.08907969795281509, \"is_central\": \"no\"}, {\"x\": 0.28231284393317024, \"id\": 4, \"community\": \"2\", \"y\": 0.06144180346211253, \"is_central\": \"no\"}, {\"x\": 0.20924144653777046, \"id\": 5, \"community\": \"2\", \"y\": 0.07358121775706167, \"is_central\": \"no\"}, {\"x\": 0.21058049738425882, \"id\": 6, \"community\": \"2\", \"y\": 0.06513414641735353, \"is_central\": \"no\"}, {\"x\": 0.2735223829326983, \"id\": 7, \"community\": \"2\", \"y\": 0.07164451799613736, \"is_central\": \"no\"}, {\"x\": 0.2858410918363624, \"id\": 8, \"community\": \"2\", \"y\": 0.08009334352883232, \"is_central\": \"no\"}, {\"x\": 0.27856298881071917, \"id\": 9, \"community\": \"2\", \"y\": 0.09154594050975798, \"is_central\": \"no\"}, {\"x\": 0.20595742614062706, \"id\": 10, \"community\": \"2\", \"y\": 0.14588579669116095, \"is_central\": \"no\"}, {\"x\": 0.1602994927743264, \"id\": 11, \"community\": \"2\", \"y\": 0.09918589460426964, \"is_central\": \"no\"}, {\"x\": 0.14340048891091195, \"id\": 12, \"community\": \"2\", \"y\": 0.10340329927972684, \"is_central\": \"no\"}, {\"x\": 0.18663132449362335, \"id\": 13, \"community\": \"2\", \"y\": 0.13433733897838734, \"is_central\": \"no\"}, {\"x\": 0.18586204868298753, \"id\": 14, \"community\": \"2\", \"y\": 0.16749211517500212, \"is_central\": \"no\"}, {\"x\": 0.17573924264549753, \"id\": 15, \"community\": \"2\", \"y\": 0.14319322131666967, \"is_central\": \"no\"}, {\"x\": -0.07761935479169774, \"id\": 16, \"community\": \"5\", \"y\": -0.94000716765174, \"is_central\": \"no\"}, {\"x\": -0.07401406503186662, \"id\": 17, \"community\": \"5\", \"y\": -0.8965666719477022, \"is_central\": \"no\"}, {\"x\": -0.05612050668302144, \"id\": 18, \"community\": \"0\", \"y\": -0.01768164671407569, \"is_central\": \"yes\"}, {\"x\": -0.0386660772673068, \"id\": 19, \"community\": \"0\", \"y\": -0.05897078740977869, \"is_central\": \"no\"}, {\"x\": -0.06321557290680196, \"id\": 20, \"community\": \"0\", \"y\": -0.10507750890540686, \"is_central\": \"no\"}, {\"x\": 0.058197338132539224, \"id\": 21, \"community\": \"2\", \"y\": 0.04054154351798948, \"is_central\": \"no\"}, {\"x\": -0.10435783249725597, \"id\": 22, \"community\": \"0\", \"y\": -0.04194020355486868, \"is_central\": \"no\"}, {\"x\": -0.008948305588105252, \"id\": 23, \"community\": \"0\", \"y\": -0.05504141076884899, \"is_central\": \"no\"}, {\"x\": 0.05520866651359483, \"id\": 24, \"community\": \"0\", \"y\": -0.14337579861081434, \"is_central\": \"no\"}, {\"x\": 0.07314904598050838, \"id\": 25, \"community\": \"0\", \"y\": 0.018843802477964286, \"is_central\": \"no\"}, {\"x\": 0.08126935347307138, \"id\": 26, \"community\": \"0\", \"y\": 0.046609146158610254, \"is_central\": \"no\"}, {\"x\": -0.012391752374116128, \"id\": 27, \"community\": \"1\", \"y\": 0.05130248030961548, \"is_central\": \"no\"}, {\"x\": 0.16785441410916155, \"id\": 28, \"community\": \"2\", \"y\": 0.060067062225702864, \"is_central\": \"no\"}, {\"x\": -0.06656634161106342, \"id\": 29, \"community\": \"0\", \"y\": 0.02072802436799425, \"is_central\": \"no\"}, {\"x\": -0.13184619813604853, \"id\": 30, \"community\": \"0\", \"y\": 0.0239530046935114, \"is_central\": \"no\"}, {\"x\": -0.10927812066240415, \"id\": 31, \"community\": \"0\", \"y\": -0.08734890286142297, \"is_central\": \"no\"}, {\"x\": -0.07733357836765282, \"id\": 32, \"community\": \"0\", \"y\": -0.09150374006610085, \"is_central\": \"no\"}, {\"x\": -0.06601643841065934, \"id\": 33, \"community\": \"0\", \"y\": -0.0655494401056117, \"is_central\": \"no\"}, {\"x\": 0.0032855872505247415, \"id\": 34, \"community\": \"0\", \"y\": -0.021647749997618267, \"is_central\": \"no\"}, {\"x\": -0.01673381052039805, \"id\": 35, \"community\": \"0\", \"y\": -0.023753091559328095, \"is_central\": \"no\"}, {\"x\": 0.05734925339610061, \"id\": 36, \"community\": \"1\", \"y\": 0.010343123664195639, \"is_central\": \"no\"}, {\"x\": 0.09071083209925236, \"id\": 37, \"community\": \"3\", \"y\": -0.08992370202512684, \"is_central\": \"no\"}, {\"x\": -0.008018244466440517, \"id\": 38, \"community\": \"1\", \"y\": 0.10072445639961984, \"is_central\": \"no\"}, {\"x\": 0.04309348428219574, \"id\": 39, \"community\": \"1\", \"y\": 0.2053099652866237, \"is_central\": \"no\"}, {\"x\": 0.03993349451976024, \"id\": 40, \"community\": \"1\", \"y\": 0.2382904291101742, \"is_central\": \"no\"}, {\"x\": 0.060834102272328154, \"id\": 41, \"community\": \"1\", \"y\": 0.2963564369834344, \"is_central\": \"no\"}, {\"x\": 0.04211946342367626, \"id\": 42, \"community\": \"1\", \"y\": 0.2993487255455769, \"is_central\": \"no\"}, {\"x\": -0.03881531608244006, \"id\": 43, \"community\": \"1\", \"y\": 0.04540671878796461, \"is_central\": \"no\"}, {\"x\": -0.04924921034069172, \"id\": 44, \"community\": \"1\", \"y\": 0.102421470202217, \"is_central\": \"no\"}, {\"x\": 0.014345250393342512, \"id\": 45, \"community\": \"1\", \"y\": 0.09666990489701383, \"is_central\": \"no\"}, {\"x\": 0.06539720733271068, \"id\": 46, \"community\": \"1\", \"y\": 0.08609400131444722, \"is_central\": \"no\"}, {\"x\": 0.007108527109453355, \"id\": 47, \"community\": \"1\", \"y\": 0.11003438822517303, \"is_central\": \"yes\"}, {\"x\": 0.03284354606986613, \"id\": 48, \"community\": \"1\", \"y\": 0.10465010441704334, \"is_central\": \"no\"}, {\"x\": 0.06934001875360078, \"id\": 49, \"community\": \"1\", \"y\": 0.14488308920109902, \"is_central\": \"no\"}, {\"x\": 0.01908768168133293, \"id\": 50, \"community\": \"1\", \"y\": 0.14941301779317726, \"is_central\": \"no\"}, {\"x\": -0.022636151635812406, \"id\": 51, \"community\": \"1\", \"y\": 0.12274076369864406, \"is_central\": \"no\"}, {\"x\": -0.07810978486531617, \"id\": 52, \"community\": \"1\", \"y\": 0.2544256558594014, \"is_central\": \"no\"}, {\"x\": -0.08308573848455948, \"id\": 53, \"community\": \"1\", \"y\": 0.2214746864725049, \"is_central\": \"no\"}, {\"x\": -0.012499975810208244, \"id\": 54, \"community\": \"1\", \"y\": 0.0743061072952657, \"is_central\": \"no\"}, {\"x\": 0.05452234251722276, \"id\": 55, \"community\": \"1\", \"y\": 0.160441589610005, \"is_central\": \"no\"}, {\"x\": 0.09412333274359878, \"id\": 56, \"community\": \"1\", \"y\": 0.08676444262522966, \"is_central\": \"no\"}, {\"x\": 0.19750003762175775, \"id\": 57, \"community\": \"1\", \"y\": 0.04815107032506278, \"is_central\": \"no\"}, {\"x\": -0.18241099698058447, \"id\": 58, \"community\": \"0\", \"y\": 0.032024336890226164, \"is_central\": \"no\"}, {\"x\": -0.1559017209514067, \"id\": 59, \"community\": \"0\", \"y\": 0.021592614097687188, \"is_central\": \"no\"}, {\"x\": -0.12520490671870255, \"id\": 60, \"community\": \"0\", \"y\": -0.07745856899409895, \"is_central\": \"no\"}, {\"x\": 0.0048703552552857585, \"id\": 61, \"community\": \"3\", \"y\": -0.11040957072110401, \"is_central\": \"yes\"}, {\"x\": 0.060586395520634266, \"id\": 62, \"community\": \"3\", \"y\": -0.2107327427687022, \"is_central\": \"no\"}, {\"x\": -0.11572944960277982, \"id\": 63, \"community\": \"0\", \"y\": -0.023077593783232188, \"is_central\": \"no\"}, {\"x\": -0.08203963210578026, \"id\": 64, \"community\": \"0\", \"y\": -0.006091601590310482, \"is_central\": \"no\"}, {\"x\": -0.1702165144146252, \"id\": 65, \"community\": \"0\", \"y\": -0.001404905897743331, \"is_central\": \"no\"}, {\"x\": -0.022640902960118864, \"id\": 66, \"community\": \"3\", \"y\": -0.11038841407886545, \"is_central\": \"no\"}, {\"x\": 0.042361526319305065, \"id\": 67, \"community\": \"3\", \"y\": -0.13333806121176184, \"is_central\": \"no\"}, {\"x\": 0.03471850621848811, \"id\": 68, \"community\": \"1\", \"y\": 0.027941958673130613, \"is_central\": \"no\"}, {\"x\": -0.05404415963374475, \"id\": 69, \"community\": \"1\", \"y\": 0.0555737666564064, \"is_central\": \"no\"}, {\"x\": 0.011819622080482078, \"id\": 70, \"community\": \"1\", \"y\": 0.029445565255388533, \"is_central\": \"no\"}, {\"x\": -0.10834854337472752, \"id\": 71, \"community\": \"0\", \"y\": 0.009031121091285323, \"is_central\": \"no\"}, {\"x\": -0.1685060948873108, \"id\": 72, \"community\": \"0\", \"y\": -0.05601789641213182, \"is_central\": \"no\"}, {\"x\": 0.5115868445874617, \"id\": 73, \"community\": \"6\", \"y\": 0.9261996655449791, \"is_central\": \"no\"}, {\"x\": 0.4883291644532202, \"id\": 74, \"community\": \"6\", \"y\": 0.8842431905126642, \"is_central\": \"no\"}, {\"x\": -0.15725665594995872, \"id\": 75, \"community\": \"0\", \"y\": -0.06183290734652462, \"is_central\": \"no\"}, {\"x\": -0.13440995012367013, \"id\": 76, \"community\": \"0\", \"y\": -0.035573320781739935, \"is_central\": \"no\"}, {\"x\": -0.1674280211177842, \"id\": 77, \"community\": \"0\", \"y\": -0.012273254905121884, \"is_central\": \"no\"}, {\"x\": -0.1796591076086675, \"id\": 78, \"community\": \"0\", \"y\": -0.10029407843374245, \"is_central\": \"no\"}, {\"x\": 0.12897107520534695, \"id\": 79, \"community\": \"3\", \"y\": -0.1105890148315894, \"is_central\": \"no\"}, {\"x\": 0.11968141191543366, \"id\": 80, \"community\": \"3\", \"y\": -0.17155214344222655, \"is_central\": \"no\"}, {\"x\": 0.17195002667354403, \"id\": 81, \"community\": \"3\", \"y\": -0.19676009520547832, \"is_central\": \"no\"}, {\"x\": 0.18326963790906847, \"id\": 82, \"community\": \"3\", \"y\": -0.18375314690383282, \"is_central\": \"no\"}, {\"x\": 0.10847349281750544, \"id\": 83, \"community\": \"3\", \"y\": -0.057042196238195696, \"is_central\": \"no\"}, {\"x\": 0.21369006215689948, \"id\": 84, \"community\": \"2\", \"y\": 0.1902963496879098, \"is_central\": \"no\"}, {\"x\": 0.19056353716074514, \"id\": 85, \"community\": \"2\", \"y\": 0.15642328640415717, \"is_central\": \"no\"}, {\"x\": 0.1983824078622018, \"id\": 86, \"community\": \"2\", \"y\": 0.19598317965489814, \"is_central\": \"no\"}, {\"x\": 0.22441207019825102, \"id\": 87, \"community\": \"2\", \"y\": 0.17847843085648182, \"is_central\": \"no\"}, {\"x\": -0.07809806699308035, \"id\": 88, \"community\": \"0\", \"y\": -0.14026940956002884, \"is_central\": \"no\"}, {\"x\": 0.00986576584186687, \"id\": 89, \"community\": \"1\", \"y\": 0.08031211042643539, \"is_central\": \"no\"}, {\"x\": 0.005731416596121094, \"id\": 90, \"community\": \"1\", \"y\": 0.17761324046325702, \"is_central\": \"no\"}, {\"x\": 0.16440400899713334, \"id\": 91, \"community\": \"1\", \"y\": -0.024083091381361547, \"is_central\": \"no\"}, {\"x\": 0.15937579571068833, \"id\": 92, \"community\": \"1\", \"y\": -0.04195568206221384, \"is_central\": \"no\"}, {\"x\": 0.027239467393576043, \"id\": 93, \"community\": \"1\", \"y\": 0.08938023744814323, \"is_central\": \"no\"}, {\"x\": -0.014229000506406199, \"id\": 94, \"community\": \"1\", \"y\": 0.15649185613013583, \"is_central\": \"no\"}, {\"x\": 0.03684315176773381, \"id\": 95, \"community\": \"1\", \"y\": 0.13846405716536056, \"is_central\": \"no\"}, {\"x\": -0.05061792141094029, \"id\": 96, \"community\": \"1\", \"y\": 0.21777227600247498, \"is_central\": \"no\"}, {\"x\": 0.03141336674461058, \"id\": 97, \"community\": \"1\", \"y\": 0.062183868103891184, \"is_central\": \"no\"}, {\"x\": -0.1373101562119631, \"id\": 98, \"community\": \"0\", \"y\": -0.005371636152092576, \"is_central\": \"no\"}, {\"x\": -0.03800130853345794, \"id\": 99, \"community\": \"0\", \"y\": -0.09253436619166476, \"is_central\": \"no\"}, {\"x\": 0.023332033262655517, \"id\": 100, \"community\": \"0\", \"y\": -0.040032722379539636, \"is_central\": \"no\"}, {\"x\": 0.22787150157572195, \"id\": 101, \"community\": \"2\", \"y\": 0.23794535711803252, \"is_central\": \"no\"}, {\"x\": -0.09926376548922529, \"id\": 102, \"community\": \"0\", \"y\": -0.12170590325524416, \"is_central\": \"no\"}, {\"x\": -0.17606769760215724, \"id\": 103, \"community\": \"0\", \"y\": 0.015576909333247556, \"is_central\": \"no\"}, {\"x\": -0.17409306862461033, \"id\": 104, \"community\": \"0\", \"y\": -0.031377524456192796, \"is_central\": \"no\"}, {\"x\": 0.039903417854644616, \"id\": 105, \"community\": \"3\", \"y\": -0.17873055666970827, \"is_central\": \"no\"}, {\"x\": 0.3782975408220712, \"id\": 106, \"community\": \"7\", \"y\": -0.9585953960927783, \"is_central\": \"no\"}, {\"x\": 0.3960108209369903, \"id\": 107, \"community\": \"7\", \"y\": -1.0, \"is_central\": \"no\"}, {\"x\": -0.14847023407178417, \"id\": 108, \"community\": \"0\", \"y\": 0.04412350134247153, \"is_central\": \"no\"}, {\"x\": -0.21748147572590495, \"id\": 109, \"community\": \"0\", \"y\": 0.09593019893154699, \"is_central\": \"no\"}, {\"x\": -0.16554649083433273, \"id\": 110, \"community\": \"0\", \"y\": 0.0613612565078103, \"is_central\": \"no\"}, {\"x\": -0.21839355981992586, \"id\": 111, \"community\": \"0\", \"y\": 0.19133920636625884, \"is_central\": \"no\"}, {\"x\": -0.20270155781575283, \"id\": 112, \"community\": \"0\", \"y\": -0.08330058587597301, \"is_central\": \"no\"}, {\"x\": -0.21525766957998949, \"id\": 113, \"community\": \"0\", \"y\": -0.10051157056248572, \"is_central\": \"no\"}, {\"x\": -0.2276345604322558, \"id\": 114, \"community\": \"0\", \"y\": -0.056471172319069785, \"is_central\": \"no\"}, {\"x\": -0.2507342385511679, \"id\": 115, \"community\": \"0\", \"y\": -0.13008608621181492, \"is_central\": \"no\"}, {\"x\": -0.2657705605074746, \"id\": 116, \"community\": \"0\", \"y\": -0.10237814066108729, \"is_central\": \"no\"}, {\"x\": -0.00444784078162752, \"id\": 117, \"community\": \"3\", \"y\": -0.18765976946808133, \"is_central\": \"no\"}, {\"x\": -0.03900210541452407, \"id\": 118, \"community\": \"0\", \"y\": -0.1499724806300101, \"is_central\": \"no\"}, {\"x\": -0.0915877212667124, \"id\": 119, \"community\": \"1\", \"y\": 0.11701018058264209, \"is_central\": \"no\"}, {\"x\": -0.1275593951692764, \"id\": 120, \"community\": \"1\", \"y\": 0.11875604489352103, \"is_central\": \"no\"}, {\"x\": 0.06311721348021446, \"id\": 121, \"community\": \"1\", \"y\": 0.20757439495810204, \"is_central\": \"no\"}, {\"x\": -0.01484911107013809, \"id\": 122, \"community\": \"1\", \"y\": 0.22357741515042764, \"is_central\": \"no\"}, {\"x\": 0.006382036204762352, \"id\": 123, \"community\": \"1\", \"y\": 0.22084645561869615, \"is_central\": \"no\"}, {\"x\": -0.11589057331876794, \"id\": 124, \"community\": \"1\", \"y\": 0.3497996671693867, \"is_central\": \"no\"}, {\"x\": 0.012050096233286714, \"id\": 125, \"community\": \"1\", \"y\": 0.012727612759410905, \"is_central\": \"no\"}, {\"x\": -0.08725800601701894, \"id\": 126, \"community\": \"1\", \"y\": 0.13847821207887648, \"is_central\": \"no\"}, {\"x\": -0.1536562263982753, \"id\": 127, \"community\": \"0\", \"y\": -0.08360286417691555, \"is_central\": \"no\"}, {\"x\": 0.01793498624792611, \"id\": 128, \"community\": \"3\", \"y\": -0.1857666690597249, \"is_central\": \"no\"}, {\"x\": -0.0013334346347747073, \"id\": 129, \"community\": \"3\", \"y\": -0.229445870323091, \"is_central\": \"no\"}, {\"x\": 0.019788470465907235, \"id\": 130, \"community\": \"3\", \"y\": -0.22757867641728233, \"is_central\": \"no\"}, {\"x\": 0.016315752571887842, \"id\": 131, \"community\": \"3\", \"y\": -0.15829498963549138, \"is_central\": \"no\"}, {\"x\": -0.133672498611843, \"id\": 132, \"community\": \"0\", \"y\": -0.11022599588864794, \"is_central\": \"no\"}, {\"x\": -0.023772059143059947, \"id\": 133, \"community\": \"3\", \"y\": -0.22515184985888426, \"is_central\": \"no\"}, {\"x\": 0.040932892363857123, \"id\": 134, \"community\": \"3\", \"y\": -0.22412387161229547, \"is_central\": \"no\"}, {\"x\": 0.03428521848756629, \"id\": 135, \"community\": \"1\", \"y\": 0.21961471482378736, \"is_central\": \"no\"}, {\"x\": 0.05032606131256557, \"id\": 136, \"community\": \"1\", \"y\": -0.015961203093154453, \"is_central\": \"no\"}, {\"x\": -0.27997890602541675, \"id\": 137, \"community\": \"0\", \"y\": -0.011526531863413061, \"is_central\": \"no\"}, {\"x\": -0.06727209544701435, \"id\": 138, \"community\": \"1\", \"y\": 0.140436626954799, \"is_central\": \"no\"}, {\"x\": -0.10667639502577143, \"id\": 139, \"community\": \"0\", \"y\": 0.07638231999526468, \"is_central\": \"no\"}, {\"x\": -0.17836078870967892, \"id\": 140, \"community\": \"0\", \"y\": -0.049201881466319465, \"is_central\": \"no\"}, {\"x\": -0.08111005054221812, \"id\": 141, \"community\": \"0\", \"y\": -0.03890183331178787, \"is_central\": \"no\"}, {\"x\": -0.04413909119047113, \"id\": 142, \"community\": \"3\", \"y\": -0.1607756139326318, \"is_central\": \"no\"}, {\"x\": 0.23861229243734594, \"id\": 143, \"community\": \"2\", \"y\": 0.12480818117104757, \"is_central\": \"no\"}, {\"x\": 0.2634363623209933, \"id\": 144, \"community\": \"2\", \"y\": 0.14861794413837914, \"is_central\": \"no\"}, {\"x\": 0.22644213244410646, \"id\": 145, \"community\": \"2\", \"y\": 0.13441820924980916, \"is_central\": \"no\"}, {\"x\": -0.11752748960995511, \"id\": 146, \"community\": \"0\", \"y\": -0.12158857212955436, \"is_central\": \"no\"}, {\"x\": -0.03145493629479878, \"id\": 147, \"community\": \"1\", \"y\": 0.20903569392972257, \"is_central\": \"no\"}, {\"x\": 0.1464062231275505, \"id\": 148, \"community\": \"1\", \"y\": -0.059838364010903235, \"is_central\": \"no\"}, {\"x\": 0.11508266807083972, \"id\": 149, \"community\": \"1\", \"y\": -0.02696447573610774, \"is_central\": \"no\"}, {\"x\": 0.13262657088278995, \"id\": 150, \"community\": \"1\", \"y\": 0.015813517971194884, \"is_central\": \"no\"}, {\"x\": 0.04684013351762619, \"id\": 151, \"community\": \"1\", \"y\": 0.11062064395722294, \"is_central\": \"no\"}, {\"x\": 0.12894303743924734, \"id\": 152, \"community\": \"1\", \"y\": -0.006738005547372787, \"is_central\": \"no\"}, {\"x\": -0.1872731340151251, \"id\": 153, \"community\": \"0\", \"y\": -0.010370550444382997, \"is_central\": \"no\"}]}};\n",
      "    var embedOpt5 = {\"mode\": \"vega-lite\"};\n",
      "\n",
      "    function showError(el5, error){\n",
      "        el5.innerHTML = ('<div class=\"error\" style=\"color:red;\">'\n",
      "                        + '<p>JavaScript Error: ' + error.message + '</p>'\n",
      "                        + \"<p>This usually means there's a typo in your chart specification. \"\n",
      "                        + \"See the javascript console for the full traceback.</p>\"\n",
      "                        + '</div>');\n",
      "        throw error;\n",
      "    }\n",
      "    const el5 = document.getElementById('vis5');\n",
      "    vegaEmbed(\"#vis5\", spec5, embedOpt5)\n",
      "      .catch(error => showError(el5, error));\n",
      "  })(vegaEmbed);\n",
      "\n",
      "</script>\n",
      "\n",
      "8 communities have been detected, and the communities detected appear in various colors.\n",
      "\n",
      "# Network Disruption\n",
      "\n",
      "As we have seen, there are some key players, and communities that are larger than others. When disruption a criminal network,it is therefore important to arrest the right persons and avoid the network resilience.\n",
      "\n",
      "The authors use a metric to measure the network resilience:\n",
      "\n",
      "$$ \\rho = 1 - \\mid \\frac{LCC_{i}(G_i) - LCC_{0}(G_0)} {LCC_{0}(G_0)} \\mid $$\n",
      "\n",
      "Where $$ LCC $$ is the Largest Connected Component, a measure of the total number of nodes of the largest connected component. A connected component of an undirected graph is a subgraph in which any two vertices are connected to each other by paths, and which is connected to no additional vertices outside of this component. *In brief, this metric tracks by how much we damaged the LCC of the network by removing one or few nodes.*\n",
      "\n",
      "In the whole graph, the connected components can be computed as such:\n",
      "\n",
      "```python\n",
      "for x in nx.connected_components(G):\n",
      "    print(x)\n",
      "```\n",
      "\n",
      "```\n",
      "{0, 1, 2}\n",
      "{3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153}\n",
      "{16, 17}\n",
      "{73, 74}\n",
      "{106, 107}\n",
      "```\n",
      "\n",
      "Just like we can visually observe, there is one major connected component. We can see the number of nodes of each sub-graph using:\n",
      "\n",
      "```python\n",
      "for x in nx.connected_components(G):\n",
      "    print(nx.number_of_nodes(G.subgraph(x)))\n",
      "```\n",
      "\n",
      "```\n",
      "3\n",
      "145\n",
      "2\n",
      "2\n",
      "2\n",
      "```\n",
      "\n",
      "The LCC is therefore the max of those, which is 145.\n",
      "\n",
      "We can therefore define a function to look for the LCC size: \n",
      "\n",
      "```python\n",
      "def lcc_size(graph):\n",
      "\n",
      "    compsize = []\n",
      "    for c in nx.connected_components(graph):\n",
      "        compsize.append(nx.number_of_nodes(graph.subgraph(c)))\n",
      "    return max(compsize)\n",
      "```\n",
      "\n",
      "The idea behind the metric is to iteratively remove:\n",
      "- either the node with the highest collective influence, in a sequential manner\n",
      "- or the top 5 nodes with the highest collective influence, in a \"block\" manner\n",
      "\n",
      "And see how the metric evolves. If you want to learn more about collective influence, make sure to check [this article](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5080555/).\n",
      "\n",
      "To gradually remove the nodes (in a sequential manner), the authors propose a function:\n",
      "\n",
      "```python\n",
      "import array\n",
      "import copy\n",
      "\n",
      "toremove = array.array('i', [])\n",
      "nrem = 5\n",
      "names = ['id', 'data']\n",
      "formats = ['int32', 'int32']\n",
      "dtype = dict(names=names, formats=formats)\n",
      "\n",
      "def disruption_sequence(Graph, weight=None):\n",
      "\n",
      "    Gor = copy.deepcopy(Graph)\n",
      "    lccinit = lcc_size(Graph) \n",
      "    dflcc = pd.DataFrame()\n",
      "    dflccvar = pd.DataFrame()\n",
      "\n",
      "    dictx = dict()\n",
      "    dicty = dict()\n",
      "    kiter = 0\n",
      "    toremove = array.array('i', [])\n",
      "    \n",
      "    while Gor.number_of_nodes() > nrem:\n",
      "\n",
      "        i = 0\n",
      "\n",
      "        while i < nrem:\n",
      "            if Gor.number_of_nodes() <= nrem:\n",
      "                break\n",
      "\n",
      "            toremove = array.array('i', [])\n",
      "            dictx[kiter] = lcc_size(Gor)\n",
      "            dicty[kiter] = 1 - (abs((lcc_size(Gor) - lccinit) / lccinit))\n",
      "            toremove = collective_influence_centality(Gor, toremove, weight=weight)\n",
      "            \n",
      "            Gor.remove_node(toremove[0])\n",
      "            toremove.pop(0)\n",
      "            kiter += 1\n",
      "            i += 1\n",
      "            \n",
      "    dflcc['No'] = list(dictx.keys())\n",
      "    dflccvar['No'] = dflcc['No']\n",
      "    dflcc['centrality'] = list(dictx.values())\n",
      "    dflccvar['centrality'] = list(dicty.values())\n",
      "    \n",
      "    return dflcc, dflccvar\n",
      "\n",
      "dflcc, dflccvar = disruption_sequence(G)\n",
      "```\n",
      "\n",
      "We store the LCC and the score of the network given the sequential disruption. We can then plot the result:\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(dflcc['No'], dflcc['centrality'])\n",
      "plt.title(\"LCC Size with sequential disruption\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/LCC.png)\n",
      "\n",
      "We can also plot the score described above:\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(dflccvar['No'], dflccvar['centrality'])\n",
      "plt.title(\"Centrality score with sequential disruption\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/var.png)\n",
      "\n",
      "For the block-removal, one can simply remove 5 nodes before re-computing the collective influence candidates to remove:\n",
      "\n",
      "```python\n",
      "def disruption_block(Graph, weight=None):\n",
      "\n",
      "    Gor = copy.deepcopy(Graph)\n",
      "    lccinit = lcc_size(Graph) \n",
      "    dflcc = pd.DataFrame()\n",
      "    dflccvar = pd.DataFrame()\n",
      "\n",
      "    dictx = dict()\n",
      "    dicty = dict()\n",
      "    kiter = 0\n",
      "    toremove = array.array('i', [])\n",
      "\n",
      "    while Gor.number_of_nodes() > nrem:\n",
      "        i = 0\n",
      "\n",
      "        toremove = array.array('i', [])\n",
      "        dictx[kiter] = lcc_size(Gor)\n",
      "        dicty[kiter] = 1 - (abs((lcc_size(Gor) - lccinit) / lccinit))\n",
      "        toremove = collective_influence_centality(Gor, toremove, weight=weight)\n",
      "        \n",
      "        while i < nrem:\n",
      "            if Gor.number_of_nodes() <= nrem:\n",
      "                break\n",
      "            Gor.remove_node(toremove[0])\n",
      "            toremove.pop(0)\n",
      "            kiter += 1\n",
      "            i += 1\n",
      "    \n",
      "    dflcc['No'] = list(dictx.keys())\n",
      "    dflccvar['No'] = dflcc['No']\n",
      "    dflcc['centrality'] = list(dictx.values())\n",
      "    dflccvar['centrality'] = list(dicty.values())\n",
      "    \n",
      "    return dflcc, dflccvar\n",
      "```\n",
      "\n",
      "And as before, we can compute the LCC and the score with the number of iterations:\n",
      "\n",
      "```python\n",
      "dflcc, dflccvar = disruption_block(G)\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(dflcc['centrality'])\n",
      "plt.title(\"LCC Size with block disruption\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/LCC2.png)\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(dflccvar['centrality'])\n",
      "plt.title(\"Centrality score with block disruption\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/var2.png)\n",
      "\n",
      "Note, the results are quite similar in terms when you compare the number of persons who had to be arrested. \n",
      "\n",
      "# Discussion\n",
      "\n",
      "The authors discuss that betweenness centrality is the best metric to apply (better than collective influence), given the focus it has on information bottleneck. 5% of the network is often responsible for more than 70% of the information (and this is real-world data !).\n",
      "\n",
      "The dataset provided is static (meaning that we do not know how the network will react to a disruption). Overall, the metric provided is interesting, and this kind of approach could be explored in criminal network investigations.\n",
      "---\n",
      "title: Conditions\n",
      "layout: post\n",
      "tags: [da_tuto]\n",
      "subtitle : \"Data Analysis Basics\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# If, Elif, Else\n",
      "\n",
      "In Python, there is a really easy way to execute some code only if a certain condition is met, with an if-statement.\n",
      "\n",
      "```python\n",
      "var_a = 2\n",
      "var_b = 5\n",
      "\n",
      "if var_a + var_b > 0:\n",
      "\tprint(\"Total is positive\")\n",
      "elif var_a + var_b == 0:\n",
      "\tprint(\"Total is 0\")\n",
      "else:\n",
      "\tprint(\"Total is negative\")\n",
      "```\n",
      "\n",
      "This code will check for the condition that var_a plus var_b is greater than 0. If it is the case, we print that the total is positive.\n",
      "\n",
      "At the second statement, we use the `elif` statement, meaning : \"else, if this condition is met, do that...\". Notice how to test for equality (and not assign a value), we do not use a single equal sign, but two. This is a simple thing to remember.\n",
      "\n",
      "Finally, we use the `else` statement for any other case.\n",
      "\n",
      "We could also have tested that the sum of both is greater or equal to 0.\n",
      "\n",
      "```python\n",
      "var_a = 2\n",
      "var_b = 5\n",
      "\n",
      "if var_a + var_b >= 0:\n",
      "\tprint(\"Total is greater or equal to 0\")\n",
      "else:\n",
      "\tprint(\"Total is negative\")\n",
      "```\n",
      "\n",
      "The \"greater or equal\" sign is simply a greater sign, followed by a equal sign.\n",
      "\n",
      "> If you found the article useful or see ways in which it could be improved, please leave a comment :)\n",
      "---\n",
      "title: Introduction to Hadoop\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Parallel and Distributed Computing\"\n",
      "---\n",
      "\n",
      "The annual volume of data produced worldwide is rising exponentially. Emerging markets and IoT nowadays represent more than half of the data produced in the world, and it keeps rising.\n",
      "\n",
      "We qualify of Big Data anything too large or too complex for traditional data processing applications.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/data_vol.jpg)\n",
      "\n",
      "The amount of data produced by us from the beginning of time until 2003 was 5 billion gigabytes. If you pile up the data in the form of disks it may fill an entire football field. The same amount was created in every two days in 2011, and every ten minutes in 2013, and most likely every minute in 2020.\n",
      "\n",
      "# Limits of Vertical Scalability\n",
      "\n",
      "Challenges arise when we need to capture, store, transform, transfer, analyze and present large volumes of data.\n",
      "\n",
      "Capacities of hard drives have increased, but the rate at which data can be read have not kept up. We have reached the limits of the vertical scalability, and need to develop horizontal scalability. Moore's law is the observation that the number of transistors in dense integrated circuit doubles about every two years. It is up to now generally true, but the rate seems to slow down.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/41.jpg)\n",
      "\n",
      "This is however technically challenging to do since parallelizing complex computations is not trivial. This is why Google (Jeff Dean) came up with MapReduce in 2004. The original paper can be found [here](https://static.googleusercontent.com/media/research.google.com/fr//archive/mapreduce-osdi04.pdf).\n",
      "\n",
      "MapReduce divides the task we want to do on the data into small parts and assigns them to several computers (a cluster), and aggregates the results from them.\n",
      "\n",
      "Before moving on, let's define some simple terminology :\n",
      "- **A Cluster** is a group of machines that acts as a single system.\n",
      "- **A Node** is a single machine among the cluster.\n",
      "\n",
      "# Hadoop\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/hadoop.jpg)\n",
      "\n",
      "## Main Concept\n",
      "\n",
      "Hadoop is an ensemble of distributed technologies, written in Java, to store and deal with a large volume of data (>To).\n",
      "\n",
      "To get the big picture, Hadoop makes use of a whole cluster. For each operation, we use the processing power of all machines.\n",
      "\n",
      "\n",
      "There are 4 big steps in MapReduce :\n",
      "- **Map **: apply some arbitrary function to all inputs and output a key-value pair\n",
      "- **Combine **: start combining the outputs from close outputs\n",
      "- **Shuffle **: sort outputs by keys\n",
      "- **Reduce **: Key per key combine outputs with an arbitrary function\n",
      "\n",
      "## What is Hadoop made of?\n",
      "Hadoop is made of some core services, as described in \"Hadoop, The Definitive Guide\" by O'Reilly :\n",
      "\n",
      "- **Core**: A set of components and interfaces for distributed filesystems and general I/O (serialization, Java RPC, persistent data structures).\n",
      "- **MapReduce**: A distributed data processing model and execution environment that runs on large clusters of commodity machines.\n",
      "- **HDFS**: A distributed filesystem that runs on large clusters of commodity machines.\n",
      "- **YARN **: A framework for the application management in a cluster.\n",
      "\n",
      "And other services :\n",
      "- **Pig**: A data flow language and execution environment for exploring very large datasets. Pig runs on HDFS and MapReduce clusters.\n",
      "- **HBase**: A distributed, column-oriented database. HBase uses HDFS for its underlying storage and supports both batch-style computations using MapReduce and point queries (random reads).\n",
      "- **ZooKeeper **: A distributed, highly available coordination service. ZooKeeper provides primitives such as distributed locks that can be used for building distributed applications.\n",
      "- **Hive **: A distributed data warehouse. Hive manages data stored in HDFS and provides a query language based on SQL (and which is translated by the runtime engine to MapReduce jobs) for querying the data.\n",
      "\n",
      "**Hadoop is used to develop applications that perform statistical analysis on large amounts of data.**\n",
      "\n",
      "## History\n",
      "\n",
      "- 2004: Google publishes the MapReduce paper\n",
      "- 2006: Hadoop is integrated into Apache\n",
      "- 2014: Apache launches Spark\n",
      "- ...\n",
      "\n",
      "Today, Hadoop is a collection of related subprojects that fall under the umbrella of infrastructure for distributed computing. These projects are hosted by the Apache Software Foundation, which provides support for a community of open-source software projects.\n",
      "\n",
      "Hadoop was created by *Doug Cutting and Mike Cafarella* in *2005*. Doub was working at Yahoo! and wanted to support distribution for the Nutch search engine project, a production-ready tool developed by Apache.\n",
      "\n",
      "It was developed to support distribution for the Nutch search engine project. Doug, who was working at Yahoo! at the time and is now Chief Architect of Cloudera, named the project after his son's toy elephant. Cutting's son was 2 years old at the time and just beginning to talk. He called his beloved stuffed yellow elephant \"Hadoop\" (with the stress on the first syllable). Now 12, Doug's son often exclaims, \"Why don't you say my name, and why don't I get royalties? I deserve to be famous for this!\"\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/cutting.jpg)\n",
      "\n",
      "## What changes with Hadoop?\n",
      "\n",
      "In traditional approaches, the user interacts with a centralized system which queries the databases.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/1.jpg)\n",
      "\n",
      "With Hadoop, this paradigm changes. The task is no longer centralized but split into several workers.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/2.jpg)\n",
      "\n",
      "For the big picture, you should remember that HDFS is used to store the data, and MapReduce to perform actions on the data.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/3.jpg)\n",
      "\n",
      "## How does Hadoop work?\n",
      "\n",
      "- The input data is divided into uniformly-sized blocks of 128Mb or 64Mb.\n",
      "- Each file is distributed to a given cluster node, and even to several cluster nodes to handle failure of a node.\n",
      "- A Master node keeps track of where each file is sent.\n",
      "- HDFS is plugged on top of the local file system to supervise the processing.\n",
      "- Hadoop performs a *sort* between the map and reduces stages.\n",
      "- It then sends the sorted data to a given machine and displays the result.\n",
      "\n",
      "### *Why is Hadoop used ?*\n",
      "\n",
      "Hadoop is used because :\n",
      "- it can handle a large amount of data quickly\n",
      "- all the steps mentioned above are automatic\n",
      "- it is fully open source\n",
      "- it is compatible with all platforms since written in Java\n",
      "- we can add servers and remove some dynamically \n",
      "- it handles failure cases\n",
      "\n",
      "### *When to use Hadoop ?*\n",
      "\n",
      "Hadoop should **only** be used if :\n",
      "- your local machine can't handle the computation\n",
      "- the computation can be parallelized\n",
      "- you can't use Spark \n",
      "- you have no other choice\n",
      "\n",
      "### *What are the limits of Hadoop ?*\n",
      "\n",
      "- The shuffle part is really expensive in terms of computations\n",
      "- The files should be transferred to HDFS, and this is expensive\n",
      "- The community around Hadoop is not active anymore\n",
      "- Spark is almost systematically chosen\n",
      "\n",
      "## How to install and use Hadoop?\n",
      "\n",
      "- From scratch, using a Linux VM and following [this tutorial](https://www.tutorialspoint.com/hadoop/hadoop_enviornment_setup.htm) which relies on the [GitHub of hadoop](https://github.com/apache/hadoop)\n",
      "- Using packaged solutions developed by Cloudera, Hortonworks or MapR. Hadoop Distributions pull together all the enhancement projects present in the Apache repository and present them as a unified product so that organizations don’t have to spend time on assembling these elements into a single functional component.\n",
      "\n",
      "Here is a small summary of the advantages and disadvantages of each solution :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/32.jpg)\n",
      "\n",
      "Hortonworks and Cloudera, the two tech giants of Big Data, have merged in October 2018, and are now worth over 3 billions $ combined.\n",
      "\n",
      "This is all for this first article. In the next article, we'll cover MapReduce and HDFS.\n",
      "\n",
      "> Conclusion: I hope this high-level overview was clear and helpful. I'd be happy to answer any question you might have in the comments section.\n",
      "---\n",
      "title: Getting started without setup\n",
      "layout: post\n",
      "tags: [da_tuto]\n",
      "subtitle : \"Data Analysis Basics\"\n",
      "---\n",
      "\n",
      "<style>\n",
      ".button {\n",
      "  display: inline-block;\n",
      "  padding: 15px 25px;\n",
      "  font-size: 24px;\n",
      "  cursor: pointer;\n",
      "  text-align: center;\n",
      "  text-decoration: none;\n",
      "  outline: none;\n",
      "  color: #fff;\n",
      "  background-color: #7187bd;\n",
      "  border: none;\n",
      "  border-radius: 15px;\n",
      "  box-shadow: 0 9px #999;\n",
      "}\n",
      "\n",
      ".button:hover {background-color: #7187bd}\n",
      "\n",
      ".button:active {\n",
      "  background-color: #7187bd;\n",
      "  box-shadow: 0 5px #666;\n",
      "  transform: translateY(4px);\n",
      "}\n",
      "</style>\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "If for whatever reason, you don't want or cannot setup your computer, there are 2 options:\n",
      "- use the interpreter page I provide\n",
      "- use Google Colab \n",
      "\n",
      "# Interperter on my site\n",
      "\n",
      "The interpreter that I embedded on my site is simple and hosted by Tricket.io. When you read the articles, simply open the page on the side.\n",
      "\n",
      "<button class=\"button\" onclick=\"location.href='https://maelfabien.github.io/python_compil/'\" type=\"button\">Open Interpreter</button>\n",
      "\n",
      "# Google Colab\n",
      "\n",
      "Google Colab is much more flexible, lets you install libraries and use CPUs and GPUs. You just need to log in using your Google Account.\n",
      "\n",
      "<button class=\"button\" onclick=\"location.href='https://colab.research.google.com/'\" type=\"button\">Open Google Colab</button>\n",
      "\n",
      "---\n",
      "title: TPU survival guide on Google Colaboratory \n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Google Cloud Platform\"\n",
      "---\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gc.jpg)\n",
      "\n",
      "Google now offers TPUs on Google Colaboratory. In this article, we'll see what is a TPU, what TPU brings compared to CPU or GPU, and cover an example of how to train a model on TPU and how to make a prediction.\n",
      "\n",
      "# What is a TPU?\n",
      "\n",
      "TPU stands for Tensor Processing Unit. It is an AI accelerator application-specific integrated circuit (ASIC). TPUs have been developed by Google in 2016 at Google I/O. However, TPUs have already been in Google data centers since 2015. \n",
      "\n",
      "The chip is specifically designed for TensorFlow framework for neural network machine learning. Current TPU versions are already 3rd generation TPUs, launched in May 2018. Edge TPUs have also been launched in July 2018 for ML models for edge computing. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/tpu_img.jpg)\n",
      "\n",
      "The TPUs have been designed, verified, built and deployed in just under 15 months, whereas typical ASIC development takes years.\n",
      "\n",
      "If you'd like to know more on TPUs, check <span style=\"color:blue\">[Google's blog](https://cloud.google.com/blog/products/gcp/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu)</span> .\n",
      "\n",
      "# How do TPU work?\n",
      "\n",
      "## Quantization in neural networks\n",
      "\n",
      "TPU use a technique called *quantization* to reduce execution time. Quantization is an optimization technique that uses an 8-bit integer to approximate an arbitrary value between a pre-set minimum and maximum value. Therefore, instead of using 16-bit or even 32-bit floating point operations, quantization dramatically reduces the computational requirements by maintaining quite similar precision of floating-point calculations.\n",
      "\n",
      "The process of quantization is illustrated as follows on Google's blog :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/quanti.jpg)\n",
      "\n",
      "Memory usage drops when using quantization. For example, Google states that when applied to Inception image recognition challenge, memory usage gets compressed from 91MB to 23 MB. \n",
      "\n",
      "TPUs use integer rather than floating-point operations, which highly reduces the hardware footprint and improves the computation time by up to 25 times.\n",
      "\n",
      "## TPU instruction set\n",
      "\n",
      "TPU is designed to be flexible enough to accelerate computation times of many kinds of neural networks model. \n",
      "\n",
      "Modern CPUs are influenced by the Reduced Instruction Set Computer (RISC) design style. The idea of RISC is to define simple instructions (load, store, add, multiply) and execute them as fast as possible.\n",
      "\n",
      "TPUs use Complex Instruction Set Computer (CISC) style as an instruction set. CISC focus on implementing high-level instructions that run complex tasks such as multiply-and-add many times with each instruction.\n",
      "\n",
      "## What is a TPU made of?\n",
      "\n",
      "TPUs are made of several computational resources :\n",
      "- Matric Multiplier Unit (MXU): 65,536 8-bit multiply-and-add units for matrix operations\n",
      "- Unified Buffer (UB): 24MB of SRAM that works as registers\n",
      "- Activation Unit (AU): Hardwired activation functions\n",
      "\n",
      "Here's an example of some high-levels instructions specifically designed for neural network inference that control how the MXU, UB, and AU work :\n",
      "- Read_Host_Memory : Read data from memory\n",
      "- Read_Weights : Read weights from memory\n",
      "- MatrixMultiply/Convolve: Multiply or convolve with the data and weights, accumulate the results\n",
      "- Activate : Apply activation functions\n",
      "- Write_Host_Memory : Write results to memory\n",
      "\n",
      "Google has created a compiler and software stack that translates API calls from TensorFlow graphs inti TPU instructions following this schema :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/tpu_stack.jpg)\n",
      "\n",
      "## Parallel processing on MXU\n",
      "\n",
      "Typical RISC processors provide instructions for multiplication or addition of numbers. They're called scalar processors. It might, therefore, take time to execute large matrix operations. \n",
      "\n",
      "Extensions of instructions set such as SSE and AVX allow matrix operations through vector processing where the same operation is conducted across a large number of data elements at the same time.\n",
      "\n",
      "In TPU, Google designed the MXUas a matrix processor that processes hundreds of thousands of operations in a single clock cycle.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/scalar.jpg)\n",
      "\n",
      "## Systolic array\n",
      "\n",
      "CPUs are made to run pretty much any calculation. Therefore, CPU store values in registers and a program sends a set of instructions to the Arithmetic Logic Unit to read a given register, perform an operation and register the output into the right register. This comes at some cost in terms of power and chip area.\n",
      "\n",
      "For an MXU, matrix multiplication reuses both inputs many times, as illustrated below :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/systolic.gif)\n",
      "\n",
      "Data flows in through the chip in waves. \n",
      "\n",
      "## How efficient are TPUs?\n",
      "\n",
      "The TPU MXU contains $$ 256 * 256 = 65'536 $$ ALUs. That means a TPU can process 65,536 multiply-and-adds for 8-bit integers every cycle. Because a TPU runs at 700MHz, a TPU can compute :\n",
      "$$ 65,536 × 700,000,000 = 46 × 10^{12} $$ multiply-and-add operations or 92 Teraops per second in the matrix unit.\n",
      "\n",
      "That being said, we can now move on to the practical part of this tutorial. Let's use TPUs on Google Colab!\n",
      "\n",
      "# Connect the TPU and test it\n",
      "\n",
      "First of all, connect to Google Colab : <span style=\"color:blue\">[https://colab.research.google.com/](https://colab.research.google.com/)</span> and create a new notebook in Python 3. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/collab1.jpg)\n",
      "\n",
      "The first step is to modify the hardware of your notebook :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/runtime.jpg)\n",
      "\n",
      "Switch the hardware accelerator to TPU :\n",
      "![image](https://maelfabien.github.io/assets/images/tpu.jpg)\n",
      "\n",
      "Now, we'll test if the TPU is well configured on the notebook :\n",
      "```python\n",
      "import os\n",
      "try:\n",
      "    device_name = os.environ['COLAB_TPU_ADDR']\n",
      "    TPU_ADDRESS = 'grpc://' + device_name\n",
      "    print('Found TPU at: {}'.format(TPU_ADDRESS))\n",
      "except KeyError:\n",
      "    print('TPU not found')\n",
      "```\n",
      "If everything works well, you should observe something similar to :\n",
      "```python \n",
      "Found TPU at: grpc://address\n",
      "```\n",
      "\n",
      "# Load and process the data\n",
      "\n",
      "We'll import basic packages at first :\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "```\n",
      "\n",
      "We'll load the IMDB movie review dataset :\n",
      "```python\n",
      "import tensorflow as tf\n",
      "tf.set_random_seed(1)\n",
      "\n",
      "from tensorflow.keras.datasets import imdb\n",
      "```\n",
      "Notice that we'll always use the `tensorflow.keras` implementation, and not the `keras` implementation directly. Indeed, to run a model on TPU, it is essential.\n",
      "\n",
      "We'll have to process the data later on using `tensorflow.keras` :\n",
      "```python\n",
      "from tensorflow.keras.models import Sequential\n",
      "from tensorflow.keras.preprocessing import sequence\n",
      "from tensorflow.keras.layers import Dense, Activation, Embedding, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
      "from tensorflow.keras import Model\n",
      "from tensorflow.keras import backend as K\n",
      "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
      "```\n",
      "\n",
      "In the IMDB movie review dataset, we'll only consider the most common words :\n",
      "```python\n",
      "top_words = 15000 \n",
      "max_review_length = 250\n",
      "INDEX_FROM = 3\n",
      "epochs = 20\n",
      "embedding_vector_length = 32\n",
      "\n",
      "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words, index_from=INDEX_FROM)\n",
      "```\n",
      "\n",
      "`X_train` is now an array that contains lists of indexes. The indexes corresponding to the index of the word in the vocabulary. `y_train` contains labels (0 or 1) that describe the emotion of the review (positive or negative).\n",
      "\n",
      "The reviews have different lengths. In order to normalize the length of the inputs, we \"pad\" the sequences to a defined length :\n",
      "```python\n",
      "X_train = pad_sequences(X_train,max_review_length)\n",
      "X_test = pad_sequences(X_test,max_review_length)\n",
      "```\n",
      "# Build and fit the model \n",
      "\n",
      "Alright, now the next step is to build a model that can predict whether a comment is negative or positive. LSTM's can be useful since there is a dependency between the beginning and the end of a sentence. We'll build a simple architecture accordingly with :\n",
      "- an Embedding layer\n",
      "- an LSTM layer\n",
      "- a Dense layer for the output\n",
      "\n",
      "The model is really simple and does not prevent overfitting...\n",
      "\n",
      "```\n",
      "K.clear_session()\n",
      "model = Sequential()\n",
      "\n",
      "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
      "model.add(LSTM(100))\n",
      "model.add(Dense(1, activation='sigmoid'))\n",
      "\n",
      "model.summary()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lstm.jpg)\n",
      "\n",
      "Then, compile the model :\n",
      "```python\n",
      "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "```\n",
      "\n",
      "At that point, you may want to simply run `model.fit`. But you first need to transform your model in order to make it understandable by your TPU :\n",
      "\n",
      "```python\n",
      "def model_to_tpu(model):\n",
      "    return tf.contrib.tpu.keras_to_tpu_model( model, strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)))\n",
      "```\n",
      "\n",
      "Once this function is defined, just modify your model :\n",
      "```python\n",
      "model = model_to_tpu(model)\n",
      "```\n",
      "\n",
      "Now, you can move on to the fitting of the model !\n",
      "```python\n",
      "model.fit(X_train, y_train, epochs=epochs, batch_size=64, validation_data=(X_test, y_test))\n",
      "```\n",
      "\n",
      "Finally, we can save the model weights:\n",
      "\n",
      "```python\n",
      "model.save_weights('./tpu_model.h5', overwrite=True)\n",
      "```\n",
      "\n",
      "We have already used our test set to evaluate the performance during the training. However, if you have a validation set for the validation accuracy during the training, you can evaluate the performance on your test set using :\n",
      "\n",
      "```python\n",
      "model.evaluate(X_test, y_test, batch_size=128 * 8)\n",
      "```\n",
      "\n",
      "Notice that the `batch_size` is set to eight times of the model input `batch_size` since the input samples are evenly distributed to run on 8 TPU cores. This will require some modifications in prediction.\n",
      "\n",
      "# Performance of the model\n",
      "\n",
      "In this section, I'll try to compare the performance of Googe Colab TPU, GPU, and CPU on 20 epochs. \n",
      "\n",
      "## TPU performance\n",
      "\n",
      "The TPU model has been fitted in less than **5** minutes. The average time per sample in each epoch is around **680 us**.\n",
      "\n",
      "```\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 17s 690us/sample - loss: 0.0344 - acc: 0.9888 - val_loss: 0.7723 - val_acc: 0.8302\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 17s 665us/sample - loss: 0.0196 - acc: 0.9946 - val_loss: 1.1661 - val_acc: 0.8204\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 18s 706us/sample - loss: 0.0105 - acc: 0.9972 - val_loss: 1.3740 - val_acc: 0.8103\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 17s 674us/sample - loss: 0.0102 - acc: 0.9972 - val_loss: 1.1836 - val_acc: 0.8290\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 17s 687us/sample - loss: 0.0352 - acc: 0.9889 - val_loss: 1.0536 - val_acc: 0.8258\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 17s 670us/sample - loss: 0.0293 - acc: 0.9915 - val_loss: 0.9991 - val_acc: 0.8302\n",
      "\n",
      "````\n",
      "The model has largely overfitted.\n",
      "\n",
      "## GPU performance\n",
      "\n",
      "From the runtime menu, switch the hardware accelerator to GPU. The GPU is now way longer to run.  A single epoch takes around 5 minutes. The average computing time per sample in each epoche is now **12 ms**. The overall model ran in around 2.5 hours.\n",
      "\n",
      "This means that on average, the model on TPU runs 17 times faster than on GPU! This is pretty close to the 25X faster announced. \n",
      "\n",
      "Regarding the performance of the model, the GPU model reaches higher results around 84.5 %.\n",
      "\n",
      "One would need more iterations to assess whether the difference is significant or not. We can guess that on LSTM computations like this one with a large number of variables, quantization's impact can be observed on the final result.\n",
      "\n",
      "## CPU performance\n",
      "\n",
      "Well, if a model runs on 2 hours on a GPU, you have to be patient to run it on the CPU. LSTM models would typically run over a whole night, so I decided to not run it for the moment. \n",
      "\n",
      "# Make a prediction\n",
      "\n",
      "If you try to predict a TPU, you might encounter this issue :\n",
      "```\n",
      "AssertionError: batch_size must be divisible by the number of TPU cores in use (6 vs 8)\n",
      "```\n",
      "As discussed above, since the samples are evenly distributed to run on 8 TPU cores, your sample to predict needs to have a shape that is a multiple of 8. This might be problematic. To overcome this problem, we'll convert the model back to CPU :\n",
      "\n",
      "```python\n",
      "model = model.sync_to_cpu()\n",
      "y_pred = model.predict(X_test)\n",
      "```\n",
      "\n",
      "We now have the prediction of our model, but the model's training is now around 20 times faster!\n",
      "\n",
      "> **Conclusion** : I hope this introduction to Google Colab TPU's was helpful. If you have any question, don't hesitate to drop a comment!\n",
      "---\n",
      "title: Lab - VM and Storage for Earthquake Data\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Road to Google Cloud Platform Certification\"\n",
      "---\n",
      "\n",
      "In this lab, our aim will be to create a VM instance to process real earthquake data and make the analysis publicly available.\n",
      "\n",
      "# Creating a VM on Compute Engine\n",
      "\n",
      "First, go to : [https://cloud.google.com](https://cloud.google.com). Open the console, go to the side menu, and click on: Compute Engine > VM Instances.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_6.jpg)\n",
      "\n",
      "The initialization of Compute Engine might take several minutes. Alright, Compute Engine is now up and running. Click on \"Create\" to create new VM instances :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_7.jpg)\n",
      "\n",
      "We will analyze Earth Quake data for what comes next, so we name the VM accordingly. Set your region (whatever your requirements are). We will stick with the standard machine (1 CPU, 3.75GB memory), but of course, all settings can be modified. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_8.jpg)\n",
      "\n",
      "To be able to write to cloud storage from the VM we will need to allow full access to all Cloud APIs. We will access the VM through SSH, and not HTTP or HTTPS.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_9.jpg)\n",
      "\n",
      "Click on \"Create\", wait a few minutes, and the VM should be up and running :)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_10.jpg)\n",
      "\n",
      "If you click on \"SSH\", a terminal page will launch. Notice that there is absolutely no program installed.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_11.jpg)\n",
      "\n",
      "To install `git` for example, run the following command :\n",
      "\n",
      "```bash\n",
      "sudo apt-get install git\n",
      "```\n",
      "\n",
      "This will allow us to access our code repository through the VM. Google prepared a GitHub repository with the necessary data for the Earthquake demo. To clone the repo, simply run :\n",
      "\n",
      "```bash\n",
      "git clone https://www.github.com/GoogleCloudPlatform/training-data-analyst\n",
      "```\n",
      "\n",
      "Once the data has been loaded, simply go in the folder :\n",
      "\n",
      "```bash\n",
      "cd training-data-analyst/\n",
      "cd courses\n",
      "cd bdml_fundamentals/\n",
      "cd demos\n",
      "cd earthquakevm/\n",
      "```\n",
      "\n",
      "Notice that there is a file called  `ingest.sh`. To view its content, run : \n",
      "\n",
      "```bash\n",
      "less ingest.sh\n",
      "```\n",
      "\n",
      "It removes existing files and makes a `wget` on the file to download. To quit the editor, type `:wq`. The earthquake data comes from USGS. There is also a Python file called `transform.py` that parses and transforms the input data. At that point, there are however many missing libraries in our VM. In this demo, Google packaged all necessary libraries in the `install_missing.sh` script. \n",
      "\n",
      "It contains the following lines (you can check it with `cat install_missing.sh`) :\n",
      "\n",
      "```bash\n",
      "sudo apt-get update\n",
      "sudo apt-get -y -qq --fix-missing install python3-mpltoolkits.basemap python3-numpy python3-matplotlib python3-requests\n",
      "```\n",
      "\n",
      "We can run the command to install missing libraries : \n",
      "\n",
      "```bash\n",
      "./install_missing.sh\n",
      "```\n",
      "\n",
      "Now, download the data by running : \n",
      "\n",
      "```bash\n",
      "./ingest.sh \n",
      "```\n",
      "\n",
      "There is now a file `earthquakes.csv` ! You can check its content by running :\n",
      "\n",
      "```bash\n",
      "head earthquakes.csv\n",
      "```\n",
      "\n",
      "This displays the following lines :\n",
      "\n",
      "```\n",
      "time,latitude,longitude,depth,mag,magType,nst,gap,dmin,rms,net,id,updated,place,type,horizontalError,depthError,mag,Error,magNst,status,locationSource,magSource\n",
      "2019-07-30T19:49:53.860Z,35.8401667,-117.6665,4.07,1.37,ml,24,80,0.06113,0.19,ci,ci38673143,2019-07-30T19:53:42.289,Z,\"24km ESE of Little Lake, CA\",earthquake,0.26,0.61,0.134,19,automatic,ci,ci\n",
      "2019-07-30T19:49:03.730Z,35.9245,-117.7173333,5.78,1.97,ml,29,40,0.05914,0.13,ci,ci38673135,2019-07-30T19:52:54.321, Z,\"17km E of Little Lake, CA\",earthquake,0.17,0.51,0.238,25,automatic,ci,ci\n",
      "...\n",
      "```\n",
      "\n",
      "The file `transform.py` transforms the data into a PNG file. Execute the content of the file :\n",
      "\n",
      "\n",
      "```bash\n",
      "./transform.py\n",
      "```\n",
      "\n",
      "It created a file called `earthquakes.png`:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_12.jpg)\n",
      "\n",
      "How can we get this file and read it on our Cloud Storage bucket? Go back to GCP, and click on Storage in the lateral menu. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_13.jpg)\n",
      "\n",
      "We will create a bucket.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_14.jpg)\n",
      "\n",
      "The name of the bucket has to be globally unique. Set the name, choose the storage location (EU in my case), and leave the default value for the object access control: \"Set object-level and bucket-level permissions\". Finally, click on \"Create\", and the bucket is ready! Back to the terminal window, you can check the content of the bucket with the following command:\n",
      "\n",
      "```bash\n",
      "gsutil ls gs://earthquake_mael\n",
      "```\n",
      "\n",
      "(Where earthquake_mael is the name I gave to my bucket)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_15.jpg)\n",
      "\n",
      "To copy the content of the VM instance to the Bucket, use the copy function in gsutils :\n",
      "\n",
      "```bash\n",
      "gsutil cp earthquakes.* gs://earthquake_mael\n",
      "```\n",
      "\n",
      "The bucket now contains the files we copied!\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_16.jpg)\n",
      "\n",
      "You can click on the `earthquakes.png` file, and this is what it looks like :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/earthquakes.jpg)\n",
      "\n",
      "At that point, we don't need the VM instance anymore. To stop paying, simply pause the VM from Compute Engine (or delete it if you don't need it ever again) :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_17.jpg)\n",
      "\n",
      "Suppose that now, we want to make our image public, as a static web app. How can we do that? Go back to the storage part, select all file you want to make available, and click on \"Permissions\" :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_18.jpg)\n",
      "\n",
      "We then need to create a user and give it rights to view the content stored. Call the user `allUsers`.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_19.jpg)\n",
      "\n",
      "Since the objects are now public, you can click on the public link provided (mine is [here](https://storage.googleapis.com/earthquake_mael/earthquakes.htm)) :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_20.jpg)\n",
      "\n",
      "Anyone with the public link can access the following HTML page that represents earthquakes this week :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_21.jpg)\n",
      "\n",
      "---\n",
      "title: Speaker Verification using I-vector features\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Speech Processing\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "The motivation behind the i-vector is to provide an intermediate representation between :\n",
      "- high-dimensional GMM super-vector\n",
      "- low-dimensional MFCC features\n",
      "\n",
      "It models the total variability of both the speaker and the channel.\n",
      "\n",
      "# I-vector feature extraction\n",
      "\n",
      "I-vectors represent the GMM super-vector using a single total-variability subspace.\n",
      "\n",
      "---\n",
      "title: Create Streaming Data Pipelines - Week 2 Module 1\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Road to Google Cloud Platform Certification\"\n",
      "---\n",
      "\n",
      "Nowadays, data in a company come from a variety of sources :\n",
      "- IoT sensors\n",
      "- Images\n",
      "- Transactions\n",
      "- CSV files\n",
      "- ...\n",
      "\n",
      "Building efficient data engineering pipelines is challenging, and variety of data does not help. Another element is the volume and the magnitude of data processed. We also need to do our processing in close to real-time. There is a need for velocity in terms of the solution we build. There are also constraints in terms of transaction. We need to handle the late arrival of a message...\n",
      "\n",
      "# Message oriented architectures with Cloud Pub/Sub\n",
      "\n",
      "In IoT applications, we need to handle :\n",
      "- data streaming from various processes or devices that do not even talk to each other\n",
      "- allow other services to subscribing to new messages that we're publishing out\n",
      "- scale to handle large volume and not lose any message\n",
      "- reliability to keep all the messages and remove any duplicate found\n",
      "\n",
      "Google Cloud Sub / Pub offers reliable real-time messaging. It's the publisher-subscriber model. Pub/Sub is a distributed messaging service that can receive messages from a variety of different streams, upstream data systems like gaming events, IoT devices, applications streams, and more.\n",
      "\n",
      "It ensures the delivery of messages and passes them to subscribing application. Pub/Sub auto-scales, and encrypts the messages. \n",
      "\n",
      "Upstream data is ingested into Cloud Pub/Sub as the first point of contact with our system. Cloud Pub/Sub reads, stores, and then publishes out any subscribers of this particular topic, for example, Cloud Dataflow.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_104.jpg)\n",
      "\n",
      "A central piece of Pub/Sub is the topic. A topic is a bit like a radio antenna. A publisher can send data to a topic, and the topic might have no subscribers or a lot of them.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_105.jpg)\n",
      "\n",
      "There can be 0, 1 or many publishers and subscribers.\n",
      "\n",
      "# Streaming Data Pipelines\n",
      "## Design Streaming Data Pipelines with Apache Beam\n",
      "\n",
      "Now that we captured all the messages from different sources with Cloud Sub / Pub, we need to pipe in all that data into our Data Warehouse for analysis. \n",
      "\n",
      "Data Engineers need to solve two different problems :\n",
      "- design data pipelines :\n",
      "    - will it work both in batch and streaming?\n",
      "    - does the SDK support my transformations?\n",
      "    - are there existing solutions?\n",
      "- implement them\n",
      "\n",
      "Usually, all this is done using Apache Beam. Apache Beam lets us start from existing templates for our pipeline code. It is a portable data processing programming model that can run into a highly distributed fashion.\n",
      "\n",
      "Apache Beam has 3 advantages :\n",
      "- it is unified, and a single programming model can be used for both batch and streaming data\n",
      "- it is portable, and we can execute our pipeline on multiple environments\n",
      "- it is extensible since we can write new SDK, IO connectors and transformation libraries\n",
      "\n",
      "Beam pipelines are written in Java, Go or Python. It creates a model representation of our code that is portable across many runners. Runners pass off our model to execution environments. For example, here is an example of a pipeline structure :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_106.jpg)\n",
      "\n",
      "## Implement Streaming Pipelines on Cloud DataFlow\n",
      "\n",
      "When implementing a data pipeline, consider these questions :\n",
      "- How much maintenance overhead is involved? -> Little\n",
      "- Is the infrastructure reliable? -> Built on Google Infrastructure\n",
      "- How is scaling handled? -> Autoscale workers\n",
      "- How can I monitor and alert? -> Integrated with StackDriver\n",
      "- Am I locked into a vendor? -> Run Apache Beam elsewhere\n",
      "\n",
      "DataFlow is serverless, since resource provisioning, reliability, monitoring... are managed by Google. So what does DataFlow do exactly?\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_107.jpg)\n",
      "\n",
      "- It receives a job from Apache Beam\n",
      "-It optimizes the execution graph of the model to remove inefficiencies. \n",
      "- It schedules out work in a distributed fashion to workers and scales as needed.\n",
      "- It will auto-heal in the event of faults with those workers. \n",
      "- It will re-balance automatically to best utilize the underlying workers.\n",
      "- It will connect to a variety of data syncs to produce a result (BigQuery for example)\n",
      "- Cloud Dataflow is managing all the compute and storage elastically to fit the demand of your streaming data pipeline.\n",
      "\n",
      "Here is a list of all templates provided to start from :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_108.jpg)\n",
      "\n",
      "---\n",
      "title: AWS Cloud Practitioner - Core Services\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Amazon Web Services\"\n",
      "---\n",
      "\n",
      "In this series of articles, I'll present to you the main concepts of Cloud Computing as presented by AWS on their online certification: \"Cloud Practitioner\".\n",
      "\n",
      "<embed src=\"https://maelfabien.github.io/assets/images/AWS_1.pdf\" type=\"application/pdf\" width=\"600px\" height=\"500px\" />\n",
      "\n",
      "---\n",
      "title: Introduction to Python's environment\n",
      "layout: post\n",
      "tags: [da_tuto]\n",
      "subtitle : \"Data Analysis Basics\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# History of Python\n",
      "\n",
      "Python was developped by Guido van Rossum in the Netherlands at the end of the 80s. It lead to the creation of the Python Software fundation in 2001. In 2008, Python 3.x was introduced. It is now since 2020 the only supported version.\n",
      "\n",
      "# What is Python?\n",
      "\n",
      "Python is an *object-oriented* programming language, in the sense that you can declare objects which have properties and methods. It is considered as a high-level programming language in the sense that the user does not have to deal with memory allocation for example, as opposed to C which is a low-level language.\n",
      "\n",
      "# Why Python?\n",
      "\n",
      "Python became really popular since it's easy to learn, easy to install, has lots of open source packages. It can be used to prototype easily. However, Python does not scale really well on large tasks and large volumes of data. We usually prefer to use Scala or C++ in such cases.\n",
      "\n",
      "# How to program in Python?\n",
      "\n",
      "There are again many options to program in Python. It will depend on what you expect and what you are used to:\n",
      "- a raw file, saved as a .py file is a Python file that can be executed as `python myfile.py`\n",
      "- use a text editor such as Sublime Text or Atom. The hard way, typos are easy, not recommended.\n",
      "- use an IDE such as PyCharm (includes everything with clicks and drag/drop such as a debugger and Github connect). Great for projects.\n",
      "- using Google Colab and other online hosted services. These services allow you to code and run your code online. Good if you need GPU access for example.\n",
      "- using Jupyter Notebooks or Jupyter Lab, what most data analyst / scientists do to explore their data and prototype. This opens a web browser tabs which runs locally and lets you code using their visual interface. It creates notebooks, `.ipynb` extensions which contains code, text and images.\n",
      "\n",
      "My personal pick for my projects:\n",
      "- exploratory data analysis and prototyping with Jupyter Notebooks\n",
      "- move the code once done in Python files with Sublime Text\n",
      "- if additional computing power is needed, move to Google Colab or other hosted services\n",
      "\n",
      "In groups, working with PyCharm works well since it includes a good debugging feature.\n",
      "\n",
      "# Using Jupyter Notebooks\n",
      "\n",
      "To start Jupyter Notebook, go to your terminal, in the folder you'd like to create the notebook in and run:\n",
      "\n",
      "```bash\n",
      "jupyter notebook\n",
      "```\n",
      "\n",
      "A page similar to this one (but with different files inside) should open.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/jn_0.png)\n",
      "\n",
      "It is simply a graphical display of the current content of your folder. We will create a Jupyter notebook by clicking on \"New > Python 3\".\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/jn_1.png)\n",
      "\n",
      "Don't worry if you don't have exactly the same tabs as me, I have some extra installs for the moment. This will create a Python kernel, a small instance of Python, running in your browser, that uses the computing power of your machine. You should now see:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/jn_2.png)\n",
      "\n",
      "You can give your notebook a name, as add cells in the notebook. Cells can be 2 types:\n",
      "- Markdown, text with a bit of formating\n",
      "- Code\n",
      "\n",
      "To write a title, select the cell by clicking on it, and select \"Markdown\" is the \"Code\" dropdown menu.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/jn_3.png)\n",
      "\n",
      "You can now type the title of the notebook in the cell. If you don't know much about Markdown, it's a hybrid between .txt files and HTML. It lets you type HTML code with shortcuts and regular text. For example, to put a title in bold as a `<h1>` title in HTML, simply type:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/jn_4.png)\n",
      "\n",
      "If you execute the cell (\"Execute\" button, or Ctrl + Enter), you'll see the title:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/jn_5.png)\n",
      "\n",
      "To add another cell, click on \"Insert > Cell below\", or simply on the keyboard \"b\". We will now add content with regular text, text in italic, bold, italic and bold, and lists:\n",
      "\n",
      "```markdown\n",
      "My text goes here. \n",
      "\n",
      "Text in *italic*\n",
      "Text in **bold**\n",
      "Text in ***italic and bold***\n",
      "\n",
      "Lists are:\n",
      "- easy\n",
      "- to\n",
      "- make\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/jn_6.png)\n",
      "\n",
      "This is all you need to know for the moment on Jupyter Notebooks. We will dive deeper into Python programming in the next articles.\n",
      "\n",
      "> If you found the article useful or see ways in which it could be improved, please leave a comment :)\n",
      "---\n",
      "title: Install Apache Cassandra on an AWS EC2 Cluster\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Amazon Web Services\"\n",
      "---\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ec2_cass.jpg)\n",
      "\n",
      "The Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data. We'll see how to configure Cassandra on an AWS EC2 cluster and create a resilient architecture that is big-data ready.\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "## SSH Connection to the nodes\n",
      "\n",
      "In the architecture we considered, we essentially focused on deploying Cassandra on slave nodes. The steps detailed below can also be used for deploying Cassandra on your Masters. \n",
      "\n",
      "The first step is to establish an SSH connection with your Slave nodes. Recall :\n",
      "``` bash\n",
      "ssh -i \"<path to your keyPair directory>/Cluster_test_Key_Pair.pem\" ubuntu@<copy the public DNS> \n",
      "```\n",
      "\n",
      "All the steps below should be made for all the slave nodes. \n",
      "\n",
      "## Install Java SDK 8\n",
      "\n",
      "Ones your instances are up and running and the SSH tunnel is established, we need to install Java SDK. Java 8 is indeed required to run Cassandra. Luckily, the installation process is quite simple.\n",
      "\n",
      "Run the following command :\n",
      "``` bash\n",
      "sudo apt install openjdk-8-jre-headless\n",
      "```\n",
      "\n",
      "Once done, we need to define some exports that will allow us to launch Cassandra easily. Open VI to edit the ``` bashrc ``` file  :\n",
      "``` bash\n",
      "vi ~/.bashrc\n",
      "```\n",
      "\n",
      "Then, add the following lines to the file (you might have to type the letter \"i\" to insert newlines) :\n",
      "\n",
      "```\n",
      "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
      "export JRE_HOME=$JAVA_HOME/jre\n",
      "export PATH=$PATH:$JAVA_HOME/bin:$JAVA_HOME/jre/bin\n",
      "````\n",
      "To quit and save changes, press ‘ESC’ and then write ``` :wq ``` and press ‘ENTER’.\n",
      "\n",
      "Once you're back on the terminal, execute the following line: ``` source ~/.bashrc ```\n",
      "\n",
      "## Install Cassandra\n",
      "\n",
      "a. From <span style=\"color:blue\">[Apache Cassandra's website](http://www.apache.org/dyn/closer.lua/cassandra/3.11.3/apache-cassandra-3.11.3-bin.tar.gz)</span>, copy the download link (version of January 2019) :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Cassandra_Website.jpg)\n",
      "\n",
      "b. Install Apache-Cassandra on your instances\n",
      "\n",
      "From your terminal, go to : ``` /ubuntu/home/ ```\n",
      "\n",
      "Them, execute :\n",
      "``` wget http://wwwftp.ciril.fr/pub/apache/cassandra/3.11.3/apache-cassandra-3.11.3-bin.tar.gz ```\n",
      "\n",
      "If a new version of Cassandra has been released, make sure to replace the link above by the latest version.\n",
      "\n",
      "You should see something similar :\n",
      "![image](https://maelfabien.github.io/assets/images/Cassandra_Wget.jpg)\n",
      "\n",
      "c. The files are compressed. The next step is to uncompress them and extract the software :\n",
      "\n",
      "``` bash\n",
      "tar -xv apache-cassandra-3.11.3-bin.tar.gz\n",
      "```\n",
      "\n",
      "Then, remove the .tar.gz file :\n",
      "```\n",
      "rm apache-cassandra-3.11.3-bin.tar.gz\n",
      "```\n",
      "![image](https://maelfabien.github.io/assets/images/Cassandra_Extract.jpg)\n",
      "\n",
      "d. Replicate the steps above on each slave\n",
      "\n",
      "\n",
      "## Configuration of your 5 nodes cluster\n",
      "\n",
      "We will need to modify 2 files :\n",
      "- ``` cassandra.yaml ```\n",
      "- ``` cassandra-rackdc.properties ```\n",
      "\n",
      "Those two files are located in the ``` conf ``` directory :\n",
      "``` bash\n",
      "cd apache-cassandra-3.11.3/conf/\n",
      "```\n",
      "\n",
      "a. Modify the ``` cassandra.yaml ``` file :\n",
      "\n",
      "Open the file :\n",
      "``` vi cassandra.yaml ```\n",
      "\n",
      "Change the following fields :\n",
      "- ``` cluster_name ``` :  give the name you want (e.g Cluster1)\n",
      "- ``` listen_address``` : Give it a private IP address specific to this node\n",
      "- ``` rpc_address ``` : Give it again this private IP address specific to this node\n",
      "- ``` seed_provider ``` : A private IP address common to all instances\n",
      "- ```endpoint_snitch``` :  Set it to ```Ec2Snitch```\n",
      "\n",
      "Save and quit. Here's a quick example :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Cassandra_yaml.jpg)\n",
      "\n",
      "Repeat those steps on the 5 Slave nodes.\n",
      "\n",
      "b. Modify the ``` cassandra-rackdc.properties ``` file :\n",
      "\n",
      "We will consider the simplest framework here: we won't specify any rack name or data center name. Just comment on the two lines that are not commented :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Cassandra_rack.jpg)\n",
      "\n",
      "The configuration is now ready!\n",
      "\n",
      "## Try the connection between every Cassandra node\n",
      "\n",
      "Go in the ``` bin ``` directory :\n",
      "``` bash\n",
      "$ cd ./bin\n",
      "```\n",
      "\n",
      "For each node, execute the following command : \n",
      "```\n",
      "./cassandra \n",
      "```\n",
      "\n",
      "Some lines contain the keyword “Handshaking” which means that the nodes communicate.\n",
      "\n",
      "There is a command to directly describe the connections of your cluster :\n",
      "```\n",
      "./nodetool describecluster\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Cassandra_Final.jpg)\n",
      "\n",
      "Your Slave nodes with Apache-Cassandra are now configured! \n",
      "\n",
      "> *Conclusion *: The next step is to install and configure Zookeeper for the resilience of Spark!\n",
      "---\n",
      "title: Expectation Maximization for Gaussian Mixture Models and Hidden Markov Models\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Speech Processing\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "For a course at EPFL, I recently gave a presentation on Expactation Maximization for Gaussian Mixture Models and Hidden Markov Models. The presentation had nice feedbacks, and I thought that including it here could be useful:\n",
      "\n",
      "<div style=\"width:100%; text-align:justify; align-content:left; display:inline-block;\">\n",
      "<embed src=\"https://maelfabien.github.io/assets/files/EM.pdf\" type=\"application/pdf\" width=\"100%\" height=\"138px\" />\n",
      "</div>\n",
      "\n",
      "<br>\n",
      "\n",
      "I summarized this presentation on Towards Data Science, right [here](https://towardsdatascience.com/expectation-maximization-for-gmms-explained-5636161577ca).\n",
      "\n",
      "I also made an interactive Dash web application. Here's a preview of what you can find in it:\n",
      "\n",
      "<iframe width=\"700\" height=\"500\" src=\"https://www.youtube.com/embed/hxr-UijYbpk\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
      "\n",
      "<br>\n",
      "\n",
      "The Github of the project can be found [here](https://github.com/maelfabien/EM_GMM_HMM).\n",
      "---\n",
      "title: AWS Cloud Practitioner - Cloud Concepts\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Amazon Web Services\"\n",
      "---\n",
      "\n",
      "In this series of articles, I'll present to you the main concepts of Cloud Computing as presented by AWS on their online certification: \"Cloud Practitioner\".\n",
      "\n",
      "> **Cloud computing**  refers to the on-demand delivery of IT resources and applications via the Internet with Pay-As-You-Go pricing. \n",
      "\n",
      "It allows companies to avoid buying their infrastructures, in which case the would be subject to under or overuse. \n",
      "\n",
      "AWS can be :\n",
      "- Initiated within seconds\n",
      "- Treat these a temporary and disposable \n",
      "- Freeform the inflexibility and constraints\n",
      "- More Agile and efficient\n",
      "\n",
      "3 main factors influence agility :\n",
      "- **Speed**\n",
      "    -  Global reach within a moment’s notice\n",
      "    - New resources a click away\n",
      "- **Experimentation**\n",
      "    - Experiment more often\n",
      "    - Operations as code\n",
      "    - Safely experiment\n",
      "    - Spin up servers in minutes\n",
      "    - Retour of re-purpose servers\n",
      "    - Testing different configurations\n",
      "    - AWS CloudFormation gives templated environments\n",
      "- **Culture of innovation**\n",
      "    - Experiment quickly with low cost/risk\n",
      "    - Experiment more often\n",
      "\n",
      "The AWS Infrastructure :\n",
      "- Allows **elasticity, scalability and reliability** of computing resources\n",
      "- Is distributed over regions: physical locations, containing multiple availability zones\n",
      "- Availability zones (AZs): one or more discrete data centers, which offer power, networking, and connectivity in separate facilities\n",
      "\n",
      "The AWS is highly available :\n",
      "- Fault tolerance: Application operational during component failure, built-in redundancy of components\n",
      "- Highly available: Systems always functioning and accessible. Downtime is minimized as much as possible, without human intervention. \n",
      "\n",
      "**Elasticity** is the power to scale computing resources up or down easily, while only paying for the actual resources used.\n",
      "\n",
      "Elasticity allows customers to :\n",
      "- Quickly deploy new applications\n",
      "- Instantly scale up as the workload grows\n",
      "- Instantly shut down resources that are no longer required\n",
      "- Scale down and don’t pay for the infrastructure \n",
      "\n",
      "The overall infrastructure allows to :\n",
      "- Use services at own pace\n",
      "- Adapt consumption\n",
      "- Launch new services or products\n",
      "- Accommodate new strategic directions\n",
      "- Run a wide range of applications\n",
      "- Auto-scaling and elastic load balancing for scale up or down based on demand\n",
      "- Deploy the system in multiple regions with lower latency and a better experience\n",
      "- Innovative services and cutting-edge technology\n",
      "- Virtually, support any workload\n",
      "\n",
      "**Security** and compliance :\n",
      "- Customer retains control over regions where data is located\n",
      "- Security auditing often periodic and manual\n",
      "- AWS cloud provides governance capabilities\n",
      "- Industry-leading capabilities that meet the strictest security requirements\n",
      "\n",
      "Data centers :\n",
      "- Offer state of the art electronic surveillance\n",
      "- Multi-factor access control systems\n",
      "- Staffed 24x7\n",
      "- Access is a strictly least-privileged basis \n",
      "- Environmental systems minimize the impact of disruptions\n",
      "- Multiple regions and availability zones enable resiliency\n",
      "\n",
      "**Reliability** is the ability of a system to recover from system failures. Also focused on the ability to dynamically acquire resources to meet demand and mitigate disruptions. \n",
      "\n",
      "A reliable system must have a well-planned foundation that can :\n",
      "- Handle changes in demand\n",
      "- Detect failure and automatically heal itself\n",
      "\n",
      "**Pay as you go** pricing :\n",
      "- Avoids dedicating resources to infrastructure\n",
      "- Redirects focus on innovation and invention\n",
      "- Reduces procurement complexity\n",
      "- Enables elasticity\n",
      "- Adapts to changes in business needs\n",
      "- Improves responsiveness to changes\n",
      "- Reduces the risk of over-provisioning or missing capacity\n",
      "---\n",
      "title: Run ML models in SQL with BigQuery ML - Week 1 Module 3\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Road to Google Cloud Platform Certification\"\n",
      "---\n",
      "\n",
      "In Google Cloud, one can build Machine Learning models straight into BigQuery, using SQL! Through this module, we will create Demand forecasting models using BigQuery ML.\n",
      "\n",
      "# Introduction to BigQuery\n",
      "\n",
      "BigQuery is an easy to use Data Warehouse. It's a petabyte-scale fully-managed service, and has several advantages :\n",
      "- it's serverless\n",
      "- it offers flexible pricing models\n",
      "- data encryption and security for regulatory requirements (restrictions on columns and row visibility\n",
      "- geospatial data types and functions \n",
      "- foundation for BI and AI\n",
      "\n",
      "It's a common workload to prototype ML models rapidly, and it is the bridge for data analysts and all users to your data.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_79.jpg)\n",
      "\n",
      "Big Query is essentially both a fast SQL query engine (BigQuery Query Service), and a managed storage for datasets (BigQuery Storage Service). Both services are fully managed and linked by a Petabit network.\n",
      "\n",
      "The storage service relies on Google Colossus file storage system. This storage system also powers Google Photos for example. This storage service can do both :\n",
      "- bulk data ingestions (huge amount of data)\n",
      "- and streaming data ingestion (real-time data)\n",
      "\n",
      "To launch a Query service, you can do it :\n",
      "- through the command line\n",
      "- through the WebUI\n",
      "- through 7 REST APIs\n",
      "\n",
      "The Query Service runs interactive or batch queries. It can connect to CSV files in Cloud Storage, but also to other services through connectors (Cloud Dataproc or Google Sheets for example).\n",
      "\n",
      "Both services run together to optimize the syntax of the SQL query that is running. \n",
      "\n",
      "BigQuery offers free monthly processing for up to 1TB.\n",
      "\n",
      "# BigQuery Query Service\n",
      "\n",
      "From the Query Editor, we'll explore San Francisco bike-sharing data :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_83.jpg)\n",
      "\n",
      "Type the following command (with the back-hyphens) :\n",
      "\n",
      " ```\n",
      " `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips`\n",
      "```\n",
      "\n",
      "This is the link to the data. On macOS, simply hold the Command button, and click on the link of the data :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_84.jpg)\n",
      "\n",
      "It opens a description of the table, gives you details about the number of rows, the size of the file... You can click on the Preview tab, without even running a query, to see the first 100 rows of the dataset.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_85.jpg)\n",
      "\n",
      "If you click on the \"Query table\" button, it creates the default SQL format query :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_86.jpg)\n",
      "\n",
      "If you don't want to type the name of the columns in the SELECT query, you can click on the column names from the Schema.\n",
      "\n",
      "An example query would be :\n",
      "\n",
      "```\n",
      "SELECT trip_id, start_station_name FROM `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips` LIMIT 1000\n",
      "```\n",
      "\n",
      "If you click on the Format button, it automatically re-formats the SQL query in a nice way :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_87.jpg)\n",
      "\n",
      "To get the stations in which we have the most rentals, we can run the following command :\n",
      "\n",
      "```\n",
      "# Top 10 station by Volume\n",
      "SELECT\n",
      "    start_station_name,\n",
      "COUNT(trip_id) AS num_trips\n",
      "FROM\n",
      "    `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips`\n",
      "GROUP BY\n",
      "    start_station_name\n",
      "ORDER BY\n",
      "    num_trips DESC\n",
      "LIMIT\n",
      "    1000\n",
      "```\n",
      "\n",
      "To filter only on rentals that occurred after 2017, apply a filter on the WHERE :\n",
      "\n",
      "```\n",
      "# Top 10 station by Volume since 2018\n",
      "SELECT\n",
      "    start_station_name,\n",
      "COUNT(trip_id) AS num_trips\n",
      "FROM\n",
      "    `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips`\n",
      "WHERE\n",
      "    start_date > '2017-12-31 00:00:00 UTC'\n",
      "GROUP BY\n",
      "    start_station_name\n",
      "ORDER BY\n",
      "    num_trips DESC\n",
      "LIMIT\n",
      "    1000\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_88.jpg)\n",
      "\n",
      "We can save a query as a Table to get a static table in the result (not a view). To do so, create an empty database in your project's resources. I called mine `bikes`. Your architecture should be as follows :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_89.jpg)\n",
      "\n",
      "To save a table, simply add a CREATE OR REPLACE argument at first :\n",
      "\n",
      "```\n",
      "# Top 10 station by Volume since 2018\n",
      "CREATE OR REPLACE TABLE bikes.after_2017\n",
      "SELECT\n",
      "    start_station_name,\n",
      "COUNT(trip_id) AS num_trips\n",
      "FROM\n",
      "    `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips`\n",
      "WHERE\n",
      "    start_date > '2017-12-31 00:00:00 UTC'\n",
      "GROUP BY\n",
      "    start_station_name\n",
      "ORDER BY\n",
      "    num_trips DESC\n",
      "LIMIT\n",
      "    1000\n",
      "```\n",
      "\n",
      "The table is then added to the `bikes` database! To create a view, simply switch REPLACE table by VIEW :\n",
      "\n",
      "```\n",
      "# Top 10 station by Volume since 2018\n",
      "CREATE OR REPLACE VIEW bikes.after_2017\n",
      "SELECT\n",
      "    start_station_name,\n",
      "COUNT(trip_id) AS num_trips\n",
      "FROM\n",
      "    `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips`\n",
      "WHERE\n",
      "    start_date > '2017-12-31 00:00:00 UTC'\n",
      "GROUP BY\n",
      "    start_station_name\n",
      "ORDER BY\n",
      "    num_trips DESC\n",
      "LIMIT\n",
      "    1000\n",
      "```\n",
      "\n",
      "To further explore the data, you can use DataStudio since it has a BigQuery connector.\n",
      "\n",
      "Let's talk a little bit about IAM Project roles and security. There are several levels of content access managed through roles :\n",
      "- Viewer: Can start a job in the project\n",
      "- Editor: Can create a dataset in the project\n",
      "- Owner: Can list all datasets in the project, delete and create datasets\n",
      "\n",
      "Dataset users should have the minimum permissions needed for their role. We use separated projects or datasets for different environments (DEV, QA, PRD...)\n",
      "\n",
      "We also audit roles periodically.\n",
      "\n",
      "# BigQuery Storage Service\n",
      "\n",
      "In addition to super-fast a super-fast query system, BigQuery can also ingest data from a large variety of sources :\n",
      "- Cloud Storage\n",
      "- Google Drive\n",
      "- Cloud Bigtable\n",
      "- CSV, JSON...\n",
      "\n",
      "BigQuery is automatically replicated, backed-up, set up to auto-scaling... It's a fully managed service. You can also directly query files that are outside the scope of BigQuery Managed Storage, although it's not as optimized as for the files inside the scope. This is typically useful when ingesting external datasets from other services of a company.\n",
      "\n",
      "BigQuery also allows for streaming records through API. The max input file size is 1MB, and the max output is 1000 files per second per project (If need more, consider Cloud Bigtable). It allows querying data without waiting for a full batch load.\n",
      "\n",
      "Big Query natively supports arrays as data types :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_90.jpg)\n",
      "\n",
      "It also supports STRUCTs, that satisfy the notions of Normalization of our tables.\n",
      "\n",
      "# Insights from geographic data\n",
      "\n",
      "BigQuery natively supports Geographic Information System (GIS) function to get insights from geographic data. To deal with GIS data in your SQL queries, apply the following template :\n",
      "\n",
      "```\n",
      "SELECT\n",
      "    ST_GeogPoint(longitude, latitude) AS point,\n",
      "    name\n",
      "FROM\n",
      "    `bigquery-public-data.noaa_hurricanes.hurricanes`\n",
      "WHERE\n",
      "    name LIKE '%MARIA%'\n",
      "    AND ST_DWithin(st_geogfromtext('POLYGON((-179 26, -179 48, -10 48, -10 26, -100 -10.1, -179 26))'), ST_GeogPoint(longitude, latitude), 10)\n",
      "```\n",
      "\n",
      "This query creates a table of the geographic points of all hurricanes names MARIA within a given region, using the NOAA dataset.\n",
      "\n",
      "To plot it, we can use GeoViz, a BigQuery tool that uses Google Map API. Access the tool here : [https://bigquerygeoviz.appspot.com/](https://bigquerygeoviz.appspot.com/).\n",
      "\n",
      "Then, select your project ID and paste your SQL Query :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_99.jpg)\n",
      "\n",
      "This is only a basic exploration in GeoViz, but the tool is really powerful.\n",
      "\n",
      "# BigQuery ML\n",
      "\n",
      "More than 60% of ML models in Google operate on structured data. CNNs and LSTMs represent up to 35 % of the rest.\n",
      "\n",
      "As a general guideline, here's the most simple type of model you should consider in each case :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_100.jpg)\n",
      "\n",
      "Here's a scenario of lifetime value (how much profit we can expect from a customer) prediction on Google E-commerce datasets. The goal is to target high-value customers.\n",
      "\n",
      "We have the following columns :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_101.jpg)\n",
      "\n",
      "BigQuery ML will handle the train and test split. The 2 majors steps are to build the model and to fit it. The learning rate is auto-tuned. It also handles regularization. \n",
      "\n",
      "The models currently supported in BigQuery ML are the following :\n",
      "- Linear Regression\n",
      "- Binary and Multiclass Logistic Regression\n",
      "\n",
      "Other models are currently being added. Other interesting features are :\n",
      "- Built-in model evaluation for standard metrics\n",
      "- Model weight inspection\n",
      "- Feature distribution analysis\n",
      "\n",
      "Here are the key steps in the model creation :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_102.jpg)\n",
      "\n",
      "The key steps to build the model are essentially :\n",
      "\n",
      "```\n",
      "CREATE MODEL\n",
      "ML.EVALUATE\n",
      "ML.PREDICT\n",
      "```\n",
      "\n",
      "## Key features\n",
      "\n",
      "Model creation :\n",
      "\n",
      "```\n",
      "CREATE OR REPLACE MODEL\n",
      "    `mydataset.mymodel`\n",
      "OPTIONS\n",
      "    ( model_type='linear_reg',\n",
      "    input_label_cols='sales',\n",
      "    ls_init_learn_rate=.15,\n",
      "    l1_reg=1,\n",
      "    max_iterations=5 ) AS\n",
      "```\n",
      "\n",
      "View features information :\n",
      "\n",
      "```\n",
      "SELECT\n",
      "    *\n",
      "FROM\n",
      "    ML.FEATURE_INFO(MODEL\n",
      "`bracketology.ncaa_model`)\n",
      "```\n",
      "\n",
      "Training progress :\n",
      "\n",
      "```\n",
      "SELECT\n",
      "    *\n",
      "FROM\n",
      "    ML.TRAINING_INFO(MODEL\n",
      "`bracketology.ncaa_model`)\n",
      "```\n",
      "\n",
      "Inspect the model weights :\n",
      "\n",
      "```\n",
      "SELECT\n",
      "    category, \n",
      "    weight\n",
      "FROM\n",
      "    UNNEST((\n",
      "        SELECT\n",
      "            category_weights\n",
      "        FROM\n",
      "            ML.WEIGHTS(MODEL\n",
      "        `bracketology.ncaa_model`)\n",
      "        WHERE\n",
      "            processed_input = 'seed'))\n",
      "    features like 'school_ncaa'\n",
      "        ORDER BY weight DESC\n",
      "```\n",
      "\n",
      "Evaluate a model :\n",
      "\n",
      "```\n",
      "SELECT\n",
      "    *\n",
      "FROM\n",
      "    ML.EVALUATE(MODEL\n",
      "`bracketology.ncaa_model`)\n",
      "```\n",
      "\n",
      "Make batch predictions :\n",
      "\n",
      "```\n",
      "CREATE OR REPLACE TABLES `bracketology.predictions`\n",
      "AS (\n",
      "\n",
      "SELECT * FROM ML.PREDICT(MODEL\n",
      "`bracketology.ncaa_model`,\n",
      "\n",
      "(SELECT * FROM `data-to-insights.ncaa.2018_tournament_results`)\n",
      ")\n",
      ")\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_103.jpg)\n",
      "\n",
      "---\n",
      "title: Speaker Adaptation\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Speech Processing\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "When training an ASR system, mismatch can occur between training and test data, due to various sources of variation. Speaker adaptation can help fill this gap.\n",
      "\n",
      "# Sources of variation\n",
      "\n",
      "## Acoustic model\n",
      "\n",
      "Lots of speaker-specific variations exist in the **acoustic model**:\n",
      "- speaking styles\n",
      "- accents\n",
      "- speech poduction anatomy\n",
      "\n",
      "There are also variations not related to the speaker itself, but to the channel (telephone, distance to microphone, reverberation...).\n",
      "\n",
      "## Pronunciation model\n",
      "\n",
      "Each speaker has a specific way to pronounce some words.\n",
      "\n",
      "## Language model\n",
      "\n",
      "Test data might be related to a specific topic, or to user-specific documents. The language model should then be adapted.\n",
      "\n",
      "To handle all of these potential variations, we perform **speaker adaptation**. The aim is to reduce the mismatch between test data and trained models.\n",
      "\n",
      "ASR Systems can be said to be:\n",
      "- Speaker Independent (SI), which does not require too much data for each speaker, but has high WER\n",
      "- Speaker Dependent (SD), which requires a lot of data per speaker\n",
      "- Speaker Adaptative, that builds on top of an SI system and requires only a fraction of speaker-specific training data from an SD system\n",
      "\n",
      "# Speaker Adaptation (SA)\n",
      "\n",
      "Adaptation can be:\n",
      "- supervised, if word level transcriptions of adaptation data is known\n",
      "- unsupervised, if no transcripts are available\n",
      "\n",
      "It can also be:\n",
      "- static, if we adapt on adaptation data in a block\n",
      "- dynamic, if we adapt incrementally\n",
      "\n",
      "Finally, we want SA to be:\n",
      "- compact, few speaker-dependent parameters\n",
      "- efficient, requires low computational resources\n",
      "- flexible, applicable to different models\n",
      "\n",
      "## Model-based adaptation\n",
      "\n",
      "We can adapt the parameters of acoustic models to better match the observed data, using:\n",
      "- Maximum A Posteriori (MAP) adaptation of HMM/GMM\n",
      "- Maximum Likelihood Linear Regression (MLLR) of GMMs, and cMLLR\n",
      "- Learning Hidden Unit Contributions (LHUC) for NNs\n",
      "\n",
      "# Adaptation of HMM-GMMs\n",
      "\n",
      "## MAP training of GMMs\n",
      "\n",
      "MAP training will balance the estimated parameters of the Speaker Independent data and the new adaptation data.\n",
      "\n",
      "Using Maximum Likelihood Estimate (MLE) os Speaker Independent model, the mth Gaussian in the jth state has the following mean:\n",
      "\n",
      "$$ \\mu_{mj} = \\frac{\\sum_n \\gamma_{jm}(n) x_n}{\\sum_n \\gamma_{jm}(n)} $$\n",
      "\n",
      "Where $$ \\gamma_{jm} $$ is the component occupation probability. The aim of a MLE is to maximize: $$ P(X \\mid \\lambda) $$ where $$ \\lambda $$ are the model parameters.\n",
      "\n",
      "The MAP estimate of the adapted model uses the Speaker Independent models as a prior probability distribution over model parameters to estimate speaker-specific data. MAP training maximizes: $$ P(\\lambda \\mid X) ∝ P(X \\mid \\lambda) P_0(\\lambda) $$\n",
      "\n",
      "It gives a new mean:\n",
      "\n",
      "$$ \\hat{\\mu} = \\frac{ \\tau \\mu_0 + \\sum_n \\gamma(n)x_n} {\\tau + \\sum_n \\gamma(n)} $$\n",
      "\n",
      "Where:\n",
      "- $$ \\tau $$ is a weight factor for the weight of the SI estimate (between 0 and 20 typically)\n",
      "- $$ x_n $$ is the adaptation vector at time $$ n $$\n",
      "- $$ \\gamma(n) $$ is the probability of this Gaussian at this time\n",
      "- $$ \\mu_0 $$ is the prior mean\n",
      "\n",
      "As the amount of training data increases, the MAP estimate converges to the ML one.\n",
      "\n",
      "## Maximum Likelihood Linear Regression (MLLR)\n",
      "\n",
      "One of the issues with MAP is that with few adaptation data, most Gaussians will not be adapted. If instead we share the adaptation across the Gaussians, each adaptation data can affect many of, or all, the Gaussians.\n",
      "\n",
      "As its name suggests, MLLR will apply a linear transformation on the Gaussian parameters estimated by MLE:\n",
      "\n",
      "$$ \\hat{\\mu} = A \\mu + b $$\n",
      "\n",
      "Hence, if the observations have $$ d $$ -dimensions, then A is $$d \\times d $$ and b has $$ d $$ dimensions.\n",
      "\n",
      "It can be re-written as:\n",
      "\n",
      "$$ \\hat{\\mu} = W \\eta $$\n",
      "\n",
      "Where:\n",
      "- $$ W = [bA] $$\n",
      "- $$ \\eta = [1 \\mu^T]^T $$\n",
      "\n",
      "We then estimate $$ W $$ to maximize the MLE on the adaptation data. Such transformations can then be applied to all classes, or to a set of Gaussian sharing a transform, called a **regression class**.\n",
      "\n",
      "We must then determine the number of regression classes. This is usually small (1, 2 (speech / non-speech), one per broad class, one per context-independent phone class...). We can obtain the number of regression classes by building a regression class tree, in a similar manner to a clustering tree.\n",
      "\n",
      "As usual in MLE, we maximize the log-likelihood (LL), which turns out to be easier to solve. The LL is defined as:\n",
      "\n",
      "$$ L = \\sum_r \\sum_n \\gamms_r(n) \\log (K_r \\exp(-\\frac{1}{2}(x_n - W \\eta_r)^T \\Sigma_r^{-1} (x_n - W \\eta_r))) $$\n",
      "\n",
      "Where $$ r $$ defines the different regression classes. Note that there is bi closed form solution if $$ \\sigma $$ is a full covariance matrix, and it can be solved if $$ \\sigma $$ is diagonal.\n",
      "\n",
      "We apply the mean-only MLLR, by adapting only the mean, and it usually improves WER by 10-15% (relative). 1 minute of adaptation speech is more or less equal, in terms of model performance, to 30 minutes of speech in speaker dependent models.\n",
      "\n",
      "## Constrained MLLR (cMLLR)\n",
      "\n",
      "In **constrained MLLR (cMLLR)**, we use the the same linear transform for both the mean and the covariance. This is also called feature space MLLR (fMLLR), since it's equivalent to applying a linear transform to the data:\n",
      "\n",
      "$$ \\hat{\\mu} = A^{'} \\mu - b^{'} $$\n",
      "\n",
      "$$ \\hat{\\Sigma} = A^{'} \\Sigma A^{'}^T $$\n",
      "\n",
      "There are no closed form solution, this is solved iteratively.\n",
      "\n",
      "## Speaker-Adaptative Training (SAT)\n",
      "\n",
      "In SAT, we adapt the base models to the training speaker while training, using MLLR or fMLLR for each training speaker. It results in higher training likelihoods and improved recognition results, but increases the complexity and the storage requirements. SAT can be seen as a type of speaker normalization at training time.\n",
      "\n",
      "# Adaptation of HMM-DNNs\n",
      "\n",
      "## cMLLR feature transformation\n",
      "\n",
      "We can use the HMM/GMM system that we train to estimate the tied state, to also estimate a single cMLLR transform for a given speaker. We then use this to transform the input speech of the DNN for the target speaker.\n",
      "\n",
      "## Linear Input Network (LIN)\n",
      "\n",
      "In a Linear Input Network (LIN), a single linear input layer is trained to map input speaker-dependent speech to speaker independent network.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_56.png)\n",
      "\n",
      "In training, the LIN is fixed and in testing, the main speaker-independent network is fixed. \n",
      "\n",
      "## Speaker codes with i-vectors\n",
      "\n",
      "We can also learn a short speaker code vector for talker, and combine that with the input feature vector, to compute transformed features. This allows an adaptation on speaker specific means. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_57.png)\n",
      "\n",
      "i-vectors are used as speaker codes. i-vectors are fixed-dimensional representations $$ \\lambda_s $$ for a speaker $$ s $$. They model the difference between the means trained on all data $$ \\mu_0 $$ and the speaker specific means $$ \\mu_s $$:\n",
      "\n",
      "$$ \\mu_s = \\mu_0 + M \\lambda_S $$\n",
      "\n",
      "i-vectors are derived from a factor analysis, and widely used in speaker identification.\n",
      "\n",
      "## Learning Hidden Unit Contributions (LHUC)\n",
      "\n",
      "In LHUC, we add a learnable speaker dependent amplitude to each hidden unit, which allows us to learn amplitudes from data, per speaker, and it \"embeds\" in a sense the adaptation part.\n",
      "\n",
      "Below are the results, in terms of WER, of various adaptation methods, and their combinations.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_58.png)\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "If you want to improve this article or have a question, feel free to leave a comment below :)\n",
      "\n",
      "References:\n",
      "- [ASR 12, University of Edimburgh](http://www.inf.ed.ac.uk/teaching/courses/asr/2019-20/asr12-adapt.pdf)\n",
      "- [Steve Renals course on Speaker Adaptation](https://www.inf.ed.ac.uk/teaching/courses/asr/2008-9/asr-adapt-1x2.pdf)\n",
      "\n",
      "---\n",
      "title: MapReduce Jobs in Python (4/4)\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Parallel and Distributed Computing\"\n",
      "---\n",
      "\n",
      "Big Data tools are great, but if you have to learn a new language every time you use a new tool, this slows down the development of such tools. For this reason, it is possible to submit Python scripts to Hadoop using a Map-Reduce framework. Let's consider the WordCount example.\n",
      "\n",
      "Any job in Hadoop must have two phases:\n",
      "- Mapper\n",
      "- and Reducer. \n",
      "\n",
      "## Hadoop Streaming\n",
      "\n",
      "Hadoop Streaming is the canonical way of supplying any executable to Hadoop as a mapper or reducer, including standard Unix tools or Python scripts. The executable must read from stdin and write to stdout using agreed-upon semantics.\n",
      "\n",
      "- First, create a mapper that attaches the value 1 to every single word in the document. Copy-paste this code into a mapper.py file. \n",
      "\n",
      "```python\n",
      "#!/usr/bin/env python\n",
      "\n",
      "import sys\n",
      "\n",
      "for line in sys.stdin:\n",
      "    line = line.strip()\n",
      "    words = line.split()\n",
      "    for word in words:\n",
      "        print('%s\\t%s' % (word, 1))\n",
      "```\n",
      "\n",
      "- Then, create the reducer. Try to understand then copy-paste this code into a reducer.py file. \n",
      "\n",
      "```python\n",
      "#!/usr/bin/env python\n",
      "\n",
      "from operator import itemgetter\n",
      "import sys\n",
      "\n",
      "current_word = None\n",
      "current_count = 0\n",
      "word = None\n",
      "\n",
      "for line in sys.stdin:\n",
      "    line = line.strip()\n",
      "    word, count = line.split('\\t', 1)\n",
      "\n",
      "    try:\n",
      "        count = int(count)\n",
      "    except ValueError:\n",
      "        continue\n",
      "\n",
      "    if current_word == word:\n",
      "        current_count += count\n",
      "    else:\n",
      "        if current_word:\n",
      "            print('%s\\t%s' % (current_word, current_count))\n",
      "        current_count = count\n",
      "        current_word = word\n",
      "\n",
      "    if current_word == word:\n",
      "        print('%s\\t%s' % (current_word, current_count))\n",
      "```\n",
      "\n",
      "Save the two files in the VM, at the root : `[raj_ops@sandbox-hdp ~]`, using `scp` :\n",
      "\n",
      "- `scp -P 2222 Desktop/Hadoop/mapper.py raj_ops@localhost:mapper.py`\n",
      "\n",
      "- `scp -P 2222 Desktop/Hadoop/reducer.py raj_ops@localhost:reducer.py`\n",
      "\n",
      "\n",
      "We can try our program locally using the following command :\n",
      "\n",
      "`echo \"foo foo quux labs foo bar quux\" | python mapper.py | sort -k1,1 | python reducer.py`\n",
      "\n",
      "If it returns :\n",
      "\n",
      "- `bar    1`\n",
      "- `foo    3`\n",
      "- `labs    1`\n",
      "- `quux    2`\n",
      "\n",
      "Then we can move on to the next step.\n",
      "\n",
      "We are ready to launch the job for the terminal of the VM (after SSH connexion):\n",
      "\n",
      "`hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar -input TP/input -output TP/output_python -mapper ./mapper.py -reducer ./reducer.py -file ./mapper.py -file ./reducer.py`\n",
      "\n",
      "Note that the files mapper.py and reducer.py must be specified twice on the command line: the first time points Hadoop at the executables, while the second time tells Hadoop to distribute the executables around to all the nodes in the cluster.\n",
      "\n",
      "We run the Java class `hadoop-streaming` but using our Python files mapper.py and reduce.py as the MapReduce process.\n",
      "\n",
      "You'll see something like this :\n",
      "\n",
      "`19/05/19 20:20:36 INFO mapreduce.Job: Job job_1558288385722_0012 running in uber mode : false`\n",
      "\n",
      "`19/05/19 20:20:36 INFO mapreduce.Job:  map 0% reduce 0%`\n",
      "\n",
      "`19/05/19 20:20:53 INFO mapreduce.Job:  map 100% reduce 0%`\n",
      "\n",
      "`19/05/19 20:21:09 INFO mapreduce.Job:  map 100% reduce 84%`\n",
      "\n",
      "`19/05/19 20:21:12 INFO mapreduce.Job:  map 100% reduce 98%`\n",
      "\n",
      "`19/05/19 20:21:13 INFO mapreduce.Job:  map 100% reduce 100%`\n",
      "\n",
      "First, we cover the Map, then the reduce. The output is created in the TP/output_python of HDFS. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/40.jpg)\n",
      "\n",
      "You can read the output and you'll see :\n",
      "\n",
      "```\n",
      "\"    278\n",
      "\"#Muscular    1\n",
      "\"'Come    1\n",
      "\"'Dieu    1\n",
      "\"'Dio    1\n",
      "\"'From    1\n",
      "\"'Grant    1\n",
      "\"'I    4\n",
      "\"'No    1\n",
      "\"'Now    1\n",
      "\"'Russia    1\n",
      "...\n",
      "```\n",
      "\n",
      "## mrJob\n",
      "\n",
      "mrjob is an open-source Python framework that wraps Hadoop Streaming and is actively developed by Yelp. Since Yelp operates entirely inside Amazon Web Services, mrjob’s integration with EMR is incredibly smooth and easy (using the boto package). Check this [link](https://github.com/Yelp/mrjob) for more information.\n",
      "\n",
      "```python\n",
      "from mrjob.job import MRJob\n",
      "import re\n",
      "\n",
      "WORD_RE = re.compile(r\"[\\w']+\")\n",
      "\n",
      "class MRWordFreqCount(MRJob):\n",
      "\n",
      "    def mapper(self, _, line):\n",
      "        for word in WORD_RE.findall(line):\n",
      "            yield (word.lower(), 1)\n",
      "\n",
      "    def combiner(self, word, counts):\n",
      "        yield (word, sum(counts))\n",
      "\n",
      "    def reducer(self, word, counts):\n",
      "        yield (word, sum(counts))\n",
      "```\n",
      "\n",
      "## dumbo\n",
      "\n",
      "dumbo is another Python framework that wraps Hadoop Streaming. It seems to enjoy relatively broad usage but is not developed as actively as mrjob at this point. It is one of the earlier Python Hadoop APIs and is very mature. However, its documentation is lacking, which makes it a bit harder to use.\n",
      "\n",
      "## hadoopy\n",
      "\n",
      "hadoopy is another Streaming wrapper that is compatible with dumbo. Similarly, it focuses on typed bytes serialization of data and directly writes typed bytes to HDFS.\n",
      "\n",
      "## Summary \n",
      "\n",
      "Hadoop Streaming outperforms other approaches, so simply remember that this approach works well.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/39.jpg)\n",
      "\n",
      "> Conclusion: I hope this tutorial was clear and helpful. I'd be happy to answer any question you might have in the comments section.\n",
      "---\n",
      "title: Voice Activity Detection\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Speech Processing\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "> Voice activity detection is a field which consists in identifying whether someone is speaking or not at a given moment. It can be useful to launch a vocal assistant or detect emergency situations.\n",
      "\n",
      "In this article, we will cover the main concepts behind classical approaches to voice activity detection, and implement them in Python is a small web application using Streamlit. This article is inspired by the [following repository](https://github.com/marsbroshok/VAD-python).\n",
      "\n",
      "# High-level overview\n",
      "\n",
      "It can be useful at first to give a high level overview of the classical approaches to Voice Activity Detection:\n",
      "- Read the input file and convert is to mono\n",
      "- Move a window of 20ms along the audio data\n",
      "- Calculate for each window the ratio between energy of speech band and total energy for window\n",
      "- If ratio is higher than a pre-defined threshold (e.g 60%), label windows as speech\n",
      "- Apply median filter with length of 0.5s to smooth detected speech regions\n",
      "- Represent speech regions as intervals of time\n",
      "\n",
      "The application we will build is the following:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/app_speech_0.png)\n",
      "\n",
      "# Read the input file and convert it to mono\n",
      "\n",
      "In this exercise, we will only consider the case of mono signals and not stereo, meaning that we must have a single series of values, not 2. We read the files using Scipy's wavfile module, and convert it to mono if there are 2 lists of values returned (stereo) by applying a mean of both series.\n",
      "\n",
      "```python\n",
      "import scipy.io.wavfile as wf\n",
      "\n",
      "filename = 'test.wav'\n",
      "\n",
      "def _read_wav(wave_file):\n",
      "\t# Read the input\n",
      "\trate, data = wf.read(wave_file)\n",
      "\tchannels = len(data.shape)\n",
      "\tfilename = wave_file\n",
      "\n",
      "\t# Convert to mono\n",
      "\tif channels == 2 :\n",
      "\t\tdata = np.mean(data, axis=1, dtype=data.dtype)\n",
      "\t\tchannels = 1\n",
      "\treturn data\n",
      "\n",
      "read_file = _read_wav(filename)\n",
      "```\n",
      "\n",
      "You can plot the signal in order to see which regions should be detected. In my case, the sample file contains 2 to 3 speech regions.\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(np.arange(len(data)), data)\n",
      "plt.title(\"Raw audio signal\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/audio_0.png)\n",
      "\n",
      "You can already notice that there is a notion a threshold that appears. From what moment do we assume that someone is speaking? How do we split 2 regions? We'll answer those questions as we dive deeper into the solution.\n",
      "\n",
      "# Rolling Window\n",
      "\n",
      "The solution will take the form of a rolling window on the input data. We will determine the energy in the frequency range that usually is associated to speech, and the energy of the whole band. If the ratio is larger than a threshold, we can assume that someone is speaking. \n",
      "\n",
      "We first need to define some constants that we will use:\n",
      "\n",
      "```python\n",
      "SAMPLE_START = 0\n",
      "SPEECH_START_BAND = 300\n",
      "SPEECH_END_BAND = 3000\n",
      "SAMPLE_WINDOW = 0.02\n",
      "SAMPLE_OVERLAP = 0.01\n",
      "THRESHOLD = 0.6\n",
      "```\n",
      "\n",
      "Here's what the constants are used for:\n",
      "- `SAMPLE_START` : the start index,\n",
      "- `SPEECH_START_BAND` : the minimum frequency of a human voice\n",
      "- `SPEECH_END_BAND` : the maximum frequency of a human voice\n",
      "- `SAMPLE_WINDOW` : a 20 ms window size on which we run the algorithm\n",
      "- `SAMPLE_OVERLAP` : the amount by which we shift the window size at each step\n",
      "- `THRESHOLD` : the threshold for the energy ratio under which a sound is not tagged as a voice\n",
      "\n",
      "The rolling window will have the following format:\n",
      "\n",
      "```python\n",
      "while (SAMPLE_START < (len(data) - SAMPLE_WINDOW)):\n",
      "    \n",
      "    # Select only the region of the data in the window\n",
      "    SAMPLE_END = SAMPLE_START + SAMPLE_WINDOW\n",
      "    if SAMPLE_END >= len(data): \n",
      "        SAMPLE_END = len(data)-1\n",
      "\n",
      "    data_window = data[SAMPLE_START:SAMPLE_END]\n",
      "    \n",
      "    # Detect speech here\n",
      "    \n",
      "    # Increment \n",
      "    SAMPLE_START += SAMPLE_OVERLAP\n",
      "```\n",
      "\n",
      "# Speech Ratio\n",
      "\n",
      "Within this data window, we now need to determine the speech ratio:\n",
      "    \n",
      "$$ speech_{ratio} = \\frac{\\sum energy_{voice}}{\\sum energy_{full}} $$\n",
      "\n",
      "To determine the voice energy, we will only consider frequencies between 300 and 3'000 Hz, as they correspond to human voice frequencies. \n",
      "\n",
      "The first thing we need to do is to compute the range of possible frequencies at the defined rate and given the audio sequence:\n",
      "\n",
      "```python\n",
      "def _calculate_frequencies(audio_data):\n",
      "    data_freq = np.fft.fftfreq(len(audio_data),1.0/rate)\n",
      "    data_freq = data_freq[1:]\n",
      "    return data_freq\n",
      "```\n",
      "\n",
      "This will return regular values between -8'000 and 8'000. The energy transported by a wave is directly proportional to the square of the amplitude of the wave, which can be computed using a Fast Fourrier Transform.\n",
      "\n",
      "```python\n",
      "def _calculate_energy(audio_data):\n",
      "    data_ampl = np.abs(np.fft.fft(audio_data))\n",
      "    data_ampl = data_ampl[1:]\n",
      "    return data_ampl ** 2\n",
      "```\n",
      "\n",
      "We then connect the energy with the frequency by creating a dictionary whose keys are the absolute value of the frequency, and values are the corresponding energy at that frequency.\n",
      "\n",
      "```python\n",
      "def _connect_energy_with_frequencies(data):\n",
      "    \n",
      "    data_freq = _calculate_frequencies(data)\n",
      "    data_energy = _calculate_energy(data)\n",
      "    \n",
      "    energy_freq = {}\n",
      "    for (i, freq) in enumerate(data_freq):\n",
      "        if abs(freq) not in energy_freq:\n",
      "            energy_freq[abs(freq)] = data_energy[i] * 2\n",
      "    return energy_freq\n",
      "\n",
      "energy_freq = _connect_energy_with_frequencies(data)\n",
      "sum_full_energy = sum(energy_freq.values())\n",
      "```\n",
      "\n",
      "The variable `energy_freq` should return :\n",
      "\n",
      "```python\n",
      "{0.4166666666666667: 388888371.0778143,\n",
      " 0.8333333333333334: 378650788.74457765,\n",
      " 1.25: 139749533.30109847,\n",
      " 1.6666666666666667: 703141467.1534827,\n",
      " 2.0833333333333335: 2622893493.5843244,\n",
      " 2.5: 2214362080.232078,\n",
      " ...\n",
      " ```\n",
      "\n",
      "As stated above, we suppose that a human voice will be anywhere between 300 and 3'000 Hz. Therefore, we sum the energy corresponding such frequencies in the time window, and we can compare it with the full sum of energies.\n",
      "\n",
      "```python\n",
      "def _sum_energy_in_band(energy_frequencies):\n",
      "    sum_energy = 0\n",
      "    for f in energy_frequencies.keys():\n",
      "        if SPEECH_START_BAND < f < SPEECH_END_BAND:\n",
      "            sum_energy += energy_frequencies[f]\n",
      "    return sum_energy\n",
      "\n",
      "sum_voice_energy = _sum_energy_in_band(energy_freq)\n",
      "```\n",
      "\n",
      "Finally, we can define the speech ratio as being the quotien between the sum of the speech energy in the time window and the sum of the total energy. \n",
      "\n",
      "```python\n",
      "speech_ratio = sum_voice_energy/sum_full_energy\n",
      "speech_ratio\n",
      "```\n",
      "\n",
      "In this sample, it gave me : `0.68923`.\n",
      "\n",
      "# Combining the loop and the speech ratio\n",
      "\n",
      "So far, we estimated the speech ratio on the whole audio file, without using a rolling window. It is now time to combine both approaches. We will store in `speech_ratio_list` a list of all the speech ratios in the loop.\n",
      "\n",
      "```python\n",
      "speech_ratio_list = []\n",
      "detected_voice = []\n",
      "mean_data = []\n",
      "\n",
      "SAMPLE_START = 0\n",
      "\n",
      "while (SAMPLE_START < (len(data) - SAMPLE_WINDOW)):\n",
      "    \n",
      "    # Select only the region of the data in the window\n",
      "    SAMPLE_END = SAMPLE_START + SAMPLE_WINDOW\n",
      "    if SAMPLE_END >= len(data): \n",
      "        SAMPLE_END = len(data)-1\n",
      "\n",
      "    data_window = data[SAMPLE_START:SAMPLE_END]\n",
      "    mean_data.append(np.mean(data_window))\n",
      "\n",
      "    # Full energy\n",
      "    energy_freq = _connect_energy_with_frequencies(data_window)\n",
      "    sum_full_energy = sum(energy_freq.values())\n",
      "    \n",
      "    # Voice energy\n",
      "    sum_voice_energy = _sum_energy_in_band(energy_freq)\n",
      "    \n",
      "    # Speech ratio\n",
      "    speech_ratio = sum_voice_energy/sum_full_energy\n",
      "    speech_ratio_list.append(speech_ratio)\n",
      "    detected_voice.append(speech_ratio > THRESHOLD)\n",
      "    \n",
      "    # Increment \n",
      "    SAMPLE_START += SAMPLE_OVERLAP\n",
      "```\n",
      "\n",
      "We can now compare the speech ratio list with the threshold over time:\n",
      "\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(speech_ratio_list)\n",
      "plt.axhline(THRESHOLD, c='r')\n",
      "plt.title(\"Speech ratio list vs. threshold\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/audio_1.png)\n",
      "\n",
      "We can also compare the raw signal with moments we detected a voice:\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(np.array(mean_data), alpha=0.4, label=\"Not detected\")\n",
      "plt.plot(np.array(detected_voice) * np.array(mean_data), label=\"Detected\")\n",
      "plt.legend()\n",
      "plt.title(\"Detected vs. non-detected region\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/audio_2.png)\n",
      "\n",
      "You can try to play with the speech ratio threshold and the window size to see how it affects the detection.\n",
      "\n",
      "# Smoothing the regions\n",
      "\n",
      "The output is interesting but would require some smoothing if we want to detect smooth regions in which a user speaks. We'll go for a median filter and apply it on the speech ratio's list.\n",
      "\n",
      "```python\n",
      "def _median_filter (x, k):\n",
      "    assert k % 2 == 1, \"Median filter length must be odd.\"\n",
      "    assert x.ndim == 1, \"Input must be one-dimensional.\"\n",
      "    k2 = (k - 1) // 2\n",
      "    \n",
      "    y = np.zeros((len(x), k), dtype=x.dtype)\n",
      "    y[:,k2] = x\n",
      "    for i in range (k2):\n",
      "        j = k2 - i\n",
      "        y[j:,i] = x[:-j]\n",
      "        y[:j,i] = x[0]\n",
      "        y[:-j,-(i+1)] = x[j:]\n",
      "        y[-j:,-(i+1)] = x[-1]\n",
      "    return np.median(y, axis=1)\n",
      "```\n",
      "\n",
      "We can the apply it to a region\n",
      "\n",
      "\n",
      "```python\n",
      "SPEECH_WINDOW = 0.5\n",
      "\n",
      "def _smooth_speech_detection(detected_voice):\n",
      "    window = 0.02\n",
      "    median_window=int(SPEECH_WINDOW/window)\n",
      "    if median_window % 2 == 0 : \n",
      "        median_window = median_window - 1\n",
      "    median_energy = _median_filter(detected_voice, median_window)\n",
      "    \n",
      "    return median_energy\n",
      "```\n",
      "\n",
      "We can now apply this to the pipeline defined above:\n",
      "\n",
      "```python\n",
      "speech_ratio_list = []\n",
      "detected_voice = []\n",
      "mean_data = []\n",
      "\n",
      "SAMPLE_START = 0\n",
      "\n",
      "while (SAMPLE_START < (len(data) - SAMPLE_WINDOW)):\n",
      "    \n",
      "    # Select only the region of the data in the window\n",
      "    SAMPLE_END = SAMPLE_START + SAMPLE_WINDOW\n",
      "    if SAMPLE_END >= len(data): \n",
      "        SAMPLE_END = len(data)-1\n",
      "    data_window = data[SAMPLE_START:SAMPLE_END]\n",
      "    mean_data.append(np.mean(data_window))\n",
      "    # Full energy\n",
      "    energy_freq = _connect_energy_with_frequencies(data_window)\n",
      "    sum_full_energy = sum(energy_freq.values())\n",
      "    \n",
      "    # Voice energy\n",
      "    sum_voice_energy = _sum_energy_in_band(energy_freq)\n",
      "    \n",
      "    # Speech ratio\n",
      "    speech_ratio = sum_voice_energy/sum_full_energy\n",
      "    speech_ratio_list.append(speech_ratio)\n",
      "    detected_voice.append(int(speech_ratio > THRESHOLD))\n",
      "    \n",
      "    # Increment \n",
      "    SAMPLE_START += SAMPLE_OVERLAP\n",
      "    \n",
      "detected_voice = _smooth_speech_detection(np.array(detected_voice))\n",
      "```\n",
      "\n",
      "Finally, the detected regions are these ones :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(np.array(detected_voice), label=\"Detected\")\n",
      "plt.legend()\n",
      "plt.title(\"Detected vs. non-detected region\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/audio_3.png)\n",
      "\n",
      "We can plot once again the regions on the raw signal in which the voice has been detected:\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(np.array(mean_data), alpha=0.4, label=\"Not detected\")\n",
      "plt.plot(np.array(detected_voice) * np.array(mean_data), label=\"Detected\")\n",
      "plt.legend()\n",
      "plt.title(\"Detected vs. non-detected region\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/audio_4.png)\n",
      "\n",
      "# Pros and Cons\n",
      "\n",
      "The main advantages of this approach is that :\n",
      "- it runs really fast\n",
      "- it is easily explainable\n",
      "- it is simple to implement\n",
      "- it does not take language into account\n",
      "\n",
      "The limits of such approach is that :\n",
      "- there are many hyperparameters to choose from\n",
      "- we must specify manually the range of frequency corresponding to a human voice\n",
      "- this range is not unique to humans, an an animal or a car could be interprete as a human\n",
      "\n",
      "> *Conclusion*: In this article, we introduced the concept of voice activity detection. In the next article, we'll see how to create a web application to deploy our algorithm using Streamlit.\n",
      "---\n",
      "title: Setup your computer\n",
      "layout: post\n",
      "tags: [da_tuto]\n",
      "subtitle : \"Before we start\"\n",
      "---\n",
      "\n",
      "# Necessary packages\n",
      "\n",
      "There are several packages that we will need for this training.\n",
      "\n",
      "## MacOS\n",
      "\n",
      "All you need to do is to install these packages via Homebrew. Homebrew is a package manager that helps install missing softwares on MacOs. The command line below installs Homebrek, zsh, wget, vim and git.\n",
      "\n",
      "```bash\n",
      "/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n",
      "brew update\n",
      "brew install wget zsh vim git\n",
      "```\n",
      "\n",
      "## Ubuntu \n",
      "\n",
      "On Ubuntu, your installs can be made this way:\n",
      "\n",
      "```bash\n",
      "sudo apt update\n",
      "sudo apt install -y make git zsh curl vim wget xclip\n",
      "```\n",
      "\n",
      "## Windows\n",
      "\n",
      "Finally, on Windows, go to [Git's website](https://git-scm.com/downloads) and download git. You can follow [this tutorial](https://www.linode.com/docs/development/version-control/how-to-install-git-on-linux-mac-and-windows/). \n",
      "\n",
      "# Install Python\n",
      "\n",
      "We then must install Python. There are many ways to install Python:\n",
      "- from Python's website\n",
      "- from Anaconda's website\n",
      "\n",
      "Anaconda is a Python distribution which allows you to install Python as well as 400 core packages for data manipulation, visualization and machine learning.\n",
      "\n",
      "Simply follow the installation guide [here](https://www.anaconda.com/download/).\n",
      "\n",
      "> If you found the article useful or see ways in which it could be improved, please leave a comment :)\n",
      "---\n",
      "title: Illustrating EM for GMMs and HMMs\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "I recently gave a talk on EM for GMMs and HMMs at EPFL and published the slides [here](https://maelfabien.github.io/machinelearning/GMM/). For the sake of the presentation, I built an interactive web application using Dash, Plotly, scikit-learn, open-cv and hmm-learn. In the app, I included:\n",
      "- GMM generated data exploration\n",
      "- K-Means vs. GMMs performance on overlapping clusters\n",
      "- Fitting EM on GMM data\n",
      "- EM-GMM for gender detection\n",
      "- Vector Quantization with k-Means\n",
      "- Background substraction with GMMs\n",
      "- Breast cancer data clustering with GMMs\n",
      "- AIC-BIC over the number of components\n",
      "- HMM-GMM data generation\n",
      "- HMM-GMM training and visualization\n",
      "- HMM-GMM for spoken digit speech recognition\n",
      "\n",
      "<iframe width=\"700\" height=\"500\" src=\"https://www.youtube.com/embed/hxr-UijYbpk\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
      "\n",
      "<br>\n",
      "\n",
      "I summarized the presentation mentioned above on Towards Data Science, right [here](https://towardsdatascience.com/expectation-maximization-for-gmms-explained-5636161577ca).\n",
      "\n",
      "---\n",
      "title: Introduction to Time Series\n",
      "layout: post\n",
      "tags: [statistics]\n",
      "subtitle : \"Time Series\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "{% highlight matlab %}\n",
      "{% endhighlight %}\n",
      "\n",
      "# I. What is a time series?\n",
      "\n",
      "Time series is a series of data collected with the same unit over several successive periods. \n",
      "\n",
      "Examples of time series include :\n",
      "- daily exchange rate\n",
      "- yearly inflation\n",
      "- consumption of a certain good per month\n",
      "- ...\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_1.jpg)\n",
      "\n",
      "Time series are used for :\n",
      "- forecasting\n",
      "- estimating causal effects\n",
      "- estimating correlation over time\n",
      "\n",
      "We'll define the following notations :\n",
      "- The value of $$ y $$ at time $$ t $$ is given by $$ y_t $$\n",
      "- The data points are : $$ y_1, ..., y_T $$\n",
      "- The first difference is given by : $$ \\Delta y_t = y_t - y_{t-1} $$\n",
      "\n",
      "# II. Illustration using Open Data\n",
      "\n",
      "## 1. The data\n",
      "\n",
      "To illustrate the main concepts related to time series, we'll be working with time series of Open Power System Data ([OPSD](https://open-power-system-data.org/)) for Germany. \n",
      "\n",
      "The data set includes daily electricity consumption, wind power production, and solar power production between 2006 and 2017. \n",
      "```\n",
      "- Date — The date (yyyy-mm-dd format)\n",
      "- Consumption — Electricity consumption in GWh\n",
      "- Wind — Wind power production in GWh\n",
      "- Solar — Solar power production in GWh\n",
      "- Wind+Solar — Sum of wind and solar power production in GWh\n",
      "```\n",
      "\n",
      "The data can be downloaded [here](https://raw.githubusercontent.com/jenfly/opsd/master/opsd_germany_daily.csv)\n",
      "\n",
      "Start by importing the following packages :\n",
      "\n",
      "```python\n",
      "### General import\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn import preprocessing\n",
      "import statsmodels.api as sm\n",
      "\n",
      "### Time Series\n",
      "from statsmodels.tsa.ar_model import AR\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from pandas.tools.plotting import autocorrelation_plot\n",
      "from statsmodels.tsa.arima_model import ARIMA\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "#from statsmodels.tsa.sarimax_model import SARIMAX\n",
      "\n",
      "### LSTM Time Series\n",
      "from keras.models import Sequential  \n",
      "from keras.layers import Dense  \n",
      "from keras.layers import LSTM  \n",
      "from keras.layers import Dropout \n",
      "from sklearn.preprocessing import MinMaxScaler  \n",
      "```\n",
      "\n",
      "Then, load the data :\n",
      "```python\n",
      "df = pd.read_csv('opsd_germany_daily.csv', index_col=0)\n",
      "df.head(10)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_2.jpg)\n",
      "\n",
      "Then, make sure to transform the dates into `datetime` format in pandas :\n",
      "\n",
      "```python\n",
      "df.index = pd.to_datetime(df.index)\n",
      "```\n",
      "\n",
      "Using the `df.describe()` function, we observe that there are almost half of the solar data points that are missing. This is because the series does not start at the same time. \n",
      "\n",
      "## 2. Distributions\n",
      "\n",
      "We can now take a look at the distribution of the different variables we measured :\n",
      "\n",
      "```python\n",
      "# Distribution of the consumption\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist(df['Consumption'], bins=100)\n",
      "plt.title(\"Distribution of the consumption\")\n",
      "plt.xlabel(\"Electricity consumption in GWh\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_3.jpg)\n",
      "\n",
      "```python\n",
      "# Distribution of the wind power production\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist(df['Wind'], bins=100)\n",
      "plt.title(\"Distribution of the wind power production\")\n",
      "plt.xlabel(\"Wind power production in GWh\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_4.jpg)\n",
      "\n",
      "```python\n",
      "# Distribution of the solar power production\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist(df['Solar'], bins=100)\n",
      "plt.title(\"Distribution of the solar power production\")\n",
      "plt.xlabel(\"Solar power production in GWh\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_5.jpg)\n",
      "\n",
      "## 3. Time Series\n",
      "\n",
      "First of all, how does the overall production compare with the overall consumption?\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(df['Consumption'], linewidth = 0.5)\n",
      "plt.plot(df['Wind+Solar'], linewidth = 0.5)\n",
      "plt.title(\"Consumption vs. Production\")\n",
      "plt.show()\n",
      "```\n",
      "![image](https://maelfabien.github.io/assets/images/ts_17.jpg)\n",
      "\n",
      "Let's now take a look at the consumption over time :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(df['Consumption'], linewidth = 0.5)\n",
      "plt.title(\"Consumption over time\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_6.jpg)\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(df['Consumption'], linewidth = 0.5, linestyle = \"None\", marker='.')\n",
      "plt.title(\"Consumption over time\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_9.jpg)\n",
      "\n",
      "```\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(df['Wind'], linewidth = 0.5)\n",
      "plt.title(\"Wind production over time\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_7.jpg)\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(df['Solar'], linewidth = 0.5)\n",
      "plt.title(\"Solar production over time\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_8.jpg)\n",
      "\n",
      "We observe large seasonal trends over time. \n",
      "- The solar production is much smaller during winter times. \n",
      "- The wind production is, however, larger during winter times, and the consumption as well.\n",
      "- There is an increasing trend in the production of both solar and wind power over time.\n",
      "- There is a large number of points in consumption located in the highest part of the time series, and some points lying under this curve.\n",
      "\n",
      "## 4. Change scale\n",
      "\n",
      "Let's now change scale and analyze the data per year, month and week.\n",
      "\n",
      "### Yearly\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(df.loc['2017-01':'2017-12']['Consumption'], linewidth = 0.5)\n",
      "plt.title(\"Electricity Consumption in 2017\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_10.jpg)\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(df.loc['2017-01':'2017-12']['Wind'], linewidth = 0.5)\n",
      "plt.title(\"Wind Production in 2017\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_11.jpg)\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(df.loc['2017-01':'2017-12']['Solar'], linewidth = 0.5)\n",
      "plt.title(\"Solar Production in 2017\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_12.jpg)\n",
      "\n",
      "We observe better the seasonality for consumption and production.\n",
      "\n",
      "### Weekly\n",
      "\n",
      "To understand the weekly trends, we can take a period of 3 weeks. I've added red line on Sundays, to better understand the pattern through the week :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(df.loc['2017-12-09':'2017-12-31']['Consumption'], linewidth = 0.5)\n",
      "plt.title(\"Electricity Consumption in December 2017\")\n",
      "plt.axvline(\"2017-12-10\", c='r')\n",
      "plt.axvline(\"2017-12-17\", c='r')\n",
      "plt.axvline(\"2017-12-24\", c='r')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_13.jpg)\n",
      "\n",
      "There seems to be a larger consumption and Tuesday, Wednesday and Thursday. The consumption is much lower on weekends.\n",
      "\n",
      "There is no point in analyzing trends of production weekly since there is no correlation between the day of the week and the sun/wind. We do not have access to hourly consumption data, so we can't dive deeper into this analysis.\n",
      "\n",
      "## 5. Box plots\n",
      "\n",
      "So far, we explored the trends visually. Box plots are a nice way to present the trend information visually as well as the confidence intervals to validate our hypothesis. We must start by creating new columns for the year, month and day of the week.\n",
      "\n",
      "```python\n",
      "df['year'] = df.index.year\n",
      "df['month'] = df.index.month\n",
      "df['day'] = df.index.weekday_name\n",
      "```\n",
      "\n",
      "We'll take a look at the consumption :\n",
      "\n",
      "```python \n",
      "plt.figure(figsize=(12,8))\n",
      "sns.boxplot(data=df, x='year', y='Consumption')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_14.jpg)\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "sns.boxplot(data=df, x='month', y='Consumption')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_15.jpg)\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "sns.boxplot(data=df, x='day', y='Consumption')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_16.jpg)\n",
      "\n",
      "There is a significant effect of the day of the week over the consumption. The month also has a large effect.\n",
      "\n",
      "## 6. Handling missing values\n",
      "\n",
      "There are several ways to handle missing values. A pretty common way to do this is to use Forward Filling. This simply means that when there is a missing value, you take the previously known value and duplicate it. This is pretty much the best approximation we can make without any further information.\n",
      "\n",
      "```python\n",
      "df = df.fillna(method='ffill')\n",
      "```\n",
      "\n",
      "We can also fill backward starting from the next value, using `bfill`.\n",
      "\n",
      "### Rolling Mean\n",
      "\n",
      "The rolling mean of a time series produces a smoother version than the original series. How is this achieved?\n",
      "\n",
      "Over a given window, for example here, a window of 7 days, we take the average of all the days within the window.\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(df.loc['2017-11':'2017-12']['Consumption'])\n",
      "plt.axvline('2017-11-09', color = 'red')\n",
      "plt.axvline('2017-11-16', color = 'red')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_20.jpg)\n",
      "\n",
      "This overall allows a much smoother series :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(df['Consumption'], label=\"Consumption\")\n",
      "plt.plot(df['Consumption'].rolling('90D').mean(), label=\"Rolling Mean\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_21.jpg)\n",
      "\n",
      "Instead of defining an average over a given number of observations within a window, we can take the mean of all the observations up to the given point. This is called expanding!\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(df['Consumption'], label=\"Consumption\")\n",
      "plt.plot(df['Consumption'].expanding().mean(), label=\"Rolling Mean\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ts_22.jpg)\n",
      "\n",
      "> **Conclusion **: We have now covered the basics of time series exploration. In the next articles, we'll cover trends, seasonality, stationarity, ergodicity and many other concepts.\n",
      "---\n",
      "title: Image Alignment and Image Warping\n",
      "layout: post\n",
      "tags: [computervision]\n",
      "subtitle : \"Computer Vision\"\n",
      "---\n",
      "\n",
      "Let's continue our work on image alignment. We shall cover further details on image warping.\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "For what comes next, we'll work a bit in Python. Import the following packages :\n",
      "\n",
      "```python\n",
      "import cv2\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "```\n",
      "\n",
      "So far, we saw how to :\n",
      "- detect features (Harris corner detector, Laplacian of Gaussians for blobs, difference of Gaussians for fast approximation of the LOG)\n",
      "- the properties of the ideal feature\n",
      "- the properties of the feature descriptor\n",
      "- matching of local features\n",
      "- feature distance metrics\n",
      "\n",
      "We'll cover into further details image alignment.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_47.jpg)\n",
      "\n",
      "# I. Image Warping\n",
      "\n",
      "There is a geometric relationship between these 2 images :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_48.jpg)\n",
      "\n",
      "An image warping is a change of domain of an image : $$ g(x) = f(h(x)) $$. This might include translation, rotation or aspect change. These changes are said to be global parametric warping: $$ p' = T(p) $$, since the transformation can easily be described by few parameters and is the same for every input point.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_49.jpg)\n",
      "\n",
      "To build the transformed image, we usually apply an inverse-warping :\n",
      "- for every pixel $$ x' $$ in $$ g(x') $$ :\n",
      "- compute the source location $$ x = \\hat{h}(x') $$\n",
      "- resample $$ f(x) $$ at location $$ x $$ and copy to $$ g(x') $$\n",
      "\n",
      "This allows $$ \\hat{h}(x') $$ to be defined for all pixels in $$ g(x') $$.\n",
      "\n",
      "## 1. Linear transformations\n",
      "\n",
      "We'll now cover the different types of linear transformations that we can apply to an image using inverse-warping.\n",
      "\n",
      "### a. Uniform Scaling\n",
      "\n",
      "Scaling by factor $$ s $$ :\n",
      "\n",
      "$$ S =  \\begin{pmatrix} s & 0 \\\\ 0 & s \\end{pmatrix} $$\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_50.jpg)\n",
      "\n",
      "### b. Rotation\n",
      "\n",
      "Rotation by angle $$ \\theta $$ :\n",
      "\n",
      "$$ R =  \\begin{pmatrix} cos \\theta & - sin \\theta \\\\ sin \\theta & cos \\theta \\end{pmatrix} $$\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_51.jpg)\n",
      "\n",
      "### c. 2D Mirror about the Y-axis\n",
      "\n",
      "$$ T =  \\begin{pmatrix} -1 & 0 \\\\ 0 & 1 \\end{pmatrix} $$\n",
      "\n",
      "### d. 2D Miror accross line $$ y = x $$\n",
      "\n",
      "$$ T =  \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} $$\n",
      "\n",
      "### e. All 2D linear transformations\n",
      "\n",
      "In summary, the linear transforms we can apply are :\n",
      "- scale\n",
      "- rotation\n",
      "- shear\n",
      "- mirror\n",
      "\n",
      "$$  \\begin{pmatrix} x' \\\\ y \\end{pmatrix} =  \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}  \\begin{pmatrix} x \\\\ y \\end{pmatrix} $$\n",
      "\n",
      "The transformation should respect the following properties :\n",
      "- origin maps origin\n",
      "- lines map to lines\n",
      "- parallel lines remain parallel\n",
      "- ratios are preserved\n",
      "- closed under composition\n",
      "\n",
      "## 2. Translation\n",
      "\n",
      "The trick is to add one more coordinate to build homogenous image coordinates.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_52.jpg)\n",
      "\n",
      "## 3. Affine transformation\n",
      "\n",
      "An affine transformation is any transformation that combines linear transformations and translations. For example :\n",
      "\n",
      "$$ \\begin{pmatrix} x' \\\\ y' \\\\ w' \\end{pmatrix} = \\begin{pmatrix} a & b & c \\\\ d & e & f \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ w \\end{pmatrix} $$\n",
      "\n",
      "In affine transformations, the origin does not always have to map the origin.\n",
      "\n",
      "## 4. Homography\n",
      "\n",
      "The Homography transform is also called projective transformation or planar perspective map.\n",
      "\n",
      "$$ H = \\begin{pmatrix} a & b & c \\\\ d & e & f \\\\ g & h & 1 \\end{pmatrix} $$\n",
      "\n",
      "Homographic transformations simply respect the following properties :\n",
      "- lines map to lines\n",
      "- closed under composition\n",
      "\n",
      "The different transformations can be summarized this way :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_53.jpg)\n",
      "\n",
      "With homographies, points at infinity become finite vanishing points.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_54.jpg)\n",
      "\n",
      "# II. Computing transformations\n",
      "\n",
      "Given a set of matches between images $$ A $$ and $$ B $$, we must find the transform $$ T $$ that best agrees with the matches.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_55.jpg)\n",
      "\n",
      "## 1. Translation\n",
      "\n",
      "The displacement of match $$ i $$ is $$ (x_i' - x_i, y_i' - y_i) $$ where $$ x_i' = x_i + x_t $$ and $$ y_i' = y_i + y_t $$. We want therefore to solve :\n",
      "\n",
      "$$ (x_t, y_t) = ( \\frac {1} {n} \\sum_i x_i' - x_i, \\frac {1} {n} \\sum_i y_i' - y_i ) $$\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_56.jpg)\n",
      "\n",
      "We face an overdetermined system of equations, which can be solved by least squares.\n",
      "\n",
      "$$ r_{x_i}(x_t) = x_i + x_t - x_i' $$\n",
      "\n",
      "$$ r_{y_i}(y_t) = y_i + y_t - y_i' $$\n",
      "\n",
      "The goal is to minimize the sum of squared residuals :\n",
      "\n",
      "$$ C(x_t, y_t) = \\sum_i (r_{x_i}(x_t)^2 + r_{y_i}(y_t)^2) $$\n",
      "\n",
      "We can rewrite the problem in matrix form :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_57.jpg)\n",
      "\n",
      "And the solution heads : $$ t = (A^T A)^{-1} Ab $$\n",
      "\n",
      "## 2. Affine transformation\n",
      "\n",
      "$$ \\begin{pmatrix} x' \\\\ y' \\\\ w' \\end{pmatrix} = \\begin{pmatrix} a & b & c \\\\ d & e & f \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ w \\end{pmatrix} $$\n",
      "\n",
      "We can write the residuals as :\n",
      "\n",
      "$$ r_{x_i}(a,b,c,d,e,f) = (ax_i + by_i + c) - x_i' $$\n",
      "\n",
      "$$ r_{y_i}(a,b,c,d,e,f) = (dy_i + ey_i + f) - y_i' $$\n",
      "\n",
      "And rewrite the cost function as :\n",
      "\n",
      "$$ C(a,b,c,d,e,f) = \\sum_i ( r_{x_i}(a,b,c,d,e,f)^2 + r_{y_i}(a,b,c,d,e,f)^2 ) $$\n",
      "\n",
      "Which can be rewritten in matrix form as :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_58.jpg)\n",
      "\n",
      "Let's develop a more general formulation. We have $$ x' = f(x,p) $$ a parametric transformation. The Jacobian of the transformation $$ f $$ with respect to the motion parameters $$ p $$ determines the relationship between the amount of motion $$ \\Delta x = x' - x $$ and the unknown parameters $$ \\Delta x = x' - x = J(x)p $$ . We note that $$ J = \\frac {\\delta f} {\\delta p} $$.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_59.jpg)\n",
      "\n",
      "The sum of squared residuals is then :\n",
      "\n",
      "$$ E_{LLS} = \\sum_i {\\mid \\mid J(x_i)p - \\Delta x_i \\mid \\mid_2 }^2 $$\n",
      "\n",
      "The solution yields :\n",
      "\n",
      "$$ Ap = b $$ where :\n",
      "- we define $$ A = \\sum_i J^T(x_i)J(x_i) $$ the Hessian\n",
      "- and $$ b = \\sum_i J^T(x_i) \\Delta x_i $$\n",
      "\n",
      "Up to now, we considered only a perfect matching accuracy. It's however only rarely the case. We can weight the least squares problem :\n",
      "\n",
      "$$ E_{WLS} = \\sum_i {\\Sigma_i}^2 \\mid \\mid r_i \\mid \\mid $$ . If the $$ \\delta_i $$ are fixed, the solution to apply is : $$ p = ( \\sum^T A^T A \\Sigma)^-1 \\sigma^T Ab $$ with $$ \\Sigma $$ a matix containing for each observation the noise level.\n",
      "\n",
      "\n",
      "> **Conclusion **: I hope this article on image alignment, transformations, and warping was helpful. Don't hesitate to drop a comment if you have any question.\n",
      "---\n",
      "title: Traitement Automatique du Langage Naturel en Français (TAL)\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Stat4Decision\"\n",
      "search: false\n",
      "---\n",
      "\n",
      "# I. A quoi sert le Traitement Automatique du Langage Naturel (TAL)?\n",
      "\n",
      "Le traitement du Langage Naturel est un des domaines de recherche les plus actifs en science des données actuellement. C'est un domaine à l'intersection du Machine Learning et de la linguistique. \n",
      "\n",
      "> Le traitement de langage naturel a pour but d'extraire des informations et une signification d'un contenu textuel.\n",
      "\n",
      "Le Traitement Automatique du Langage naturel (TAL) ou Natural Language Processing (NLP) en anglais trouve de nombreuses applications dans la vie de tous les jours:\n",
      "- traduction de texte (DeepL par exemple)\n",
      "- correcteur orthographique\n",
      "- résumé automatique d'un contenu\n",
      "- synthèse vocale\n",
      "- classification de texte\n",
      "- analyse d'opinion/sentiment\n",
      "- prédiction du prochain mot sur smartphone\n",
      "- extraction des entités nommées depuis un texte\n",
      "- ...\n",
      "\n",
      "La plupart des ressources disponibles actuellement sont en anglais, ce qui implique que la plupart des modèles pré-entrainés sont également spécifiques à la langue anglaise. Cependant, il existe des librairies et des outils en français pour accomplir les tâches mentionnées ci-dessus. Nous allons les voir dans cet article.\n",
      "\n",
      "# II. Les grands principes\n",
      "\n",
      "Le TAL est généralement composé de deux à trois grandes étapes:\n",
      "- Pré-traitement : une étape qui cherche à standardiser du texte afin de rendre son usage plus facile\n",
      "- Représentation du texte comme un vecteur : Cette étape peut être effectuée via des techniques de sac de mots (Bag of Words) ou Term Frequency-Inverse Document Frequency (Tf-IdF). On peut également apprendre des représentations vectorielles (embedding) par apprentissage profond.\n",
      "- Classification, trouver la phrase la plus similaire... (optionnel).\n",
      "\n",
      "Dans cet article, nous allons couvrir les tâches de TAL les plus communes pour lesquelles des outils spécifiques au français existent.\n",
      "\n",
      "Nous utiliserons principalement SpaCy. [SpaCy](https://spacy.io/) est une jeune librairie (2015) qui offre des modèles pré-entrainés pour diverses applications, y compris la reconnaissance d'entités nommées. SpaCy est la principale alternative à NLTK (Natural Language Tool Kit), la librairie historique pour le TAL avec Python, et propose de nombreuses innovations et options de visualisation qui sont très intéressantes.\n",
      "\n",
      "Après avoir installé la librairie SpaCy (`pip install spacy`), il faut télécharger les modèles français.\n",
      "\n",
      "```bash\n",
      "python -m spacy download fr_core_news_sm\n",
      "```\n",
      "\n",
      "Ce modèle est un réseau convolutionnel entrainé sur deux corpus, WikiNER et Sequoia, ce qui représente de gros volumes de données en français (typiquement plusieurs dizaines de Go).\n",
      "\n",
      "Dans un notebook Jupyter, on peut alors importer SpaCy et charger le modèle français.\n",
      "\n",
      "```python\n",
      "import spacy\n",
      "nlp = spacy.load(\"fr_core_news_sm\")\n",
      "```\n",
      "\n",
      "On va également créer une phrase d'exemple pour toutes les tâches de TAL que nous allons illustrer.\n",
      "\n",
      "```python\n",
      "test = \"Bouygues a eu une coupure de réseau à Marseille\"\n",
      "```\n",
      "\n",
      "# III. TAL en Français\n",
      "\n",
      "## 1. Tokenisation\n",
      "\n",
      "La tokenisation cherche à transformer un texte en une série de tokens individuels. Dans l'idée, chaque token représente un mot, et identifier des mots semble être une tâche relativement simple. Mais comment gérer en français des exemples tels que: \"J'ai froid\". Il faut que le modèle de tokenisation sépare le \"J'\" comme étant un premier mot.\n",
      "\n",
      "SpaCy offre une fonctionnalité de tokenisation en utilisant la fonction `nlp`. Cette fonction est le point d'entrée vers toutes les fonctionnalités de SpaCy. Il sert à représenter le texte sous une forme interprétable par la librairie.\n",
      "\n",
      "```python\n",
      "def return_token(sentence):\n",
      "    # Tokeniser la phrase\n",
      "    doc = nlp(sentence)\n",
      "    # Retourner le texte de chaque token\n",
      "    return [X.text for X in doc]\n",
      "```\n",
      "\n",
      "Ainsi, en appliquant cette tokenisation à notre phrase, on obtient:\n",
      "\n",
      "```python\n",
      "return_token(test)\n",
      "```\n",
      "\n",
      "`['Bouygues', 'a', 'eu', 'une', 'coupure', 'de', 'réseau', 'à', 'Marseille']`\n",
      "\n",
      "## 2. Enlever les mots les plus fréquents\n",
      "\n",
      "Certains mots se retrouvent très fréquemment dans la langue française. En anglais, on les appelle les \"stop words\". Ces mots, bien souvent, n'apportent pas d'information dans les tâches suivantes. Lorsque l'on effectue par exemple une classification par la méthode Tf-IdF, on souhaite limiter la quantité de mots dans les données d'entraînement.\n",
      "\n",
      "Les \"stop words\" sont établis comme des listes de mots. Ces listes sont généralement disponibles dans une librairie appelée NLTK (Natural Language Tool Kit), et dans beaucoup de langues différentes. On accède aux listes en français de cette manière:\n",
      "\n",
      "```python\n",
      "from nltk.corpus import stopwords\n",
      "stopWords = set(stopwords.words('french'))\n",
      "```\n",
      "\n",
      "```python\n",
      "{'ai',\n",
      " 'aie',\n",
      " 'aient',\n",
      " 'aies',\n",
      " 'ait',\n",
      " 'as',\n",
      " 'au',\n",
      " 'aura',\n",
      " 'aurai',\n",
      " 'auraient',\n",
      " 'aurais',\n",
      " ...\n",
      "```\n",
      "\n",
      "Pour filtrer le contenu de la phrase, on enlève tous les mots présents dans cette liste:\n",
      "\n",
      "```python\n",
      "clean_words = []\n",
      "for token in return_token(test):\n",
      "    if token not in stopWords:\n",
      "        clean_words.append(token)\n",
      "\n",
      "clean_words\n",
      "```\n",
      "\n",
      "```\n",
      "['Bouygues', 'a', 'coupure', 'réseau', 'Marseille', '.']\n",
      "```\n",
      "\n",
      "## 3. Tokenisation par phrases\n",
      "\n",
      "On peut également appliquer une tokenisation par phrase afin d'identifier les différentes phrases d'un texte. Cette étape peut à nouveau sembler facile, puisqu'a priori, il suffit de couper chaque phrase lorsqu'un point est rencontré (ou un point d'exclamation ou d'interrogation).\n",
      "\n",
      "Mais que se passerait-t-il dans ce cas-là?\n",
      "\n",
      "`Bouygues a eu une coupure de réseau à Marseille. La panne a affecté 300.000 utilisateurs.`\n",
      "\n",
      "Il faut donc une compréhension du contexte afin d'effectuer une bonne tokenisation par phrase.\n",
      "\n",
      "```python\n",
      "def return_token_sent(sentence):\n",
      "    # Tokeniser la phrase\n",
      "    doc = nlp(sentence)\n",
      "    # Retourner le texte de chaque phrase\n",
      "    return [X.text for X in doc.sents]\n",
      "```\n",
      "\n",
      "On applique alors la tokenisation à la phrase mentionnée précédemment.\n",
      "\n",
      "```python\n",
      "return_token_sent(\"Bouygues a eu une coupure de réseau à Marseille. La panne a affecté 300.000 utilisateurs.\")\n",
      "```\n",
      "\n",
      "La tokenisation des phrases retourne alors:\n",
      "\n",
      "```\n",
      "['Bouygues a eu une coupure de réseau à Marseille.',\n",
      " 'La panne a affecté 300.000 utilisateurs.']\n",
      "```\n",
      "\n",
      "## 4. Stemming\n",
      "\n",
      "Le stemming consiste à réduire un mot dans sa forme \"racine\". Le but du stemming est de regrouper de nombreuses variantes d'un mot comme un seul et même mot. Par exemple, une fois que l'on applique un stemming sur \"Chiens\" ou \"Chien\", le mot résultant est le même. Cela permet notamment de réduire la taille du vocabulaire dans les approches de type sac de mots ou Tf-IdF. \n",
      "\n",
      "Un des stemmers les plus connus est le Snowball Stemmer. Ce stemmer est diponible en français.\n",
      "\n",
      "```python\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "stemmer = SnowballStemmer(language='french')\n",
      "\n",
      "def return_stem(sentence):\n",
      "    doc = nlp(sentence)\n",
      "    return [stemmer.stem(X.text) for X in doc]\n",
      "```\n",
      "\n",
      "Si on applique ce stemmer à notre phrase d'exemple:\n",
      "\n",
      "```python\n",
      "return_stem(test)\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "['bouygu', 'a', 'eu', 'une', 'coupur', 'de', 'réseau', 'à', 'marseil', '.']\n",
      "```\n",
      "\n",
      "## 5. Reconnaissance d'entités nommées (NER)\n",
      "\n",
      "La reconnaissance d'entités nommées cherche à détecter les entités telles que des personnes, des entreprises ou des lieux dans un texte. Cela s'effectue très facilement avec SpaCy.\n",
      "\n",
      "\n",
      "```python\n",
      "def return_NER(sentence):\n",
      "    # Tokeniser la phrase\n",
      "    doc = nlp(sentence)\n",
      "    # Retourner le texte et le label pour chaque entité\n",
      "    return [(X.text, X.label_) for X in doc.ents]\n",
      "```\n",
      "\n",
      "Puis on peut appliquer la fonction à une phrase d'exemple.\n",
      "\n",
      "```python\n",
      "return_NER(test)\n",
      "```\n",
      "\n",
      "Les entités identifiées sont les suivantes:\n",
      "\n",
      "```\n",
      "[('Bouygues', 'ORG'), ('Marseille', 'LOC')]\n",
      "```\n",
      "\n",
      "Bouygues est reconnue comme une organisation, et Marseille comme un lieu.\n",
      "\n",
      "Spacy offre des \"Visualizers\", des outils graphiques qui permettent d'afficher les résultats de reconnaissances d'entités nommées ou d'étiquetage par exemple. \n",
      "\n",
      "```python\n",
      "from spacy import displacy\n",
      "\n",
      "doc = nlp(test)\n",
      "displacy.render(doc, style=\"ent\", jupyter=True)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/s4d2_0.png)\n",
      "\n",
      "On peut également préciser des options à DisplaCy pour n'afficher que les organisations, d'une certaine couleur:\n",
      "\n",
      "```python\n",
      "doc = nlp(test)\n",
      "colors = {\"ORG\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\"}\n",
      "options = {\"ents\": [\"ORG\"], \"colors\": colors}\n",
      "\n",
      "displacy.render(doc, style=\"ent\", jupyter=True, options=options)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/s4d2_1.png)\n",
      "\n",
      "## 6. L’étiquetage morpho-syntaxique\n",
      "\n",
      "L’étiquetage morpho-syntaxique ou Part-of-Speech (POS) Tagging en anglais essaye d'attribuer une étiquette à chaque mot d'une phrase mentionnant la fonctionnalité grammaticale d'un mot (Nom propre, adjectif, déterminant...).\n",
      "\n",
      "```python\n",
      "def return_POS(sentence):\n",
      "    # Tokeniser la phrase\n",
      "    doc = nlp(sentence)\n",
      "    # Retourner les étiquettes de chaque token\n",
      "    return [(X, X.pos_) for X in doc]\n",
      "```\n",
      "\n",
      "Les étiquettes identifiées sont les suivants:\n",
      "\n",
      "```python\n",
      "return_POS(test)\n",
      "```\n",
      "\n",
      "```\n",
      "[(Bouygues, 'ADJ'),\n",
      " (a, 'AUX'),\n",
      " (eu, 'VERB'),\n",
      " (une, 'DET'),\n",
      " (coupure, 'NOUN'),\n",
      " (de, 'ADP'),\n",
      " (réseau, 'NOUN'),\n",
      " (à, 'ADP'),\n",
      " (Marseille, 'PROPN')]\n",
      "```\n",
      "\n",
      "On remarque que Bouygues n'est pas identifié comme un nom propre, mais comme un Adjectif. Pour le reste, les tags sont corrects.\n",
      "\n",
      "Spacy dispose également d'une option de visualisation qui nous permet simplement d'afficher les étiquettes identifiées ainsi que les dépendances entre ces étiquettes.\n",
      "\n",
      "```python\n",
      "doc = nlp(test)\n",
      "displacy.serve(doc, style=\"dep\")\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/s4d2_2.png)\n",
      "\n",
      "## 7. Embedding par mot\n",
      "\n",
      "Avec SpaCy, on peut facilement récupérer le vecteur correspondant à chaque mot une fois passé dans le modèle pré-entrainé en français.\n",
      "\n",
      "Cela nous sert à représenter chaque mot comme étant un vecteur de taille 96.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "def return_word_embedding(sentence):\n",
      "    # Tokeniser la phrase\n",
      "    doc = nlp(sentence)\n",
      "    # Retourner le vecteur lié à chaque token\n",
      "    return [(X.vector) for X in doc]\n",
      "```\n",
      "\n",
      "Ainsi, lorsqu'appliqué à la phrase test, on obtient:\n",
      "\n",
      "```python\n",
      "return_word_embedding(test)\n",
      "```\n",
      "\n",
      "```\n",
      "[array([ -1.8685186 ,   1.3645297 ,  -2.3505871 ,  -1.233012  ,\n",
      "         -3.702136  ,   1.3316352 ,  -1.3532144 ,  -3.879726  ,\n",
      "         -7.051861  ,  -2.8570302 ,  -2.409908  ,   3.3500502 ,\n",
      "          3.8512042 ,  -0.5462021 ,  -1.7187259 ,  -5.341373  ,\n",
      "          ...\n",
      "```\n",
      "\n",
      "Cette information nous sert notamment lorsque l'on cherche à caractériser la similarité entre deux mots ou deux phrases. \n",
      "\n",
      "## 8. Similarité entre phrases\n",
      "\n",
      "Afin de déterminer la similarité entre deux phrases, nous allons opter pour une méthode très simple :\n",
      "- déterminer l'embedding moyen d'une phrase en moyennant l'embedding de tous les mots de la phrase\n",
      "- calculer la distance entre deux phrases par simple distance euclidienne\n",
      "\n",
      "Cela s'effectue très facilement avec SpaCy !\n",
      "\n",
      "```python\n",
      "def return_mean_embedding(sentence):\n",
      "    # Tokeniser la phrase\n",
      "    doc = nlp(sentence)\n",
      "    # Retourner la moyenne des vecteurs pour chaque phrase\n",
      "    return np.mean([(X.vector) for X in doc], axis=0)\n",
      "```\n",
      "\n",
      "On peut alors tester notre fonction avec plusieurs phrases:\n",
      "\n",
      "```python\n",
      "test_2 = \"Le réseau sera bientot rétabli à Marseille\"\n",
      "test_3 = \"La panne réseau affecte plusieurs utilisateurs de l'opérateur\"\n",
      "test_4 = \"Il fait 18 degrés ici\"\n",
      "```\n",
      "\n",
      "Ici, on s'attend à ce que la phrase 2 soit la plus proche de la phrase test, puis la phrase 3 et 4.\n",
      "\n",
      "Avec Numpy, la distance euclidienne se calcule simplement :\n",
      "\n",
      "```python\n",
      "np.linalg.norm(return_tensor(test)-return_tensor(test_2))\n",
      "np.linalg.norm(return_tensor(test)-return_tensor(test_3))\n",
      "np.linalg.norm(return_tensor(test)-return_tensor(test_4))\n",
      "```\n",
      "\n",
      "On obtient alors les résultats suivants:\n",
      "\n",
      "```python\n",
      "16.104986\n",
      "17.035103\n",
      "22.039303\n",
      "```\n",
      "\n",
      "La phrase 2 est bien identifiée comme la plus proche, puis la phrase 3 et 4. On peut également utiliser cette approche pour classifier si une phrase appartient à une classe ou à une autre.\n",
      "\n",
      "## 9. Transformers 🤗\n",
      "\n",
      "Si vous avez suivi l'actualité du TAL ces derniers mois, vous avez surement entendu parlé des Transformers, des modèles état-de-l'art qui apprennent des représentations vectorielles à partir d'un texte d'entrée et qui dépassent les capacités humaines sur certains points. Ces modèles peuvent être pré-entraînés et sont mis à disposition par Hugging Face, une startup spécialisée dans le traitement de langage naturel.\n",
      "\n",
      "Les modèles les plus récents mis à disposition par Hugging Face spécifiques au français sont entraînés avec XLM, une architecture qui a depuis été battue par de nombreux autres modèles, dont BERT. \n",
      "\n",
      "BERT est également disponible dans une version multi-linguage, entrainé sur le Wikipedia de plus de 104 langues, sous le nom de : `bert-base-multilingual-cased`.\n",
      "\n",
      "Les transformers sont particulièrement bons pour:\n",
      "- la génération de texte\n",
      "- apprendre une représentation vectorielle\n",
      "- prédire si une phrase est la suite d'une autre\n",
      "- les tâches de questions/réponses\n",
      "\n",
      "Nous allons ici donner un exemple de comment utiliser les transformers pour prédire si une phrase est la suite d'une autre. On trouve des applications de ceci lorsque l'on cherche par exemple à segmenter des fils de discussions en plusieurs blocs distincts. \n",
      "\n",
      "On suppose ici que PyTorch et Transformers sont installés (`pip install transformers`). \n",
      "\n",
      "```python\n",
      "import torch\n",
      "from transformers import *\n",
      "```\n",
      "\n",
      "Ensuite, on doit charger un tokenizer et un modèle.\n",
      "\n",
      "```python\n",
      "# Tokeniser, Model\n",
      "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
      "model = BertForNextSentencePrediction.from_pretrained('bert-base-multilingual-cased')\n",
      "model.eval()\n",
      "```\n",
      "\n",
      "La ligne `model.eval()` permet de passer le modèle en mode évaluation.\n",
      "\n",
      "Par la suite, on passe au modèle une séquence de texte convertie en tokens, et des délimitations des phrases:\n",
      "\n",
      "```python\n",
      "text = \"Comment ça va ? Bien merci, un peu stressé avant l'examen\"\n",
      "# Texte tokenisé\n",
      "tokenized_text = tokenizer.tokenize(text)\n",
      "# Convertir le texte en indexs\n",
      "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
      "segments_ids = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "```\n",
      "\n",
      "On transforme le tout en tenseurs interprétables par le modèle:\n",
      "\n",
      "```python\n",
      "# Transformer en tenseurs\n",
      "tokens_tensor = torch.tensor([indexed_tokens])\n",
      "segments_tensors = torch.tensor([segments_ids])\n",
      "```\n",
      "\n",
      "Finalement, on prédit si la deuxième phrase est la suite de la première:\n",
      "\n",
      "```python\n",
      "predictions = model(tokens_tensor, segments_tensors)\n",
      "```\n",
      "\n",
      "Si l'élément en position 0 est plus grand que celui en position 1, la phrase est la suite de la première. Autrement, c'est une nouvelle discussion qui commence.\n",
      "\n",
      "```python\n",
      "if np.argmax(predictions) == 0:\n",
      "    print(\"Suite\")\n",
      "else:\n",
      "    print(\"Pas la suite\")\n",
      "```\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "Nous avons couvert dans cet article de nombreuses applications de Traitement Automatique du Langage Naturel en Français. Bien que de très nombreuses ressources existent exclusivement en Anglais, il existe tout de même des outils intéressants pour le Français, que votre problème de TAL concerne une classification de texte par Tf-IdF, ou une approche par Deep Learning ou Transformers.\n",
      "\n",
      "Liens utiles:\n",
      "- [SpaCy](https://spacy.io/)\n",
      "- [NLTK](https://www.nltk.org/)\n",
      "- [Hugging Face](https://huggingface.co/)\n",
      "---\n",
      "title: MapReduce, Illustrated\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Parallel and Distributed Computing\"\n",
      "---\n",
      "\n",
      "MapReduce is a programming paradigm that allows processing a large amount of data by initially splitting the data into blocks, sending the blocks to different clusters to perform operations, and aggregating the results.\n",
      "\n",
      "Let's consider a simple WordCount exercise here. From an input text, we will count how many times each word appears, and rank the final list by occurrence. MapReduce is of course not needed for such task, and a simple Python script on your computer would be fine. But what if your input text is a whole book? Thousands of books? Millions of books? ...\n",
      "\n",
      "What we'll describe here scales easily to a large amount of data.\n",
      "\n",
      "# The algorithm\n",
      "\n",
      "## Step 1: Elect the Master\n",
      "\n",
      "In the first step, we consider a set of machines which together form a cluster. We need to define which machine/node will become a Master, and which ones will become Workers.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/5.jpg)\n",
      "\n",
      "The role of the Master is to :\n",
      "- split the input data on different machines\n",
      "- remember which machine handles which part of the data\n",
      "- handle duplicates to avoid loss of information if a worker fails\n",
      "- aggregate the outputs and sort the final list\n",
      "\n",
      "## Step 2: Split the input data\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/4.jpg)\n",
      "\n",
      "The input data is split into blocks of 64Mb. A 1Ko text file might use a whole 64Mb data node inside a cluster, so it is preferable to have rather big files. Optimization tools are provided by Hadoop.\n",
      "\n",
      "## Step 3: Map\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/6.jpg)\n",
      "\n",
      "In the Map, the Master returns for each split :\n",
      "- the node name to which the split should be sent\n",
      "- the mapping (Key, value) where value is equal to 1 for every word of the split\n",
      "\n",
      "## Step 4: Shuffle\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/7.jpg)\n",
      "\n",
      "In the Shuffle step, the Map-Reduce algorithm groups the words by similarity (group a dictionary by key). It is called Shuffle because the initial splits are no longer used.\n",
      "\n",
      "## Step 5: Reduce\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/8.jpg)\n",
      "\n",
      "In the Reduce step, we simply compute the sum of all values for a given key. This is simply the sum of all the 1's of the key. Remember that this step is still parallelized, so the Master still handles how the different key-value attributes are stored and computed across the different machines.\n",
      "\n",
      "## Step 6: Sorting\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/9.jpg)\n",
      "\n",
      "The last step is to sort by (first) value, then by key, and return the final list as a `.txt` file.\n",
      "\n",
      "## The Big Picture\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/10.jpg)\n",
      "\n",
      "# Replication factor and cluster types\n",
      "\n",
      "## Replication factor\n",
      "As we discussed above, Hadoop MapReduce can handle the failure of a node. When does failure occur?\n",
      "- Connection with a node is lost (network)\n",
      "- The node itself breaks (over 5'000 machines working full time in a data center, expect several to break down each day)\n",
      "\n",
      "How do we handle failure?\n",
      "- In the Map step, the splits are not sent to simply one machine, but each machine stores several data splits, in case other machines would break.\n",
      "- This is called the **replication factor**.\n",
      "- In case the Master fails, we have a second Master up and running!\n",
      "\n",
      "By default, every data block is replicated 3 times, so distributed 3 times on different DataNodes, and each NameNode is replicated 2 to 3 times. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Hadoop/11.jpg)\n",
      "\n",
      "## Cluster types\n",
      "\n",
      "As a recap, there are 3 types of nodes on a Hadoop Cluster :\n",
      "- **Edge** node: This is an entry point for the client, as we do not want him to connect straight to the Master.\n",
      "- **Master** node: Hosts all servers and administration of the MapReduce algorithm.\n",
      "- **Worker** nodes: Store the data and the different computations. If we face a higher demand, we simply dynamically add workers.\n",
      "\n",
      "### Other operations\n",
      "\n",
      "We presented a toy example using WordCount. We could also try to build Map, Combine and Reduce functions for other tasks :\n",
      "\n",
      "#### Find Minimum (or maximum) : \n",
      "\n",
      "- Map : Identity function `f(x) = (x, 1)`\n",
      "- Combine: Take the smallest key among each chunk (We could drop duplicates using the value associated with each key)\n",
      "- Reduce: Return the smallest key\n",
      "\n",
      "#### Find the mean : \n",
      "\n",
      "- Map : Identity function `f(x) = (x, 1)`\n",
      "- Combine : Take the sum of key and sum of values : `f( (a,b), (c,d) ) = (a+c, b+d)`\n",
      "- Reduce : Return the sum of keys / sum of values\n",
      "\n",
      "#### Find the median : \n",
      "\n",
      "We make the assumption that there is a limited number of different numbers :\n",
      "- Map : Identity function within a list `f(x) = [(x, 1)]`\n",
      "- Combine : Take the sum of key and sum of values : `f( [(x, 1)], [(x, 1)], [(y, 1)] ) = [(x, 2), (y, 1)]`\n",
      "- Reduce : Sort and return the median\n",
      "\n",
      "> Conclusion: I hope this high-level overview was clear and helpful. I'd be happy to answer any question you might have in the comments section.\n",
      "---\n",
      "title: Tests, p-values, restrictions\n",
      "layout: post\n",
      "tags: [statistics]\n",
      "subtitle : \"Statistical Hypothesis testing\"\n",
      "---\n",
      "\n",
      "Any parameter estimation requires statistical hypothesis testing. The point of such a test is to assess whether a given parameter is significant or not given the initial hypothesis. \n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# I. Definition and context\n",
      "\n",
      "Let's suppose we are interested in the following problem. We want to assess whether the demand for ice cream depends on the outside temperature. The model we would build would look like this :\n",
      "\n",
      "$$ y_i = \\beta_0 + \\beta_1 * x_i + u_i $$\n",
      "\n",
      "where :\n",
      "- $$ y_i $$ is the icecream demand on a given day\n",
      "- $$ \\beta_0 $$ a constant parameter\n",
      "- $$ \\beta_1 $$ the parameter that assesses the impact of temperature on icecream demand\n",
      "- $$ x_i  $$ the average temparature for a given day\n",
      "- $$ u_i  $$ the residuals\n",
      "\n",
      "# II. Compute the expectation and the variance\n",
      "\n",
      "By OLS method, we build an estimator for $$ \\beta_0 $$ and $$ \\beta_1 $$. We'll call these estimators $$ \\hat{\\beta}_0 $$ and $$ \\hat{\\beta}_1 $$, which can be described by three metrics :\n",
      "- an expectation\n",
      "- a bias\n",
      "- a variance\n",
      "\n",
      "The expectation of the parameter corresponds... to its expected value. Nothing really new here : $$ E(\\hat{\\beta}_j) $$\n",
      "\n",
      "The bias corresponds to how far we are from the actual value. It is given by : $$ E(\\hat{\\beta}_j) - {\\beta}_j $$ . The true bias is typically unknown, as we try to estimate $$ {\\beta}_j $$ . If the bias is 0, we say that the estimator is unbiased.\n",
      "\n",
      "The variance defines the stability of our estimator regarding the observations. Indeed, the features might be highly spread, which would mean a pretty big variance. \n",
      "\n",
      "It can be shown that the variance of $$ {\\beta}_1 $$ is given by :\n",
      "$$ \\hat{\\sigma}{_\\hat{\\beta_1}} = \\frac{\\hat{\\sigma}} {\\sqrt{\\sum(X_i – \\bar{X})^2}} $$\n",
      "\n",
      "And the one of  $$ {\\beta}_0 $$ by :\n",
      "$$ \\hat{\\sigma}{_\\hat{\\beta_0}} = \\hat{\\sigma} \\sqrt{\\frac{1} {n} + \\frac{\\sum(X_i)^2} {\\sum(X_i – \\bar{X})^2}} $$\n",
      "\n",
      "Where the estimated variance $$ \\hat{\\sigma} $$ is defined by : $$ \\hat{\\sigma} = \\sqrt\\frac{\\sum(Y_i – \\hat{Y}_i)^2} {n – p-1} $$ . This is an unbiaised estimator of the variance.\n",
      "\n",
      "# III. Statistical hypothesis testing\n",
      "\n",
      "## 1. Two-sided tests\n",
      "\n",
      "### T-Stat and critical value\n",
      "\n",
      "For each parameter, we want to test whether the parameter in question has a real impact on the output or not, to avoid adding dimensions that bring no significant information. In the linear regression : $$ \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1{X}_{i} $$ , it would mean testing whether the Betas are significantly different from 0 or not. \n",
      "\n",
      "To do so, we proceed to a statistical test. If our aim is the state if the parameter is significantly different from 0, we are doing a test with :\n",
      "- $$ H_0 $$ the null hypothesis : $$ {\\beta}_j = 0 $$ \n",
      "- and $$ H_1 $$ the alternative hypothesis : $$ {\\beta}_j ≠ 0 $$ . \n",
      "\n",
      "> Some further theory is needed here : Recall the Central Limit Theorem.\n",
      "$$ \\sqrt{n} \\frac{\\bar{Y}_n - {\\mu}} {\\sigma} $$ converges to $$ ∼ {N(0,1)} $$ as n tends to infinity if $$ {\\sigma} $$ is knowm.\n",
      "\n",
      "In case $$ {\\sigma} $$ is unknown, Slutsky's Lemma states that $$ \\sqrt{n} \\frac{\\bar{Y}_n - {\\mu}} {\\hat{\\sigma}} $$ converges to $$ ∼ {N(0,1)} $$ if $$ {\\hat{\\sigma}} $$ converges to $$ {\\sigma} $$ .\n",
      "\n",
      "Most of the time, $$ {\\sigma} $$ is unknown. From this point, it can be shown that :\n",
      "$$ \\hat{T}_j = \\frac{\\hat{\\beta}_j - {\\beta}_j} {\\hat{\\sigma}_j} ∼ {\\tau}_{n-p-1} $$ where $$ {\\tau}_{n-p-1} $$ and $$ n-p-1 $$ is the degrees of freedom (p is the dimension, equal to 1 here).\n",
      "\n",
      "This metric is called the T-Stat, and it allows us to perform a hypothesis test. The 0 in the numerator can be replaced by any value we would like to test actually.\n",
      "\n",
      "The T-Stat can be decomposed this way :\n",
      "- $$ \\hat{\\beta}_j $$ is the estimated parameter\n",
      "- $$ {\\beta}_j $$ is the value of the true parameter we are testing\n",
      "- $$ \\hat{\\beta}_j - {\\beta}_j $$ follows a Normal distribution \n",
      "- $$  {\\hat{\\sigma}_j}^2 = {\\hat{\\sigma}^2}_{\\hat{\\beta_1}} $$ follows a Chi Square Distribution\n",
      "- a ratio of a Normal over the square root of a Chi Square is a Student distribution\n",
      "\n",
      "How to interpret the T-Stat?\n",
      "\n",
      "The T-Stat should be compared with the Critical Value. The critical value is the quantile of the corresponding Student distribution at a given level of $$ {\\alpha} $$. If a coefficient is significant at a level $$ {\\alpha} $$ , this means that the T-Stat is above or under the quantiles of the Student Distribution.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Graph3.jpg){:height=\"50%\" width=\"50%\"}\n",
      "\n",
      "A parameter is said to be significant if its value is significantly different from 0, i.e $$ \\hat{T}_j = \\frac{\\hat{\\beta}_j - 0} {\\hat{\\sigma}_j} $$ is larger than the critical value.\n",
      "\n",
      "> If the T-Stat is smaller than the critical value, we cannot reject the null hypothesis $$ H_0 $$.\n",
      "\n",
      "### The p-value\n",
      "\n",
      "Another interpretation is that the probability that the coefficient estimate is not in the interval $$ [- {t}_{1-{\\alpha}/2}; + {t}_{1-{\\alpha}/2} ] $$ is smaller than $$ {\\alpha} $$ . This probability is called the p-value and is defined by :\n",
      "\n",
      "$$ p_{value} = Pr( |\\hat{T}_j| > |{t}_{1-{\\alpha}/2}|) $$\n",
      "\n",
      "### Confidence interval\n",
      "\n",
      "Using the CLT, one can set a confidence interval around an estimate of a parameter.\n",
      "\n",
      "The lower bound and the upper bound are determined by the critical value of the student distribution at a level $$ {\\alpha} $$, and by the standard deviation of the parameter.\n",
      "$$ {\\beta}_1 ± {t}_{1-{\\alpha}/2} * \\hat{\\sigma}_{\\hat{\\beta_1}} $$\n",
      "\n",
      "## 2. One-sided tests\n",
      "\n",
      "Implicitely, when we defined the hypothesis to test :\n",
      "- Null Hypothesis $$ H_0 $$ the null hypothesis : $$ {\\beta}_j = 0 $$ \n",
      "- and $$ H_1 $$ the alternative hypothesis : $$ {\\beta}_j ≠ 0 $$ \n",
      "\n",
      "we implied a bi-lateral test. Indeed, when we define $$ H_1 $$, we state that both a negative or a positive value of the parameter are considered as failures of $$ H_0 $$.\n",
      "\n",
      "Now, let's redefine the hypothesis :\n",
      "- Null Hypothesis $$ H_0 : {\\beta}_j = 0 $$ \n",
      "- and $$ H_1 : {\\beta}_j > 0 $$\n",
      "\n",
      "In this case, we are only interested in one side of the distribution :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/uni_test.jpg)\n",
      "\n",
      "# III. Joint hypothesis\n",
      "\n",
      "Let's add some new variables to our model to explain and predict the icecreams consumption.\n",
      "\n",
      "$$ y_i = \\beta_0 + \\beta_1 * {X_1}_i + \\beta_2 * {X_2}_i + u_i $$\n",
      "\n",
      "## 1. One parameter against another\n",
      "\n",
      "Now, what happens if we want to test one parameter against another?\n",
      "\n",
      "For example, we could define the new hypothesis this way :\n",
      "- Null Hypothesis $$ H_0 : \\beta_1 = \\beta_2 $$ \n",
      "- and $$ H_1 : \\beta_1 > \\beta_2 $$\n",
      "\n",
      "The T-Stat would become :\n",
      "\n",
      "$$ \\hat{T}_j = \\frac{\\hat{\\beta}_1 - \\hat{\\beta}_2} {\\hat{\\sigma}_{\\hat{\\beta}_1 - \\hat{\\beta}_2}} $$\n",
      "\n",
      "The standard deviation is however quite difficult to estimate. For this reason, we define a new parameter : $$ \\theta_1 = \\beta_1 - \\beta_2 $$ . This way, we can just redefine our hypothesis :\n",
      "- Null Hypothesis $$ H_0 : \\theta_1 = 0 $$ \n",
      "- and $$ H_1 : \\theta_1 < 0 $$\n",
      "\n",
      "We can replace $$ \\theta_1 $$ in our model :\n",
      "\n",
      "$$ y_i = \\beta_0 + \\beta_1 * {X_1}_i + \\beta_2 * {X_2}_i + u_i $$\n",
      "\n",
      "$$ y_i = \\beta_0 + (\\theta_1 + \\beta_2) * {X_1}_i + \\beta_2 * {X_2}_i + u_i $$\n",
      "\n",
      "$$ y_i = \\beta_0 + \\theta_1 * {X_1}_i + \\beta_2 * ({X_2}_i + {X_1}_i) + u_i $$\n",
      "\n",
      "## 2. Multiple restrictions\n",
      "\n",
      "We might also want to test several hypothesis at once. For example, in a more complex model :\n",
      "\n",
      "$$ y_i = \\beta_0 + \\beta_1 * {X_1}_i + \\beta_2 * {X_2}_i + \\beta_3 * {X_3}_i  + \\beta_4 * {X_4}_i  + u_i $$\n",
      "\n",
      "We might want to test the following joint hypothesis :\n",
      "- Null Hypothesis $$ H_0 : \\beta_2 = 0, \\beta_3 = 0, \\beta_4 = 0 $$ \n",
      "- and $$ H_1 : H_0 $$ is not true\n",
      "\n",
      "The one thing is to avoid using individual student tests. The intuition we rather use is the following: if all the coefficients are not jointly significant, the sum of the squared errors should not diminish if we delete some variables. In other words, under a constrained model where $$ \\beta_2 = 0, \\beta_3 = 0, \\beta_4 = 0 $$, the Sum of Squared Residuals does not change.\n",
      "\n",
      "Recall that we define the Sum of Squared Residuals (SSR) as :\n",
      "\n",
      "$$ SSR = \\sum \\hat{u_i}^2 $$\n",
      "\n",
      "The constrained model would become :\n",
      "\n",
      "$$ y_i = \\beta_0 + \\beta_1 * {X_1}_i + u_i $$\n",
      "\n",
      "To test the hypothesis defined above, we define the F-Stat :\n",
      "\n",
      "$$ \\hat{F}_j = \\frac { \\frac { SSR_c - SSR_{uc} } {q} } {\\frac {SSR_{uc}} {n-k-1}} $$ \n",
      "\n",
      "Where :\n",
      "- $$ SSR_c $$ is the sum of squared residuals for the constrained model\n",
      "- $$ SSR_{uc} $$ is the sum of squared residuals for the unconstrained model\n",
      "- $$ q $$ is the number of restrictions we apply\n",
      "\n",
      "Under those hypothesis, the F-Stat follows a Fisher distribution : $$ \\hat{F}_j ∼ {F}_{n-q-1} $$\n",
      "\n",
      "We can use the Fisher distribution to find the critical value $$ C $$. As before, we reject $$ H_0 $$ if $$ \\hat{F}_j > C $$.\n",
      "\n",
      "If we apply only one constraint, the F-Stat is similar to the T-Stat.\n",
      "\n",
      "> **Conclusion **: We have covered the most common tests to apply in the linear regression framework. Don't hesitate to drop a comment if you have a question.\n",
      "---\n",
      "title: Working with Amazon S3 buckets\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Amazon Web Services\"\n",
      "---\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/s3_head.jpg)\n",
      "\n",
      "Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "## Create a Bucket\n",
      "\n",
      "S3 buckets can be created from the console, or directly by following this link :  <span style=\"color:blue\">[https://s3.console.aws.amazon.com/s3/home?region=us-east-1#](https://s3.console.aws.amazon.com/s3/home?region=us-east-1#)</span> ).\n",
      "\n",
      "Click on Create bucket :\n",
      "![image](https://maelfabien.github.io/assets/images/s3.jpg){:height=\"80%\" width=\"80%\"}\n",
      "\n",
      "You must give your bucket a unique name :\n",
      "![image](https://maelfabien.github.io/assets/images/s3_2.jpg){:height=\"80%\" width=\"80%\"}\n",
      "\n",
      "You can keep the other default parameters, and confirm the creation of the bucket.\n",
      "\n",
      "## Download AWS CLI\n",
      "\n",
      "For some operations like checking the volume of an S3 bucket, using AWS CLI is useful. In your terminal, run the following command :\n",
      "``` bash\n",
      "$ pip install awscli --upgrade\n",
      "```\n",
      "\n",
      "Then, type :\n",
      "```\n",
      "$ aws configure\n",
      "```\n",
      "\n",
      "Answer the different questions. Make sure to have the ``` credentials.csv ```  file you downloaded when you created a user group. If you don't have one, I invite you to check my article on how to run a Zeppelin notebook on EMR.\n",
      "\n",
      "``` bash\n",
      "AWS Access Key ID [None]: XXX\n",
      "AWS Secret Access Key [None]: XXX\n",
      "Default region name [None]: us-east-1\n",
      "Default output format [None]: text\n",
      "````\n",
      "\n",
      "## Check content of an S3 bucket\n",
      "\n",
      "Finally, you can check the content of your S3 bucket pretty easily :\n",
      "\n",
      "```bash\n",
      "$ aws s3 ls --summarize --human-readable --recursive s3://mael-fabien-test-bucket/\n",
      "````\n",
      "\n",
      "Which should return for your newly created bucket :\n",
      "``` bash \n",
      "Total Objects: 0\n",
      "Total Size: 0 Bytes\n",
      "```\n",
      "---\n",
      "title: Activation Functions\n",
      "layout: post\n",
      "tags: [deeplearning]\n",
      "subtitle : \"Deep Learning with PyTorch\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "When building your Deep Learning model, activation functions are an important choice to make.  In this article, we'll review the main activation functions, their implementations in Python, and advantages/disadvantages of each. \n",
      "\n",
      "# Linear Activation\n",
      "\n",
      "Linear activation is the simplest form of activation. In that case, $$ f(x) $$ is just the identity. If you use a linear activation function the wrong way, your whole Neural Network ends up being a regression :\n",
      "\n",
      "$$ \\hat{y} = \\sigma(h) = h = W^{(2)} h_1 = W^{(1)} W^{(2)} X = W' X $$\n",
      "\n",
      "Linear activations are only needed when you're considering a regression problem, as a last layer. The whole idea behind the other activation functions is to create non-linearity, to be able to model highly non-linear data that cannot be solved by a simple regression !\n",
      "\n",
      "# ReLU\n",
      "\n",
      "ReLU stands for **Rectified Linear Unit**. It is a widely used activation function. The formula is simply the maximum between $$ x $$ and 0 :\n",
      "\n",
      "$$ f(x) = max(x, 0) $$\n",
      "\n",
      "To implement this in Python, you might simply use :\n",
      "\n",
      "```python\n",
      "def relu(x) :\n",
      "    return max(x, 0)\n",
      "```\n",
      "\n",
      "The derivative of the ReLU is :\n",
      "- $$ 1 $$ if $$ x $$ is greater than 0\n",
      "- $$ 0 $$ if $$ x $$ is smaller or equal to 0\n",
      "\n",
      "```python\n",
      "def der_relu(x):\n",
      "    if x <= 0 :\n",
      "        return 0\n",
      "    if x > 0 :\n",
      "        return 1\n",
      "````\n",
      "\n",
      "Let's simulate some data and plot them to illustrate this activation function :\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Data which will go through activations\n",
      "x = np.linspace(-10,10,100)\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(x, list(map(lambda x: relu(x),x)), label=\"relu\")\n",
      "plt.plot(x, list(map(lambda x: der_relu(x),x)), label=\"derivative\")\n",
      "plt.title(\"ReLU\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_24.jpg)\n",
      "  \n",
      "| Advantage | Easy to implement and quick to compute |\n",
      "| Disadvantage | Problematic when we have lots of negative values, since the outcome is always 0 and leads to the death of the neuron |\n",
      "\n",
      "# Leaky-ReLU\n",
      "\n",
      "Leaky-ReLU is an improvement of the main default of the ReLU, in the sense that it can handle the negative values pretty well, but still brings non-linearity. \n",
      "\n",
      "$$ f(x) = max(0.01x, x) $$\n",
      "\n",
      "The derivative is also simple to compute :\n",
      "- $$ 1 $$ if $$ x>0 $$\n",
      "- $$ 0.01 $$ else\n",
      "\n",
      "```python\n",
      "def leaky_relu(x):\n",
      "    return max(0.01*x,x)\n",
      "\n",
      "def der_leaky_relu(x):\n",
      "    if x < 0 :\n",
      "        return 0.01\n",
      "    if x >= 0 :\n",
      "        return 1\n",
      "```\n",
      "\n",
      "And we can plot the result of this activation function :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(x, list(map(lambda x: leaky_relu(x),x)), label=\"leaky-relu\")\n",
      "plt.plot(x, list(map(lambda x: der_leaky_relu(x),x)), label=\"derivative\")\n",
      "plt.title(\"Leaky-ReLU\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_25.jpg)\n",
      "\n",
      "| Advantage | Leaky-ReLU overcomes the problem of the death of the neuron linked to a zero-slope. |\n",
      "| Disadvantage | The factor 0.01 is arbitraty, and can be tuned (PReLU for parametric ReLU) |\n",
      "\n",
      "# ELU\n",
      "\n",
      "Exponential Linear Units (ELU) try to make the mean activations closer to zero, which speeds up learning.\n",
      "\n",
      "$$ f(x) = x $$ if $$ x>0 $$, and $$ a(e^x -1) $$ otherwise, where $$ a $$ is a positive constant.\n",
      "\n",
      "The derivative is :\n",
      "- $$ 1 $$ if $$ x > 0 $$\n",
      "- $$ a * e^x $$ otherwise \n",
      "\n",
      "If we set $$ a $$ to 1 :\n",
      "\n",
      "```python\n",
      "def elu(x):\n",
      "    if x > 0 :\n",
      "        return x\n",
      "    else : \n",
      "        return (np.exp(x)-1)\n",
      "\n",
      "def der_elu(x):\n",
      "    if x > 0 :\n",
      "        return 1\n",
      "    else :\n",
      "        return np.exp(x)\n",
      "```\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(x, list(map(lambda x: elu(x),x)), label=\"elu\")\n",
      "plt.plot(x, list(map(lambda x: der_elu(x),x)), label=\"derivative\")\n",
      "plt.title(\"ELU\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_30.jpg)\n",
      "\n",
      "| Advantage | Can achieve higher accuracy than ReLU |\n",
      "| Disadvantage | Same as ReLU, and a needs to be tuned |\n",
      "\n",
      "# Softplus\n",
      "\n",
      "The Softplus function is a continuous approximation of ReLU. It is given by :\n",
      "\n",
      "$$ f(x) = log(1+e^x) $$\n",
      "\n",
      "The derivative of the softplus function is :\n",
      "\n",
      "$$ f'(x) = \\frac{1}{1+e^x}e^x $$\n",
      "\n",
      "You can implement them in Python :\n",
      "\n",
      "```python\n",
      "def softplus(x):\n",
      "    return np.log(1+np.exp(x))\n",
      "\n",
      "def der_softplus(x):\n",
      "    return 1/(1+np.exp(x))*np.exp(x)\n",
      "```\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(x, list(map(lambda x: softplus(x),x)), label=\"softplus\")\n",
      "plt.plot(x, list(map(lambda x: der_softplus(x),x)), label=\"derivative\")\n",
      "plt.title(\"Softplus\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_26.jpg)\n",
      "\n",
      "| Advantage | Softplus is continuous and might have good properties in terms of derivability. It is interesting to use it when the values are between 0 and 1. |\n",
      "| Disadvantage | As ReLU, problematic when we have lots of negative values, since the outcome gets really close to 0 and might lead to the death of the neuron |\n",
      "\n",
      "# Sigmoid\n",
      "\n",
      "Sigmoid is one of the most common activation functions in litterature these days. The sigmoid function has the following form :\n",
      "\n",
      "$$ f(x) = \\frac{1}{1+e^{-x}} $$\n",
      "\n",
      "The derivative of the sigmoid is :\n",
      "\n",
      "$$ f'(x) = - e^{-x} \\frac{1}{(1+e^{-x})^2} = \\frac {1 + e^{-x} -1}{(1+e^{-x})^2} $$\n",
      "\n",
      "$$ = \\frac {1}{1+e^{-x}} (1-\\frac {1}{1+e^{-x}}) = f(x)(1-f(x)) $$\n",
      "\n",
      "In Python :\n",
      "\n",
      "```python\n",
      "def softmax(x):\n",
      "    return 1/(1+np.exp(-x))\n",
      "\n",
      "def der_softmax(x):\n",
      "    return 1/(1+np.exp(-x)) * (1-1/(1+np.exp(-x)))\n",
      "```\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(x, list(map(lambda x: softmax(x),x)), label=\"softmax\")\n",
      "plt.plot(x, list(map(lambda x: der_softmax(x),x)), label=\"derivative\")\n",
      "plt.title(\"Softmax\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_27.jpg)\n",
      "\n",
      "It might not be obvious when considering data between $$ -10 $$ and $$ 10 $$ only, but the sigmoid is subject to the vanishing gradient problem. This means that the gradient will tend to vanish as $$ x $$ takes large values.\n",
      "\n",
      "Since the gradient is the sigmoid times 1 minus the sigmoid, the gradient can be efficiently computed. If we keep track of the sigmoids, we can compute the gradient really quickly.\n",
      "\n",
      "| Advantage | Interpretability of the output mapped between 0 and 1, compute gradient quickly |\n",
      "| Disadvantage | Vanishing Gradient |\n",
      "\n",
      "# Hyperbolic Tangent\n",
      "\n",
      "Hyperbolic tangent is quite similar to the sigmoid function, expect is maps the input between -1 and 1.\n",
      "\n",
      "$$ f(x) = tanh(x) = \\frac {sinh(x)}{cosh(x)} = \\frac {e^x - e^{-x}}{e^x + e^{-x}} $$\n",
      "\n",
      "The derivative is computed in the following way :\n",
      "\n",
      "$$ f'(x) = \\frac{\\partial}{\\partial x} \\frac{sinh(x)}{cosh(x)} $$\n",
      "\n",
      "$$ = \\frac { \\frac{\\partial}{\\partial x} sinh(x) cosh(x) -  \\frac{\\partial}{\\partial x} cosh(x) sinh(x) } {cosh^2(x)} $$\n",
      "\n",
      "$$ = \\frac { cosh^2(x) -  sinh^2(x) } {cosh^2(x)} $$\n",
      "\n",
      "$$ = 1 - \\frac { sinh^2(x) } { cosh^2(x) } $$\n",
      "\n",
      "$$ = 1 - tanh^2(x) $$\n",
      "\n",
      "Implement this in Python :\n",
      "\n",
      "```python\n",
      "def hyperb(x):\n",
      "    return (np.exp(x)-np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
      "\n",
      "def der_hyperb(x):\n",
      "    return 1 - ((np.exp(x)-np.exp(-x)) / (np.exp(x) + np.exp(-x)))**2\n",
      "```\n",
      "\n",
      "And plot it :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(x, list(map(lambda x: hyperb(x),x)), label=\"hyperbolic\")\n",
      "plt.plot(x, list(map(lambda x: der_hyperb(x),x)), label=\"derivative\")\n",
      "plt.title(\"Hyperbolic\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_28.jpg)\n",
      "\n",
      "| Advantage | Efficient since it has mean 0 in the middle layers between -1 and 1 |\n",
      "| Disadvantage | Vanishing gradient too |\n",
      "\n",
      "# Arctan\n",
      "\n",
      "This activation function maps the input values in the range $$ (− \\pi / 2, \\pi / 2) $$. It is the inverse of a hyperbolic tangent function.\n",
      "\n",
      "$$ f(x) = arctan(x) $$\n",
      "\n",
      "And the derivative :\n",
      "\n",
      "$$ f'(x) = \\frac {1} {1+x^2} $$\n",
      "\n",
      "In Python :\n",
      "\n",
      "```python\n",
      "def arctan(x):\n",
      "    return np.arctan(x)\n",
      "\n",
      "def der_arctan(x):\n",
      "    return 1 / (1+x**2)\n",
      "```\n",
      "\n",
      "And plot it :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(x, list(map(lambda x: arctan(x),x)), label=\"arctan\")\n",
      "plt.plot(x, list(map(lambda x: der_arctan(x),x)), label=\"derivative\")\n",
      "plt.title(\"Arctan\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_31.jpg)\n",
      "\n",
      "| Advantage | Simple derivative |\n",
      "| Disadvantage | Vanishing gradient |\n",
      "\n",
      "---\n",
      "title: Introduction to Online Learning\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Advanced Machine Learning\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "Large volumes of data are nearly impossible to process using **batch learning**. Batch learning is the classical process by which we train a model of a given batch of the training data. With tens of millions of data points, Random Forest algorithms, for example, become nearly impossible to compute in a reasonable time.\n",
      "\n",
      "Therefore, 2 options exist :\n",
      "- use better hardware: more powerful computers, or parallelized computation. This is however costly, and not always possible.\n",
      "- build online learning algorithms\n",
      "\n",
      "# What is Online learning?\n",
      "\n",
      "## Concept\n",
      "\n",
      "Online learning is the process by which we :\n",
      "- make an initial guess model\n",
      "- pick one observation from the training data and recalibrate the weights on each input parameter\n",
      "- we make one pass over the data, sequentially, as they come in from the stream\n",
      "\n",
      "Thanks to Online learning, we are allowed to :\n",
      "- make one pass on the input data, which is time-efficient\n",
      "- not store all points in the learning procedure, which is memory efficient\n",
      "\n",
      "However, when using Online learning :\n",
      "- the features we previously built might not be relevant anymore\n",
      "- if the data changes over time, the model is not relevant anymore\n",
      "- if the network connection is down, the model cannot produce an output anymore\n",
      "- it is hard to state whether an algorithm performs well since there is no test set typically\n",
      "\n",
      "## Illustration\n",
      "\n",
      "Let's create a small example for classification purposes :\n",
      "- say that we have 10 voters, i.e people who are going to vote on the classification task\n",
      "- we initialize the weight of each person to 1\n",
      "\n",
      "We build a model based on those features :\n",
      "- if a person is right when predicting the output, we keep its weight constant\n",
      "- if a person is wrong when predicting the output, we divide its weight by a factor > 1\n",
      "\n",
      "This way, we are making sure that the weight of the voters that tend to predict the wrong classes is small, whereas the weight of the voters that predict correctly the output remains high\n",
      "\n",
      "## When to use Online Learning\n",
      "\n",
      "We should use Online Learning when :\n",
      "- there is too much data to fit in memory, or the training time of a batch learning algorithm takes too long for our application\n",
      "- the application is itself inherently online\n",
      "\n",
      "# Mathematical motivation\n",
      "\n",
      "Suppose that we tell someone to randomly say 0 or 1, and try to teach an algorithm to predict what the user is about to say. Using batch learning, the accuracy would typically not exceed 50%. There is however an inherent nonrandomness in the sequences that humans can generate.\n",
      "\n",
      "That being said, we could be able to develop an algorithm that predicts the right sequence with an accuracy slightly higher than 50%.\n",
      "\n",
      "## Deterministic Strategy\n",
      "\n",
      "Suppose that we try to predict the person's random sequence with a deterministic strategy : $$ A = (\\hat{y_1}, ... \\hat{y_n}) $$ where $$ y = (y_1, ..., y_n) \\in \\{-1,1\\} $$ is the sequence generated by the user and $$ \\hat{y} =  \\hat{y}(y_1, ..., y_n) $$ is the corresponding predicted sequence.\n",
      "\n",
      "We make an average number of mistakes equal to :\n",
      "\n",
      "$$ \\frac{1}{n} \\sum_t 1{\\hat{y_t} ≠ y_t} $$\n",
      "\n",
      "If we systematically predict the wrong output, the average loss will be 1.\n",
      "\n",
      "## Randomized Strategy\n",
      "\n",
      "Using a randomized strategy, our algorithm is denoted by : $$ A=(q_1, ..., q_n) $$ with $$ q_t = q_t(y_1, ..., y_{t-1}) \\in [-1,1] $$ \n",
      "\n",
      "The average number of mistakes we'll make over a given sequence using this randomized algorithm is now :\n",
      "\n",
      "$$ l(A; y_1, ..., y_n) = E(\\frac{1}{n} \\sum_t 1{\\hat{y_t} ≠ y_t}) $$\n",
      "\n",
      "An algorithm guessing randomly will systematically reach an average loss of $$ \\frac {1}{2} $$, so there is no longer a \"bad sequence\" that would lead to an average loss of 1.\n",
      "\n",
      "We would be winning if you could fin an algorithm for which $$ l(A; y_1, ..., y_n)  $$ is always smaller than 50%. \n",
      "\n",
      "If we consider Randemacher random variables (unbiaised coin flips) $$ \\epsilon_1, ... ,\\epsilon_n $$, then any algorithm will necessarly make more mistakes on some sequences, and less mistakes on other, with an average of 50% :\n",
      "\n",
      "$$ \\frac{1}{2^n} \\sum_{y_1, ..., y_n} l(A; y_1, ..., y_n) = E_{\\epsilon_1, ... ,\\epsilon_n}l(A; \\epsilon_1, ... ,\\epsilon_n) = \\frac{1}{2} $$\n",
      "\n",
      "## Sequence patterns\n",
      "\n",
      "In practice, asking a human to produce random sequences brings a bias, since we might find some patterns within the generated sequences. If we care about predicting correctly some sequence, typically the ones generated by humans, while accepting that we'll perform poorly on others, we might find an algorithm $$ A $$ that suits our needs.\n",
      "\n",
      "We have some idea of the algorithms we'll encounter in practice. But how do we build an algorithm that takes this into account?\n",
      "\n",
      "We denote $$ \\phi(y_1, ..., y_n) $$ as the target expected value we'd like to achieve on a sequence $$ y_1, ..., y_n $$. In other words, it's the next value of the sequence. We compute the values of $$ \\phi $$ on all such sequences. \n",
      "\n",
      "In a sense, $$ \\phi $$ is a function on the binary hypercube $$ \\{-1,1\\}^n $$. We define $$ \\phi : \\{±1\\}^n → R $$. Under the uniform distribution on $$ \\{±1\\}^n $$, $$ E_{\\phi} = \\frac{1}{2} $$\n",
      "\n",
      "\n",
      "> **Lemma** : If $$ E_{\\phi(\\epsilon_1, ... ,\\epsilon_n)} = \\frac{1}{2} $$, there is a randomized algorithm such that $$ l(A; y_1, ..., y_n) = \\phi(y_1, ..., y_n) $$ for all sequences.\n",
      "\n",
      "> **Corollary** : For any $$ \\phi : \\{±1\\}^n $$ and $$ E_{\\phi} ≥ \\frac{1}{2} $$, then, there exists a randomized algorithm $$ A $$ making, on average over its randomization, no more than $$ \\phi(y_1, ..., y_n) $$ number of mistakes for any sequence.\n",
      "\n",
      "If we are predicting node labels, for example, we might tilt $$ \\phi $$ towards smooth labeling to respect the manifold property and respect an expected homogeneity among a graph structure.\n",
      "\n",
      "## Constructing $$ \\phi $$\n",
      "\n",
      "One of the first method to construct a good $$ \\phi $$ in to combine several good predictors into a meta-predictor. After that, we may define a collection of $$ \\phi $$'s that predict human-generated sequences according to some finite state machines and create a meta-predictor that does as well as the best of them on the given sequence.\n",
      "\n",
      "Performing as well as a finite collection of predictors is called **experts setting**. Assume that we have access to the predictions of each of the k expert, say $$ P_1, ..., P_k $$. On round $$ t $$, they produce a number, called **advice**, $$ P_j(y_1, ..., y_{t-1}) \\in \\{-1, 1\\} $$.\n",
      "\n",
      "For each $$ j \\in k $$, define :\n",
      "\n",
      "$$ \\phi^j(y_1, ..., y_n) = \\frac{1}{n} \\sum_t 1 \\{ P_j(y_1, ..., y_{t-1}) ≠ y_t \\} $$, the performance of expert $$ j $$, which is known only at the end. Our aim is to do as well as the best expert, say :\n",
      "\n",
      "$$ min_{j \\in k} \\phi^j(y_1, ..., y_n) $$\n",
      "\n",
      "Under uniform distribution, the expectation of this function is less than $$ {1}{2} $$. Recall the first corollary, in which we stated that in order to have a decent accuracy on human generated sequences, we need to have $$ E_{\\phi} ≥ \\frac{1}{2} $$. We will add a vanishing term to the expression defined above :\n",
      "\n",
      "$$ \\phi(y_1, ..., y_n) = min_{j \\in k} \\phi^j(y_1, ..., y_n) + \\sqrt{ \\frac {c \\log(k)}{n}} $$\n",
      "\n",
      "## Example\n",
      "\n",
      "Suppose that we have information that human tend to have an imbalance between the 0's and the 1's in the sequences they generate. Then, there is a simple algorithm that will make at most :\n",
      "\n",
      "$$ min(\\bar{p}, 1-\\bar{p}) + O(1/\\sqrt{n}) $$\n",
      "\n",
      "The method does not need to know the proportion. If the sequence consists of 80% of 1's, the algorithm will roughly make 20% of errors if n is large. To conclude this introduction, we can easily build a prediction method that will win over any unbalanced sequence entered by a human.\n",
      "\n",
      "> **Conclusion** : That's it ! I hope this introduction to Online Learning was clear. Don't hesitate to drop a comment if you have any question.\n",
      "---\n",
      "title: Face Classification using XCeption\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "In this challenge, our aim was to develop face classification algorithms using Deep Learning Architectures. I have explored hand-made CNNs, Inception, XCeption, VGG16, DenseNet or ResNet networks for binary classification purposes.\n",
      "\n",
      "The report paper I submitted can be found here :\n",
      "\n",
      "<embed src=\"https://maelfabien.github.io/assets/images/DataChallenge.pdf\" type=\"application/pdf\" width=\"600px\" height=\"500px\" />\n",
      "\n",
      "---\n",
      "title: Predicting the next hit song (Part 2)\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Applied Data Science\"\n",
      "search: false\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "In the first part of this two-part series, we explored basic models and data enrichments for our hit song classifier. In this article, we will try to push our model a little more by attempting to improve its performance through better data enrichment and feature engineering. \n",
      "Before we get started, let’s recall the context.\n",
      "\n",
      "# The context\n",
      "\n",
      "There’s no shortage of articles and papers trying to explain why a song became a hit, and the features hit songs share. Here, we will try to go a bit further and build a hit song classifier. To build such a classifier, we’ll typically need a lot of data enrichment because there is no single source of data that can help with such a large task. \n",
      "\n",
      "We will use the following sources to help us build the dataset:\n",
      "- Google Trends\n",
      "- Spotify\n",
      "- Billboard\n",
      "- Genius.com\n",
      "\n",
      "We will consider a song a hit only if it reached the top 10 of the most popular songs of the year. Otherwise, it does not count as a hit.\n",
      "\n",
      "In part one we used data from the Billboard Year-End Hot100 Singles Chart between 2010 and 2018. We then enriched the data using Spotify’s API. Our model achieved an accuracy of 93% on the test set.\n",
      "\n",
      "\n",
      "# Data Enrichment through Genius.com\n",
      "\n",
      "[Genius.com](http://genius.com/) is a great resource if you are looking for song lyrics. It offers a great API, all of which is packaged in a great library called lyricsgenius. Start by installing the package (instructions can be found on [GitHub](https://github.com/johnwmillr/LyricsGenius)).\n",
      "\n",
      "You will have to get a token from [Genius.com developer's website](https://docs.genius.com/).\n",
      "\n",
      "Start by importing the package:\n",
      "\n",
      "```python\n",
      "import lyricsgenius as genius\n",
      "api = genius.Genius('YOUR_TOKEN_GOES_HERE')\n",
      "```\n",
      "\n",
      "As before, the API has a powerful search functionality:\n",
      "\n",
      "```python\n",
      "def lookup_lyrics(song):\n",
      "    try :\n",
      "        return api.search_song(song).lyrics\n",
      "    except :\n",
      "        return None\n",
      "```\n",
      "\n",
      "You’ll need to create a column “lyrics” that contains the lyrics of each song. This one might take some time.\n",
      "\n",
      "```python\n",
      "df['lyrics'] = df['lookup'].apply(lambda x: lookup_lyrics(x))\n",
      "```\n",
      "\n",
      "Notice how some of the text is not clean and contains `\\n` to denote a new line or has text between brackets to split sections:\n",
      "\n",
      "```python\n",
      "def clean_txt(song):\n",
      "    song = ' '.join(song.split(\"\\n\"))\n",
      "    song = re.sub(\"[\\[].*?[\\]]\", \"\", song)\n",
      "    return song\n",
      "\n",
      "df['lyrics'] = df['lyrics'].apply(lambda x: clean_txt(x))\n",
      "df = df.dropna() #Drop song if we don't have lyrics\n",
      "```\n",
      "\n",
      "Some features we could add are:\n",
      "- The length of the lyrics\n",
      "- The number of unique words used\n",
      "- The length of the lyrics without stopwords\n",
      "- The number of unique words used without stopwords\n",
      "\n",
      "We will use NLTK stop words list in English. However, we should also consider that some of the songs of the Billboard Year-End Hot100 Singles Chart are not English songs.\n",
      "\n",
      "```python\n",
      "from nltk.corpus import stopwords \n",
      "from nltk.tokenize import word_tokenize \n",
      "stop_words = set(stopwords.words('english'))\n",
      "\n",
      "def len_lyrics(song):\n",
      "    return len(song.split())\n",
      "\n",
      "def len_unique_lyrics(song):\n",
      "    return len(list(set(song.split())))\n",
      "\n",
      "def rmv_stop_words(song):\n",
      "    song = [w for w in song.split() if not w in stop_words] \n",
      "    return len(song)\n",
      "\n",
      "def rmv_set_stop_words(song):\n",
      "    song = [w for w in song.split() if not w in stop_words] \n",
      "    return len(list(set(song)))\n",
      "```\n",
      "\n",
      "Next, apply this to the dataset :\n",
      "\n",
      "```python\n",
      "df['len_lyrics'] = df['lyrics'].apply(lambda x: len_lyrics(x))\n",
      "df['len_unique_lyrics'] = df['lyrics'].apply(lambda x: len_unique_lyrics(x))\n",
      "df['without_stop_words'] = df['lyrics'].apply(lambda x: rmv_stop_words(x))\n",
      "df['unique_without_stop_words'] = df['lyrics'].apply(lambda x: rmv_set_stop_words(x))\n",
      "```\n",
      "\n",
      "## Data exploration\n",
      "\n",
      "Just like in the first article, some data exploration might bring us additional insights.\n",
      "\n",
      "How many words are used in the lyrics?\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist(df[df['len_lyrics']<2000]['len_lyrics'], bins=70) #Not plot outliers\n",
      "plt.title(\"Number of words\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_15.png)\n",
      "\n",
      "The histogram above does not represent outliers, but a few songs count over 2000 words. On average, there are 467 words in a song and 166 unique words. This can be verified by:\n",
      "\n",
      "```python\n",
      "np.mean(df['len_lyrics'])\n",
      "np.mean(df['len_unique_lyrics'])\n",
      "```\n",
      "\n",
      "The ratio of unique words over total words is 35%. We can also plot the distribution of this ratio:\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist(df['len_unique_lyrics']/df['len_lyrics'], bins=50)\n",
      "plt.title(\"Ratio Unique Words over total words\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_20.png)\n",
      "\n",
      "The vast majority of the songs do not exceed 40% of unique words, which reflects the balance that hit songs reach between repetitive lyrics and a diversified vocabulary. The vast majority of To illustrate the diversity of the vocabulary used in the songs, we can compute the ratio of words that are not stop words over all words:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_21.png)\n",
      "\n",
      "When we remove the stop words, the average ratio is now much higher. A large part of the vocabulary used in those songs seems to be made of stop words.  This is it for the count of words. Now, wWhat are the most common words that singers use in their songs?\n",
      "\n",
      "```python\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "word_cloud = df['lyrics'].values\n",
      "\n",
      "str1 = ' '.join(word_cloud)\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(str(str1))\n",
      "\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.imshow(wordcloud, interpolation='bilinear')\n",
      "plt.axis(\"off\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_16.png)\n",
      "\n",
      "We won’t spend too much time commenting this, but “yeah,” “oh,” and “baby” should definitely be on your hit-song to-do list.\n",
      "\n",
      "## Lyrics sentiment\n",
      "\n",
      "Should a song be positive? Negative? Neutral? To assess the positiveness of a song and its intensity, we will use Valence Aware Dictionary and sEntiment Reasoner (VADER), a lexicon and rule-based sentiment analysis tool, available on [Github](https://github.com/cjhutto/vaderSentiment). This method relies on lexicons, and has over 7500 words annotated by linguists. This kind of algorithm was used before the rise of Natural Language Processing, but can still be useful in cases like this one where we do not have labeled data or trained models for song sentiment classification.\n",
      "\n",
      "\n",
      "```python\n",
      "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
      "analyzer = SentimentIntensityAnalyzer()\n",
      "\n",
      "df['sentimentVaderPos'] = df['lyrics'].apply(lambda x: analyzer.polarity_scores(x)['pos'])\n",
      "df['sentimentVaderNeg'] = df['lyrics'].apply(lambda x: analyzer.polarity_scores(x)['neg'])\n",
      "df['sentimentVaderComp'] = df['lyrics'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
      "df['sentimentVaderNeu'] = df['lyrics'].apply(lambda x: analyzer.polarity_scores(x)['neu'])\n",
      "```\n",
      "\n",
      "We can also create a feature that is the difference between the positive and the negative score:\n",
      "\n",
      "```python\n",
      "df['Vader'] = df['sentimentVaderPos'] - df['sentimentVaderNeg']\n",
      "```\n",
      "\n",
      "What are the sentiments expressed in the songs?\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist(df['Vader'], bins=50)\n",
      "plt.axvline(0, c='r')\n",
      "plt.title(\"Average sentiment\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_17.png)\n",
      "\n",
      "On average, the sentiment is slightly positive. Some songs have strong sentiments attached to them (i.e more than 0.5 in absolute value), but most songs have sentiments that are more controlled. This approach is however limited since it derives the average sentiment of a song by averaging the word sentiments, but does not understand the content and the context.\n",
      "\n",
      "## New model\n",
      "\n",
      "Now, let’s train a new model and see whether the performance was improved. First, we create the train and test sets and apply oversampling:\n",
      "\n",
      "```python\n",
      "X = df.drop([\"Artist_Feat\", \"Artist\", \"Artist_Feat_Num\", \"Title\", \"Hit\", \"lookup\", \"release_date\", \"genres\", \"lyrics\"], axis=1)\n",
      "y = df[\"Hit\"]\n",
      "\n",
      "sm = SMOTE(random_state=42)\n",
      "X_res, y_res = sm.fit_resample(X, y)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_res,y_res, test_size=0.2, random_state=42) \n",
      "```\n",
      "\n",
      "Then, we define the random forest classifier and train the model:\n",
      "\n",
      "```python\n",
      "rf=RandomForestClassifier(n_estimators=100)\n",
      "rf.fit(X_train, y_train)\n",
      "\n",
      "y_pred = rf.predict(X_test)\n",
      "accuracy_score(y_pred, y_test)\n",
      "```\n",
      "\n",
      "The accuracy score improved by close to 5% and reaches 98.3%.\n",
      "\n",
      "What are the most important features in this new model?\n",
      "\n",
      "```python\n",
      "importances = rf.feature_importances_\n",
      "indices = np.argsort(importances)\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.title('Feature Importances')\n",
      "plt.barh(range(len(indices)), importances[indices], align='center')\n",
      "plt.yticks(range(len(indices)), [X.columns[i] for i in indices])\n",
      "plt.xlabel('Relative Importance')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_18.png)\n",
      "\n",
      "The order of the important features remains the same, but the compound sentiment feature is now one of the most important features.\n",
      "\n",
      "# Making predictions \n",
      "\n",
      "## Prediction function\n",
      "\n",
      "We can build a predictor that takes the name of the song and the singer as an input, creates the features, and outputs the probability of a song being a hit. Since the algorithm has never been trained on songs from 2019, we can feed it with recent songs and observe the outcome.\n",
      "\n",
      "Let’s recall the whole pipeline first:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_22.png)\n",
      "\n",
      "Let’s build this pipeline and try it with “Lover” by Taylor Swift, a song that was recently released when we wrote this article:\n",
      "\n",
      "```python\n",
      "def model_prediction(artist, title):\n",
      "    \n",
      "    df_pred = pd.DataFrame.from_dict({\n",
      "        \"Artist\":[artist], \n",
      "        \"Title\":[title]})\n",
      "        \n",
      "    df_pred[\"Featuring\"] = df_pred.apply(lambda row: featuring(row['Artist']), axis=1)\n",
      "    df_pred[\"Artist_Feat\"] = df_pred.apply(lambda row: featuring_substring(row['Artist']), axis=1)\n",
      "    df_pred['Title_Length'] = df_pred['Title'].apply(lambda x: num_words(x))\n",
      "    df_pred['lookup'] = df_pred['Title'] + \" \" + df_pred[\"Artist_Feat\"]\n",
      "    df_pred['available_markets'], df_pred['release_date'], df_pred['total_followers'],\n",
      "    df_pred['genres'], df_pred['popularity'], df_pred['acousticness'], df_pred['danceability'],\n",
      "    df_pred['duration_ms'], df_pred['energy'], df_pred['instrumentalness'], df_pred['key'],\n",
      "    df_pred['liveness'], df_pred['loudness'], df_pred['speechiness'], df_pred['tempo'],\n",
      "    df_pred['time_signature'], df_pred['valence'] = zip(*df_pred['lookup'].map(artist_info))\n",
      "    df_pred['release_date'] = pd.to_datetime(df_pred['release_date'])\n",
      "    df_pred['month_release'] = df_pred['release_date'].apply(lambda x: x.month)\n",
      "    df_pred['day_release'] = df_pred['release_date'].apply(lambda x: x.day)\n",
      "    df_pred['weekday_release'] = df_pred['release_date'].apply(lambda x: x.weekday())\n",
      "    df_pred['lookup'] = df_pred['Title'] + \" \" + df_pred[\"Artist\"]\n",
      "    df_pred['lyrics'] = df_pred['lookup'].apply(lambda x: lookup_lyrics(x))\n",
      "    df_pred['lyrics'] = df_pred['lyrics'].apply(lambda x: clean_txt(x))\n",
      "    df_pred['len_lyrics'] = df_pred['lyrics'].apply(lambda x: len_lyrics(x))\n",
      "    df_pred['len_unique_lyrics'] = df_pred['lyrics'].apply(lambda x: len_unique_lyrics(x))\n",
      "    df_pred['without_stop_words'] = df_pred['lyrics'].apply(lambda x: rmv_stop_words(x))\n",
      "    df_pred['unique_without_stop_words'] = df_pred['lyrics'].apply(lambda x: rmv_set_stop_words(x))\n",
      "    df_pred['sentimentVaderPos'] = df_pred['lyrics'].apply(lambda x: analyzer.polarity_scores(x)['pos'])\n",
      "    df_pred['sentimentVaderNeg'] = df_pred['lyrics'].apply(lambda x: analyzer.polarity_scores(x)['neg'])\n",
      "    df_pred['sentimentVaderComp'] = df_pred['lyrics'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
      "    df_pred['sentimentVaderNeu'] = df_pred['lyrics'].apply(lambda x: analyzer.polarity_scores(x)['neu'])\n",
      "    df_pred['Vader'] = df_pred['sentimentVaderPos'] - df_pred['sentimentVaderNeg']\n",
      "\n",
      "    X = df_pred.drop([\"Artist_Feat\", \"Artist\", \"Title\", \"lookup\", \"release_date\", \"genres\", \"lyrics\"], axis=1).astype(float)\n",
      "    y_pred = rf.predict_proba(X)\n",
      "    \n",
      "    print(\"It's a NOT hit with probability : \" + str(y_pred[0][0]))\n",
      "    print(\"It's a hit with probability : \" + str(y_pred[0][1]))\n",
      "    \n",
      "    return y_pred\n",
      "```\n",
      "\n",
      "We can create an interactive form the Notebook to ask the user for the name of the artist, title of the song, and output the prediction.\n",
      "\n",
      "```python\n",
      "from ipywidgets import widgets, interact\n",
      "\n",
      "artist = widgets.Text()\n",
      "title = widgets.Text()\n",
      "\n",
      "ui = widgets.HBox([artist, title])\n",
      "\n",
      "def f(artist, title):\n",
      "    return model_prediction(artist, title)\n",
      "```\n",
      "\n",
      "And in the next cell, type :\n",
      "\n",
      "```python\n",
      "interact(f, artist='Taylor Swift', title='Lover')\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl5_19.png)\n",
      "\n",
      "According to our algorithm, there is only a 22% chance that the song “Lover” by Taylor Swift will make it to the top 10 of the most popular songs of 2019. And this is probably the case, since the song \"only\" [peaked at number 10 on Billboard Hot 100 for a few days.](https://en.wikipedia.org/wiki/Lover_(Taylor_Swift_song))\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "Through this article, we illustrated the importance of external data sources for most data science problems. A good enrichment dataset can boost the performance of your model.  Relevant feature engineering can help gain additional performance.\n",
      "\n",
      "Here is a performance summary of the different steps of our model:\n",
      "\n",
      "| Description | Model | Performance |\n",
      "| -- | -- | -- |\n",
      "| Data from billboard | Decision Tree | F1-Score : 6.6% |\n",
      "| Enrich with Spotify and oversample | Random Forest | Accuracy : 93% |\n",
      "| Enrich with Genius | Random Forest | Accuracy : 98% |\n",
      "\n",
      "Sources and resources:\n",
      "- [Lyricsgenius](https://github.com/johnwmillr/LyricsGenius)\n",
      "\n",
      "\n",
      "---\n",
      "title: Container Dwell Time prediction\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "More than 80% of the goods consumed worldwide are transported in containers. The invention of the container is no less than the greatest revolution in our modern society since Internet. It is the basis of the modern supply chain and the globalization that has taken place over the last 30 years.\n",
      "Every day, millions of containers are loaded and unloaded from cargo ships. Terminal operators unload containers from ships and store them while waiting for the trucking company to pick them up. We would like to predict the dwell time for each container.\n",
      "\n",
      "The Github of the project can be found here :\n",
      "\n",
      "<div class=\"github-card\" data-github=\"maelfabien/Cargo-Dwell-Time\" data-width=\"100%\" data-height=\"\" data-theme=\"default\"></div>\n",
      "<script src=\"//cdn.jsdelivr.net/github-cards/latest/widget.js\"></script>\n",
      "\n",
      "<br>\n",
      "\n",
      "# Exploratory data analysis\n",
      "\n",
      "## Features\n",
      "\n",
      "The features available in the given data set are the following :\n",
      "- idcontainer : the reference of the container\n",
      "- feet : length of the container\n",
      "- reefer : is the container reefer or not\n",
      "- hazardous : is the container dangerous or not \n",
      "- weight : weight of the container\n",
      "- estimatedtimearrival : estimated arrival time of the container \n",
      "- actualtimearrival : actual arrival time of the container\n",
      "- idportcall : reference of the ship’s stopover\n",
      "- shippingline : shipping company\n",
      "- freightforwarder \n",
      "- truckingcompany \n",
      "- finaldestination \n",
      "- dwelltime\n",
      "\n",
      "## Data Preparation\n",
      "\n",
      "Several steps have done in order to clean the data set :\n",
      "- replace NaN in reefer and hazardous columns by 0s\n",
      "- replace missing weight values by the average weight\n",
      "- replace missing estimated arrival time by the actual arrival time\n",
      "- create a new category for missing freightforwarder values\n",
      "\n",
      "Then, in order to handle the diversity of the data types, all the variables are converted to categorical. The following changes have been made in order to allow this transformation :\n",
      "- the weights have been embedded into a set of 20 categories\n",
      "- the estimated arrival date has been removed and replaced by a time difference, in days, between the estimated and actual arrival date\n",
      "- from the actual arrival date, we extracted the month, the day of the month and the day of the week\n",
      "- the final destinations that are present less than 2 times in the data set have been grouped in a ”isRare” category\n",
      "\n",
      "Some containers might come back several time during a given period. There- fore, among the train set, a label was created to state whether a container had already been seen during the year of observation or not. Indeed, the average dwell time of containers already seen was around 10% smaller.\n",
      "Finally, some outliers were noticed essentially regarding the time difference between the estimated and actual arrival time. Some containers arrived 330 days earlier than expected given those data. Given the small volume of observations concerned, it was safe to remove them. Finally, the data set is passed through a One-Hot Encoder in order to deal with the categorical features created.\n",
      "\n",
      "## Distributions \n",
      "\n",
      "Distribution of the estimated and actual times in the train and test set :\n",
      "![alt text](https://maelfabien.github.io/assets/images/AS_Images/time.png)\n",
      "\n",
      "The train set contains data collected in 2017. The test set contains data collected in 2018. The year of the estimated and actual dates can obviously not be used. We do expect the column of the months to be quite relevant since the test set is essentially collected on the first months of the year.\n",
      "The weights of the containers follow a multimodal distribution. The split of 20 categories was randomly chosen. Other splits might have been applied and might produce better results.\n",
      "\n",
      "![alt text](https://maelfabien.github.io/assets/images/AS_Images/weights.png)\n",
      "\n",
      "Most containers arrived between Tuesday and Thursday. The dwell time distribution has a lognormal shape. Some containers stay more than 3 months in the port. \n",
      "\n",
      "![alt text](https://maelfabien.github.io/assets/images/AS_Images/dwell.png)\n",
      "\n",
      "We would require an anomaly prediction algorithm in order to predict those subcases. We will focus on the most basic cases in which the dwell time is smaller than 50 days and delete the 16 outliers.\n",
      "A feature importance can be established from the random forest regressor. The most importance features in the train set appear to be :\n",
      "- the final destination\n",
      "- the weight\n",
      "- the reference of the ship’s stopover\n",
      "- the trucking company\n",
      "\n",
      "The circle of correlation indicates that :\n",
      "- the actual day and the dwell time are correlated and when projected on a 2D plan with a PCA, go along the same direction. \n",
      "- the later we are in the month, the longer the dwell time\n",
      "the actual day name is inversely proportional to the dwell time. The further in the week we are, the shorter the dwell time (since the containers should be gone before the week-end)\n",
      "- the time difference between the estimation and the actual arrival time is also negatively correlated with the dwell time. A late container should be gone faster once it has arrived\n",
      "\n",
      "![alt text](https://maelfabien.github.io/assets/images/AS_Images/circle.png)\n",
      "\n",
      "## Model and results\n",
      "\n",
      "The metric chosen is the Mean Absolute Pourcentage Error (MAPE).\n",
      "\n",
      "![alt text](https://maelfabien.github.io/assets/images/AS_Images/mape.png)\n",
      "\n",
      "Different approaches have been tested on this problem :\n",
      "- a K-Nearest Neighbors Regressor. The model performs poorly whether we perform first a dimension reduction with a MAPE of 140.\n",
      "- a Random Forest Regressor which achieves a MAPE of 110\n",
      "- a XGBoost Regressor which achieves a MAPE of 80\n",
      "- an Artificial Neural Network (ANN) that acheives a MAPE of 69\n",
      "\n",
      "Overall, the problem appears quite challenging due to the diversity of the data types and the limited amount of trainign examples. The best results have been achieved by the ANN due to the underlying complexity of the data. However, the train set only has 13’500 samples, which is a major restriction compared to the large amount of data needed by deep learning algorithms.\n",
      "The model architecture was inspired by a recent paper by Ioanna Kourounioti : ”Development of Models Predicting Dwell Time of Import Containers in Port Container Terminals – An Artificial Neural Networks Application”. The model is made of 4 hidden layers. There is a large trend for overfitting in this data set. Therefore, several drop out and batch normalization layers are necessary.\n",
      "There is room for a large amount of improvements on the structure proposed. There is a total of 99’401 parameters. There are some overfitting issues that could also be solved by a better deep learning model. Overall, the confidence interval at 95%, in hour, is [6.51 ; 53.54].\n",
      "\n",
      "![alt text](https://maelfabien.github.io/assets/images/AS_Images/model.png)\n",
      "\n",
      "---\n",
      "title: How do Neural Networks learn ?\n",
      "layout: post\n",
      "tags: [deeplearning]\n",
      "subtitle : \"Deep Learning with PyTorch\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "Let's break down the maths behing Neural Networks. There are two main processes going on when a NN tries to learn a decision boundary :\n",
      "- the feedforward process\n",
      "- the back-propagation\n",
      "\n",
      "They apply respectively to move forward with the weights we learned in order to compute the outcome, and to move backwards to update the weights once the outcome (an therefore the error) has been computed. You guessed it, the hard part is the back-propagation.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_19.png)\n",
      "\n",
      "# The Chain Rule\n",
      "\n",
      "No spoil, but we'll need to compute a big derivatives at some point in the learning process. How can we efficiently break down big derivatives into smaller steps ? Using the chain rule, yes.  The chain rule can be applied here to **drastically** decrease the numbers of gradients to compute and the complexity of the calculus.\n",
      "\n",
      "What the chain rule states is that when composing functions, the derivatives multiply :\n",
      "\n",
      "$$ \\frac {\\partial}{\\partial t} f(x(t)) = \\frac {\\partial f} {\\partial x} \\frac {\\partial x} {\\partial t} $$\n",
      "\n",
      "And in more complex cases :\n",
      "\n",
      "$$ \\frac {\\partial}{\\partial t} f(x(t), y(t)) = \\frac {\\partial f} {\\partial x} \\frac {\\partial x} {\\partial t} + \\frac {\\partial f} {\\partial y} \\frac {\\partial y} {\\partial t} $$\n",
      "\n",
      "# Notation\n",
      "\n",
      "> \"Notation in Deep Learning is important\", My Mom\n",
      "\n",
      "We wiil first cover the basic 1 Hidden Layer Neural Net case before extending to Deep Neural Nets. We'll introduce basic notations :\n",
      "- $$ h = Wx + b $$ is the outcome of an input that went through a neuron\n",
      "- $$ \\hat{y} = \\sigma(h) $$ is $$ h $$ to which we applied an activation function\n",
      "- $$ E $$ is the error. It could be the cross-entropy or the Mean Squared Error for example.\n",
      "\n",
      "Let's represent visually the Neuron Net we are talking about, and where the derivatives apply :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_20.png)\n",
      "\n",
      "The derivatives apply in the back-propagation. In the back propagation, we compute : $$ E = (\\frac{\\partial}{\\partial W_{11}^{(1)}}E, \\cdots, \\frac{\\partial}{\\partial W_{31}^{(2)}}E) $$\n",
      "\n",
      "We will see in the back-propagation section how to break down this problem.\n",
      "\n",
      "# The Feedforward Process\n",
      "\n",
      "The feedforward process is the process in Neural Networks that turns an input into an output. It describes the mathematical operations that allow to turn the input into an output. For example, on the example displayed above, with 2 hidden layers and 1 final layer :\n",
      "\n",
      "$$ \\hat{y} = \\sigma \\circ W^{(2)} \\sigma \\circ W^{(1)} (x) $$\n",
      "\n",
      "$$ \\hat{y} = \\sigma (h) = \\sigma (W_{11}^{(2)}  \\sigma(h_1) + W_{21}^{(2)}  \\sigma(h_2) +  W_{31}^{(2)} $$\n",
      "\n",
      "Where :\n",
      "\n",
      "$$ W^{(2)} = \\begin{pmatrix} \n",
      "W_{11}^{(2)} & W_{12}^{(2)} \\\\\n",
      "W_{21}^{(2)} & W_{22}^{(2)} \\\\\n",
      "W_{31}^{(2)} & W_{32}^{(2)}\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "$$ W^{(1)} = \\begin{pmatrix} \n",
      "W_{11}^{(1)} & W_{12}^{(1)} & W_{13}^{(1)} \\\\\n",
      "W_{21}^{(1)} & W_{22}^{(1)}  & W_{23}^{(1)} \\\\\n",
      "W_{31}^{(1)} & W_{32}^{(1)} & W_{33}^{(1)}\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "The error function will remain the same when computing how far out predicted output is from the true output :\n",
      "\n",
      "$$ CE(W) = - \\frac{1}{m} \\sum_i y_i \\log{p_i} + (1-y_i) \\log{1-p_i} $$\n",
      "\n",
      "The cost function is the sum of the losses of $$ CE $$ over all training examples $$ m $$ :\n",
      "\n",
      "$$ E = \\frac{1}{m} \\sum_m CE_m $$\n",
      "\n",
      "## The Back-propagation\n",
      "\n",
      "The second step of the process is to apply the Back-propagation. What this basically means is that we mapped our input to an output, and based on the performance of this mapping, we should be able to addjust the weights and bias in each layer accordingly. \n",
      "\n",
      "We can compute the gradients with respect to each and every weight in the layers.\n",
      "\n",
      "$$ E = (\\frac{\\partial}{\\partial W_{11}^{(1)}}E, \\cdots, \\frac{\\partial}{\\partial W_{31}^{(2)}}E) $$\n",
      "\n",
      "To update the gradient, we will do it individually :\n",
      "\n",
      "$$ W_{ij}^{(k)}* = W_{ij}^{(k)} - \\alpha  \\frac{\\partial E}{\\partial W_{ij}^{(k)}} $$\n",
      "\n",
      "Feed-forwarding is nothing more than composing some functions. Therefore, back-propagation is nothing more than applying the chain rule to split the gradients into multiplicative derivatives.\n",
      "\n",
      "We can compute the derivative of the error function as :\n",
      "\n",
      "$$ \\nabla E = (\\frac{\\partial E}{\\partial W_{11}^{(1)}}, \\cdots, \\frac{\\partial E}{\\partial W_{31}^{(2)}}) $$\n",
      "\n",
      "Let's break down the first derivative, since it's the deepest dependency we have in our network so far :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_21.png)\n",
      "\n",
      "$$ \\frac{\\partial E}{\\partial W_{11}^{(1)}} =  \\frac{\\partial E}{\\partial \\hat{y}}  \\frac{\\partial \\hat{y}}{\\partial h} \\frac{\\partial h}{\\partial h_1} \\frac{\\partial h_1}{\\partial W_{11}^{(1)}}  $$\n",
      "\n",
      "Where $$ h_i $$ is the value taken by the output of a neuron at each step. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nn_23.png)\n",
      "\n",
      "For example, in the first step :\n",
      "\n",
      "$$ h_1 = W_{11}^{(1)} x_1 +  W_{21}^{(1)} x_2 +  W_{31}^{(1)} $$\n",
      "\n",
      "$$ h_2 = W_{12}^{(1)} x_1 +  W_{22}^{(1)} x_2 +  W_{32}^{(1)} $$\n",
      "\n",
      "$$ h = W_{11}^{(2)} \\sigma(h_1) +  W_{21}^{(2)} \\sigma(h_2) +  W_{31}^{(2)} $$\n",
      "\n",
      "Notice how when computing the derivative with respect to one of the first weights of the network, you need the whole \"chain\" to be computed. And there is not only one way to reach neuron, since in deeper networks, we might go through other neurons. Therefore, by computing the partial derivatives, we will re-use them often. \n",
      "\n",
      "We will need the derivative of the sigmoid function in the partial derivatives later on :\n",
      "\n",
      "$$ \\sigma'(x) =  \\frac{\\partial}{\\partial x} \\frac {1}{1+e^{-x}} $$\n",
      "\n",
      "$$ = \\frac{e^{-x}}{(1+e^{-x})^2} = \\frac {1}{1+e^{-x}} \\frac {e^{-x}}{1+e^{-x}} = \\sigma(x) (1-\\sigma(x)) $$\n",
      "\n",
      "Using the expression of $$ h $$ we just defined, we can compute the partial derivatives !  For simplicity, we'll use the mean squared error as an error metric : $$ MSE(y, \\hat{y}) = \\frac{1}{2}(\\hat{y}-y)^2 $$\n",
      "\n",
      "$$ \\frac{\\partial E}{\\partial \\hat{y}} = \\frac{\\partial \\frac{1}{2}(\\hat{y}-y)^2 }{\\partial \\hat{y}} = \\hat{y} - y $$\n",
      "\n",
      "$$ \\frac{\\partial \\hat{y}}{\\partial h} = \\sigma'(h) =  \\sigma(h) (1-\\sigma(h)) $$\n",
      "\n",
      "For the next derivative : $$ \\frac{\\partial h }{\\partial h_1} $$, recall that $$ h = W_{11}^{(2)} \\sigma(h_1) +  W_{21}^{(2)} \\sigma(h_2) +  W_{31}^{(2)} $$. Therefore :\n",
      "\n",
      " $$ \\frac{\\partial h }{\\partial h_1} = W_{11}^{(2)} \\sigma'(h_1) = W_{11}^{(2)} \\sigma(h_1) (1-\\sigma(h_1)) $$\n",
      " \n",
      " Finally, we need to compute $$ \\frac{\\partial h_1 }{\\partial W_{11}^{(1)}} $$. Recall that $$ h_1 = W_{11}^{(1)} x_1 +  W_{21}^{(1)} x_2 +  W_{31}^{(1)} $$. Therefore :\n",
      " \n",
      "  $$ \\frac{\\partial h_1 }{\\partial W_{11}^{(1)}} = x_1 $$\n",
      "  \n",
      "  We now have characterized all the steps of the learning process. For deeper neuronal nets, we're just doing more of this steps, and making sure to re-use the gradients already computed accross layers.\n",
      "  \n",
      "  Computing the partial derivatives is useless if we don't update the weights using **gradient descent** :\n",
      "  \n",
      "  $$ W_{ij}^{(k)} = W_{ij}^{(k)}  - \\alpha \\frac{\\partial E}{\\partial W_{ij}^{(k)}} $$ \n",
      "  \n",
      "  Where $$ \\alpha $$ is the learning rate. Progressively, we'll be shifting in the right direction !\n",
      "  \n",
      "  ![image](https://maelfabien.github.io/assets/images/nn_22.jpg)\n",
      "\n",
      "> At this point, all that's left is to re-apply the feedforward process with the updated weights, and so on, until you reach your maximum number of iterations or any other stopping criteria.\n",
      "\n",
      "---\n",
      "title: How to run a Zeppelin notebook locally?\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Amazon Web Services\"\n",
      "---\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/zep.jpg)\n",
      "\n",
      "Zeppelin notebooks are web-based notebooks that enable data-driven, interactive data analytics and collaborative documents with SQL, Scala, Spark and much more. Zeppelin also offers built-in visualizations and allows multiple users when configured on a cluster. In this article, I am going to go through the basic steps that allow you to configure Zeppelin the easy way, locally.\n",
      "\n",
      "The following procedure was tested on macOS.\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "## Install\n",
      "\n",
      "First of all, let's install the dependencies we will later on need. If you do not have homebrew installed, start by running this line in your terminal :\n",
      "``` bash\n",
      "$ /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n",
      "```\n",
      "Then, we need to install Java, Scala, Spark, and Zeppelin.\n",
      "\n",
      "``` bash\n",
      "$ brew cask install java8\n",
      "$ brew install scala\n",
      "$ brew install apache-spark\n",
      "$ brew install apache-zeppelin\n",
      "```\n",
      "Your Zeppelin will be installed here :\n",
      "```\n",
      "/usr/local/Cellar/apache-zeppelin\n",
      "```\n",
      "\n",
      "## Running Zeppelin\n",
      "\n",
      "To run Zeppelin and create a Zeppelin notebook, run the following command :\n",
      "\n",
      "```\n",
      "$ zeppelin-daemon.sh start\n",
      "```\n",
      "\n",
      "If everything goes well, you should see this :\n",
      "```\n",
      "Zeppelin start                                             [  OK  ]\n",
      "```\n",
      "There are some useful Zeppelin commands one should know :\n",
      "\n",
      "```\n",
      "$ zeppelin-daemon.sh start -> To start the Daemon\n",
      "$ zeppelin-daemon.sh stop -> To stop the Daemon\n",
      "$ zeppelin-daemon.sh restart -> To restart the Daemon\n",
      "```\n",
      "\n",
      "Your Zeppelin Notebook should be accessible from the following link :\n",
      "``` http://localhost:8080/ ```\n",
      "\n",
      "At that point, you should see this :\n",
      "![image](https://maelfabien.github.io/assets/images/notebook.jpg)\n",
      "\n",
      "Click on \"Create new note\", leave the Spark interpreter as the default one.\n",
      "![image](https://maelfabien.github.io/assets/images/note.jpg)\n",
      "\n",
      "\n",
      "## Test your Zeppelin configuration\n",
      "\n",
      "The default interpreter language is Spark-Scala. In the first cell, simply type :\n",
      "``` scala\n",
      "val i = 1\n",
      "```\n",
      "\n",
      "It should return :\n",
      "```\n",
      "i: Int = 1\n",
      "```\n",
      "\n",
      "There are also Python interpreters in Zeppelin. You can try it the following way :\n",
      "\n",
      "``` python\n",
      "%python\n",
      "import numpy as np\n",
      "```\n",
      "\n",
      "If no error is displayed, well, congrats! Your Zeppelin notebook is ready! \n",
      "\n",
      "\n",
      "> **Conclusion **: Your Zeppelin is now configured locally. The next article will include the next step of our road to big data analysis: starting a Zeppelin Notebook on an AWS EMR instance!\n",
      "---\n",
      "title: The important advice I wish I received \n",
      "layout: post\n",
      "tags: [da_tuto]\n",
      "subtitle : \"Data Analysis Basics\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "In your journey to becoming a data analyst, you'll encounter some difficulties and challenges. There are some important skills and notions to remember through this journey.\n",
      "\n",
      "# Keep it simple\n",
      "\n",
      "99.5% of your problems in programming won't involve complex solutions. You can do a lot by just manipulating lists, dictionaries, loops...\n",
      "\n",
      "Trying to keep your code simple, documented and explainable is highly important. Follow the PEP8 standards (we'll talk about it later) and everything will be alright.\n",
      "\n",
      "# Code, Code, Code\n",
      "\n",
      "This is a hard truth about programming, you'll have to practice it almost daily to get better. Think about it as learning a new language. If you want to speak russian, you'll barely improve if you read a textbook once every two weeks. \n",
      "\n",
      "Programming is the same, you learn a language, a logic, keywords, syntax... This needs to be practiced almost daily when you learn.\n",
      "\n",
      "# Get out of your comfort zone\n",
      "\n",
      "Sometimes, we identify a certain way of solving a problem, and tend to approach the next problems with this same solution. It's generally a bad idea. We do it because we are lazy. We understood a solution, we remember it, but it's not always optimal. It's not always the most straight-forward, the fastest, the shortest... Challenge yourself.\n",
      "\n",
      "# Learn how to search\n",
      "\n",
      "It is extremely likely (and almost sure) that the problem you are currently stuck on has already been solved somewhere on the internet. You just need to know how to look for it. \n",
      "\n",
      "Google is a good starting point. By making enough Python-related queries, Google will adapt its results on your result page. The first times, you might want to search \"python add element to list\", but after a few weeks, if you forgot again how to do it, simply typing \"add elem to list\" will land the same results.\n",
      "\n",
      "You'll often find results of Stack Overflow, a giant tech forum, really useful. Some of the answers might be on Github too, look for a similar package, a similar project, how things were implemented...\n",
      "\n",
      "# Keep learning\n",
      "\n",
      "Data analysis and data science are fields that evolve really quickly. Tools you master today might not be relevant anymore in 2 or 3 years. You need to adapt, keep training and keep learning. In this training, you'll learn about Python. But Python is already being challenged by other languages which might become leaders in 5-6 years, who knows...\n",
      "\n",
      "You also might want to follow relevant experts on LinkedIn or subscribe to some newsletters to see the news coming. This is great to identify new softwares, papers, libraries... to try.\n",
      "\n",
      "# Share your projects\n",
      "\n",
      "It's easy to share a project using Github or a personal blog. Do it. It will force you to write good code, to provide a write-up of your work and it will definitely show recruiters (if this is what you are looking for) that you know what you talk about.\n",
      "\n",
      "> If you found the article useful or see ways in which it could be improved, please leave a comment :)\n",
      "---\n",
      "title: Deploy a container on GCP\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "In the previous article, we managed to build a container from a simple web application using Spacy, Streamlit and Docker. We ran the container locally. In this article, we'll go a bit further and deploy the container on a container-specialized VM instance on Google Cloud Platform.\n",
      "\n",
      "# Pushing the image to Docker Hub\n",
      "\n",
      "The first step is to push our image to Docker Hub. This can be done by command line. You must first check that you are logged in Docker Hub. Try to run : `docker login` and if `Login Succeeded` is displayed, you are all set.\n",
      "\n",
      "Otherwise, create a DockerHub account from DockerHub's website and log-in:\n",
      "\n",
      "```bash\n",
      "docker login --username=XXX --email=XXX@XXX.XX\n",
      "```\n",
      "\n",
      "Then, you must tag your image. This tag will be used as an identifier once you want to pull it or for others to pull it. You must first find the image'sI ID using : `docker images`. Copy the id of the corresponding image, and run:\n",
      "\n",
      "```bash\n",
      "docker tag IMAGE_ID YOUR_USERNAME/app:latest\n",
      "```\n",
      "\n",
      "The image is now ready to be pushed !\n",
      "\n",
      "```bash\n",
      "docker push YOUR_USERNAME/app:latest\n",
      "```\n",
      "\n",
      "You can now check that it is accessible from your DockerHub's account.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hub.png)\n",
      "\n",
      "# Checking the Instance\n",
      "\n",
      "I wrote above that the instance already has Docker installed and the container is already running. Let's check that !\n",
      "\n",
      "Connect in SSH to the machine by clicking on SSH.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_3.png)\n",
      "\n",
      "You can then check the images (`docker images`) and the running containers ('docker ps').\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_4.png)\n",
      "\n",
      "# Starting a GCP Instance\n",
      "\n",
      "From the Google Cloud Platform's Console, go to Compute Engine > VM Instances. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_0.png)\n",
      "\n",
      "We will use the default machine. Check the button \"Deploy a container image to this VM Instance\". This will automatically select a specialized VM instance (Container-Optimized OS) with a pre-installed Docker. It will also automatically run the specified Docker container.\n",
      "\n",
      "You can specify the name of the container as simply as : YOUR_USERNAME/app (if you named it app).\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_1.png)\n",
      "\n",
      "Then, make sure to allow HTTP/HTTPs traffic depending on the port you expose your app to (80 or 443). If you expose your web application on a custom port, as I did (8501), you will need to set a specific Firewall rule.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_2.png)\n",
      "\n",
      "If you use the default ports, you are now ready ! To access the web application, simply click on the External IP address of the instance.\n",
      "\n",
      "# Specifying a firewall rule\n",
      "\n",
      "If you use Flask or Streamlit, there are chances that you exposed your application on a certain port. In my case, 8501. \n",
      "\n",
      "In the side menu, go to VPC Network > Firewall rules. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rule_0.png)\n",
      "\n",
      "Click on Create a New Rule.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rule_1.png)\n",
      "\n",
      "You can apply it to a specific instance or to the whole network. Then, you must specify Source IP ranges, and allow a specific port (image below displays port 5000, not 8501).\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rule_2.png)\n",
      "\n",
      "Once created, the rule is now activated on the running instance. \n",
      "\n",
      "To run your web application, simply type:\n",
      "\n",
      "`http://INSTANCE_EXTERNAL_IP:PORT`\n",
      "\n",
      "And replace the external IP and the port with your values. Your application is now accessible to anyone, and the hosting typically costs around 25$ per month with this instance type.\n",
      "---\n",
      "title: Full guide to Linear Regression (2/2)\n",
      "layout: post\n",
      "tags: [statistics]\n",
      "subtitle : \"Linear Model\"\n",
      "---\n",
      "\n",
      "Before starting this series of articles on Machine Learning, I thought it might be a good idea to go through some Statistical recalls. This first article is an introduction to some more detailed articles on statistics. I will be illustrating some concepts using Python codes. \n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "This is the part 2/2 of our series on Linear Regression.\n",
      "\n",
      "# Random Design Matrix\n",
      "\n",
      "So far, a hidden hypothesis was set without being explicitly defined: $$ X $$ should be deterministic. Indeed, we should be able to have full control over how $$ X $$ is measured. This is what we call a fixed design matrix. Let me quickly introduce an extension of what has been covered so far.\n",
      "\n",
      "## Random Design\n",
      "\n",
      "In real-life situations, $$ X $$ is rarely fixed. Indeed, many external components might influence the measures you are currently making. For example, some geological data collected over time could depend on the temperature, the atmospheric pressure or other external factors... And it might not be taken into account.\n",
      "\n",
      "For this reason, we introduce the random design matrices where : $$ X_i = X_i(w) $$ is a random variable. The hypothesis on $$ \\epsilon $$ slightly change :\n",
      "- $$ E({\\epsilon}_i|X_i) = 0 $$ \n",
      "- $$ Var({\\epsilon}_i|X_i) = {\\sigma}^2 $$\n",
      "\n",
      "\n",
      "## Consequences\n",
      "\n",
      "The minimization problem remains the same as previously :\n",
      "$$ argmin \\sum(Y_i - X \\hat{\\beta})^2 $$ .\n",
      "\n",
      "The difference relies in the associated theoretical results. Recall the Gram matrix that we previously defined :\n",
      "$$ G = X^TX $$. \n",
      "It should now be written as follows :\n",
      "$$ G = E(X^TX) $$.\n",
      "\n",
      "The limit results become :\n",
      "$$ \\frac {1} {\\sqrt{n}} * ({\\hat{\\sigma}_n - {\\sigma}}) $$ tends as n goes to infinity to $$ N(0, E(X^TX)^{-1}{\\sigma}^2) $$\n",
      "\n",
      "Notice that it is now a limit result. \n",
      "\n",
      "## What changes?\n",
      "\n",
      "What should be remembered from this small article? Well, the direct consequence we face in this case is a change in the confidence interval as the limit distribution is not the same as previously.\n",
      "\n",
      "Previously, we had : \n",
      "\n",
      "$$ {\\beta}_{n,k} ± {t}_{1-{\\alpha}/2} * \\hat{\\sigma}_n $$, \n",
      "$$ t $$ being the quantile of the student distribution\n",
      "\n",
      "This result is constrained to the gaussian hypothesis, which is quite restrictive. \n",
      "\n",
      "Now that we introduced the random design matrix, the confidence interval becomes :\n",
      "\n",
      "$$ {\\beta}_{n,k} ± ({\\Phi}^{-1})_{1-{\\alpha}/2} * \\hat{\\sigma}_n $$ , $$ {\\Phi}^{-1} $$ being the quantile of the Normal distribution. \n",
      "\n",
      "# Normal Regression Model\n",
      "\n",
      "## Concept\n",
      "\n",
      "Recall that in a multi-dimensional linear regression, we have :  $$ Y =  X {\\beta}+ {\\epsilon} $$\n",
      "\n",
      "And the following conditions on $$ {\\epsilon} $$ :\n",
      "- $$ E({\\epsilon}) = 0 $$ , i.e a white noise condition\n",
      "- $$ {\\epsilon}_i ∼ iid  {\\epsilon} $$ for all i = 1,...,n, i.e a homoskedasticity condition\n",
      "\n",
      "Notice also that we never specified any distribution for $$ {\\epsilon} $$. This is where the Normal law comes in. This time, we make the hypothesis that : $$ {\\epsilon}_i ∼ iid  N(0, {\\sigma}^2) $$. \n",
      "\n",
      "Independence implies that :\n",
      "- $$ E({\\epsilon}_i|X_i) = E({\\epsilon}) = 0 $$\n",
      "- $$ Var({\\epsilon}_i|X_i) = {\\sigma}^2 $$\n",
      "\n",
      "Joint normality is not required in the normal regression model. We simply want to have that the conditional distribution of $$ Y $$ given $$ X $$ is normal.\n",
      "\n",
      "## Maximum Likelihood Estimation (MLE)\n",
      "\n",
      "First, we should observe that the previous condition on the conditional distribution of $$ Y $$ can be translated into :\n",
      "\n",
      "$$ f(y|X) = \\frac {1} {(2{\\pi}{\\sigma}^2)^{1/2}} e^{- \\frac {1} {2{\\sigma}^2} (Y - X {\\beta})^2} $$\n",
      "\n",
      "Under the assumption that the observations are independent, the conditional density becomes :\n",
      "$$ f(y_1, y_2, ... | x_1, x_2,...) = \\prod {f(y_i | x_i)} $$\n",
      "\n",
      "$$ = \\prod {\\frac {1} {(2{\\pi}{\\sigma}^2)^{1/2}} e^{- \\frac {1} {2{\\sigma}^2} (y_i - x_i {\\beta})^2} } $$\n",
      "\n",
      "$$ = \\frac {1} {(2{\\pi}{\\sigma}^2)^{n/2}} e^{- \\frac {1} {2{\\sigma}^2} \\sum (y_i - x_i {\\beta})^2} $$\n",
      "\n",
      "$$ = L({\\beta}, {\\sigma}^2) $$\n",
      "\n",
      "$$ L $$ is called the likelihood function. Our aim is to find the Maximum Likelihood Estimation (MLE), i.e the values of $$ {\\beta} $$ such that the likelihood function is maximal. A natural interpretation is to identify the values of $$ {\\beta}, {\\sigma}^2 $$ that are the most likely. We can sum up the maximization problem as follows :\n",
      "\n",
      "$$ ( \\hat{\\beta}, \\hat{\\sigma}^2 ) = {argmax}_{ ({\\beta}, {\\sigma}^2) }  L( {\\beta}, {\\sigma}^2 ) $$\n",
      "\n",
      "You might have noticed that it is not always simple to work with products in a likelihood function. Therefore, we introduce the log-likelihood function : \n",
      "\n",
      "$$ l({\\beta}, {\\sigma}^2) = log L({\\beta}, {\\sigma}^2) $$\n",
      "\n",
      "$$ = log f(y_1, y_2, ... | x_1, x_2,...) $$\n",
      "\n",
      "$$ = - \\frac {n} {2} log (2{\\pi}{\\sigma}^2) - \\frac {1} {2{\\sigma}^2} \\sum (y_i - x_i {\\beta})^2 $$\n",
      "\n",
      "The maximization problem can be re-expressed as :\n",
      "\n",
      "$$ (\\hat{\\beta}, \\hat{\\sigma}^2) = {argmax}_{( {\\beta}, {\\sigma}^2 )}  log L({\\beta}, {\\sigma}^2) $$\n",
      "\n",
      "## First Order conditions\n",
      "\n",
      "The MLE is usually identified numerically. In our case, we can explore it algebraically, by identifying $$ \\hat{\\beta}, \\hat{\\sigma}^2 $$ that jointly solve the First Order Conditions :\n",
      "\n",
      "$$ (1) : \\frac {d} {d {\\beta}} log L({\\beta}, {\\sigma}^2) = 0 $$\n",
      "\n",
      "$$ (2) : \\frac {d} {d {\\sigma}} log L({\\beta}, {\\sigma}^2) = 0 $$\n",
      "\n",
      "It can be pretty easily shown that the MLE will give us the same results as the OLS procedure. Indeed, $$ \\hat{\\beta} = {(X^TX)^{-1}X^TY} = \\hat{\\beta_{OLS}} $$\n",
      "\n",
      "And :\n",
      "\n",
      "$$ \\hat{\\sigma}^2 = \\frac {1} {n} \\sum (y_i - x_i {\\beta} )^2 = { \\hat{\\sigma}_{OLS} }^2 $$\n",
      "\n",
      "The last step is to plug-in the estimators in the initial problem :\n",
      "\n",
      "$$ l({\\beta}, {\\sigma}^2) = - \\frac {n} {2} log (2{\\pi}{\\sigma}^2) - \\frac {1} {2{\\sigma}^2} \\sum (y_i - x_i {\\beta})^2 $$\n",
      "\n",
      "$$ = - \\frac {n} {2} log  (2{\\pi}\\hat{\\sigma}^2) - \\frac {n} {2} $$\n",
      "\n",
      "# Pseudo Least Squares\n",
      "\n",
      "Do you remember when we defined the Gram matrix as $$ X^TX $$ ? To define the OLS estimator, we defined the Gram matrix as invertible. In other words, $$ Ker(X) = {0} $$ . But what happens when it is not the case?\n",
      "\n",
      "## When does it happen?\n",
      "\n",
      "It might happen that the Gram matrix is not invertible. But what would that mean? And how does it happen?\n",
      "- it arises from a non-unique OLS solution\n",
      "- it typically is the case when the dimension of our design matrix $$ X $$ is larger than the number of observations itself, e.g collecting a lot of data per patient in a small medical study\n",
      "- it implies that the pseudo-inverse should be used instead of the inverse of the Gram matrix\n",
      "\n",
      "\n",
      "## Singular Value Decomposition\n",
      "\n",
      "If we cannot invert the Gram matrix, we are stuck in the algebraic derivation of the OLS model at the following expression :\n",
      "\n",
      "$$ (X^TX){\\beta} = X^TY $$\n",
      "\n",
      "We do not have a unique solution, but a whole set of solutions defined by : $$ \\hat{\\beta} + Ker(X) $$.\n",
      "\n",
      "In order to compute the inverse of $$ X^TX $$, we need to use the Singular Value Decomposition (SVD). The SVD is a matrix decomposition theorem that states the following :\n",
      "\n",
      "> Any matrix $$ X_{(n*n)} $$ can be de decomposed as $$ X = USV^T $$ \n",
      "Where : \n",
      "- $$ U $$ is a $$ (n*n) $$ matrix, and $$ U^TU = I_n $$, contains left singular vectors\n",
      "- $$ S $$ is a $$ (n*p) $$ matrix that contains the eigen values of X on the diagonal, and 0s elsewhere.\n",
      "- $$ V $$ is a $$ (p*p) $$ matrix, and $$ V^TV = I_p $$, contains right singular vectors\n",
      "\n",
      "We can express $$ X $$ as : $$ X = \\sum S_iV_i{U_i}^T $$\n",
      "\n",
      "The whole point of performing an SVD is to define : $$ X^+ = \\sum {S_i}^{-1}V_i{U_i}^T $$. $$ X^+ $$ is called the pseudo-inverse of a matrix. This pseudo-inverse will allow us to compute the pseudo inverse of the Gram matrix :\n",
      "\n",
      "$$ \\hat{\\beta} = (X^TX)^+ X^TY = (VSU^TUSV^T)^+(VSU^T)Y $$\n",
      "\n",
      "$$ = (VS^2V^T)^+(VSU^T)Y = VS^{-2}SU^TY = VS^{-1}U^TY = X^+Y $$\n",
      "\n",
      "The estimator has now a different form. This means that we can compute again the bias and the variance of this new estimator.\n",
      "\n",
      "The bias becomes : $$ E(\\hat{\\beta}) - {\\beta} = X^+E(Y) - {\\beta} = X^+(X{\\beta}) - {\\beta} $$\n",
      "$$ = (X^+X - I_p){\\beta} $$ where $$ X^+X $$. The estimator will be biaised, except if $$ X^+X - I_p = 0 $$ which means $$ Ker(X) = {0} $$.\n",
      "\n",
      "The variance becomes : $$ Var(\\hat{\\beta}_n) = (X^TX)^+{\\sigma}^2 $$\n",
      "$$ = (VSU^TUSV^T)^+{\\sigma}^2 = (VS^{-2}V^T){\\sigma}^2 $$\n",
      "$$ = \\sum \\frac {({\\sigma}_i)^2} {(S_i)^2} V_i {V_i}^T $$\n",
      "\n",
      "# Transformations\n",
      "\n",
      "Let's get back to the simple problem in which we wanted to assess the demand for ice cream depending on the outside temperature. The model we built looked like this :\n",
      "\n",
      "$$ y_i = \\beta_0 + \\beta_1 * x_i + u_i $$\n",
      "\n",
      "where :\n",
      "- $$ y_i $$ is the icecream demand on a given day\n",
      "- $$ \\beta_0 $$ a constant parameter\n",
      "- $$ \\beta_1 $$ the parameter that assesses the impact of temperature on icecream demand\n",
      "- $$ x_i  $$ the average temparature for a given day\n",
      "- $$ u_i  $$ the residuals\n",
      "\n",
      "However, the assumption that the model is purely linear is strong, that is pretty much never met in practice. We can apply some transformations to the model to make it more flexible.\n",
      "\n",
      "## 1. Log\n",
      "\n",
      "$$ log(y_i) = \\beta_0 + \\beta_1 * x_i + u_i $$\n",
      "\n",
      "This also means that 1% change of $$ y_i $$ : $$  \\delta_{y_i} $$ is equal to $$ 100 * \\beta_1 * \\delta_{x_i} $$ .\n",
      "\n",
      "## 2. Log-Log\n",
      "\n",
      "$$ log(y_i) = \\beta_0 + \\beta_1 * log(x_i) + u_i $$\n",
      "\n",
      "## 3. Square Root\n",
      "\n",
      "$$ y_i = \\beta_0 + \\beta_1 * \\sqrt {x_i} + u_i $$\n",
      "\n",
      "## 4. Quadratic\n",
      "\n",
      "$$ y_i = \\beta_0 + \\beta_1 * {X_1}_i + \\beta_2 * {X_{2i}}^2 + u_i $$\n",
      "\n",
      "This implies that :\n",
      "\n",
      "$$ \\frac { \\delta_{ {X_1}_i } } { \\delta_{ {X_2}_i } } = \\beta_1 + 2 * \\beta_2 * {X_2}_i  $$\n",
      "\n",
      "## 5. Non-linear\n",
      "\n",
      "$$ y_i = \\frac {1} {\\beta_0 + \\beta_1 * {x}_i } + u_i $$\n",
      "\n",
      "This is a simple example of a non-linear transform.\n",
      "\n",
      "# Boolean and Categorical Variables \n",
      "\n",
      "Up to now, we mostly considered cases in which the variables were continuous (temperature for example). But the variety of data you might deal with might include categorical or boolean variables!\n",
      "\n",
      "## I. Binary Variables\n",
      "\n",
      "Let's get back to our icecream demand forecasting problem :\n",
      "\n",
      "$$ y_i = \\beta_0 + \\beta_1 * x_i + u_i $$\n",
      "\n",
      "where :\n",
      "- $$ y_i $$ is the icecream demand on a given day\n",
      "- $$ \\beta_0 $$ a constant parameter\n",
      "- $$ \\beta_1 $$ the parameter that assesses the impact of temperature on icecream demand\n",
      "- $$ x_i  $$ the average temparature for a given day\n",
      "- $$ u_i  $$ the residuals\n",
      "\n",
      "A binary variable that might be interesting to predict icecream's consumption is the fact that there is public holiday on the day considered or not :\n",
      "\n",
      "$$ y_i = \\beta_0 + \\beta_1 * {X_1}_i + \\beta_2 * {X_2}_i + u_i $$\n",
      "\n",
      "where :\n",
      "- $$ {X_2}_i = 1 $$ is there is public holiday this day\n",
      "- $$ {X_2}_i = 0 $$ otherwise\n",
      "\n",
      "Therefore, we can see $$ \\beta_2 $$ as :\n",
      "\n",
      "$$ \\beta_2 = E (y_i \\mid holiday, {X_1}_i) - E (y_i \\mid no-holiday, {X_1}_i ) $$\n",
      "\n",
      "Public holiday has an impact on icecream demand if $$ \\beta_2 $$ is significantly different from 0. This can be seen as a classical hypothesis testing :\n",
      "- Null hypothesis $$ H_0 : \\beta_2 = 0 $$\n",
      "- and $$ H_0 : \\beta_2 ≠ 0 $$\n",
      "\n",
      "## II. Categorical Variables\n",
      "\n",
      "We might also have more than 2 categories. For example, regarding our public holiday variable, we'll now be interested in which state/region of France is on a public holiday (there are 3 overall, to avoid have everybody sharing the same week of holidays). \n",
      "\n",
      "$$ y_i = \\beta_0 + \\beta_1 * {X_1}_i + \\beta_2 * {X_2}_i + \\beta_3 * {X_3}_i + \\beta_4 * {X_4}_i + u_i $$\n",
      "\n",
      "where :\n",
      "- $$ {X_2}_i = 1 $$ is zone A is on holiday\n",
      "- $$ {X_3}_i = 1 $$ is zone B is on holiday\n",
      "- $$ {X_4}_i = 1 $$ is zone C is on holiday\n",
      "\n",
      "The reference case we implicitly consider here is the case in which there is no public holiday. The three terms are here to evaluate a difference with the reference case.\n",
      "\n",
      "We can run individual or joint tests on the different coefficients of the regression.\n",
      "\n",
      "## III. Interaction variable\n",
      "\n",
      "The intuition behind the interaction variable is quite simple. Suppose we want to test the effect of temperature on consumption if zone A is on holiday. \n",
      "\n",
      "The problem can be formulated as follows :\n",
      "\n",
      "$$ y_i = \\beta_0 + \\beta_1 * {X_1}_i + \\beta_2 * {X_2}_i + \\theta_2 * {X_2}_i *  {X_1}_i + \\beta_3 * {X_3}_i + \\beta_4 * {X_4}_i + u_i $$\n",
      "\n",
      "Here, $$ \\theta_2 $$ measures the effect of temperature on icecream consumption when zone A is on holiday. \n",
      "\n",
      "We can run student tests on $$ \\theta_2 $$ directly.\n",
      "\n",
      "We  can now enrich our model and add several interaction terms :\n",
      "\n",
      "$$ y_i = \\beta_0 + \\beta_1 * {X_1}_i + \\beta_2 * {X_2}_i + \\theta_2 * {X_2}_i *  {X_1}_i + \\beta_3 * {X_3}_i + \\theta_3 * {X_3}_i *  {X_1}_i  + \\beta_4 * {X_4}_i + \\theta_4 * {X_4}_i *  {X_1}_i + u_i $$\n",
      "\n",
      "If we want to test the difference between the groups, we should make a Fisher test with a null hypothesis $$ H_0 : \\theta_2 = 0, \\theta_3 = 0, \\theta_4 = 0 $$.\n",
      "\n",
      "# Dealing with heteroscedasticity\n",
      "\n",
      "Heteroscedasticity might be an issue when conducting hypothesis tests. How can we define heteroscedasticity? How can we detect it? How can we overcome this issue? \n",
      "\n",
      "## I. What is heteroscedasticity?\n",
      "\n",
      "Heteroscedasticity (or heteroskedasticity) refers to the case in which the variability is unequal across the range of values. \n",
      "\n",
      "Recall that in the linear regression framework : $$ y = \\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k + u $$. The fundamental hypothesis is that : $$ E (u \\mid X_1, X_2, ..., X_k) = 0 $$. Under this hypothesis, the OLS estimator is the Best Linear Unbiaised Estimator (BLUE). \n",
      "\n",
      "By normality hypothesis, under homoscedasticity, $$ u \\sim N(0, \\sigma^2) $$ and $$ Var (u \\mid X_1, ... X_k) = \\sigma^2 $$.\n",
      "\n",
      "For example, if we try to predict the income in terms of the age of a person :\n",
      "- in case of homoscedasticity, the variance is constant over the age of the person, i.e \n",
      "- in case of heteroscedasticity, the variance is increasing over the age of the person\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hetero.jpg)\n",
      "\n",
      "Under heteroscedasticity, $$ Var (u_i \\mid X_i) = {\\sigma_i}^2 $$.\n",
      "\n",
      "## II. Transform the variables in `log`\n",
      "\n",
      "In most cases, to control for heteroscedasticity, there is an easy trick: transform the variables in log. \n",
      "\n",
      "$$ log(y) = \\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k + u $$\n",
      "\n",
      "Most of the time, this will do the trick!\n",
      "\n",
      "For example, in Matlab :\n",
      "\n",
      "```matlab\n",
      "y = log(hprice1(:,1));\n",
      "[n,k] = size(hprice1);\n",
      "\n",
      "X = [ones(n,1), hprice1(:,[3,4,5])]\n",
      "beta = inv(X'*X)*X'*y\n",
      "```\n",
      "\n",
      "## III. Robust inference to heteroscedasticity \n",
      "\n",
      "### Concept\n",
      "\n",
      "*Idea *: Build a standard error robust to any kind of heteroscedasticity. \n",
      "\n",
      "The estimators take the following form :\n",
      "\n",
      "$$ \\hat{\\beta_1} = \\beta_1 + \\frac {\\sum_i (X_i - \\bar{X})^2 \\hat{u_i}^2} { \\sum_i (X_i - \\bar{X})^2} $$\n",
      "\n",
      "This estimator is robust to any kind of heteroscedasticity. The variance of the estimator is defined by :\n",
      "\n",
      "$$ Var( \\hat{\\beta_1} ) = \\frac { \\sum_i ( X_i - \\bar{X})^2 {\\sigma_i}^2 } { { SSR_x }^2 } $$\n",
      "\n",
      "This requires $$ \\beta_1 $$ to be known and $$ u_i $$ too. We can define $$ \\hat{u_i} $$ the residual of the estimation. Using White's formula:\n",
      "\n",
      "$$ \\frac { \\sum_i (X_i - \\bar{X})^2 \\hat{u_i}^2 } { { SSR_x }^2 } $$ is robust to any kind of heteroscedasticity. \n",
      "\n",
      "Therefore, if $$ r_{ij} $$ is the residual of the regression of $$ X_{j} $$ on the other independant variables, we have :\n",
      "\n",
      "$$ \\hat{Var}( \\hat{\\beta_j} ) = \\frac {\\sum_i \\hat{r_{ij}}^2 \\hat{u_i}^2 } { {SSR_j}^2 } $$\n",
      "\n",
      "It is sufficient to modify the reference standard error during further tests. \n",
      "\n",
      "### Detect heteroscedasticity \n",
      "\n",
      "How can we detect heteroscedasticity ? A simple test hypothesis can be used :\n",
      "\n",
      "$$ H_0 : Var(u \\mid X_1, ..., X_k) = \\sigma^2 $$\n",
      "\n",
      "## IV. Linear form residuals\n",
      "\n",
      "The residuals in case of heteroscedasticity might depend on :\n",
      "- time, with an index $$ i $$, as we have seen up to now\n",
      "- other features\n",
      "\n",
      "In the second case, the residuals should have the following form :\n",
      "\n",
      "$$ u^2 = \\delta_0 + \\delta_1 X_1 + ... + \\delta_k X_k + V $$\n",
      "\n",
      "We can test the hypothesis $$ H_0 : \\delta_1 = \\delta_2 = ... = \\delta_k = 0 $$\n",
      "\n",
      "## V. Heteroscedasticity with a constant shift\n",
      "\n",
      "This time, the hypothesis is the following :\n",
      "\n",
      "$$ Var(u \\mid x_i) = \\sigma^2 h(x_i) $$\n",
      "\n",
      "Therefore, we might need to modify the residuals :\n",
      "\n",
      "$$ E( ( \\frac {u_i} {\\sqrt{h_i} } )^2 ) = \\frac {E({u_i}^2)} {h_i} = \\frac { \\sigma^2 h_i} {h_i} = \\sigma^2 $$\n",
      "\n",
      "Now, let's use the transformed model to fit our regression :\n",
      "\n",
      "$$ \\frac {y_i} {\\sqrt{h_i} } = \\frac {\\beta_0} {\\sqrt{h_i} } + \\beta_1 \\frac { X_{i1} } { \\sqrt{h_i} } + ... + \\beta_k \\frac { X_{ik} } { \\sqrt{h_i} } + \\frac {u_i} { \\sqrt{h_i} } $$\n",
      "\n",
      "We obtain a weighted least squares problem :\n",
      "\n",
      "$$ \\frac {\\sum_i (y_i - b_0 - b_1 X_{i1} - ... - b_k X_{ik})^2 } {h_i} $$\n",
      "\n",
      "In practice, it is hard to find those weights. For this reason, we usually apply what we call generalized least squares (GLS). \n",
      "\n",
      "# Generalized Least Squares\n",
      "\n",
      "In the previous sections, we highlighted the need for models and estimators that handle heteroscedasticity. We'll introduce Generalized Least Squares, a more general framework. \n",
      "\n",
      "Recall that in the OLS framework, the Best Linear Unbiaised Estimator was defined by :\n",
      "\n",
      "$$ b = (X' X)^{-1} X' Y = \\beta + (X'X)^{-1}X' \\epsilon $$\n",
      "\n",
      "We have seen previously that under heteroscedasticity, this estimator was no longer the best. It remained, however, a Linear Unbiased Estimator. \n",
      "\n",
      "Suppose that we know $$ \\Omega $$ the covariance of the errors such that it can be split the following way by Cholesky decomposition: $$ \\Omega^{-1} = P'P $$, $$ P $$  a triangular matrix. \n",
      "\n",
      "In this case, we can build the GLS estimator by scaling the initial regression problem :\n",
      "\n",
      "$$ Py = PX \\beta + P \\epsilon $$\n",
      "\n",
      "We can define new shifted variables :\n",
      "\n",
      "$$ Y^{*} = X^{*} \\beta + \\epsilon^{*} $$\n",
      "\n",
      "The resulting estimator is therefore modified too :\n",
      "\n",
      "$$ \\hat{\\beta} = (X^{'*} X^{*})^{-1} X^{'*} Y^{*} = (X'P'PX)^{-1}X'P'Py = (X' \\Omega^{-1} X)^{-1} X' \\Omega^{-1} y $$\n",
      "\n",
      "The variance of the estimator becomes :\n",
      "\n",
      "$$ Var( \\hat{\\beta} \\mid X*) = \\sigma^2(X'*X'*)^{-1} = \\sigma^2(X' \\Omega^{-1} X)^{-1} $$\n",
      "\n",
      "## 1. Weighted Least Squares\n",
      "\n",
      "The Weighted Least Squares is a special case of the GLS framework. \n",
      "\n",
      "Under heteroscedasticity, we might face : $$ Var( \\epsilon_i \\mid X_i) = {\\sigma_i}^2 = \\sigma^2 w_i $$\n",
      "\n",
      "In such case, we can specify $$ \\Omega^{-1} $$, which has diagnoal elements of the type $$ \\frac {1} {w_i} $$\n",
      "\n",
      "If we replace the next value of $$ \\Omega $$ in the GLS estimator :\n",
      "\n",
      "$$ \\hat{\\beta} = ( \\sum_i w_i X_i X_i' )^{-1} ( \\sum_i w_i X_i y_i) $$\n",
      "\n",
      "The estimator is consistent whatever the weights being used. \n",
      "\n",
      "## 2. Feasible Least Squares\n",
      "\n",
      "Usually, we don't know the covariance of the errors, but it usually depends on some parameters that can be estimated. This is the main idea behind Feasible Least Squares.\n",
      "\n",
      "$$ \\hat{ \\omega} = \\Omega ( \\hat{ \\theta} ) $$ \n",
      "\n",
      "We can use this estimator in the estimator's formula :\n",
      "\n",
      "$$ \\hat {\\hat { \\beta } } = (X' \\hat{ \\Omega^{-1}} X)^{-1} X' \\hat{\\Omega^{-1}} Y $$\n",
      "\n",
      "The Github repository of this article can be found [here](https://github.com/maelfabien/Machine_Learning_Tutorials).\n",
      "---\n",
      "title: Move Scala Dataframes to Cassandra\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"GDelt Project\"\n",
      "---\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/scala_cassandra.jpg)\n",
      "\n",
      "To store the data we downloaded previously, it is now essential to prepare them and split them into different sub-tables. This will allow us to create smaller data sets, and move those sets to Cassandra.\n",
      "\n",
      "# Split the data sets\n",
      "\n",
      "Suppose we're building a pretty easy request in which we would like to know the number of articles per day, language and country in which the event took place.\n",
      "\n",
      "Mentions contains for a given EventID the language in which the article was written.\n",
      "```scala\n",
      "val mentions_1 = mentionsDF.withColumn(\"_tmp\", $\"value\").select(\n",
      "    $\"_tmp\".getItem(0).as(\"globaleventid\"),\n",
      "    $\"_tmp\".getItem(14).as(\"language\")\n",
      "    )\n",
      "```\n",
      "\n",
      "The country of the event, as well as the day of the event, are in the Export table. Instead of selecting all columns (more than 50), we'll focus on some specific ones :\n",
      "\n",
      "```scala\n",
      "val events_1 = exportDF.withColumn(\"_tmp\", $\"value\").select(\n",
      "    $\"_tmp\".getItem(0).as(\"globaleventid\"),\n",
      "    $\"_tmp\".getItem(1).as(\"day\"),\n",
      "    $\"_tmp\".getItem(33).as(\"numarticles\"),\n",
      "    $\"_tmp\".getItem(53).as(\"actioncountry\")\n",
      "    )\n",
      "```\n",
      "\n",
      "We can also replicate those steps for the translated data :\n",
      "\n",
      "```scala\n",
      "val mentions_trans_1 = mentionsDF_trans.withColumn(\"_tmp\", $\"value\").select(\n",
      "    $\"_tmp\".getItem(0).as(\"globaleventid\"),\n",
      "    $\"_tmp\".getItem(14).as(\"language\")\n",
      "    )\n",
      "val events_trans_1 = exportDF_trans.withColumn(\"_tmp\", $\"value\").select(\n",
      "    $\"_tmp\".getItem(0).as(\"globaleventid\"),\n",
      "    $\"_tmp\".getItem(1).as(\"day\"),\n",
      "    $\"_tmp\".getItem(33).as(\"numarticles\"),\n",
      "    $\"_tmp\".getItem(53).as(\"actioncountry\")\n",
      "    )\n",
      "```\n",
      "\n",
      "# Join the tables\n",
      "\n",
      "Once we selected the essential columns of both tables, we can join the tables :\n",
      "```scala\n",
      "val df_events_1 = events_1.union(events_trans_1)\n",
      "val df_mentions_1 = mentions_1.union(mentions_trans_1)\n",
      "\n",
      "// Join events and mentions\n",
      "val df_1 = df_mentions_1.join(df_events_1,\"GlobalEventID\")\n",
      "```\n",
      "\n",
      "# Build the Cassandra Table\n",
      "\n",
      "Start `cqlsh` from the terminal of your instance and create a table to welcome the data :\n",
      "```SQL\n",
      "CREATE TABLE q1_1(\n",
      "day int,\n",
      "language text,\n",
      "actioncountry text,\n",
      "numarticles int,\n",
      "PRIMARY KEY (day, language, actioncountry));\n",
      "```\n",
      "\n",
      "Make sure to have the name for the fields, and no capital letters. It happened to cause some troubles in our project.\n",
      "\n",
      "# Write the data in Cassandra\n",
      "\n",
      "Once the data set has been created, since Scala Spark is a lazy evaluation framework, we have to compute the data set and load the data into Cassandra at the same time :\n",
      "\n",
      "```scala\n",
      "df_1.write.cassandraFormat(\"q1_1\", \"gdelt_datas\").save()\n",
      "val df_1_1 = spark.read.cassandraFormat(\"q1_1\", \"gdelt_datas\").load()\n",
      "df_1_1.createOrReplaceTempView(\"q1_1\")\n",
      "```\n",
      "\n",
      "It might take some time (several minutes). Once done, all your data for this specific query are in Cassandra!\n",
      "\n",
      "# Query Cassandra Tables\n",
      "\n",
      "Since we prepared the data to fit the queries, our queries are really simple to make in Zeppelin :\n",
      "```z.show(spark.sql(\"\"\" SELECT * FROM q1_1 ORDER BY NumArticles DESC LIMIT 10 \"\"\"))```\n",
      "\n",
      "The results will be displayed directly in Zeppelin :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/q1_2.jpg)\n",
      "\n",
      "> **Conclusion **: This project is now over! We have loaded several GB of zipped files in S3, built a resilient architecture using AWS, Cassandra and ZooKeeper, and finally manipulated and transferred the data to make fast, simple queries on large data sets.\n",
      "---\n",
      "title: Local features, Detection, Description and Matching\n",
      "layout: post\n",
      "tags: [computervision]\n",
      "subtitle : \"Computer Vision\"\n",
      "---\n",
      "\n",
      "Have you ever wondered how panoramas are formed? How can several pictures be combined? This is done through local features.\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "For what comes next, we'll work a bit in Python. Import the following packages :\n",
      "\n",
      "```python\n",
      "import cv2\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "```\n",
      "\n",
      "# I. Local features\n",
      "\n",
      "The key idea behind local features is to identify interest points, extract vector feature descriptor around each interest point and determine the correspondence between descriptors in two views.\n",
      "\n",
      "Essentially, it can be illustrated this way :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_35.jpg)\n",
      "\n",
      "> Global features describe the image as a whole to the generalize the entire object whereas the local features describe the image patches (key points in the image) of an object.\n",
      "\n",
      "How can we find these local features? By following this simple procedure :\n",
      "- detection: identify interest points\n",
      "- tracking: search in a small neighborhood around each detected feature when images are taken from nearby points\n",
      "- matching: determine the correspondence between descriptors in two views\n",
      "\n",
      "We want our match to be reliable, invariant t geometric (translation, rotation, scale) and photometric (brightness, exposure) differences in the two views.\n",
      "\n",
      "Local features are used for image alignment, panoramas, 3D reconstitution, motion tracking, object recognition, indexing, and database retrieval...\n",
      "\n",
      "# II. Detection\n",
      "\n",
      "## 1. Detection theory\n",
      "\n",
      "We want to look for unusual image regions. Textureless patches are nearly impossible to localize. We need to look for patches with large contrast changes, implying large gradients that vary along at least two orientations. This would be the case for corners for example.\n",
      "\n",
      "Suppose we consider only a small window of pixels. We can define flat regions, edges, and corners the following way :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_36.jpg)\n",
      "\n",
      "But how do we measure the change? What metric can we use?\n",
      "\n",
      "We can sum up the squares differences (SSD) : \n",
      "\n",
      "$$ E(u,v) = \\sum_{x,y ∈ W} [I(x+u, y+v) - I(x,y)]^2 $$\n",
      "\n",
      "We can also compare two image patches using a weighted summed square difference : \n",
      "\n",
      "$$ E_{WSSD}(u) = \\sum_{i} w(p_i) [I_1(p_i+u) - I_0(p_i)]^2 $$\n",
      "\n",
      "where :\n",
      "- the two images being compared are $$ I_0 $$ and $$ I_1 $$\n",
      "- the displacement vector is $$ u(u_x, u_y) $$\n",
      "- we have $$ w(p) $$ a spatially varying weighted function\n",
      "\n",
      "We are interested in how stable this metric is with respect to small variations in $$ u $$. This is defined by the auto-correlation function :\n",
      "\n",
      "$$ E_{AC}(\\Delta u) = \\sum_{i} w(p_i) [I_1(p_i+ \\Delta u) - I_0(p_i)]^2 $$\n",
      "\n",
      "Using Taylor series : $$ I_0 (p_i + \\Delta u) ≈ I_0 (p_i) + ∇ I_0 (p_i) \\Delta u $$\n",
      "\n",
      "With a bit of calculus, we can appriximate the autocorrelation as :\n",
      "\n",
      "$$ E_{AC} (\\Delta u) = \\Delta u^T A \\Delta u $$ where $$ A = \\sum_u \\sum_v w (u,v)  \\begin{pmatrix} {I_x}^2 & {I_x I_y} \\\\ {I_y I_x} & {I_y}^2 \\end{pmatrix} = w \\begin{pmatrix} {I_x}^2 & {I_x I_y} \\\\ {I_y I_x} & {I_y}^2 \\end{pmatrix} $$\n",
      "\n",
      "$$ A $$ can be interpreted as a tensor where the outer products of the gradients are convolved with a weighting function.\n",
      "\n",
      "The eigenvalues of $$ A $$ carry information regarding uncertainty. Since $$ A $$ is symmetric, the eigenvalues of $$ A $$ reveal the amount of intensity change in the two principal orthogonal gradient directions in the window.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_37.jpg)\n",
      "\n",
      "- direction of largest increase in E : $$ x_{max} $$\n",
      "- amount of increase in direction $$ x_{max} $$ : $$ \\lambda_{max} $$\n",
      "- direction of smallest increase in E : $$ x_{min} $$\n",
      "- amount of increase in direction $$ x_{min} $$ : $$ \\lambda_{min} $$\n",
      "\n",
      "How do we interpret the eigenvalues?\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_38.jpg)\n",
      "\n",
      "## 2. Harris' corner detection\n",
      "\n",
      "Let's formalize the corner detection process using Harris' method :\n",
      "- compute the gradients at each point in the image\n",
      "- compute $$ A $$ for each image window to get its corenerness scores\n",
      "- compute the eigenvalues\n",
      "- find points whose surrounding window gave a large corner response (f > threshold)\n",
      "- take the points of local maxima and perform non-maximum suppression\n",
      "\n",
      "```python\n",
      "img = cv2.imread('vision_7.jpg', 0)\n",
      "corn = img.copy()\n",
      "\n",
      "dst = cv2.cornerHarris(img,3,5,0.22)\n",
      "\n",
      "#result is dilated for marking the corners, not important\n",
      "dst = cv2.dilate(dst,None)\n",
      "\n",
      "# Threshold for an optimal value, it may vary depending on the image.\n",
      "corn[dst>0.01*dst.max()]=0\n",
      "\n",
      "plt.figure(figsize=(15,12))\n",
      "\n",
      "plt.subplot(121)\n",
      "plt.imshow(img,cmap = 'gray')\n",
      "plt.title('Original Image')\n",
      "\n",
      "plt.subplot(122)\n",
      "plt.imshow(corn,cmap = 'gray')\n",
      "plt.title('Corner Image')\n",
      "\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_39.jpg)\n",
      "\n",
      "The process of the Harris corner detection algorithm can be represented in the following way :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_40.jpg)\n",
      "\n",
      "Harris Corner Detector is rotation invariant, but not scale-invariant (zooming out can make an edge become a corner for example).\n",
      "\n",
      "How can we independently select interest points in each image such that the detections are repeatable across different scales? Well, we need to extract features at a variety of scales by using multiple resolutions in a pyramid and then matching features at the same level. We can also use a fixed window size with a Gaussian pyramid.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_41.jpg)\n",
      "\n",
      "## 3. Blob detection\n",
      "\n",
      "A Blob is a group of connected pixels in an image that shares some common property. Laplacian-of-Gaussian is a circularly symmetric operator for blob detection in 2D. We define the characteristic scale as the scale that produces the peak of Laplacian response.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_42.jpg)\n",
      "\n",
      "Interest points are local maxima in both position and scale.\n",
      "\n",
      "```python\n",
      "detector = cv2.SimpleBlobDetector_create()\n",
      "keypoints = detector.detect(img)\n",
      "\n",
      "img2 = img.copy()\n",
      "for marker in keypoints:\n",
      "    img2 = cv2.drawMarker(img2, tuple(int(i) for i in marker.pt), color=(0, 255, 255))\n",
      "\n",
      "plt.figure(figsize=(15,12))\n",
      "\n",
      "plt.subplot(121)\n",
      "plt.imshow(img,cmap = 'gray')\n",
      "plt.title('Original Image')\n",
      "\n",
      "plt.subplot(122)\n",
      "plt.imshow(img2,cmap = 'gray')\n",
      "plt.title('Blob Detection Image')\n",
      "\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_43.jpg)\n",
      "\n",
      "## 4. Ideal feature\n",
      "\n",
      "The ideal feature should have the following properties :\n",
      "- local, robust to occlusion and clutter\n",
      "- invariant to certain transformation \n",
      "- robust to noise, blur...\n",
      "- distinctive so that individual features can be matched to a large database of objects\n",
      "- quantify, so that many features can be generated for even small objects\n",
      "- accurate, precise localization\n",
      "- efficient, close to real-time\n",
      "\n",
      "Other interest point detectors include :\n",
      "- Hessian\n",
      "- Lowe\n",
      "- EBR, IBR\n",
      "- MSER \n",
      "- ...\n",
      "\n",
      "# III. Description\n",
      "\n",
      "The ideal feature descriptor to extract vector feature descriptor around each interest point should be :\n",
      "- repeatable (invariant/robust)\n",
      "- distinctive\n",
      "- compact\n",
      "- efficient\n",
      "\n",
      "A first step is to normalize the pixels value. However, this is still very sensitive to shifts or rotations...\n",
      "\n",
      "## 1. Scale Invariant Feature Transform (SIFT) descriptor\n",
      "\n",
      "We compute the gradient at each pixel in a 16 × 16 window around the detected keypoint, using the appropriate level of the Gaussian pyramid at which the key point was detected. Down weight gradients by a Gaussian fall-off function (blue circle) to reduce the influence of gradients far from the center. In each 4 × 4 quadrants, compute a gradient orientation histogram using 8 orientation histogram bins.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_44.jpg)\n",
      "\n",
      "The resulting 128 non-negative values form a raw version of the SIFT descriptor vector.\n",
      "\n",
      "It is also possible to apply a PCA at this level to reduce the 128 dimensions.\n",
      "\n",
      "[This](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_matcher/py_matcher.html) is a great article of OpenCV's documentation on these subjects.\n",
      "\n",
      "## 2. Multiscale Oriented Patches Descriptor (MOPS)\n",
      "\n",
      "How can we make a descriptor invariant to the rotation?\n",
      "\n",
      "We can rotate patch according to its dominant gradient orientation.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_45.jpg)\n",
      "\n",
      "## 3. Other descriptors\n",
      "\n",
      "- Gradient location-orientation histogram (GLOH) :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vision_46.jpg)\n",
      "\n",
      "- Steerable filters\n",
      "- moment invariants\n",
      "- complex filters\n",
      "- shape context\n",
      "\n",
      "# IV. Matching\n",
      "\n",
      "We have detected interest points and extracted a vector feature descriptor around each point of interest. We now need to determine the correspondence between descriptors in two views.\n",
      "\n",
      "To match local features, we need for example to minimize the SSD. The simplest approach would be to compare all key points and compare them all. We set a matching threshold value to identify the nearest neighbor to our point of interest.\n",
      "\n",
      "How do we measure performance?\n",
      "- True Positives\n",
      "- False Negatives\n",
      "- False Positives\n",
      "- True Negatives\n",
      "- Recall\n",
      "- FPR\n",
      "- Precision\n",
      "- Accuracy\n",
      "- ROC\n",
      "- AUC\n",
      "\n",
      "> **Conclusion **: I hope this article on image features was helpful. Don't hesitate to drop a comment if you have any question.\n",
      "---\n",
      "title: My Ph.D. process\n",
      "layout: post\n",
      "tags: [phd]\n",
      "subtitle : \"Other\"\n",
      "classes: wide\n",
      "---\n",
      "\n",
      "<style>\n",
      "* {\n",
      "  box-sizing: border-box;\n",
      "}\n",
      "\n",
      "body {\n",
      "  background-color: white;\n",
      "  font-family: Helvetica, sans-serif;\n",
      "}\n",
      "\n",
      "/* The actual timeline (the vertical ruler) */\n",
      ".timeline {\n",
      "  position: relative;\n",
      "  max-width: 1200px;\n",
      "  margin: 0 auto;\n",
      "}\n",
      "\n",
      "/* The actual timeline (the vertical ruler) */\n",
      ".timeline::after {\n",
      "  content: '';\n",
      "  position: absolute;\n",
      "  width: 6px;\n",
      "  background-color: #474e5d;\n",
      "  top: 0;\n",
      "  bottom: 0;\n",
      "  left: 50%;\n",
      "  margin-left: -3px;\n",
      "}\n",
      "\n",
      "/* Container around content */\n",
      ".container {\n",
      "  padding: 10px 40px;\n",
      "  position: relative;\n",
      "  background-color: inherit;\n",
      "  width: 50%;\n",
      "}\n",
      "\n",
      "/* The circles on the timeline */\n",
      ".container::after {\n",
      "  content: '';\n",
      "  position: absolute;\n",
      "  width: 25px;\n",
      "  height: 25px;\n",
      "  right: -17px;\n",
      "  background-color: #474e5d;\n",
      "  border: 4px solid #FF9F55;\n",
      "  top: 15px;\n",
      "  border-radius: 50%;\n",
      "  z-index: 1;\n",
      "}\n",
      "\n",
      "/* Place the container to the left */\n",
      ".left {\n",
      "  left: 0;\n",
      "}\n",
      "\n",
      "/* Place the container to the right */\n",
      ".right {\n",
      "  left: 50%;\n",
      "}\n",
      "\n",
      "/* Add arrows to the left container (pointing right) */\n",
      ".left::before {\n",
      "  content: \" \";\n",
      "  height: 0;\n",
      "  position: absolute;\n",
      "  top: 22px;\n",
      "  width: 0;\n",
      "  z-index: 1;\n",
      "  right: 30px;\n",
      "  border: medium solid #474e5d;\n",
      "  border-width: 10px 0 10px 10px;\n",
      "  border-color: transparent transparent transparent #474e5d;\n",
      "}\n",
      "\n",
      "/* Add arrows to the right container (pointing left) */\n",
      ".right::before {\n",
      "  content: \" \";\n",
      "  height: 0;\n",
      "  position: absolute;\n",
      "  top: 22px;\n",
      "  width: 0;\n",
      "  z-index: 1;\n",
      "  left: 30px;\n",
      "  border: medium solid #474e5d;\n",
      "  border-width: 10px 10px 10px 0;\n",
      "  border-color: transparent #474e5d transparent transparent;\n",
      "}\n",
      "\n",
      "/* Fix the circle for containers on the right side */\n",
      ".right::after {\n",
      "  left: -16px;\n",
      "}\n",
      "\n",
      "/* The actual content */\n",
      ".content {\n",
      "  padding: 20px 30px;\n",
      "  background-color: #474e5d;\n",
      "  color: white;\n",
      "  position: relative;\n",
      "  border-radius: 6px;\n",
      "}\n",
      "\n",
      "/* Media queries - Responsive timeline on screens less than 600px wide */\n",
      "@media screen and (max-width: 600px) {\n",
      "  /* Place the timelime to the left */\n",
      "  .timeline::after {\n",
      "  left: 31px;\n",
      "  }\n",
      "  \n",
      "  /* Full-width containers */\n",
      "  .container {\n",
      "  width: 100%;\n",
      "  padding-left: 70px;\n",
      "  padding-right: 25px;\n",
      "  }\n",
      "  \n",
      "  /* Make sure that all arrows are pointing leftwards */\n",
      "  .container::before {\n",
      "  left: 60px;\n",
      "  border: medium solid white;\n",
      "  border-width: 10px 10px 10px 0;\n",
      "  border-color: transparent white transparent transparent;\n",
      "  }\n",
      "\n",
      "  /* Make sure all circles are at the same spot */\n",
      "  .left::after, .right::after {\n",
      "  left: 15px;\n",
      "  }\n",
      "  \n",
      "  /* Make all right containers behave like the left ones */\n",
      "  .right {\n",
      "  left: 0%;\n",
      "  }\n",
      "}\n",
      "</style>\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "I am keeping track here of my Ph.D. process. I don't know if it will be of any interest for readers however.\n",
      "\n",
      "\n",
      "<div class=\"timeline\">\n",
      "  <div class=\"container right\">\n",
      "    <div class=\"content\">\n",
      "      <h2>September 2020</h2>\n",
      "      <p>Accepted workshop to describe the use of speech processing criminal investigations at TIM 2020 (Traitement de l'Information Multimodale).</p>\n",
      "    </div>\n",
      "  </div>\n",
      "  <div class=\"container left\">\n",
      "    <div class=\"content\">\n",
      "      <h2>August 2020</h2>\n",
      "      <p>Participated and won the <a href=\"https://www.createchallenge.org/\">International Create Challenge</a> organized at Idiap with an assistive device named SoundMap that uses Head-Related Transfer Function (HRTFs) to provide directional audio feedbacks for blind and visually impaired people.</p>\n",
      "    </div>\n",
      "  </div>\n",
      "  <div class=\"container right\">\n",
      "    <div class=\"content\">\n",
      "      <h2>July 2020</h2>\n",
      "      <p>Submitted paper rejected from Interspeech 2020. Working on a re-submission for ICASSP. Submitting a second paper to CoNLL 2020 on Authorship Attribution.</p>\n",
      "    </div>\n",
      "  </div>\n",
      "  <div class=\"container left\">\n",
      "    <div class=\"content\">\n",
      "      <h2>June 2020</h2>\n",
      "      <p>Finished the EPFL class: Statistical Sequence Processing with some work presented on EM for Gaussian Mixture Models and Hidden Markov Models training.</p>\n",
      "    </div>\n",
      "  </div>\n",
      "  <div class=\"container right\">\n",
      "    <div class=\"content\">\n",
      "      <h2>June 2020</h2>\n",
      "      <p>First paper submitted to Interspeech 2020: <a href=\"https://arxiv.org/abs/2006.02093\"> Improving Speaker Identification using Network Knowledge in Criminal Conversational Data</a>.</p>\n",
      "    </div>\n",
      "  </div>\n",
      "  <div class=\"container left\">\n",
      "    <div class=\"content\">\n",
      "      <h2>April 2020</h2>\n",
      "      <p>COVID-19 strikes, working from home on data processing and speaker identification.</p>\n",
      "    </div>\n",
      "  </div>\n",
      "  <div class=\"container right\">\n",
      "    <div class=\"content\">\n",
      "      <h2>March 2020</h2>\n",
      "      <p>Starting my Ph.D. at Idiap Research Institute, as an EPFL Student.</p>\n",
      "    </div>\n",
      "  </div>\n",
      "</div>\n",
      "---\n",
      "title: Introduction to Reinforcement Learning\n",
      "layout: post\n",
      "tags: [RL]\n",
      "subtitle : \"Advanced AI\"\n",
      "---\n",
      "\n",
      "I was recently recommended to take a look at David Silver's (from DeepMind) YouTube series on Reinforcement Learning. The whole course (10 videos) can be found [here](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ).\n",
      "\n",
      "The full lesson is the following:\n",
      "\n",
      "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/2pWv7GOvuf0\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "In this series, I am going to :\n",
      "- summarize the important notions taught in each class\n",
      "- add examples and code\n",
      "- illustrate some concepts with home made schemas\n",
      "- do a Deep RL project\n",
      "- go a bit further by exploring Twin Delayed DDPG and other SOTA models\n",
      "\n",
      "# Introduction to RL\n",
      "\n",
      "Reinforcement Learning is at the intersection of many fields of science :\n",
      "- Machine Learning\n",
      "- Reward System\n",
      "- Classical/Operant conditioning\n",
      "- Bounded Rationality\n",
      "- Operations Research\n",
      "- Optimal Control\n",
      "\n",
      "How is RL different from other ML paraddigms ?\n",
      "- There is no supervisor, only a reward signal\n",
      "- Feedback is delayed, not instantaneous\n",
      "- Time matters, since we consider sequences\n",
      "- The agent's actions affect the data it sees\n",
      "\n",
      "Where is RL applied ?\n",
      "- manage an investment portfolio\n",
      "- make a robot walk\n",
      "- control a power station\n",
      "- play games (Atari, Go...)\n",
      "\n",
      "Here is a simulation environment in which we trained an agent to run :\n",
      "\n",
      "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/TpWXyauJ3M8\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
      "\n",
      "## The RL learning problem\n",
      "\n",
      "A **reward** $$ R_t $$ is a feedback value. In indicates how well the agent is doing at step $$ t $$. The job of the agent is to maximize the cumulative reward. \n",
      "\n",
      "> **Reward Hypothesis** : All goals can be described by the maximisation of expected cumulative reward.\n",
      "\n",
      "Some reward examples :\n",
      "- give reward to the agent if it defeats the Go champion\n",
      "- give reward to the agent for each dollar that gets in the bank (investment portfolio)\n",
      "- give reward to the agent if the score of the game increases\n",
      "- ...\n",
      "\n",
      "The reward might be at the end of the whole process, or at each step, depending on the type of problem we consider.\n",
      "\n",
      "## The environment\n",
      "\n",
      "> This is a **Sequential Decision Making** process. We select actions to maximise the total future reward.\n",
      "\n",
      "Actions can have long term consequences, and the reward might be delayed. It may be better to sacrifice immediate reward to gain more long-term reward.\n",
      "\n",
      "- The agent makes an **observation** $$ O_t $$. \n",
      "- The agent then takes **action** $$ A_t $$. \n",
      "- Based on this action, it gets a **reward** $$ R_t $$.\n",
      "- There is a new environment, and a new observation is made\n",
      "\n",
      "It can be represented in the following way :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_0.png)\n",
      "\n",
      "The only leverage the agent has on its environment and the reward it gets is the action it takes.\n",
      "\n",
      "## History and State\n",
      "\n",
      "> The **history** is the sequence of observations, actions and rewards. It's all the observable variables up to time $$ t $$ : $$ H_t = A_1, O_1, R_1, \\cdots, A_t, O_t, R_t $$\n",
      "\n",
      "What happens next depends on this history. Our aim is simply to find the right action to take based on the history.\n",
      "\n",
      "We don't always use the whole history. Instead, we use the **state**. It is the information that is used to determine what happens next. It captures all necessary information, and is a function of the history.\n",
      "\n",
      "$$ S_t = f(H_t) $$\n",
      "\n",
      "> The **environment state** is what is used inside of the environment to determine what should come next : $$ S_t^e $$. \n",
      "\n",
      "If you want to go from A to B, to decide where to turn, you don't need a map of the whole world, but simply things that are relevant around you. This is the idea behind environment state.\n",
      "\n",
      "The environment state is however not usually visible to the agent. An agent being in an environment state does not mean that the agent gets data from the whole environment around him or captures data from this whole environment. Only the **observations** are sent to the agent.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_1.png)\n",
      "\n",
      "> The **agent state** $$ S_t^a $$ is the agent's internal representation. It is all the information used by the agent to take the action. It is a function of the history : $$ S_t^a = f(H_t) $$\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_2.png)\n",
      "\n",
      "The environment state and the agent state are both **information states**. An information state is a **Markov State**. It contains all information from the history :\n",
      "\n",
      "> A **state** $$ S_t $$ is a Markov State if and only if $$ P(S_{t+1} \\mid S_t) = P(S_{t+1} \\mid S_1, ..., S_t) $$\n",
      "\n",
      "Once a state is known, the history may be thrown away. The current state is a sufficient statistic of the future. The environment state $$ S_t^e $$ is Markov. The history $$ H_t $$ is Markov.\n",
      "\n",
      "A **fully observable environment** is an environment in which the agent directly observes the environment state : $$ O_t = S_t^a = S_t^e $$. This is a **Markov Decision Process (MDP)**.\n",
      "\n",
      "A **partially observed environment** is an environment in which the agent indirectly observes the environment. This could be the casr for a robot with limited vision, a trading agent that only knows the current prices... This is called a **Partially Observed Markov Decision Process (POMDP)**. The agent must then construct its own state representation $$ S_t^a $$, either by :\n",
      "- taking the complete history : $$ S_t^a = H_t $$\n",
      "- beliefs of environment state : $$ S_t^a = (P(S_t^e = s^1), \\cdots, P(S_t^e = s^e)) $$\n",
      "- recurrent neural network : $$ S_t^a = \\sigma(S_{t-1}^a W_s + O_t W_o) $$\n",
      "\n",
      "## The RL Agent\n",
      "\n",
      "An RL agent includes one or more of these components :\n",
      "- **Policy** : agent's bevavior function\n",
      "- **Value function** : how good is each state and/or action\n",
      "- **Model** : Agent's representation of the environment\n",
      "\n",
      "To illustrate those concepts, suppose that we are facing a really simple maze and want to go from the start position to the end by shifting by at most 1 square each time.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_4.png)\n",
      "\n",
      "### Policy\n",
      "\n",
      "The policy is the agent's behavior. It maps the state to the action. The policy can be :\n",
      "- deterministic : $$ a = \\pi(s) $$\n",
      "- stochastic : $$ \\pi(a \\mid s) = P(A=a \\mid S=s) $$\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_3.png)\n",
      "\n",
      "The arrows represent the policy $$ \\pi(s) $$ for each state $$ s $$.\n",
      "\n",
      "### Value function\n",
      "\n",
      "The value function is a prediction of expected future reward. It depends on the way we are behaving, and therefore on $$ \\pi $$. \n",
      "\n",
      "$$ V_{\\pi}(s) = E_{\\pi}(R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\cdots \\mid S_t = s ) $$\n",
      "\n",
      "The value function for a policy gives us the total reward we expect by selecting this policy. It is the sum of the discounted rewards associated to a specific policy.\n",
      "\n",
      "For example, in the maze example, we could attach the value -1 to the goal, and remove 1 at each step. Some paths as the upper right one are therefore not optimal since the value diminishes.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_5.png)\n",
      "\n",
      "The numbers represent value $$ v_{\\pi(s)} $$ of each state $$ s $$.\n",
      "\n",
      "### Model\n",
      "\n",
      "A model predicts what the environment will do next. The **transitions** $$ P $$ predicts the next state. \n",
      "\n",
      "$$ P_{ss'}^a = P(S' = s' \\mid S = s, A = a) $$\n",
      "\n",
      "The **rewards** $$ R $$ predicts the next reward :\n",
      "\n",
      "$$ R_{s}^a = E(R \\mid S = s, A = a) $$\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/rl_6.png)\n",
      "\n",
      "The grid layout represents transition model $$ P_{ss'}^a $$. The numbers represent immediate reward $$ R_s^a $$ from each state (same for all action). This is a 1 step prediction of the reward, and describes what the agent understood of the environment.\n",
      "\n",
      "The reward can however be different for each action.\n",
      "\n",
      "There is not always a model in the environment. In such case, we talk of model-free environments.\n",
      "\n",
      "### Taxonomy\n",
      "\n",
      "A RL agent can be :\n",
      "- Value Based : works with the value function itself. No policy (implicit)\n",
      "- Policy Based : works with the arrows directly, in a structured way, and tries to adjust it. No value function.\n",
      "- Actor Critic : combines both value andd policy based.\n",
      "\n",
      "A RL agent can be :\n",
      "- Model Free : Policy and/or value function. We do not model the environment around a helicopter for example.\n",
      "- Model Based : Policy and/or value function, but has a model.\n",
      "\n",
      "## Problems in RL\n",
      "\n",
      "There are 2 fundamental problems in sequential decision making :\n",
      "- Reinforcement learning : the environment is initially unknows, the agents interacts with the environment and it improves its policy.\n",
      "- Planning : a model of the environment is known, the agent performs computations with its model and improves its policy.\n",
      "\n",
      "Planning can be seen as a tree-based search to find the optimal policy.\n",
      "\n",
      "Reinforcement Learning is a trial and error learning problem. The agent should discover a good policy wthout losing too much reward.\n",
      "\n",
      "> An RL agent must balance between Exploration and Exploitation.\n",
      "\n",
      "**Exploration** finds more information about the environment. **Exploitation** exploits known information to maximise the reward.\n",
      "\n",
      "---\n",
      "title: A guide to Data Acquisition\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Applied Data Science\"\n",
      "search: false\n",
      "---\n",
      "\n",
      "In previous articles such as the hit song classifier, or the [painter prediction](https://www.explorium.ai/blog/whos-the-painter/), we observed how the performance of a data science task could be improved when we enrich our data. There are many reasons why you could and should think about your data enrichment, and many places you could get data from.\n",
      "\n",
      "Collecting new data to enrich your current one might be expensive or time-consuming. But it does not always have to be this way. In this article, we will review the most common data acquisition sources and how they can help you solve your problems.\n",
      "\n",
      "# Internal sources \n",
      "\n",
      "Most businesses have issues identifying the data they need, and even more issues identifying the data they could and should collect. However, collecting, sharing and using is probably the most efficient pipeline.\n",
      "\n",
      "## Collecting Data\n",
      "\n",
      "### Why?\n",
      "\n",
      "Some departments are not aware of the data they could collect. I have seen insurance companies not collecting sensor data from cars' rental since they have a fixed pricing strategy and did not really care about the micro-view. However, this kind of data is priceless for future business opportunities:\n",
      "- monitor your business in real-time\n",
      "- develop your own rental service\n",
      "- autonomous vehicles (estimate the length of use, the distance, risk factors...)\n",
      "- ...\n",
      "\n",
      "### How?\n",
      "\n",
      "This raises a technical concern: are you able to handle this data? Can you store them? Can you clean them, process them, and run queries and analytics on it? This can typically be a factor that limits the data acquisition.\n",
      "\n",
      "If the data comes from sensors, you could easily collect a few points per second. Within a few months of use, it would represent several PetaBytes of data. Relational databases and Python analytics could reach their limits here.\n",
      "\n",
      "### What?\n",
      "\n",
      "Most of the time, a good data acquisition strategy requires **domain knowledge**. I recently came across a good illustration of this concept. Doctors can have a hard time identifying early anorexia of patients as a mental disease since some patients want to keep losing weight and avoid being taken in charge at the hospital. This can obviously affect the effectiveness of the treatment of the patients. If you run studies on this disease, you might want doctors and patients to fill forms that would include the bias of sick patients not willing to be helped. \n",
      "\n",
      "But computer vision can definitely help on this task. Indeed, researchers are currently studying the point of gaze (i.e where you look on a screen) of sick and healthy patients when displaying pictures of persons on a computer, through the computer's webcam. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl7_0.png)\n",
      "\n",
      "They noticed that sick patients focus their attention on other parts of the image than healthy ones. Anorexia detection could be largely improved by training a model on this new data source. You definitely need medical expertise to know that this data is less biased, and relevant to collect. \n",
      "\n",
      "## Sharing Data\n",
      "\n",
      "Sharing data across departments of a company is not always an easy thing. The marketing department might have interesting data that it could share with you, and that could help your model or your analysis. Companies need to promote a data-first education: data is a valuable asset for the company, and all departments should be aware of that. A good data management strategy can enhance your business, and help you solve future problems easier and faster.\n",
      "\n",
      "## Using your data\n",
      "\n",
      "Using your collected data correctly is one of the key challenges. Feature engineering and domain knowledge are the essences of your Machine Learning solutions. One could have thousands of features and billions of observations, but if you don't select the relevant features, handle multicollinearity, create new features (e.g ratios between features), then the whole enrichment is pointless.\n",
      "\n",
      "# External sources \n",
      "\n",
      "The solution is not always internal. There are several types of external data sources which can help.\n",
      "\n",
      "## Open Data\n",
      "\n",
      "The notion of Open Data gathers all sources of data that are publically available. Tools such as [\"Google Dataset Search\"](https://toolbox.google.com/datasetsearch) help you browse these datasets. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl7_1.png)\n",
      "\n",
      "The French government has, for example, decided to publish most of its data as [open data](https://www.data.gouv.fr/fr/) (Geospatial data, population, car accidents...) with the hope of seeing some startups rise from this. We also trained our [painter classifier](https://www.explorium.ai/blog/whos-the-painter/) using open data published by an online art collection.\n",
      "\n",
      "Although open data sound promising, the quality of the data might not always perfect, and the quantity of information available is often limited. It is also time-consuming to identify the right dataset to use.\n",
      "\n",
      "## Scraping\n",
      "\n",
      "Scraping, or web crawling, is the process by which we fetch the content of a webpage and copy the relevant information in a database. It remains today a common data acquisition strategy. Some websites are trying to prevent scraping, especially if the website contains valuable information (Craigslist, eBay, Amazon...). \n",
      "\n",
      "In the hit song classifier, we scrapped Wikipedia to get the Billboard 100 from the past 9 years. There are great and powerful tools such as BeautifulSoup that help us do this in Python.\n",
      "\n",
      "## APIs\n",
      "\n",
      "Application Programming Interfaces (APIs) are a great way to access data from external companies. We used in a previous article Spotify's API to enrich a song dataset. APIs are generally quite easy to use, but have several limitations :\n",
      "- the pricing plan might change over time. This is what happened with Google Maps' API whose price has increased so much that some business had to stop using it.\n",
      "- the quantity of data you can retrieve is limited. Twitter, for example, will limit the number of tweets you can retrieve each minute.\n",
      "\n",
      "## Mechanical Turk\n",
      "\n",
      "Services such as [Amazon Mechanical Turk](https://www.mturk.com/) or [Prolific](https://www.prolific.co/) allow you to \"conduct simple data validation and research\", get answers on surveys, label your data... It remains quite cheap since it relies on lots of external workers making those \"micro-tasks\". \n",
      "\n",
      "However, the objectivity of the workers should never be what you aim for, and \"workers\" on the platform are mostly motivated by the financial aspect of the task.\n",
      "\n",
      "## Buying data\n",
      "\n",
      "Finally, some market places or social networks offer data to enrich your database. The cost might quite often be a limitation.\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "We covered the most common data sources for your data acquisition strategy. The perfect strategy relies on appropriate data acquisition (internal and/or external), and a good feature engineering. This is why we created **Explorium** (INSERT MARKETING MESSAGE HERE).\n",
      "\n",
      "---\n",
      "title: Estimate location using RSSI\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "Smart devices such as IoT sensors use low energy consuming networks such as the ones provided by Sigfox or Lora. But without using GPS networks, it becomes harder to estimate the position of the sensor. The aim of this study is to provide a geolocation estimation using Received Signal Strength Indicator in the context of IoT. \n",
      "\n",
      "The aim is to allow a geolocation of lowconsumption connected devices using the Sigfox network. State of the art modelsare able to be precise to the nearest kilometer in urban areas, and around tenkilometers in less populated areas.\n",
      "\n",
      "The GitHub of the project can be found here :\n",
      "\n",
      "<div class=\"github-card\" data-github=\"maelfabien/Received-Signal-Strength-Geo-Location\" data-width=\"100%\" data-height=\"\" data-theme=\"default\"></div>\n",
      "<script src=\"//cdn.jsdelivr.net/github-cards/latest/widget.js\"></script>\n",
      "\n",
      "The report paper can be found here :\n",
      "\n",
      "<embed src=\"https://maelfabien.github.io/assets/images/RSSI.pdf\" type=\"application/pdf\" width=\"600px\" height=\"500px\" />\n",
      "\n",
      "---\n",
      "title: Interview for DataCast\n",
      "layout: post\n",
      "tags: [thoughts]\n",
      "subtitle : \"Thoughts\"\n",
      "---\n",
      "\n",
      "I'm presenting here a summary I've written of the key concepts, illustrations, otpimisaton program and limitations for the most common types of algorithms. Don't hesitate to drop a comment !\n",
      "\n",
      "We'll cover :\n",
      "- Bayes classifier\n",
      "- Linear Discriminant Analysis (LDA)\n",
      "- Quadratic Discriminant Analysis (QDA)\n",
      "- Naive Bayes Classifier\n",
      "- Perceptron\n",
      "- Logistic Regression\n",
      "- K-Nearest Neighbors (KNN)\n",
      "- Local Averaging\n",
      "- Decision Tree\n",
      "- Support Vector Machine (SVM)\n",
      "- Bagging\n",
      "- Random Forest\n",
      "- Extra Trees\n",
      "- Boosting\n",
      "- Gradient Boosting\n",
      "- Depth Based Models\n",
      "- Depth KNN\n",
      "\n",
      "Click on the image below to load the PDF summary : \n",
      "\n",
      "<a href=\"https://github.com/maelfabien/Machine_Learning_Tutorials/blob/master/Images/supervised.pdf\">![image](https://maelfabien.github.io/assets/images/sup.jpg){:height=\"30%\" width=\"30%\"}</a>\n",
      "\n",
      "---\n",
      "title: Interactive Data Visualization\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "For a recent project, I developped an interactive data visualization tool that was deployed on a web app. The corresponding GitHub repository can be found here :\n",
      "\n",
      "<div class=\"github-card\" data-github=\"maelfabien/DataVisualization\" data-width=\"100%\" data-height=\"\" data-theme=\"default\"></div>\n",
      "<script src=\"//cdn.jsdelivr.net/github-cards/latest/widget.js\"></script>\n",
      "\n",
      "A demonstration hosted on my site of one the graphs (built using Altair) can be found [here](https://maelfabien.github.io/tsne).\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/data_viz.png)\n",
      "\n",
      "---\n",
      "title: A Guide to Hyperparameter Optimization (HPO)\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Parameters and Model Optimization\"\n",
      "search: false\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# What is Hyperparameter optimization?\n",
      "\n",
      "> Hyperparameter optimization (HPO) is the process by which we aim to improve the performance of a model by choosing the right set of hyperparameters.\n",
      "\n",
      "In this article, we will present the main hyperparameter optimization techniques, their implementations in Python, as well as some general guidelines regarding HPO.\n",
      "\n",
      "In HPO, we generally :\n",
      "- Select a set of hyperparameters to test\n",
      "- Train a model with those hyperparameters on validation data\n",
      "- Evaluate the performance of the model\n",
      "- Move on to the next set of hyperparameters\n",
      "- Keep the hyperparameters which improve the performance the most\n",
      "\n",
      "There are some major challenges in the field of HPO :\n",
      "- The computation time of a single model with a given set of hyperparameters might be long. Therefore, iterating over many hyperparameter values might be extremely long\n",
      "- The mix of parameters to optimize can be complex, with many parameters, and varying nature (continuous or categorical for example)\n",
      "- There is a risk to overfit on the training set if we aim to ultimately reach the highest performance in the training part\n",
      "- The problem gets even more complex in the field of deep learning\n",
      "\n",
      "To optimize the hyper-parameters, we tend to use a **validation set** (if available) to limit the overfitting on the train set.\n",
      "\n",
      "Let's illustrate a simple HPO over a simple decision tree. What are the hyperparameters we can impact? You might think of :\n",
      "- `criterion`\n",
      "- `max_depth`\n",
      "- `min_samples_split`\n",
      "- ...\n",
      "\n",
      "How many models should we fit to test all possible combinations of hyperparameters? Well, we can represent the problem graphically :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl4_0.jpg)\n",
      "\n",
      "Say that for every hyperparameter $$ i $$, we have $$ V_i $$ possible values to test. In that case, over 3 hyperparameters, we face $$ V_1 \\times V_2 \\times V_3 $$ combinations to test. Over a set of $$ P $$ hyperparameters, we should test $$ \\prod_{i=1}^{p} V_i = V_1 \\times V_2 \\times ... \\times V_p $$ models.\n",
      "\n",
      "If we now take into account continuous variables such as a learning rate factor, we rapidly realize that testing all the different hyperparameter combinations is impossible. We need to restrict the space of hyperparameters combinations to look for. \n",
      "\n",
      "There are two ways to reduce the search space :\n",
      "- Specify a space manually, and explore the space using transparent techniques such as Grid Search or Randomized Search\n",
      "- Use model-based techniques such as Bayesian Hyperparameter Optimization\n",
      "\n",
      "Grid search and Randomized search are the two most popular methods for hyper-parameter optimization of any model. In both cases, the aim is to test a set of parameters whose range has been specified by the users and observe the outcome in terms of performance of the model. However, the way the parameters are tested is quite different between Grid Search and Randomized Search.\n",
      "\n",
      "To illustrate the concepts presented below, we will use a Kaggle Credit Card Fraud Detection dataset. The main idea will be to compare the time, the number of combinations tested, the guarantees and the performance of each approach. \n",
      "\n",
      "The dataset can be found [here](https://www.kaggle.com/mlg-ulb/creditcardfraud). It contains transactions made by credit cards in September 2013 by European cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
      "\n",
      "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, the original features are not provided. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "```\n",
      "\n",
      "```python\n",
      "df = pd.read_csv('creditcard.csv')\n",
      "df.head()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/auto3.jpg)\n",
      "\n",
      "We will use the F1-Score metric, a harmonic mean between the precision and the recall. We will suppose that previous work on the model selection was made on the training set, and conducted to the choice of a Logistic Regression. Therefore, we need to use a validation set to select the right parameters of the logistic regression. \n",
      "\n",
      "```python\n",
      "X_train, X_val, y_test, y_val = train_test_split(df.drop(['Class'], axis=1), df['Class'], test_size=0.2)\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "# Grid Search\n",
      "\n",
      "In grid search, we try every combination of the set of hyperparameters that the user-specified. This means that we will test the following combinations for example :\n",
      "\n",
      "| Criterion | Max Depth | Min Samples Split |\n",
      "| Gini | 5 | 2 |\n",
      "| Gini | 5 | 3 |\n",
      "| Gini | 5 | 4 |\n",
      "| Gini | 5 | 5 |\n",
      "| Gini | 10 | 2 |\n",
      "| Gini | 10 | 3 |\n",
      "| Gini | 10 | 4 |\n",
      "| Gini | 10 | 5 |\n",
      "| .. | .. | .. |\n",
      "| Entropy | 50 | 5 |\n",
      "\n",
      "We can visually represent the grid search on 2 features as a sequential way to test, in order, all the combinations:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl4_1.jpg)\n",
      "\n",
      "In this simple example, the space explored can be represented as such:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl4_2.jpg)\n",
      "\n",
      "The limitations of grid search are pretty straightforward:\n",
      "- Grid search does not scale well. There is a huge number of combinations we end up testing for just a few parameters. For example, if we have 4 parameters, and we want to test 10 values for each parameter, there are : $$ 10 \\times 10 \\times 10 \\times 10 = 10'000 $$ combinations possible.\n",
      "- We have no guarantee to explore the right space. \n",
      "\n",
      "The user needs a good understanding of the underlying problem to select the right hyperparameters to test. To focus on the right regions of the hyperparameter space, the following technique might be useful:\n",
      "\n",
      "> Start with a first large-scale Grid Search, identify a region of interest in which the models perform well, and start a second Grid Search in this specific region.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl4_3.jpg)\n",
      "\n",
      "This approach can, however, be long to run and should be used if the model you are tuning does not have too many parameters, or if you don't have too much training data.\n",
      "\n",
      "Grid search is implemented in scikit-learn under the name of `GridSearchCV` (for cross-validation):\n",
      "\n",
      "```python\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "param_grid = {\n",
      "'penalty': ['l1', 'l2'],\n",
      "'C': [0.8, 1, 1.2], \n",
      "'tol': [0.00005, 0.0001, 0.00015, 0.0002], \n",
      "'fit_intercept' : [True, False], \n",
      "'warm_start' : [True, False]\n",
      "}\n",
      "\n",
      "lr = LogisticRegression()\n",
      "grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='f1_macro', return_train_score=True)\n",
      "grid_search.fit(X_val, y_val)\n",
      "````\n",
      "\n",
      "After approximately 4 minutes, the best model appears to be the following:\n",
      "\n",
      "```python\n",
      "estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "tol=0.0001, verbose=0, warm_start=False)\n",
      "```\n",
      "\n",
      "To access the results and the details of the GridSearch Cross-Validation, simply use:\n",
      "\n",
      "```python\n",
      "grid_search.cv_results_ \n",
      "```\n",
      "\n",
      "This gives access to the fitting time, the prediction time, the training score, the test score... The average F1-Score on the test set of the Cross Validations can be computed by taking the mean of the test scores array:\n",
      "\n",
      "```python\n",
      "np.mean(grid_search.cv_results_['mean_test_score'])\n",
      "```\n",
      "\n",
      "We achieve an average F1-Score of approximately `0.837` in 4 minutes and 17 seconds.\n",
      "\n",
      "# Randomized Search\n",
      "\n",
      "The randomized search follows the same goal. However, we won't test sequentially all the combinations. Instead, we try **random combinations** among the range of values specified for the hyper-parameters. We initially specify the number of random configurations we want to test in the parameter space.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl4_4.jpg)\n",
      "\n",
      "The main advantage is that we can try a broader range of values or hyperparameters within the same computation time as grid search, or test the same ones in much less time. We are however not guaranteed to identify the best combination since not all combinations will be tested. \n",
      "\n",
      "To get back to our previous example in which we used 2 successive hyperparameter techniques in a row, randomized Search can also be used as the first layer, to either speed up the process on the same set of hyperparameters, or explore a broader range of feature values.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl4_5.jpg)\n",
      "\n",
      "Mathematically, for a given number of iterations (function evaluations), say $$ N $$ in total, on a set of $$ V $$ hyperparameters :\n",
      "- A grid search will explore $$ N^{\\frac{1}{V}} $$ values for each hyperparameter\n",
      "- Whereas a randomized search will explore $$ N $$ different values for each hyperparameter\n",
      "\n",
      "The implementation in Scikit-learn is also straight forward :\n",
      "\n",
      "```python\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "\n",
      "param_grid = {\n",
      "'penalty': ['l1', 'l2'],\n",
      "'C': [0.8, 1, 1.2], \n",
      "'tol': [0.00005, 0.0001, 0.00015, 0.0002], \n",
      "'fit_intercept' : [True, False], \n",
      "'warm_start' : [True, False]\n",
      "}\n",
      "\n",
      "lr = LogisticRegression()\n",
      "rnd_search = RandomizedSearchCV(lr, param_grid, n_iter = 20, cv=5, scoring='f1_macro', return_train_score=True)\n",
      "rnd_search.fit(X_val, y_val)\n",
      "````\n",
      "\n",
      "The optimal model identified is exactly the same:\n",
      "\n",
      "```python\n",
      "estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "tol=0.0001, verbose=0, warm_start=False)\n",
      "```\n",
      "\n",
      "The average F1-Score is quite similar, although there is a slight difference due to the Cross Validation split. We managed to successfully explore the combination of parameters in less than 53 seconds. In this example, the randomized search was successful, but it's not always the case.\n",
      "\n",
      "There is a tradeoff to make between the guarantee to identify the best combination of parameters and the computation time. As mentioned, a simple trick could be to start with a randomized search to reduce the parameters space and then launch a grid search to select the optimal features within this space.\n",
      "\n",
      "The main limit of Grid Search or Randomized Search is that the point we just explored does not influence which point to evaluate next since these two approaches do not depend on an underlying model. To overcome this limitation, we will introduce Bayesian Hyperparameter Optimization \n",
      "\n",
      "# Bayesian Hyperparameter Optimization\n",
      "\n",
      "Bayesian Hyperparameter Optimization is a model-based hyperparameter optimization, in the sense that we aim to build a distribution of the loss function in terms of the value of each parameter. What are the main advantages and limitations of model-based techniques? How can we implement it in Python?\n",
      "\n",
      "Recall that in an optimization problem regarding a model's hyperparameters, the aim is to identify :\n",
      "\n",
      "$$ x^* = argmin_x f(x) $$\n",
      "\n",
      "where $$ f $$ is an expensive function. \n",
      "\n",
      "Depending on the form or the dimension of the initial problem, it might be really expensive to find the optimal value of $$ x $$. Hyperparameter gradients might also not be available. Suppose that we know all the parameters distribution. We can represent for every hyperparameter, a distribution of the loss according to its value.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ho1.jpg)\n",
      "\n",
      "Since the curve is not known, a naive approach would be the pick a few values of `x` and try to observe the corresponding values `f(x)`. We would then pick the value of `x` that gave the smallest value.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ho2.jpg)\n",
      "\n",
      "This is where Grid Search and Randomized search lie, and all other similar methods are called Sequential model-based optimization (SMBO).\n",
      "\n",
      "## Probabilistic Regression Models \n",
      "\n",
      "We try to approximate the underlying function using only the samples we have. This can essentially be done in 2 steps :\n",
      "- identify a probabilistic **surrogate model**, i.e. a model that describes the distribution the loss in terms of the underlying hyperparameter value\n",
      "- define an **acquisition function** which tells us which point to explore next\n",
      "\n",
      "### Surrogate models\n",
      "\n",
      "Although Random Forest and Tree Parzen Estimators (TPE) are quite popular choices, we will focus on Gaussian Processes (GP) for surrogate models.\n",
      "\n",
      "We suppose that the function $$ f $$, which represents the distribution of a loss in terms of the value of a hyperparameter, has a mean $$ \\mu $$ and a covariance $$ K $$, and is a realization of a Gaussian Process. The Gaussian Process is a tool used to infer the value of a function. Predictions follow a normal distribution. Therefore :\n",
      "\n",
      "$$ p(y \\mid x, D) = N(y \\mid \\hat{\\mu}, {\\hat{\\sigma}}^2) $$\n",
      "\n",
      "We use that set of predictions and pick new points where we should evaluate next. We can plot a Gaussian Process between 4 samples this way :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl4_6.jpg)\n",
      "\n",
      "Even though the true distribution is unknown (the red line), we can infer its value using Gaussian Process (confidence interval lines in green).\n",
      "\n",
      "Once we identify a new point using the acquisition function, we add it to the samples and re-build the Gaussian Process with that new information... We keep doing this until we reach the maximal number of iterations or the limit time for example. This is an iterative process.\n",
      "\n",
      "## Acquisition function\n",
      "\n",
      "How do we know which point we should evaluate next? There are two guidelines :\n",
      "- Pick points that yield, on the approximated curve, a low value. \n",
      "- Pick points in areas we have less explored.\n",
      "\n",
      "There is an exploration/exploitation trade-off to make. This tradeoff is taken into account in an *acquisition function*.\n",
      "\n",
      "The acquisition function is defined as :\n",
      "\n",
      "$$ A(x) = \\sigma(x) ( \\gamma(x) \\Phi( \\gamma(x)) + N (\\gamma(x))) $$\n",
      "\n",
      "where :\n",
      "\n",
      "- $$ \\gamma(x) = \\frac { f(x^c) - \\mu(x)} {\\sigma(x)} $$\n",
      "- $$ f(x^c) $$ the current guessed arg min, $$ \\mu(x) $$ the guessed value of the function at `x`, and $$ \\sigma(x) $$ the standard deviation of output at `x`.\n",
      "- $$ \\Phi(x) $$ and $$ N(x) $$ are the CDF and the PDF of a standard normal\n",
      "\n",
      "This acquisition function is the most common and is called the *expected improvement*. We then compute the acquisition score of each point, pick the point that has the highest activation, and evaluate $$ f(x) $$ at that point, and so on...\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/expl4_7.jpg)\n",
      "\n",
      "In this example, we would stay in the region in which we identified a previous point that gave a small loss. We can also see the loss minimization problem as a maximization problem on the chosen metric.\n",
      "\n",
      "The gaussian hyperparameter optimization process can be illustrated in the following way :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/bo.gif)\n",
      "\n",
      "This is the essence of bayesian hyperparameter optimization!\n",
      "\n",
      "## Advantages and limits of Bayesian Hyperparameter Optimization\n",
      "\n",
      "Bayesian optimization techniques can be effective in practice even if the underlying function $$ f $$ being optimized is stochastic, non-convex, or even non-continuous. \n",
      "\n",
      "Bayesian optimization is effective, but it will not solve all our tuning problems. As the search progresses, the algorithm switches from exploration — trying new hyperparameter values — to exploitation — using hyperparameter values that resulted in the lowest objective function loss.\n",
      "\n",
      "If the algorithm finds a local minimum of the objective function, it might concentrate on hyperparameter values around the local minimum rather than trying different values located far away in the domain space. Random search does not suffer from this issue because it does not concentrate on any values!\n",
      "\n",
      "## Implementation in Python\n",
      "\n",
      "Several libraries implement Gaussian Hyperparameter Optimization and rely on different theoretical aspects of this HPO method.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ho5.jpg)\n",
      "\n",
      "We will be using HyperOpt in this example since it's one of the most famous HPO libraries in Python, that can also be used for Deep Learning.\n",
      "\n",
      "## HyperOpt\n",
      "\n",
      "Import the HyperOpt packages and functions :\n",
      "\n",
      "```python\n",
      "from hyperopt import tpe\n",
      "from hyperopt import STATUS_OK\n",
      "from hyperopt import Trials\n",
      "from hyperopt import hp\n",
      "from hyperopt import fmin\n",
      "```\n",
      "\n",
      "In this example, we will try to optimize the Logistic Regression. Define the maximum number of evaluations and the maximum number of folds :\n",
      "\n",
      "```python\n",
      "N_FOLDS = 10\n",
      "MAX_EVALS = 50\n",
      "```\n",
      "\n",
      "We start by defining an objective function, i.e the function to minimize. Here, we want to maximize the cross-validation F1 Score, and therefore minimize `1 - CV(F1-Score)` :\n",
      "\n",
      "```python\n",
      "def objective(params, n_folds = N_FOLDS):\n",
      "\"\"\"Objective function for Logistic Regression Hyperparameter Tuning\"\"\"\n",
      "\n",
      "# Perform n_fold cross validation with hyperparameters\n",
      "clf = LogisticRegression(**params,random_state=0,verbose =0)\n",
      "scores = cross_val_score(clf, X_val, y_val, cv=5, scoring='f1_macro')\n",
      "\n",
      "# Extract the best score\n",
      "best_score = max(scores)\n",
      "\n",
      "# Loss must be minimized\n",
      "loss = 1 - best_score\n",
      "\n",
      "# Return all relevant information\n",
      "return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
      "```\n",
      "\n",
      "Then, we define the space, i.e the range of all parameters we want to tune :\n",
      "\n",
      "```python\n",
      "space = {\n",
      "'warm_start' : hp.choice('warm_start', [True, False]),\n",
      "'fit_intercept' : hp.choice('fit_intercept', [True, False]),\n",
      "'tol' : hp.uniform('tol', 0.00001, 0.0001),\n",
      "'C' : hp.uniform('C', 0.05, 3),\n",
      "'solver' : hp.choice('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']),\n",
      "}\n",
      "```\n",
      "\n",
      "Notice that we won't be able to compute the number of combinations tested here since we use uniform distributions for continuous variables such as `C` and the tolerance `tol`. We will limit the number of trials to 50 instead, as defined by `MAX_EVALS`. We are now ready to run the optimization :\n",
      "\n",
      "```python\n",
      "tpe_algorithm = tpe.suggest\n",
      "bayes_trials = Trials()\n",
      "best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials)\n",
      "```\n",
      "\n",
      "You'll see the progress similarly: \n",
      "\n",
      "`90%|█████████ | 45/50 [05:09<00:31,  6.21s/it, best loss: 0.06831375590721178]`\n",
      "\n",
      "The variable `best` contains the model with the best parameters. \n",
      "\n",
      "```python\n",
      "{'C': 1.602432793095192,\n",
      "'fit_intercept': 0,\n",
      "'solver': 1,\n",
      "'tol': 9.914353259315055e-05,\n",
      "'warm_start': 1}\n",
      "```\n",
      "\n",
      "The best solution identified is now different since we explored continuous distributions for several parameters.\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "The following table summarizes the performance of the different approaches:\n",
      "\n",
      "| Method | Time | Combinations tested | CV F1-Score |\n",
      "| Grid Search | 4m 17s | 96 | 0.837307 |\n",
      "| Randomized Search | 53s | 20 | 0.843494 |\n",
      "| Bayesian Optimization | 5m58s | 50 | 0,931686 |\n",
      "\n",
      "We notice that the bayesian optimization outperforms the two other approaches, and can be longer eventually. We tested more combinations of the grid search, but identifying optimal parameters as precise as the ones in bayesian optimization would have required a lot more of combinations for the grid search and the randomized search. \n",
      "\n",
      "The randomized search achieved results similar to grid search, in less than 25% of the computation time. The identification of the optimal set of hyperparameters is however not guaranteed. It is possible to specify a broader set of parameters to test in grid search and randomized search using `np.arange` function, but the underlying distribution remains discrete.\n",
      "\n",
      "In conclusion, using the Bayesian approach seems to be a good choice, since it can learn complex relations and interactions between the hyperparameters. There is however a risk that such an approach focuses only on local minima, and controlling this with a randomize search at first might be a good idea.\n",
      "\n",
      "Sources :\n",
      "- [A good Quora answer](https://www.quora.com/How-does-Bayesian-optimization-work)\n",
      "- [Bayesian Optimization Github](https://github.com/fmfn/BayesianOptimization)\n",
      "- [SigOpt](https://static.sigopt.com/773979031a2d61595b9bda23bb81a192341f11a4/pdf/SigOpt_Bayesian_Optimization_Primer.pdf)\n",
      "- [https://arxiv.org/pdf/1012.2599.pdf](https://arxiv.org/pdf/1012.2599.pdf)\n",
      "- [AutoML](https://www.automl.org/wp-content/uploads/2018/09/chapter1-hpo.pdf)\n",
      "\n",
      "---\n",
      "title: Lab - Explore a BigQuery Public Dataset\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Road to Google Cloud Platform Certification\"\n",
      "---\n",
      "\n",
      "Storing and querying massive datasets can be time-consuming and expensive without the right hardware and infrastructure. Google BigQuery is an enterprise data warehouse that solves this problem by enabling super-fast SQL queries using the processing power of Google's infrastructure.\n",
      "\n",
      "The aim of this lab is to explore public data using Big Query, create queries and upload our own data.\n",
      "\n",
      "# Public Data\n",
      "\n",
      "From the GCP console, start by clicking on Big Query in the side menu : \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_34.jpg)\n",
      "\n",
      "Then, select \"Explore public dataset\" :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_35.jpg)\n",
      "\n",
      "Type \"USA Names\" in the search bar, and select the following dataset :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_36.jpg)\n",
      "\n",
      "Click on \"View Dataset\" :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_37.jpg)\n",
      "\n",
      "The dataset will now appear in your side menu :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_38.jpg)\n",
      "\n",
      "We can use the query editor to write SQL queries in Big Query :\n",
      "\n",
      "```\n",
      "SELECT name, gender,\n",
      "SUM(number) AS total\n",
      "FROM `bigquery-public-data.usa_names.usa_1910_2013`\n",
      "GROUP BY name, gender\n",
      "ORDER BY total DESC\n",
      "LIMIT 10\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_39.jpg)\n",
      "\n",
      "The result table is displayed in the following way :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_40.jpg)\n",
      "\n",
      "# Your own data\n",
      "\n",
      "Alright, we can build the same approach using our own datasets. Download the baby names dataset from the following link : [http://www.ssa.gov/OACT/babynames/names.zip](http://www.ssa.gov/OACT/babynames/names.zip). Put the files on your desktop (or wherever you want). Open a file and observe the structure. \n",
      "\n",
      "We will now create a dataset in BigQuery. In the resources tab, click on your project's name. Then, click on \"Create Dataset\" in the central page.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_41.jpg)\n",
      "\n",
      "Give your dataset a name, and a region.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_42.jpg)\n",
      "\n",
      "Then, click on \"Create Table\" :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_43.jpg)\n",
      "\n",
      "Give your table a name, select the file (for example the year 2014) and the file format (CSV).\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_44.jpg)\n",
      "\n",
      "The upload should appear in your Job History on the side menu. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_45.jpg)\n",
      "\n",
      "Once ready, click on the table's name from the resources menu, and preview the columns.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_46.jpg)\n",
      "\n",
      "We can now create a query to retrieve the 5 most famous males names :\n",
      "\n",
      "```\n",
      "SELECT name, count\n",
      "FROM `babynames.names_2014`\n",
      "WHERE gender = 'M'\n",
      "ORDER BY count DESC LIMIT 5\n",
      "```\n",
      "\n",
      "If you click on run, you should see the following result!\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_47.jpg)\n",
      "s\n",
      "\n",
      "---\n",
      "title: Voice Activity Detection Application\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "Voice activity detection is a field which consists in identifying whether someone is speaking or not at a given moment. It can be useful to launch a vocal assistant or detect emergency situations.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "In this article, we will cover the main concepts behind classical approaches to voice activity detection, and implement them in Python is a small web application using Streamlit. This article is inspired by the [following repository](https://github.com/marsbroshok/VAD-python).\n",
      "\n",
      "# High-level overview\n",
      "\n",
      "It can be useful at first to give a high level overview of the classical approaches to Voice Activity Detection:\n",
      "- Read the input file and convert is to mono\n",
      "- Move a window of 20ms along the audio data\n",
      "- Calculate for each window the ratio between energy of speech band and total energy for window\n",
      "- If ratio is higher than a pre-defined threshold (e.g 60%), label windows as speech\n",
      "- Apply median filter with length of 0.5s to smooth detected speech regions\n",
      "- Represent speech regions as intervals of time\n",
      "\n",
      "The application we will build is the following:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/app_speech_0.png)\n",
      "\n",
      "# Read the input file and convert it to mono\n",
      "\n",
      "In this exercise, we will only consider the case of mono signals and not stereo, meaning that we must have a single series of values, not 2. We read the files using Scipy's wavfile module, and convert it to mono if there are 2 lists of values returned (stereo) by applying a mean of both series.\n",
      "\n",
      "```python\n",
      "import scipy.io.wavfile as wf\n",
      "\n",
      "filename = 'test.wav'\n",
      "\n",
      "def _read_wav(wave_file):\n",
      "\t# Read the input\n",
      "\trate, data = wf.read(wave_file)\n",
      "\tchannels = len(data.shape)\n",
      "\tfilename = wave_file\n",
      "\n",
      "\t# Convert to mono\n",
      "\tif channels == 2 :\n",
      "\t\tdata = np.mean(data, axis=1, dtype=data.dtype)\n",
      "\t\tchannels = 1\n",
      "\treturn data\n",
      "\n",
      "read_file = _read_wav(filename)\n",
      "```\n",
      "\n",
      "You can plot the signal in order to see which regions should be detected. In my case, the sample file contains 2 to 3 speech regions.\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(np.arange(len(data)), data)\n",
      "plt.title(\"Raw audio signal\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/audio_0.png)\n",
      "\n",
      "You can already notice that there is a notion a threshold that appears. From what moment do we assume that someone is speaking? How do we split 2 regions? We'll answer those questions as we dive deeper into the solution.\n",
      "\n",
      "# Rolling Window\n",
      "\n",
      "The solution will take the form of a rolling window on the input data. We will determine the energy in the frequency range that usually is associated to speech, and the energy of the whole band. If the ratio is larger than a threshold, we can assume that someone is speaking. \n",
      "\n",
      "We first need to define some constants that we will use:\n",
      "\n",
      "```python\n",
      "SAMPLE_START = 0\n",
      "SPEECH_START_BAND = 300\n",
      "SPEECH_END_BAND = 3000\n",
      "SAMPLE_WINDOW = 0.02\n",
      "SAMPLE_OVERLAP = 0.01\n",
      "THRESHOLD = 0.6\n",
      "```\n",
      "\n",
      "Here's what the constants are used for:\n",
      "- `SAMPLE_START` : the start index,\n",
      "- `SPEECH_START_BAND` : the minimum frequency of a human voice\n",
      "- `SPEECH_END_BAND` : the maximum frequency of a human voice\n",
      "- `SAMPLE_WINDOW` : a 20 ms window size on which we run the algorithm\n",
      "- `SAMPLE_OVERLAP` : the amount by which we shift the window size at each step\n",
      "- `THRESHOLD` : the threshold for the energy ratio under which a sound is not tagged as a voice\n",
      "\n",
      "The rolling window will have the following format:\n",
      "\n",
      "```python\n",
      "while (SAMPLE_START < (len(data) - SAMPLE_WINDOW)):\n",
      "    \n",
      "    # Select only the region of the data in the window\n",
      "    SAMPLE_END = SAMPLE_START + SAMPLE_WINDOW\n",
      "    if SAMPLE_END >= len(data): \n",
      "        SAMPLE_END = len(data)-1\n",
      "\n",
      "    data_window = data[SAMPLE_START:SAMPLE_END]\n",
      "    \n",
      "    # Detect speech here\n",
      "    \n",
      "    # Increment \n",
      "    SAMPLE_START += SAMPLE_OVERLAP\n",
      "```\n",
      "\n",
      "# Speech Ratio\n",
      "\n",
      "Within this data window, we now need to determine the speech ratio:\n",
      "    \n",
      "$$ speech_{ratio} = \\frac{\\sum energy_{voice}}{\\sum energy_{full}} $$\n",
      "\n",
      "To determine the voice energy, we will only consider frequencies between 300 and 3'000 Hz, as they correspond to human voice frequencies. \n",
      "\n",
      "The first thing we need to do is to compute the range of possible frequencies at the defined rate and given the audio sequence:\n",
      "\n",
      "```python\n",
      "def _calculate_frequencies(audio_data):\n",
      "    data_freq = np.fft.fftfreq(len(audio_data),1.0/rate)\n",
      "    data_freq = data_freq[1:]\n",
      "    return data_freq\n",
      "```\n",
      "\n",
      "This will return regular values between -8'000 and 8'000. The energy transported by a wave is directly proportional to the square of the amplitude of the wave, which can be computed using a Fast Fourrier Transform.\n",
      "\n",
      "```python\n",
      "def _calculate_energy(audio_data):\n",
      "    data_ampl = np.abs(np.fft.fft(audio_data))\n",
      "    data_ampl = data_ampl[1:]\n",
      "    return data_ampl ** 2\n",
      "```\n",
      "\n",
      "We then connect the energy with the frequency by creating a dictionary whose keys are the absolute value of the frequency, and values are the corresponding energy at that frequency.\n",
      "\n",
      "```python\n",
      "def _connect_energy_with_frequencies(data):\n",
      "    \n",
      "    data_freq = _calculate_frequencies(data)\n",
      "    data_energy = _calculate_energy(data)\n",
      "    \n",
      "    energy_freq = {}\n",
      "    for (i, freq) in enumerate(data_freq):\n",
      "        if abs(freq) not in energy_freq:\n",
      "            energy_freq[abs(freq)] = data_energy[i] * 2\n",
      "    return energy_freq\n",
      "\n",
      "energy_freq = _connect_energy_with_frequencies(data)\n",
      "sum_full_energy = sum(energy_freq.values())\n",
      "```\n",
      "\n",
      "The variable `energy_freq` should return :\n",
      "\n",
      "```python\n",
      "{0.4166666666666667: 388888371.0778143,\n",
      " 0.8333333333333334: 378650788.74457765,\n",
      " 1.25: 139749533.30109847,\n",
      " 1.6666666666666667: 703141467.1534827,\n",
      " 2.0833333333333335: 2622893493.5843244,\n",
      " 2.5: 2214362080.232078,\n",
      " ...\n",
      " ```\n",
      "\n",
      "As stated above, we suppose that a human voice will be anywhere between 300 and 3'000 Hz. Therefore, we sum the energy corresponding such frequencies in the time window, and we can compare it with the full sum of energies.\n",
      "\n",
      "```python\n",
      "def _sum_energy_in_band(energy_frequencies):\n",
      "    sum_energy = 0\n",
      "    for f in energy_frequencies.keys():\n",
      "        if SPEECH_START_BAND < f < SPEECH_END_BAND:\n",
      "            sum_energy += energy_frequencies[f]\n",
      "    return sum_energy\n",
      "\n",
      "sum_voice_energy = _sum_energy_in_band(energy_freq)\n",
      "```\n",
      "\n",
      "Finally, we can define the speech ratio as being the quotien between the sum of the speech energy in the time window and the sum of the total energy. \n",
      "\n",
      "```python\n",
      "speech_ratio = sum_voice_energy/sum_full_energy\n",
      "speech_ratio\n",
      "```\n",
      "\n",
      "In this sample, it gave me : `0.68923`.\n",
      "\n",
      "# Combining the loop and the speech ratio\n",
      "\n",
      "So far, we estimated the speech ratio on the whole audio file, without using a rolling window. It is now time to combine both approaches. We will store in `speech_ratio_list` a list of all the speech ratios in the loop.\n",
      "\n",
      "```python\n",
      "speech_ratio_list = []\n",
      "detected_voice = []\n",
      "mean_data = []\n",
      "\n",
      "SAMPLE_START = 0\n",
      "\n",
      "while (SAMPLE_START < (len(data) - SAMPLE_WINDOW)):\n",
      "    \n",
      "    # Select only the region of the data in the window\n",
      "    SAMPLE_END = SAMPLE_START + SAMPLE_WINDOW\n",
      "    if SAMPLE_END >= len(data): \n",
      "        SAMPLE_END = len(data)-1\n",
      "\n",
      "    data_window = data[SAMPLE_START:SAMPLE_END]\n",
      "    mean_data.append(np.mean(data_window))\n",
      "\n",
      "    # Full energy\n",
      "    energy_freq = _connect_energy_with_frequencies(data_window)\n",
      "    sum_full_energy = sum(energy_freq.values())\n",
      "    \n",
      "    # Voice energy\n",
      "    sum_voice_energy = _sum_energy_in_band(energy_freq)\n",
      "    \n",
      "    # Speech ratio\n",
      "    speech_ratio = sum_voice_energy/sum_full_energy\n",
      "    speech_ratio_list.append(speech_ratio)\n",
      "    detected_voice.append(speech_ratio > THRESHOLD)\n",
      "    \n",
      "    # Increment \n",
      "    SAMPLE_START += SAMPLE_OVERLAP\n",
      "```\n",
      "\n",
      "We can now compare the speech ratio list with the threshold over time:\n",
      "\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(speech_ratio_list)\n",
      "plt.axhline(THRESHOLD, c='r')\n",
      "plt.title(\"Speech ratio list vs. threshold\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/audio_1.png)\n",
      "\n",
      "We can also compare the raw signal with moments we detected a voice:\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(np.array(mean_data), alpha=0.4, label=\"Not detected\")\n",
      "plt.plot(np.array(detected_voice) * np.array(mean_data), label=\"Detected\")\n",
      "plt.legend()\n",
      "plt.title(\"Detected vs. non-detected region\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/audio_2.png)\n",
      "\n",
      "You can try to play with the speech ratio threshold and the window size to see how it affects the detection.\n",
      "\n",
      "# Smoothing the regions\n",
      "\n",
      "The output is interesting but would require some smoothing if we want to detect smooth regions in which a user speaks. We'll go for a median filter and apply it on the speech ratio's list.\n",
      "\n",
      "```python\n",
      "def _median_filter (x, k):\n",
      "    assert k % 2 == 1, \"Median filter length must be odd.\"\n",
      "    assert x.ndim == 1, \"Input must be one-dimensional.\"\n",
      "    k2 = (k - 1) // 2\n",
      "    \n",
      "    y = np.zeros((len(x), k), dtype=x.dtype)\n",
      "    y[:,k2] = x\n",
      "    for i in range (k2):\n",
      "        j = k2 - i\n",
      "        y[j:,i] = x[:-j]\n",
      "        y[:j,i] = x[0]\n",
      "        y[:-j,-(i+1)] = x[j:]\n",
      "        y[-j:,-(i+1)] = x[-1]\n",
      "    return np.median(y, axis=1)\n",
      "```\n",
      "\n",
      "We can the apply it to a region\n",
      "\n",
      "\n",
      "```python\n",
      "SPEECH_WINDOW = 0.5\n",
      "\n",
      "def _smooth_speech_detection(detected_voice):\n",
      "    window = 0.02\n",
      "    median_window=int(SPEECH_WINDOW/window)\n",
      "    if median_window % 2 == 0 : \n",
      "        median_window = median_window - 1\n",
      "    median_energy = _median_filter(detected_voice, median_window)\n",
      "    \n",
      "    return median_energy\n",
      "```\n",
      "\n",
      "We can now apply this to the pipeline defined above:\n",
      "\n",
      "```python\n",
      "speech_ratio_list = []\n",
      "detected_voice = []\n",
      "mean_data = []\n",
      "\n",
      "SAMPLE_START = 0\n",
      "\n",
      "while (SAMPLE_START < (len(data) - SAMPLE_WINDOW)):\n",
      "    \n",
      "    # Select only the region of the data in the window\n",
      "    SAMPLE_END = SAMPLE_START + SAMPLE_WINDOW\n",
      "    if SAMPLE_END >= len(data): \n",
      "        SAMPLE_END = len(data)-1\n",
      "    data_window = data[SAMPLE_START:SAMPLE_END]\n",
      "    mean_data.append(np.mean(data_window))\n",
      "    # Full energy\n",
      "    energy_freq = _connect_energy_with_frequencies(data_window)\n",
      "    sum_full_energy = sum(energy_freq.values())\n",
      "    \n",
      "    # Voice energy\n",
      "    sum_voice_energy = _sum_energy_in_band(energy_freq)\n",
      "    \n",
      "    # Speech ratio\n",
      "    speech_ratio = sum_voice_energy/sum_full_energy\n",
      "    speech_ratio_list.append(speech_ratio)\n",
      "    detected_voice.append(int(speech_ratio > THRESHOLD))\n",
      "    \n",
      "    # Increment \n",
      "    SAMPLE_START += SAMPLE_OVERLAP\n",
      "    \n",
      "detected_voice = _smooth_speech_detection(np.array(detected_voice))\n",
      "```\n",
      "\n",
      "Finally, the detected regions are these ones :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(np.array(detected_voice), label=\"Detected\")\n",
      "plt.legend()\n",
      "plt.title(\"Detected vs. non-detected region\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/audio_3.png)\n",
      "\n",
      "We can plot once again the regions on the raw signal in which the voice has been detected:\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(np.array(mean_data), alpha=0.4, label=\"Not detected\")\n",
      "plt.plot(np.array(detected_voice) * np.array(mean_data), label=\"Detected\")\n",
      "plt.legend()\n",
      "plt.title(\"Detected vs. non-detected region\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/audio_4.png)\n",
      "\n",
      "# Pros and Cons\n",
      "\n",
      "The main advantages of this approach is that :\n",
      "- it runs really fast\n",
      "- it is easily explainable\n",
      "- it is simple to implement\n",
      "- it does not take language into account\n",
      "\n",
      "The limits of such approach is that :\n",
      "- there are many hyperparameters to choose from\n",
      "- we must specify manually the range of frequency corresponding to a human voice\n",
      "- this range is not unique to humans, an an animal or a car could be interprete as a human\n",
      "\n",
      "> *Conclusion*: In this article, we introduced the concept of voice activity detection. In the next article, we'll see how to create a web application to deploy our algorithm using Streamlit.\n",
      "---\n",
      "title: Speaker Verification using SVM-based methods\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Speech Processing\"\n",
      "---\n",
      "\n",
      "This article requires having read the first article on Speaker Verification using [GMM-UBM method](https://maelfabien.github.io/machinelearning/Speech1/) to fully understand what this approach brings.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "In the paper [Support Vector Machines using GMM Supervectors for Speaker Verification](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.604&rep=rep1&type=pdf) by W. M. Campbell, D. E. Sturim, D. A. Reynolds, authors introduce a new approach, different from the MAP adaptation of means on the GMM models for speaker verification tasks.\n",
      "\n",
      "# GMM super-vectors\n",
      "\n",
      "SVM-based method, as GMM-UBM method, rely on GMM vectors, but in a stacked format, called GMM super-vectors. \n",
      "\n",
      "During the enrollment, we extract the means of the GMMs after adaptation with MAP from the UBM, but instead of using those to score the new test samples, the authors introduce this notion of super-vector as simply the stacked means of the mixture components. These vector is then fed as an input to a Support Vector Machine Classifier (SVM) with 2 classes.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gmm_svm.png)\n",
      "\n",
      "For example, instead of having 512 Gaussian components of dimention 26 each, we have a single vector of size $$ 512 \\times 26 = 13312 $$.\n",
      "\n",
      "# SVM classification\n",
      "\n",
      "Support Vector Machine (SVM) algorithm learns a discriminative frontier between two classes which maximizes margins. It can leverage a non-linear kernel mapping to project the data in a high-dimensional space in which it is linearly separable.\n",
      "\n",
      "The two classes to distinguish from are simply:\n",
      "- the target speaker\n",
      "- the impostor/background/population\n",
      "\n",
      "The discriminative function of the SVM is given by:\n",
      "\n",
      "$$ f(x) = \\sum_{i=1}^N \\alpha_i y_i K(x, x_i) + d $$\n",
      "\n",
      "Where:\n",
      "- $$ y_i $$ is the ground truth for the output value, either 1 or -1.\n",
      "- $$ x_i $$ is the support vector\n",
      "- $$ \\alpha_i $$ are the corresponding weights\n",
      "- $$ d $$ is a bias term\n",
      "\n",
      "The SVM can be linearly separable if we apply a kernel function $$ K(x,y) = b(x)^t b(y) $$, where $$ b(x) $$ is a mapping function from the input space to a possibly infinite dimensional space. This method is called the Kernel trick.\n",
      "\n",
      "\n",
      "And that's it ! We just need to train the SVM model on GMM super-vectors with positive and negative labels. Applying a SVM with a non-linear Kernel will identify the discriminative frontier.\n",
      "\n",
      "The prediction is straight-forward, since we just need to extract the super-vector and run it into the trained SVM.\n",
      "\n",
      "---\n",
      "title: Lab - Create a streaming data pipeline with Cloud DataFlow \n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Road to Google Cloud Platform Certification\"\n",
      "---\n",
      "\n",
      "In this lab, we will analyze data from a taxi business. The aim of the lab is to :\n",
      "- Connect to a streaming data Topic in Cloud Pub/sub\n",
      "- Ingest streaming data with Cloud Dataflow\n",
      "- Load streaming data into BigQuery\n",
      "- Analyze and visualize the results with DataStudio\n",
      "\n",
      "First, we need to confirm that the needed APIs are enabled. On the console, in the side menu, click on \"API & Services\". You should see Google Cloud Pub/Sub API and Dataflow API already enabled. If not, enable them.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_118.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_119.jpg)\n",
      "\n",
      "In this lab, messages published into Pub/Sub will be aggregated and stored in BigQuery. Therefore, we need to create a BigQuery Dataset.\n",
      "\n",
      "Open a shell, and type the following command :\n",
      "\n",
      "```\n",
      "bq mk taxirides\n",
      "```\n",
      "\n",
      "Then, to create the table, type this command :\n",
      "\n",
      "```\n",
      "bq mk \\\n",
      "--time_partitioning_field timestamp \\\n",
      "--schema ride_id:string,point_idx:integer,latitude:float,longitude:float,\\\n",
      "timestamp:timestamp,meter_reading:float,meter_increment:float,ride_status:string,\\\n",
      "\n",
      "passenger_count:integer -t taxirides.realtime\n",
      "```\n",
      "\n",
      "It creates an empty schema for `taxirides.realtime` table in which we will stream later.\n",
      "\n",
      "We will then use Cloud Storage to provide working space for our Cloud Dataflow pipeline. To do so, open the Storage tab in the menu, and create a bucket whose name is the same as your project's name :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_122.jpg)\n",
      "\n",
      "Then, we will set up a streaming data pipeline to read sensor data from Pub/Sub, compute the maximum temperature within a time window, and write this out to BigQuery.\n",
      "\n",
      "From the navigation menu, click on DataFlow :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_123.jpg)\n",
      "\n",
      "Then, click on \"Create job from template\" :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_124.jpg)\n",
      "\n",
      "- Under Cloud Dataflow template, select the Cloud Pub/Sub Topic to BigQuery template.\n",
      "- Under Cloud Pub/Sub input topic, enter `projects/pubsub-public-data/topics/taxirides-realtime`\n",
      "- Under BigQuery output table, enter `<myprojectid>:taxirides.realtime`\n",
      "- Under Temporary Location, enter `gs://<mybucket>/tmp/`\n",
      "\n",
      "For example :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_125.jpg)\n",
      "\n",
      "Click on Run job, and you'll see that your pipeline is now running! The main steps in this template are :\n",
      "- Read topic\n",
      "- Convert message to tables\n",
      "- Write records\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_126.jpg)\n",
      "\n",
      "After a few minutes, all services will be up and running, and you should see information on the rate of data coming in :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_127.jpg)\n",
      "\n",
      "We can now analyze the data in BigQuery :\n",
      "\n",
      "```\n",
      "SELECT * FROM taxirides.realtime LIMIT 10\n",
      "````\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_128.jpg)\n",
      "\n",
      "We can also perform aggregations on the stream for reporting :\n",
      "\n",
      "```\n",
      "WITH streaming_data AS (\n",
      "\n",
      "SELECT\n",
      "    timestamp,\n",
      "    TIMESTAMP_TRUNC(timestamp, HOUR, 'UTC') AS hour,\n",
      "    TIMESTAMP_TRUNC(timestamp, MINUTE, 'UTC') AS minute,\n",
      "    TIMESTAMP_TRUNC(timestamp, SECOND, 'UTC') AS second,\n",
      "    ride_id,\n",
      "    latitude, \n",
      "    longitude,\n",
      "    meter_reading,\n",
      "    ride_status,\n",
      "    passenger_count\n",
      "FROM\n",
      "    taxirides.realtime\n",
      "WHERE ride_status = 'dropoff'\n",
      "ORDER BY timestamp DESC\n",
      "LIMIT 100000\n",
      ")\n",
      "\n",
      "# calculate aggregations on stream for reporting:\n",
      "SELECT \n",
      "    ROW_NUMBER() OVER() AS dashboard_sort,\n",
      "    minute,\n",
      "    COUNT(DISTINCT ride_id) AS total_rides,\n",
      "    SUM(meter_reading) AS total_revenue,\n",
      "    SUM(passenger_count) AS total_passengers\n",
      "FROM streaming_data\n",
      "GROUP BY minute, timestamp\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_129.jpg)\n",
      "\n",
      "It shows total revenue, customers and rides every minute.\n",
      "\n",
      "We can now explore this table in Data Studio. Click on \"Open with Data Studio\", and we'll build a small dashboard to display information :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_130.jpg)\n",
      "\n",
      "Select the Bar chart option. Set minute as the dimensions as well as dashboard sort, and use the number of passengers, ride, and revenu as metrics. \n",
      "\n",
      "The data is updated every minute, just click on the button \"Refresh Data\" whenever you want to visualize the latest data.\n",
      "\n",
      "If you want to stop the streaming, stop the job from DataFlow :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_131.jpg)\n",
      "\n",
      "And choose to either cancel or drain the ingestion :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_132.jpg)\n",
      "\n",
      "---\n",
      "title: Graph Learning\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Graph Analysis and Graph Learning\"\n",
      "---\n",
      "\n",
      "So far, we covered the main kind of graphs and the basics of graph analysis. We'll now cover into more details the way we can \"learn\" in graphs.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "For what comes next, open a Jupyter Notebook and import the following packages :\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import random\n",
      "import networkx as nx\n",
      "from IPython.display import Image\n",
      "import matplotlib.pyplot as plt\n",
      "```\n",
      "\n",
      "If you have not already installed the `networkx` package, simply run :\n",
      "\n",
      "```bash\n",
      "pip install networkx\n",
      "```\n",
      "\n",
      "The following articles will be using the latest version  `2.x` of  `networkx`. NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\n",
      "\n",
      "To illustrate the different concepts we'll cover and how it applies to graphs we'll take the Karate Club example. This graph is present in the `networkx` package. It represents the relations of members of a Karate Club. However, due to a disagreement of the founders of the club, the club has recently been split in two. We'll try to illustrate this event with graphs. \n",
      "\n",
      "First, load and plot the graph :\n",
      "\n",
      "```python\n",
      "n=34\n",
      "m = 78\n",
      "G_karate = nx.karate_club_graph()\n",
      "\n",
      "pos = nx.spring_layout(G_karate)\n",
      "nx.draw(G_karate, cmap = plt.get_cmap('rainbow'), with_labels=True, pos=pos)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/karate.jpg)\n",
      "\n",
      "\n",
      "There are two main tasks in graph learning :\n",
      "- Link prediction\n",
      "- Node labeling\n",
      "\n",
      "We'll start with link prediction.\n",
      "\n",
      "# I. Link prediction\n",
      "\n",
      "In Link Prediction, given a graph $$ G $$, we aim to predict new edges. Predictions are useful to predict future relations or missing edges when the graph is not fully observed for example. \n",
      "\n",
      "In link prediction, we simply try to build a similarity measure between pairs of nodes and link the most similar (until we reach a threshold $$ k $$ for example). The question is now to identify the right similarity scores!\n",
      "\n",
      "To illustrate the different similarity scores, let's consider the following graph :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_13.jpg)\n",
      "\n",
      "Let $$ N(i) $$ be a set of neighbors of node $$ i $$. On the graph above, the neighbors of both nodes $$ i $$ and $$ j $$ can be represented as :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_15.jpg)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_16.jpg)\n",
      "\n",
      "We can build several similarity scores :\n",
      "- **Common Neighbors** : $$ S(i,j) = \\mid N(i) \\cap N(j) \\mid $$. In this example, the score would be simply 1.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_17.jpg)\n",
      "\n",
      "- **Jaccard Coefficient** : $$ S(i,j) = \\frac { \\mid N(i) \\cap N(j) \\mid } { \\mid N(i) \\cup N(j) \\mid } $$. This is a normalized common neighbors version.\n",
      "\n",
      "The intersection is the Common Neighbors, and the union is :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_18.jpg)\n",
      "\n",
      "Therefore, the Jaccard Coefficient is given by the ratio :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_19.jpg)\n",
      "\n",
      "And the value is $$ \\frac {1} {6} $$.\n",
      "- **Adamic-Adar index** : $$ S(i,j) = \\sum_{k \\in N(i)\\cap N(j) } \\frac {1} {\\log \\mid N(k) \\mid} $$\n",
      "In other words, for each common neighbor of nodes $$ i $$ and $$ j $$, we add $$ 1 $$ divided by the total number of neighbors of that node. The concept is that common elements with very large neighborhoods are less significant when predicting a connection between two nodes compared to elements shared between a small number of nodes.\n",
      "- **Preferential attachment** : $$ S(i,j) = \\mid N(i) \\mid * \\mid N(j) \\mid $$\n",
      "- We can also use community information when it is available.\n",
      "\n",
      "How do we evaluate the link prediction?\n",
      "We must hide a subset of node pairs, and predict their links based on the rules defined above. We then evaluate the proportion of correct predictions for dense graphs, or use Area under the Curve criteria for Sparse graphs.\n",
      "\n",
      "Let's implement this in Python on our Karate graph!\n",
      "\n",
      "First of all, print the information of the graph :\n",
      "\n",
      "```python\n",
      "n = G_karate.number_of_nodes()\n",
      "m = G_karate.number_of_edges()\n",
      "print(\"Number of nodes : %d\" % n)\n",
      "print(\"Number of edges : %d\" % m)\n",
      "print(\"Number of connected components : %d\" % nx.number_connected_components(G_karate))\n",
      "```\n",
      "\n",
      "Then, plot the graph itself :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "nx.draw(G_karate)\n",
      "plt.gca().collections[0].set_edgecolor(\"#000000\")\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_20.jpg)\n",
      "\n",
      "Now, let's remove some connections :\n",
      "\n",
      "```python\n",
      "# Remove 20% of the edges\n",
      "proportion_edges = 0.2\n",
      "edge_subset = random.sample(G_karate.edges(), int(proportion_edges * G_karate.number_of_edges()))\n",
      "\n",
      "# Create a copy of the graph and remove the edges\n",
      "G_karate_train = G_karate.copy()\n",
      "G_karate_train.remove_edges_from(edge_subset)\n",
      "```\n",
      "\n",
      "And plot the partially observed graph :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "nx.draw(G_karate_train)\n",
      "plt.gca().collections[0].set_edgecolor(\"#000000\") # set node border color to black\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_21.jpg)\n",
      "\n",
      "You can print the number of edges we deleted and the number of edges remaining :\n",
      "\n",
      "```python\n",
      "edge_subset_size = len(list(edge_subset))\n",
      "print(\"Number of edges deleted : %d\" % edge_subset_size)\n",
      "print(\"Number of edges remaining : %d\" % (m - edge_subset_size))\n",
      "```\n",
      "\n",
      "```\n",
      "Number of edges deleted : 15\n",
      "Number of edges remaining : 63\n",
      "```\n",
      "\n",
      "### Jaccard Coefficient\n",
      "\n",
      "```python\n",
      "# Make prediction using Jaccard Coefficient\n",
      "pred_jaccard = list(nx.jaccard_coefficient(G_karate_train))\n",
      "score_jaccard, label_jaccard = zip(*[(s, (u,v) in edge_subset) for (u,v,s) in pred_jaccard])\n",
      "```\n",
      "\n",
      "The prediction look like this, a first node, a second node and a jaccard score :\n",
      "\n",
      "```\n",
      "[(0, 32, 0.15),\n",
      "(0, 33, 0.125),\n",
      "(0, 3, 0.21428571428571427),\n",
      "(0, 9, 0.0),\n",
      "(0, 14, 0.0),\n",
      "(0, 15, 0.0),\n",
      "...\n",
      "```\n",
      "\n",
      "Then, compute the score :\n",
      "\n",
      "```python\n",
      "# Compute the ROC AUC Score\n",
      "fpr_jaccard, tpr_jaccard, _ = roc_curve(label_jaccard, score_jaccard)\n",
      "auc_jaccard = roc_auc_score(label_jaccard, score_jaccard)\n",
      "```\n",
      "\n",
      "### Adamic-Adar\n",
      "\n",
      "We can now repeat this for the Adamic-Adar Index :\n",
      "\n",
      "```python\n",
      "# Prediction using Adamic Adar \n",
      "pred_adamic = list(nx.adamic_adar_index(G_karate_train))\n",
      "score_adamic, label_adamic = zip(*[(s, (u,v) in edge_subset) for (u,v,s) in pred_adamic])\n",
      "\n",
      "# Compute the ROC AUC Score\n",
      "fpr_adamic, tpr_adamic, _ = roc_curve(label_adamic, score_adamic)\n",
      "auc_adamic = roc_auc_score(label_adamic, score_adamic)\n",
      "```\n",
      "\n",
      "### Preferential Attachment\n",
      "\n",
      "And for the preferential attachment score :\n",
      "\n",
      "```python\n",
      "# Compute the Preferential Attachment\n",
      "pred_pref = list(nx.preferential_attachment(G_karate_train))\n",
      "score_pref, label_pref = zip(*[(s, (u,v) in edge_subset) for (u,v,s) in pred_pref])\n",
      "\n",
      "fpr_pref, tpr_pref, _ = roc_curve(label_pref, score_pref)\n",
      "auc_pref = roc_auc_score(label_pref, score_pref)\n",
      "```\n",
      "\n",
      "### Plot the ROC AUC Curve\n",
      "\n",
      "The Adamic-Adar seems to outperform the other criteria on our problem :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_22.jpg)\n",
      "\n",
      "We covered the most common similarity scores for link prediction. We'll now cover into more details the node labeling algorithms.\n",
      "\n",
      "# II. Node Labeling\n",
      "\n",
      "Given a graph where some nodes are not labeled, we want to predict their labels. This is in some sense a semi-supervised learning problem.\n",
      "\n",
      "One common way to deal with such problems is to assume that there is a certain smoothness on the graph. The Smoothness assumption states that points connected via a path through high-density regions on the data are likely to have similar labels. This is the main hypothesis behind the Label Propagation Algorithm. \n",
      "\n",
      "The Label Propagation Algorithm (LPA) is a fast algorithm for finding communities in a graph using network structure alone as its guide, without any predefined objective function or prior information about the communities.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/lpa.jpg)\n",
      "\n",
      "A single label can quickly become dominant in a densely connected group of nodes, but it will have trouble crossing a sparsely connected region. \n",
      "\n",
      "How does the semi-supervised label propagation work?\n",
      "\n",
      "First, we have some data : $$ x_1, ..., x_l, x_{l+1}, ..., x_n \\in R^p $$, and labels for the first $$ l $$ points : $$ y_1, ..., y_l \\in 1...C $$\n",
      "\n",
      "We define the initial label matrix $$ Y \\in R^{n \\times C} $$ such that $$ Y_{ij} = 1 $$ if $$ x_i $$ has label $$ y_i = j $$ and $$ 0 $$ otherwise. \n",
      "\n",
      "The algorithm will generate a prediction matrix $$ F \\in R^{n \\times C} $$ which we'll detail under. Then, we predict the label of a node by finding the label that is the most likely :\n",
      "\n",
      "$$ \\hat{Y_i} = argmax_j F_{i,j} $$\n",
      "\n",
      "What is the prediction matrix $$ F $$ ?\n",
      "\n",
      "The prediction matrix is a matrix $$ F^{\\star} $$ that minimizes both smoothness and accuracy criteria. Therefore, there is a tradeoff to make between the smoothness and the accuracy of our result.\n",
      "\n",
      "The problem expression is quite complex, so I won't go into details. However, the solution is given by :\n",
      "\n",
      "$$ F^{\\star} = ( (1-\\alpha)I + L_{sym})^{-1} Y $$ where :\n",
      "- the parameter $$ \\alpha = \\frac {1} {1+\\mu} $$\n",
      "- the labels are given by $$ Y $$\n",
      "- and $$ L_{sym} $$ is the normalized Laplacian matrix of the graph\n",
      "\n",
      "If you'd like to go further on this topic, check the notions of smoothness of a graph function and manifold regularization.\n",
      "\n",
      "Alright, now, how do we implement this in Python?\n",
      "\n",
      "To have additional (binary) features that we'll use as labels, we need to work with some real-world data from Facebook! You can download the data right [here](https://snap.stanford.edu/data/egonets-Facebook.html)\n",
      "\n",
      "Put them in a folder called \"facebook\" in your repository.\n",
      "\n",
      "```python\n",
      "G_fb = nx.read_edgelist(\"facebook/414.edges\")\n",
      "n = G_fb.number_of_nodes()\n",
      "m = G_fb.number_of_edges()\n",
      "\n",
      "print(\"Number of nodes: %d\" % n)\n",
      "print(\"Number of edges: %d\" % m)\n",
      "print(\"Number of connected components: %d\" % nx.number_connected_components(G_fb))\n",
      "```\n",
      "\n",
      "```\n",
      "Number of nodes: 150\n",
      "Number of edges: 1693\n",
      "Number of connected components: 2\n",
      "```\n",
      "\n",
      "We have a sample graph of 150 nodes, for 1693 edges. There are 2 connected components, which means that there is a part of the graph that is detached from the rest.\n",
      "\n",
      "Let's now plot the graph :\n",
      "\n",
      "```\n",
      "mapping=dict(zip(G_fb.nodes(), range(n)))\n",
      "nx.relabel_nodes(G_fb, mapping, copy=False)\n",
      "pos = nx.spring_layout(G_fb)\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "nx.draw(G_fb, node_size=200, pos=pos)\n",
      "plt.gca().collections[0].set_edgecolor(\"#000000\")\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_23.jpg)\n",
      "\n",
      "The graph comes with some features. Among the features available, we are going to use feature 43, that describes whether someone attended a given school or not. We have only 2 labels (1, in red, if attended and -1, in blue, otherwise).\n",
      "\n",
      "```python\n",
      "with open('facebook/414.featnames') as f:\n",
      "    for i, l in enumerate(f):\n",
      "        pass\n",
      "\n",
      "n_feat = i+1\n",
      "\n",
      "features = np.zeros((n, n_feat))\n",
      "f = open('facebook/414.feat', 'r')\n",
      "\n",
      "for line in f:\n",
      "    if line.split()[0] in mapping:\n",
      "        node_id = mapping[line.split()[0]]\n",
      "            features[node_id, :] = list(map(int, line.split()[1:]))\n",
      "\n",
      "features = 2*features-1\n",
      "feat_id = 43\n",
      "labels = features[:, feat_id]\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "nx.draw(G_fb, cmap = plt.get_cmap('bwr'), nodelist=range(n), node_color = labels, node_size=200, pos=pos)\n",
      "plt.gca().collections[0].set_edgecolor(\"#000000\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_24.jpg)\n",
      "\n",
      "This attribute is rather smooth on the graph, so propagation should work pretty well. To illustrate how node labeling works, we'll now delete some of the node labels. We'll keep only 30% of the nodes :\n",
      "\n",
      "```python\n",
      "random.seed(5)\n",
      "proportion_nodes = 0.3\n",
      "labeled_nodes = random.sample(G_fb.nodes(), int(proportion_nodes * G_fb.number_of_nodes()))\n",
      "\n",
      "known_labels = np.zeros(n)\n",
      "known_labels[labeled_nodes] = labels[labeled_nodes]\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "nx.draw(G_fb, cmap = plt.get_cmap('bwr'), nodelist=range(n), node_color = known_labels, node_size=200, pos=pos)\n",
      "plt.gca().collections[0].set_edgecolor(\"#000000\") # set node border color to black\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_25.jpg)\n",
      "\n",
      "Alright, we are now ready to apply label propagation!\n",
      "\n",
      "```python\n",
      "alpha = 0.7\n",
      "L_sym = nx.normalized_laplacian_matrix(G_fb)\n",
      "\n",
      "Y = np.zeros((n,2))\n",
      "Y[known_labels==-1, 0] = 1\n",
      "Y[known_labels==1, 1] = 1\n",
      "I = np.identity(n)\n",
      "\n",
      "# Create the F-pred matrix\n",
      "F_pred = np.linalg.inv(I*(1-alpha) + L_sym) * Y\n",
      "# Identify the prediction as the argmax\n",
      "pred = np.array(np.argmax(F_pred, axis=1)*2-1).flatten()\n",
      "# Compute the accuracy score\n",
      "succ_rate = accuracy_score(labels, pred)\n",
      "```\n",
      "\n",
      "And plot the result :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(18, 6))\n",
      "f, axarr = plt.subplots(1, 2, num=1)\n",
      "\n",
      "# Plot true values\n",
      "plt.sca(axarr[0])\n",
      "nx.draw(G_fb, cmap = plt.get_cmap('bwr'), nodelist=range(n), node_color = labels, node_size=200, pos=pos)\n",
      "axarr[0].set_title('True labels', size=16)\n",
      "plt.gca().collections[0].set_edgecolor(\"#000000\")\n",
      "\n",
      "# Plot predicted values\n",
      "plt.sca(axarr[1])\n",
      "nx.draw(G_fb, cmap = plt.get_cmap('bwr'), nodelist=range(n), node_color = pred, node_size=200, pos=pos)\n",
      "axarr[1].set_title('Predicted labels (Success Rate: %.2f)' % succ_rate, size=16)\n",
      "plt.gca().collections[0].set_edgecolor(\"#000000\")\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/graph_26.jpg)\n",
      "\n",
      "That's it, we have our final prediction!\n",
      "\n",
      "> **Conclusion** : I hope that this article on graph learning was helpful. Don't hesitate to drop a comment if you have any question.\n",
      "\n",
      "Sources : \n",
      "- *A Comprehensive Guide to Graph Algorithms in Neo4j*\n",
      "- Networkx Documentation\n",
      "---\n",
      "title: Modéliser la distribution de données avec Python (Distribution Fitting)\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Stat4Decision\"\n",
      "search: false\n",
      "---\n",
      "\n",
      "*Cet article suppose une connaissance préalable des lois de probabilités les plus communes*.\n",
      "\n",
      "# Modéliser une distribution\n",
      "\n",
      "> La modélisation de la distribution de données (*probability distribution fitting*, ou *distribution fitting* en anglais) est le fait de trouver les paramètres de la loi de distribution de probabilité (ou de plusieurs lois candidates) qui correspond aux données que l'on cherche à modéliser. \n",
      "\n",
      "En d'autres termes, on souhaite savoir si nos données suivent par exemple une loi normale, une loi gamma, ou toute autre distribution, et les paramètres attachés à la loi. \n",
      "\n",
      "Pourquoi cherche-t-on à modéliser la distribution de nos données? L'information qu'apporte la distribution de nos données est en fait essentielle. Cela nous permet notamment déterminer la fréquence d'occurrence d'un certain phénomène. C'est une information très utilisée par les actuaires dans le monde de l'assurance afin de déterminer la probabilité qu'une perte dépasse un certain montant par exemple. Cette information est également utilisée en analyse de risque, en économie ou dans des études de fiabilité en ingénierie.\n",
      "\n",
      "La modélisation de la distribution de données est souvent utilisée dans les logiciels d'exploration de données, car elle permet de comprendre les propriétés sous-jacentes de nos données.\n",
      "\n",
      "La modélisation de la distribution de données est une tâche qui peut s'effectuer de deux manières:\n",
      "- Manuellement. Vous avez une idée de la loi de distribution, ou de quelques lois candidates. Pour chaque loi, vous trouvez les paramètres optimaux par maximum de vraisemblance (ou *Maximum Likelihood Estimation*, MLE en anglais), ou par la méthode des moments (Method of Moments) par exemple.\n",
      "- Automatiquement. Vous utilisez une librairie ou un logiciel spécialisé, qui a déjà implémenté les maximums de vraisemblance de nombreuses lois, et cherchez à trouver la meilleure loi et les meilleurs paramètres d'un coup.\n",
      "\n",
      "# Implémentation et exemple\n",
      "\n",
      "Supposons que vous travaillez dans une assurance, et disposez d'une série de données qui correspond aux montants des remboursements que l'assurance a effectué pour les différents assurés de son portefeuille \"Assurance Voiture\". En clair, dès qu'un client qui est assuré chez vous a un accident avec son véhicule, le montant que vous lui remboursez est rentré dans la série de données. Il y aura typiquement beaucoup de paiements d'un petit montant, et quelques paiements d'un montant élevé. Cependant, on peut aussi s'attendre à ce que les plus petits montants ne soient pas déclarés par les assurés, car cela peut impacter leur bonus pour les années suivantes. Si l'on représente l'histogramme des données, voici ce à quoi on peut s'attendre:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/s4d_1.png)\n",
      "\n",
      "On observe que:\n",
      "- pour des montants très faibles (<500€), les demandes de paiement sont faibles\n",
      "- un pic est atteint pour des montants avoisinnant les 1'000€\n",
      "- plus le montant augmente, moins il y a des cas de sinistres enregistrés\n",
      "\n",
      "## Modélisation d'une distribution connue\n",
      "\n",
      "Avec Python, il est simple de réaliser une modélisation automatique de la distribution de nos données. C'est ce que nous allons voir maintenant! Tout d'abord, importez les librairies suivantes:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy\n",
      "import scipy.stats\n",
      "import time\n",
      "```\n",
      "\n",
      "Nous allons ensuite simuler de fausses données d'un portefeuille d'assurance. Nous allons générer des données suivant une loi Gamma.\n",
      "\n",
      "```python\n",
      "# Nombre de valeurs à générer\n",
      "length = 30000\n",
      "bins=500\n",
      "\n",
      "# Génération des données\n",
      "data = np.random.gamma(2,1, length)\n",
      "\n",
      "# Histogramme des données\n",
      "y, x = np.histogram(data, bins=bins, density=True)\n",
      "# Milieu de chaque classe\n",
      "x = (x + np.roll(x, -1))[:-1] / 2.0\n",
      "```\n",
      "\n",
      "On peut alors représenter visuellement un histogramme données:\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.hist(data, bins=500, density=True)\n",
      "plt.title(\"Montant des paiements effectués (K€)\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/s4d_1.png)\n",
      "\n",
      "Afin d'identifier, pour une loi de distribution donnée, les paramètres correspondant au maximum de vraisemblance, nous allons utiliser la librairie [`scipy`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html), qui offre près de 80 distributions dans sa version actuelle, et identifie les paramètres optimaux par maximum de vraisemblance.\n",
      "\n",
      "Il suffit de spécifier la loi que l'on souhaite tester, et d'utiliser la méthode `fit` de Scipy pour récupérer les paramètres optimaux. Par exemple:\n",
      "\n",
      "```python\n",
      "dist_name = \"gamma\"\n",
      "\n",
      "# Paramètres de la loi\n",
      "dist = getattr(scipy.stats, dist_name)\n",
      "\n",
      "# Modéliser la loi\n",
      "param = dist.fit(data)\n",
      "```\n",
      "\n",
      "La variable `param` contient alors:\n",
      "\n",
      "```\n",
      "(1.993299446023943, -0.0005280086059209502, 0.9929125608037362)\n",
      "```\n",
      "\n",
      "Les paramètres correspondent à:\n",
      "- `shape` : 1.9933, contre une vraie valeur de 2\n",
      "- `loc` : -0.0005, contre une vraie valeur de 0\n",
      "- `scale` : 0.9929, contre une vraie valeur de 1\n",
      "\n",
      "On peut alors re-tracer la distribution de probabilité de la loi Gamma avec ces paramètres.\n",
      "\n",
      "```python\n",
      "loc = param[-2]\n",
      "scale = param[-1]\n",
      "arg = param[:-2]\n",
      "\n",
      "pdf = dist.pdf(x, loc=loc, scale=scale, *arg)\n",
      "\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(x, pdf, label=dist_name, linewidth=3) \n",
      "plt.plot(x, y, alpha=0.6)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/s4d_2.png)\n",
      "\n",
      "La PDF (probability distribution function), ou densité, de la loi Gamma avec les paramètres identifiés par Scipy est très proche des données originales. Comment peut-on cependant mesurer la distance entre les deux courbes? Il existe plusieurs mesures de distance, mais une des plus populaires est la somme de résidus au carrés (sum of squared errors, en anglais). On peut calculer la somme des résidus au carré de cette manière:\n",
      "\n",
      "```python\n",
      "sse = np.sum((y - pdf)**2)\n",
      "```\n",
      "\n",
      "Ici, la somme des résidus au carré vaut `0.04362`. Ce chiffre nous permettra de sélectionner la meilleure loi lors que nous testerons plusieurs lois à la fois. En effet, jusque-là, nous avons simplement généré des données selon une loi gamma, puis identifié les paramètres par maximum de vraisemblance pour cette même loi.\n",
      "\n",
      "## Plusieurs lois candidates\n",
      "\n",
      "Afin d'identifier la meilleure loi, il suffit de répéter le processus opéré jusque-là, mais pour toutes les lois que nous souhaitons tester. Dans un premier temps, on définit dans une liste l'ensemble des lois à tester:\n",
      "\n",
      "```python\n",
      "dist_names = ['norm', 'beta','gamma', 'pareto', 't', 'lognorm', 'invgamma', 'invgauss',  'loggamma', 'alpha', 'chi', 'chi2']\n",
      "```\n",
      "\n",
      "Ces lois correspondent globalement aux lois les plus communes, implémentées dans `scipy`. Par la suite, nous pouvons itérer afin de modéliser la distribution de nos données pour chacune de ces lois. Nous allons définir un seuil de la somme des résidus au carré en dessous duquel nous acceptons une loi comme une bonne candidate. Cela nous permet alors de réduire le temps de calcul, et de s'arrêter dès qu'une candidate satisfaisante est identifiée.\n",
      "\n",
      "```python\n",
      "sse = np.inf\n",
      "sse_thr = 0.10\n",
      "\n",
      "# Pour chaque distribution\n",
      "for name in dist_names:\n",
      "\n",
      "\t# Modéliser\n",
      "\tdist = getattr(scipy.stats, name)\n",
      "\tparam = dist.fit(data)\n",
      "\n",
      "\t# Paramètres\n",
      "\tloc = param[-2]\n",
      "\tscale = param[-1]\n",
      "\targ = param[:-2]\n",
      "\n",
      "\t# PDF\n",
      "\tpdf = dist.pdf(x, *arg, loc=loc, scale=scale)\n",
      "\t# SSE\n",
      "\tmodel_sse = np.sum((y - pdf)**2)\n",
      "\n",
      "\t# Si le SSE est ddiminué, enregistrer la loi\n",
      "\tif model_sse < sse :\n",
      "\t\tbest_pdf = pdf\n",
      "\t\tsse = model_sse\n",
      "\t\tbest_loc = loc\n",
      "\t\tbest_scale = scale\n",
      "\t\tbest_arg = arg\n",
      "\t\tbest_name = name\n",
      "\n",
      "\t# Si en dessous du seuil, quitter la boucle\n",
      "\tif model_sse < sse_thr :\n",
      "\t\tbreak\n",
      "\n",
      "```\n",
      "\n",
      "Ensuite, on affiche la loi sélectionnée, les paramètres identifiés et on représente le tout graphiquement.\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(x, y, label=\"Données\")\n",
      "plt.plot(x, best_pdf, label=best_name, linewidth=3)\n",
      "plt.legend(loc='upper right')\n",
      "plt.show()\n",
      "\n",
      "# Détails sur la loi sélectionnée\n",
      "print(\"Selected Model : \", best_name)\n",
      "print(\"Loc. param. : \", best_loc)\n",
      "print(\"Scale param. : \", best_scale)\n",
      "print(\"Other arguments : \", best_arg)\n",
      "print(\"SSE : \", sse)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/s4d_3.png)\n",
      "\n",
      "Ici, la loi Beta est identifiée comme meilleure candidate en 3.17 secondes, car on passe par cette loi dans la boucle avant la loi Gamma et qu'elle satisfait le critère du seuil de la somme des résidus au carré. Si on descend le seuil à 0, la loi identiée est une loi Gamma. Notre exemple reste simpliste car les données à modéliser sont générées à partir d'une des lois candidates. Que se passe-t'il si l'on vient perturber ces données avec un bruit gaussien?\n",
      "\n",
      "```python\n",
      "y = y + np.random.randn(bins)/50\n",
      "```\n",
      "\n",
      "En perturbant `y` et en ré-exécutant le code ci-dessus, la loi identifiée est une loi Gamma. La loi Beta ne satisfait plus, dans cet exemple, le critère du seuil de la somme des résidus au carré.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/s4d_4.png)\n",
      "\n",
      "# Application Web Interactive\n",
      "\n",
      "Afin de comprendre l'influence du seuil défini, des paramètres de départ de la loi générée ou de la perturbation gaussienne, il est intéressant de mettre en place une application web interactive. C'est ce que nous allons faire avec Streamlit! [Streamlit](https://streamlit.io/) est un nouveau service qui permet de déployer des applications et des modèles entrainés en quelques linges de code seulement.\n",
      "\n",
      "Voici une démonstration de l'application:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/demo_app.gif)\n",
      "\n",
      "Dans le menu latéral, l'utilisateur peut générer des données selon une loi initiale. Il peut décider des paramètres de la loi qui sert à générer les données et d'une perturbation à ajouter. Il peut ensuite ajouter une tolérance en termes de somme des résidus au carré, et allouer un budget de temps pour l'algorithme de modélisation. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/s4d_5.png)\n",
      "\n",
      "Les données sont ensuite générées automatiquement et l'algorithme de modélisation est lancé. La meilleure loi identifiée, les paramètres et le temps d'exécution sont alors affichés.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/s4d_6.png)\n",
      "\n",
      "Pour installer Streamlit, il suffit de passer par `pip`:\n",
      "\n",
      "```bash\n",
      "pip install streamlit\n",
      "```\n",
      "\n",
      "Le code de l'application Streamlit est disponible sur Github. Pour lancer l'application Streamlit en local, il suffit d'exécuter la commande suivante:\n",
      "\n",
      "```bash\n",
      "streamlit run dist.py\n",
      "```\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "A travers cet article, nous avons identifié les enjeux de la modélisation de distribution, et présenté une méthode automatisée pour identifier la loi optimale et les meilleurs paramètres par maximum de vraisemblance. Avec l'appui d'applications interactives comme Streamlit, l'exploration des différents scénarios de modélisation est bien plus intéractive.\n",
      "---\n",
      "title: How to use OpenPose on macOS ?\n",
      "layout: post\n",
      "tags: [tutorials]\n",
      "subtitle : \"Tutorials\"\n",
      "---\n",
      "\n",
      "\"OpenPose represents the first real-time multi-person system to jointly detect human body, hand, facial, and foot key points (in total 135 keypoints) on single images.\"\n",
      "OpenPose is a game-changer in pose detection. This library is proposed by the Perceptual Computing Laboratory of the Carnegie Mellon University.\n",
      "\n",
      "OpenPose offers a Python as well as a C++ API. In June 2018, a CPU-only macOS support was released. This is what we'll try right now!\n",
      "This article is a practical approach to the OpenPose library. A second article will develop the theory behind the library. \n",
      "\n",
      "NB: OpenPose License clearly states that any commercial use in the domain of sports is prohibited. This simply is a toy example for personal use.\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "## Functionalities\n",
      "\n",
      "For detailed information, please check the GitHub repository of the [project](https://github.com/CMU-Perceptual-Computing-Lab/openpose).\n",
      "\n",
      "The most important features of OpenPose include :\n",
      "- 2D multi-person real-time keypoint detection (body, hand, foot, face)\n",
      "- 3D single-person real-time keypoint detection\n",
      "\n",
      "The input should be an image, a video, your webcam, or thermic/3D cameras...\n",
      "The output would typically be your basic image/video with key points on top of it, in different file formats (PNG, JPG, AVI...), or simply the key points (JSON, XML, YML...).\n",
      "\n",
      "OpenPose works under Ubuntu (14, 16), Windows (8, 10) and Mac OSX.\n",
      "\n",
      "So what are those keypoints? Well, nothing's better than a visual illustration.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/img1.jpg)\n",
      "\n",
      "The requirements regarding your Mac are the following :\n",
      "- Around 8GB of free RAM memory\n",
      "- a CPU with at least 8 cores\n",
      "\n",
      "## Installation\n",
      "The whole installation guide can be found [here](https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/installation.md).\n",
      "\n",
      "**Step 1)**\n",
      "Clone the repository in the target folder\n",
      "\n",
      "`git clone https://github.com/CMU-Perceptual-Computing-Lab/openpose`\n",
      "\n",
      "**Step 2)**\n",
      "Install CMake GUI :\n",
      "\n",
      "`https://cmake.org/download/`\n",
      "\n",
      "Or :\n",
      "`brew cask install cmake`\n",
      "\n",
      "**Step 3)**\n",
      "Install dependencies :\n",
      "\n",
      "`bash 3rdparty/osx/install_deps.sh`\n",
      "\n",
      "**Step 4)**\n",
      "Generate caffe.pb.h manually using protoc as follows. In the directory, you installed Caffe to\n",
      "\n",
      "`protoc src/caffe/proto/caffe.proto --cpp_out=.`\n",
      " ",
      "`mkdir include/caffe/proto`\n",
      " ",
      "`mv src/caffe/proto/caffe.pb.h include/caffe/proto`\n",
      "\n",
      "\n",
      "See [this issue](https://github.com/BVLC/caffe/issues/1761) for more details.\n",
      "\n",
      "**Step 5)**\n",
      "Open CMake GUI and select the OpenPose directory as project source directory, and a non-existing or empty sub-directory (e.g., build) where the Makefile files (Ubuntu) or Visual Studio solution (Windows) will be generated. If the build does not exist, it will ask you whether to create it. Press Yes. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/img2.jpg)\n",
      "\n",
      "**Step 6)**\n",
      "Make the following adjustments to the CMake config before pressing \"Configure\":\n",
      "\n",
      "```BUILD_CAFFE``` set to false\n",
      "\n",
      "```GPU_MODE set to CPU_ONLY``` (as recommended for MacOS)\n",
      "\n",
      "```Caffe_INCLUDE_DIRS``` set to ```/usr/local/include/caffe```\n",
      "\n",
      "```Caffe_LIBS``` set to ```/usr/local/lib/libcaffe.dylib```\n",
      "\n",
      "See [this issue](https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/677) for more details.\n",
      "\n",
      "**Step 7)**\n",
      "Press configure, wait until ```Configuring Done``` appears. Everything should work well, and you should be able to click the ```Generate``` right after.\n",
      "If ever you get an error here, especially with High Sierra, please check [this issue](https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/809).\n",
      "\n",
      "**Step 8)**\n",
      "Build the project :\n",
      "\n",
      "```cd build/```\n",
      "\n",
      "```make -j`nproc` ```\n",
      "\n",
      "## Run OpenPose\n",
      "This section refers to the [Quick Start](https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/quick_start.md) section of the GitHub of the project.\n",
      "\n",
      "**Run on videos:**\n",
      "\n",
      "*Ubuntu and Mac*\n",
      "`./build/examples/openpose/openpose.bin --video examples/media/video.avi`\n",
      "\n",
      "*With face and hands*\n",
      "`./build/examples/openpose/openpose.bin --video examples/media/video.avi --face --hand`\n",
      "\n",
      "**Running on webcam:**\n",
      "\n",
      "*Ubuntu and Mac*\n",
      "`./build/examples/openpose/openpose.bin`\n",
      "\n",
      "*With face and hands*\n",
      "`./build/examples/openpose/openpose.bin --face —hand`\n",
      "\n",
      "**Running on images :**\n",
      "\n",
      "*Ubuntu and Mac*\n",
      "`./build/examples/openpose/openpose.bin --image_dir examples/media/`\n",
      "\n",
      "*With face and hands*\n",
      "`./build/examples/openpose/openpose.bin --image_dir examples/media/ --face --hand`\n",
      "\n",
      "Great, this should work fine. Try to insert a new image in your media folder and run it on your image or video.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/img3.jpg)\n",
      "\n",
      "How to save the output?\n",
      "[Several options](https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/demo_overview.md) to save the outputs exist. \n",
      "\n",
      "For example, to save a video as a .avi file, use :\n",
      "\n",
      "```./build/examples/openpose/openpose.bin --video examples/media/film.avi --write_video out.avi --write_video_fps 5```\n",
      "\n",
      "Notice that the frames per second specified here will have a dramatic impact on the length of the algorithm execution.\n",
      "For images, you can directly use ```--write_image```\n",
      "\n",
      "To save the keypoints positions as X,Y coordinates, use :\n",
      "\n",
      "```./build/examples/openpose/openpose.bin --video examples/media/film-short.avi --write_json output/ --display 0 --render_pose 0```\n",
      "\n",
      "The ```--display 0 --render_pose 0``` allows the algorithm to run without the video popup. \n",
      "\n",
      "It's time to check the final output of our work!\n",
      "\n",
      "{% include video id=\"ZreEaLSgQcc\" provider=\"youtube\" %}\n",
      "\n",
      "> **Conclusion **: I hope you enjoyed this quick tutorial on OpenPose for macOS. I am looking forward to making a more developed article on the field of pose recognition!\n",
      "\n",
      "---\n",
      "title: From text to vectors with BoW and TF-IDF\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Natural Language Processing\"\n",
      "---\n",
      "\n",
      "To analyze text and run algorithms on it, we need to represent the text as a vector. The notion of embedding simply means that we'll convert the input text into a set of numerical vectors that can be used into algorithms. There are several approaches that I'll describe in the next articles. In this article, we'll start with the simplest approach: Bag-Of-Words.\n",
      "\n",
      "For the sake of clarity, we'll call a document a simple text, and each document is made of words, which we'll call terms.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "Both Bag-Of-Words and TF-IDF methods represent a single document as a single vector.\n",
      "\n",
      "# I. What is Bag-Of-Words?\n",
      "\n",
      "## 1. Bag-Of-Words\n",
      "\n",
      "When we use Bag-Of-Words approaches, we apply a simple *word embedding* technique. Technically speaking, we take our whole corpus that has been preprocessed, and create a giant matrix :\n",
      "- the columns correspond to all the vocabulary that has ever been used with all the documents we have at our disposal\n",
      "- the lines correspond to each of the document\n",
      "- the value at each position corresponds to the number of occurrence of a given token within a given document\n",
      "\n",
      "Bag-Of-Words (BOW) can be illustrated the following way :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/nlp_2.jpg)\n",
      "\n",
      "The number we fill the matrix with are simply the raw count of the tokens in each document. This is called the ***term frequency*** (TF) approach.\n",
      "\n",
      "$$ tf_{t,d} = f_{t,d} $$\n",
      "\n",
      "where :\n",
      "- the term or token is denoted $$ t $$\n",
      "- the document is denoted $$ d $$\n",
      "- and $$ f $$ is the raw count\n",
      "\n",
      "This approach is however not so popular anymore. What are the limitations implied by this model?\n",
      "- the more frequent a word, the more importance we attach to it within each document which is logic. However, this can be problematic since common words, like cat or dog in our example, do not bring much information about the document it refers to. In other words, words that appear the most are not the most interesting to extract information from a document.\n",
      "- we could leverage the fact that the words that appear rarely bring a lot of information on the document it refers to\n",
      "\n",
      "## 2. Term Frequency Inverse Document Frequency (TF-IDF)\n",
      "\n",
      "For the reasons mentioned above, the TF-IDF methods were quite popular for a long time, before more advanced techniques like Word2Vec or Universal Sentence Encoder.\n",
      "\n",
      "In TF-IDF, instead of filling the BOW matrix with the raw count, we simply fill it with the term frequency multiplied by the inverse document frequency. It is intended to reflect how important a word is to a document in a collection or corpus.\n",
      "\n",
      "$$ tfidf_{t,d} = f_{t,d} \\times invf_{t,d}  $$\n",
      "\n",
      "$$ = f_{t,d} \\times ( \\log \\frac {M} {df_t}) $$\n",
      "\n",
      "Where :\n",
      "- the term frequency $$ f_{t,d} $$ counts the number of occurences of $$ t $$ in $$ d $$.\n",
      "- the document frequency $$ df_t $$ counts the number of documents that contain the word $$ t $$\n",
      "- M is the total number of documents in the corpus\n",
      "\n",
      "The TF-IDF value grows proportionally to the occurrences of the word in the TF, but the effect is balanced by the occurrences of the word in every other document (IDF).\n",
      "\n",
      "## 3. Measuring the similarity between documents \n",
      "\n",
      "In the vector space, a set of documents corresponds to a set of vectors in the vector space. The cosine similarity descrives the similariy between 2 vectors according to the cosine of the angle in the vector space :\n",
      "\n",
      "$$ cos_{sim}(d_1, d_2) = \\frac { \\bar{V}(d_1) \\bar{V}(d_2) } { \\mid \\bar{V}(d_1) \\mid \\mid \\bar{V}(d_2) \\mid } $$\n",
      "\n",
      "# II. Implementation in Python\n",
      "\n",
      "Let's now implement this in Python. The first step is to import NLTK library and the useful packages :\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import re\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
      "from nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\n",
      "from nltk.corpus import stopwords as sw, wordnet as wn\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras.preprocessing.sequence import pad_sequences\n",
      "import string \n",
      "```\n",
      "\n",
      "## 1. Preprocessing per document within-corpus\n",
      "\n",
      "The pre-processing will be similar to the one developed in the previous article. We'll use the `preprocess` function. This pipeline is only an example that happened to suit my needs on several NLP projects. It has many limitations, including the fact that it only handles English vocabulary.\n",
      "\n",
      "We will work with some data from the South Park series. The data set is made of all the conversations of all the characters in South Park. The data can be downloaded [here]( https://github.com/BobAdamsEE/SouthParkData). We'll focus here on the first 1000 rows of the data set.\n",
      "\n",
      "```python\n",
      "df = pd.read_csv('All-Seasons.csv')['Line'][:1000]\n",
      "df.head()\n",
      "```\n",
      "\n",
      "```\n",
      "0           You guys, you guys! Chef is going away. \\n\n",
      "1                          Going away? For how long?\\n\n",
      "2                                           Forever.\\n\n",
      "3                                    I'm sorry boys.\\n\n",
      "4    Chef said he's been bored, so he joining a gro...\n",
      "...\n",
      "```\n",
      "\n",
      "Let's now apply our preprocessing to the data set :\n",
      "\n",
      "```python\n",
      "df, vocab = preprocess(list(df))\n",
      "```\n",
      "\n",
      "The new data set will now look like this :\n",
      "\n",
      "```python\n",
      "[['you', 'guy', 'you', 'guy', 'chef', 'be', 'go', 'away'],\n",
      "['go', 'away', 'for', 'how', 'long'],\n",
      "['forever'],\n",
      "...\n",
      "```\n",
      "\n",
      "And the vocabulary, which has size 1569 here, looks like this :\n",
      "\n",
      "```\n",
      "vocab\n",
      "```\n",
      "\n",
      "```\n",
      "...\n",
      "'aaaah',\n",
      "'able',\n",
      "'aboat',\n",
      "'aborigine',\n",
      "'abort',\n",
      "'about',\n",
      "'academy',\n",
      "'acceptance',\n",
      "'acid',\n",
      "'ack',\n",
      "'across',\n",
      "'act'\n",
      "...\n",
      "```\n",
      "\n",
      "Let us now define the BOW function for Term Frequency! Note that the following implementation is by far not optimized.\n",
      "\n",
      "```python\n",
      "def generate_bow(allsentences):    \n",
      "    # Define the BOW matrix\n",
      "    bag_vector = np.zeros((len(allsentences), len(vocab)))\n",
      "    # For each sentence\n",
      "    for j in range(len(allsentences)):\n",
      "        # For each word within the sentence\n",
      "        for w in allsentences[j]:\n",
      "            # For each word within the vocabulary\n",
      "            for i,word in enumerate(vocab):\n",
      "                # If the word is in vocabulary, add 1 in position\n",
      "                if word == w: \n",
      "                    bag_vector[j,i] += 1\n",
      "    return bag_vector\n",
      "```\n",
      "\n",
      "We can then apply the BOW function to the cleaned data :\n",
      "\n",
      "```python\n",
      "bow = generate_bow(df)\n",
      "```\n",
      "\n",
      "It generates the whole matrix for the 1000 rows in 1.42s.\n",
      "\n",
      "```python\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "[0., 0., 0., ..., 0., 0., 0.],\n",
      "[0., 0., 0., ..., 0., 0., 0.],\n",
      "...\n",
      "```\n",
      "\n",
      "## 2. BoW in Sk-learn\n",
      "\n",
      "So far, we used a self-defined function. As you might guess, there is a Scikit-learn way to do this :)\n",
      "\n",
      "You might need to modify a bit the preprocessing function. Indeed, the only thing you'll want to modify is when you append the lemmatized tokens to the `clean_document` variable :\n",
      "\n",
      "```python\n",
      "cleaned_document.append(' '.join(lemmatized_tokens))\n",
      "```\n",
      "\n",
      "After which the application in Sk-learn is straightforward :\n",
      "\n",
      "```python\n",
      "df = preprocess(list(df))\n",
      "vectorizer = CountVectorizer()\n",
      "X = vectorizer.fit_transform(df).toarray()\n",
      "```\n",
      "\n",
      "The count vectorizer runs in 50ms.\n",
      "\n",
      "## 3. TF-IDF in Sk-learn\n",
      "\n",
      "We can apply TF-IDF in Sk-learn as simply as this :\n",
      "\n",
      "```\n",
      "vectorizer = TfidfVectorizer()\n",
      "X = vectorizer.fit_transform(df).toarray()\n",
      "```\n",
      "\n",
      "# III. Limits of BoW methods\n",
      "\n",
      "The reason why BOW methods are not so popular these days are the following :\n",
      "- the vocabulary size might get very, very (very) large, and handling a sparse matrix with over 100'000 features is not so cool. If you want to control it, you should set a maximum document length or a maximum vocabulary length. Both imply large biases.\n",
      "- the order of the words in the sentence does not matter, which is a major limitation\n",
      "\n",
      "For example, the sentences: \"The cat’s food was eaten by the dog in a few seconds.\" does not have the same meaning at all than \"The dog's food was eaten by the cat in a few seconds.\". To overcome the dimension's issue of BOW, it is quite frequent to apply Principal Component Analysis on top of the BOW matrix. \n",
      "\n",
      "In the next article, we'll see more evolved techniques like Word2Vec which perform much better and are currently close to state of the art.\n",
      "\n",
      "> **Conclusion** : I hope this quick introduction to Bag-Of-Words in NLP was helpful. Don't hesitate to drop a comment if you have a comment.\n",
      "\n",
      "---\n",
      "title: Create an Auto-Encoder using Keras functional API\n",
      "layout: post\n",
      "tags: [deeplearning]\n",
      "subtitle : \"Deep Neural Networks\"\n",
      "---\n",
      "\n",
      "Autoencoder is a type a neural network widely used for unsupervised dimension reduction. So, how does it work? What can it be used for? And how do we implement it in Python?\n",
      "\n",
      "The origins of autoencoders have been discussed, but one of the most likely origins of the autoencoder is a paper written in 1987 by Ballard, \"Modular Learning in Neural Networks\" which can be found [here](https://www.aaai.org/Papers/AAAI/1987/AAAI87-050.pdf).\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "## What is an autoencoder?\n",
      "\n",
      "An autoencoder is a special type of neural network architecture that can be used efficiently reduce the dimension of the input. It is widely used for images datasets for example.\n",
      "\n",
      "Let's consider an input image. The input will be sent into several hidden layers of a neural network. Those layers are used to compress the image into a smaller dimension, by reducing the dimensions of the layers as we move on. At some point, the input image will be encoded into a short code. \n",
      "\n",
      "On the other hand, we build new layers that will learn to decode the short code, to rebuild the initial image. We are now teaching a network to take an input image, reduce its dimension (encoding), and rebuild it on the other side (decoding).\n",
      "\n",
      "The network will learn by itself to gather the most important information in the short code.\n",
      "\n",
      "Therefore, all we need to do is to keep the encoding part of the model, and we have a great way to reduce the input dimension in an unsupervised way!\n",
      "\n",
      "We can summarize the network architecture as follows :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/autoencoder_structure.jpg)\n",
      "\n",
      "With an image dataset, the layers that are usually used are the following :\n",
      "- convolution layers\n",
      "- activation layers\n",
      "- max-pooling layers\n",
      "- upsampling layers\n",
      "\n",
      "Otherwise, with numerical problems, dense layers are simple to use. \n",
      "\n",
      "\"If linear activations are used, or only a single sigmoid hidden layer, then the optimal solution to an autoencoder is strongly related to principal component analysis (PCA). The weights of an autoencoder with a single hidden layer of size `p` (where `p` is less than the size of the input) span the same vector subspace as the one spanned by the first `p` principal components, and the output of the autoencoder is an orthogonal projection onto this subspace. The autoencoder weights are not equal to the principal components and are generally not orthogonal, yet the principal components may be recovered from them using the singular value decomposition\" (Wikipedia)\n",
      "\n",
      "## Variations \n",
      "\n",
      "Autoencoder can also be used for :\n",
      "\n",
      "1. Denoising autoencoder\n",
      "Take a partially corrupted input image, and teach the network to output the de-noised image.\n",
      "\n",
      "2. Sparse autoencoder\n",
      "In a Sparse autoencoder, there are more hidden units than inputs themselves, but only a small number of the hidden units are allowed to be active at the same time. This makes the training easier.\n",
      "\n",
      "3. Concrete autoencoder\n",
      "A concrete autoencoder is an autoencoder designed to handle discrete features. In the latent space representation, the features used are only user-specifier.\n",
      "\n",
      "4. Contractive autoencoder\n",
      "Contractive autoencoder adds a regularization in the objective function so that the model is robust to slight variations of input values.\n",
      "\n",
      "5. Variational autoencoder (VAE)\n",
      "Variational autoencoders (VAEs) don't learn to morph the data in and out of a compressed representation of itself. Instead, they learn the parameters of the probability distribution that the data came from. These types of autoencoders have much in common with latent factor analysis.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/vae.jpg)\n",
      "\n",
      "## Create an autoencoder in Python\n",
      "\n",
      "For this example, we'll use the MNIST dataset. Start by importing the following packages :\n",
      "\n",
      "```python\n",
      "### General Imports ###\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "### Autoencoder ###\n",
      "import tensorflow as tf\n",
      "import tensorflow.keras\n",
      "\n",
      "from tensorflow.keras import models, layers\n",
      "from tensorflow.keras.models import Model, model_from_json\n",
      "\n",
      "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, UpSampling2D, Input\n",
      "\n",
      "from tensorflow.keras.datasets import mnist\n",
      "```\n",
      "\n",
      "Then, load and reshape the data :\n",
      "\n",
      "```python\n",
      "(X_train, _), (X_test, _) = mnist.load_data()\n",
      "shape_x = 28\n",
      "shape_y = 28\n",
      "\n",
      "X_train = X_train.astype('float32') / 255.\n",
      "X_test = X_test.astype('float32') / 255.\n",
      "\n",
      "X_train = X_train.reshape(-1,shape_x,shape_y,1)\n",
      "X_test = X_test.reshape(-1,shape_x,shape_y,1)\n",
      "```\n",
      "\n",
      "Now, let's build the model!\n",
      "\n",
      "```\n",
      "input_img = Input(shape=(shape_x, shape_y, 1))\n",
      "\n",
      "# Ecoding\n",
      "x = Conv2D(16, (3, 3), padding='same', activation='relu')(input_img)\n",
      "x = MaxPooling2D(pool_size=(2,2), padding='same')(x)\n",
      "x = Conv2D(1,(3, 3), padding='same', activation='relu')(x)\n",
      "encoded = MaxPooling2D(pool_size=(2,2), padding='same')(x)\n",
      "\n",
      "# Decoding\n",
      "x = Conv2D(1,(3, 3), padding='same', activation='relu')(encoded)\n",
      "x = UpSampling2D((2, 2))(x)\n",
      "x = Conv2D(16,(3, 3), padding='same', activation='relu')(x)\n",
      "x = UpSampling2D((2, 2))(x)\n",
      "x = Conv2D(1,(3, 3), padding='same')(x)\n",
      "\n",
      "decoded = Activation('linear')(x)\n",
      "```\n",
      "\n",
      "I chose to use linear activation since we're talking about pixels values. You might also use sigmoid as the final activation function. You can visualize what is going on using the `model.summary()` function as follows :\n",
      "\n",
      "```python\n",
      "autoencoder = Model(input_img, decoded)\n",
      "autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')\n",
      "autoencoder.summary()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/auto.jpg)\n",
      "\n",
      "Using the hidden layers, we send the input image into a much lowe dimension :\n",
      "\n",
      "` 7*7*1 = 49 `\n",
      "\n",
      "Instead of :\n",
      "\n",
      "` 28*28*1 = 784 `\n",
      "\n",
      "Now, let's train the model! We don't need any `y_train` here, both the input and the output will be the train images.\n",
      "\n",
      "```python\n",
      "autoencoder.fit(X_train, X_train, nb_epoch = 15, batch_size = 64, validation_split = 0.1)\n",
      "```\n",
      "\n",
      "Save the weights of the autoencoder :\n",
      "\n",
      "```python\n",
      "# Save autoencoder weight\n",
      "json_string = autoencoder.to_json()\n",
      "autoencoder.save_weights('autoencoder.h5')\n",
      "open('autoencoder.h5', 'w').write(json_string)\n",
      "```\n",
      "\n",
      "We can build an encoding model using the first part of the model :\n",
      "\n",
      "```python\n",
      "encoder = Model(inputs = input_img, outputs = encoded)\n",
      "```\n",
      "\n",
      "We can get the encoded input with :\n",
      "\n",
      "```python\n",
      "X_train_enc = encoder.predict(X_train)\n",
      "```\n",
      "\n",
      "## Visualize the output\n",
      "\n",
      "We can simply visualize the output using the `predict` function of the autoencoding model :\n",
      "\n",
      "```python\n",
      "encoded_imgs = encoder.predict(X_test)\n",
      "decoded_imgs = autoencoder.predict(X_test)\n",
      "```\n",
      "\n",
      "To display the images, we can simply plot the entry image and the decoded image :\n",
      "```\n",
      "n = 10  \n",
      "plt.figure(figsize=(20, 4))\n",
      "\n",
      "for i in range(n):\n",
      "    # display original\n",
      "    ax = plt.subplot(3, n, i + 1)\n",
      "    plt.imshow(x_test[i].reshape(28, 28))\n",
      "    plt.gray()\n",
      "    ax.get_xaxis().set_visible(False)\n",
      "    ax.get_yaxis().set_visible(False)\n",
      "\n",
      "    # Encoded images\n",
      "    ax = plt.subplot(3, n, i + 1 + n)\n",
      "    plt.imshow(encoded_imgs[i].reshape(7, 7))\n",
      "    plt.gray()\n",
      "    ax.get_xaxis().set_visible(False)\n",
      "    ax.get_yaxis().set_visible(False)\n",
      "    \n",
      "    # display reconstruction\n",
      "    ax = plt.subplot(3, n, i + 1 + 2*n)\n",
      "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
      "    plt.gray()\n",
      "    ax.get_xaxis().set_visible(False)\n",
      "    ax.get_yaxis().set_visible(False)\n",
      "\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/enc_dec.jpg)\n",
      "\n",
      "The first row is the input image. The middle row is the encoded image. The output row is the decoded image.\n",
      "\n",
      "Our model remains quite simple, and we should add some epochs to reduce the noise of the reconstituted image. \n",
      "\n",
      "## Dense version\n",
      "\n",
      "We have just made a deep convolutional autoencoder. Another version one could think of is to treat the input images as flat images and build the autoencoder using Dense layers.\n",
      "\n",
      "```python\n",
      "input_img = Input(shape=(`shape_x * shape_y,))\n",
      "encoded = Dense(128, activation='relu')(input_img)\n",
      "encoded = Dense(64, activation='relu')(encoded)\n",
      "encoded = Dense(32, activation='relu')(encoded)\n",
      "\n",
      "decoded = Dense(64, activation='relu')(encoded)\n",
      "decoded = Dense(128, activation='relu')(decoded)\n",
      "decoded = Dense(shape_x * shape_y, activation='sigmoid')(decoded)\n",
      "```\n",
      "\n",
      "The Github repository of this article can be found [here](https://github.com/maelfabien/Machine_Learning_Tutorials).\n",
      "\n",
      "> **Conclusion **: I hope this quick introduction to autoencoder was clear. Don't hesitate to drop a comment if you have any question.\n",
      "---\n",
      "title: HMM acoustic modeling\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Speech Processing\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "Let's now dive into acoustic modeling, with the historical approach: Hidden Markov Models and Gaussian Mixture Models (HMM-GMM). \n",
      "\n",
      "# Introduction to HMM-GMM acoustic modeling\n",
      "\n",
      "Rather than covering into the maths of HMMs and GMMs in this article, I would like to invite you to read these slides that I have made on the topic of Expectation Maximization for HMMs-GMMs, it starts from very basic concepts and covers up to the end. Before going through the slides, let us just remind us what we try to do here.\n",
      "\n",
      "We want to cover the acoustic modeling, meaning that the HMM-GMM will model $$ P(X \\mid W) $$ in the diagram below.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_21.png)\n",
      "\n",
      "In the ASR course of the University of Edimburgh, this diagram illustrates where this HMM-GMM architecture takes place:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_22.png)\n",
      "\n",
      "From the utterance $$ W $$, we can break it down into words, then into subwords (or phonemes or phones), and we represent each subword as a HMM. Therefore, for each subword, we have a HMM with several hidden states, which generates features based on a GMM at each state, and these features will represent the acoustic features $$ X $$.\n",
      "\n",
      "Alright, let's jump to the slides:\n",
      "\n",
      "<div style=\"width:100%; text-align:justify; align-content:left; display:inline-block;\">\n",
      "<embed src=\"https://maelfabien.github.io/assets/files/EM.pdf\" type=\"application/pdf\" width=\"100%\" height=\"138px\" />\n",
      "</div>\n",
      "\n",
      "<br>\n",
      "\n",
      "If you follow the ASR course of the University of Edimburgh, the slides above will correspond to:\n",
      "- [ASR 02](http://www.inf.ed.ac.uk/teaching/courses/asr/2019-20/asr02-hmmgmm.pdf)\n",
      "- [ASR 03](http://www.inf.ed.ac.uk/teaching/courses/asr/2019-20/asr03-hmm-algorithms.pdf)\n",
      "- [ASR 06](http://www.inf.ed.ac.uk/teaching/courses/asr/2019-20/asr06-cdhmm.pdf)\n",
      "\n",
      "# Context-dependent phone models\n",
      "\n",
      "## Overview\n",
      "\n",
      "As seen in the slides, there are several ways to model words with HMM models. We can either consider that a single phone is represented by several hidden states of a HMM:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_23.png)\n",
      "\n",
      "Or that each phone is modeled by a single state of a HMM:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_24.png)\n",
      "\n",
      "The **acoustic phonetic context** of a speech unit describes how articulation (acoustic realization) changes depending on the surrounding units. For example, \"/n/\" is not pronounced the same in \"ten\" (alveolar) and \"tenth\" (dental).\n",
      "\n",
      "And this violates the Markov assumption of the acoustic realization being independent of the previous states. But how can we model context?\n",
      "- using pronounciations, hence leading to a pronounciation model\n",
      "- using subwords units with context:\n",
      "\t- use longer units that incorporate context, e.g. biphones or triphones, demisyllables or syllables\n",
      "\t- use multiples models for each\n",
      "\n",
      "For example, left biphones modeling would look like this:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_25.png)\n",
      "\n",
      "And triphones can be represented this way:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_26.png)\n",
      "\n",
      "Context dependent models are:\n",
      "- more specific \n",
      "- can define multiple context-dependent models to increase the state-space\n",
      "- handles incorrectness of Markov assumption\n",
      "- each model is now responsible for a smaller region of the acoustic-phonetic space\n",
      "\n",
      "## Triphones models\n",
      "\n",
      "There are 2 main types of triphones:\n",
      "- word-internal triphones: we only take triphones within a word\n",
      "- cross-word triphones: triphones can model the links between words too\n",
      "\n",
      "If we have a system with 40 phones, then the total number of triphones that can occur is: $$ 40^4 = 64000 $$. In a cross-word system, typicall 50'000 of them can occur.\n",
      "\n",
      "The number of gaussians of 50'000 3-states HMMs with 10 components per gaussian is 1.5 million. If features are 39-dimensional (12 MFCCs + energy, delta and accelaration), then each Gaussian has 790 parameters, leading to 118 million parameters! We need huge amount of training data, which ensures that all combinations are covered. Otherwise, we can explore alternatives.\n",
      "\n",
      "## Modeling infrequent triphones\n",
      "\n",
      "There are several ways to handle infrequent triphones rather than expecting large amount of training data:\n",
      "- smoothing\n",
      "- parameter sharing\n",
      "\n",
      "### **Smoothing**\n",
      "\n",
      "In smoothing back-off, the idea is to use less specific models when there's not enough data to train a specific model. If a triphone is not observed enough, we can use a biphones, or even a monophones.\n",
      "\n",
      "On the other hand, in smoothing interpolation, the idea is to combine less specific models with more specific ones, e.g. :\n",
      "\n",
      "$$ \\hat{\\lambda}^{tri} = \\alpha_3 \\lambda^{tri} + \\alpha_2 \\lambda^{bi} + \\alpha_1 \\lambda^{mono} $$\n",
      "\n",
      "Interpolation allows more triphone models to be estimated, and adds robustness by sharing data from other contexts.\n",
      "\n",
      "### **Parameter sharing**\n",
      "\n",
      "One of the most common ones is parameter sharing, where different contexts share models. This can be done in 2 ways:\n",
      "- *bottom-up*: start with all possible contexts, and merge them progressively\n",
      "- *top-down*: start with a single global context, and split progressively\n",
      "\n",
      "\n",
      "Sharing can take place at different levels:\n",
      "\n",
      "1. Sharing Gaussians, where all distributions share the same set of Gaussians but have different mixture weights, called **tied mixtures**.\n",
      "\n",
      "2. Sharing models: merge context-dependent models that are most similar, called **generalised triphones**\n",
      "\n",
      "Generalised triphones are illustrated below. These type of models are more accurate, since trained on more data.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_27.png)\n",
      "\n",
      "3. Sharing states: different models that the same states, called **state clustering**. This is what is implemented in Kaldi.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_28.png)\n",
      "\n",
      "But how do we decide which states to cluster together? \n",
      "- Bottom-up: create raw triphone models and cluster states. But unable to solve unseen triphone problems.\n",
      "- Top-down clustering: start with a parent context independent model and split successively to create context dependent models. The aim is to increase the likelihood by splitting : $$ Gain = L(S_1) + L(S_2) - L(S) $$\n",
      "\n",
      "But then the question becomes: How to find the best splits? This is done using *phonetic decision trees*. At the root od the tree, all states are shared. Then, using questions, we split the pool of states, and create leves of the tree. The questions might for example be:\n",
      "- is left context a nasal?\n",
      "- is right context a central stop?\n",
      "\n",
      "We choose the question that maximizes the likelihood of the data given the state clusters, and stop if the likelihood does not increase more than a threshold, or the amount of data in a split is not sufficient enough (like a normal decision tree).\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_29.png)\n",
      "\n",
      "In Kaldi, it generates questions automatically using a top down binary clustering. Another question remains: How do you compute the log-likelihood of a state cluster? \n",
      "\n",
      "If you consider a cluster $$ S $$ made of $$ K $$ states forming the cluster, all of them sharing a common mean $$ \\mu_S $$ and covariance $$ \\Sigma_S $$, and training data $$ X $$ generated with probability $$ \\gamma_S(X) $$ by state $$ s $$, then the likelihood of the state cluster is given by:\n",
      "\n",
      "$$ L(S) = \\sum_{s \\in S} \\sum_{x \\in X} log P(x \\mid \\mu_S, \\Sigma_S) \\gamma_S(x) $$\n",
      "\n",
      "And if the output pdfs are Gaussian, it can be shown that:\n",
      "\n",
      "$$ L(S) = - \\frac{1}{2} (log ( (2 \\pi)^d \\mid \\Sigma_S \\mid ) + d) \\sum_{s \\in S} \\sum_{x \\in X} \\gamma_S(x) $$\n",
      "\n",
      "And hence, $$ L(S) $$ only depends on the pooled state variance $$ \\Sigma_S $$ and the state occupation probability (known from the forward-backward iterations).\n",
      "\n",
      "In summary, the process applied in Kaldi is:\n",
      "- share parmeters through state clustering\n",
      "- cluster states using phonetic decision tree (with the gaussian assumption)\n",
      "- split gaussians and re-train to obtain a GMM state clustered system\n",
      "\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "If you want to improve this article or have a question, feel free to leave a comment below :)\n",
      "\n",
      "References:\n",
      "- [ASR 02, University of Edimburgh](http://www.inf.ed.ac.uk/teaching/courses/asr/2019-20/asr02-hmmgmm.pdf)\n",
      "- [ASR 03, University of Edimburgh](http://www.inf.ed.ac.uk/teaching/courses/asr/2019-20/asr03-hmm-algorithms.pdf)\n",
      "- [ASR 06, University of Edimburgh](http://www.inf.ed.ac.uk/teaching/courses/asr/2019-20/asr06-cdhmm.pdf)\n",
      "\n",
      "\n",
      "---\n",
      "title: Markov Processes\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Markov Processes and HMM\"\n",
      "---\n",
      "\n",
      "In this series of articles, we'll focus on Markov Models, where an when they should be used, and extensions such as Hidden Markov Models.\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "{% highlight matlab %}\n",
      "{% endhighlight %}\n",
      "\n",
      "We'll cover into further details when to use Markov Models. For the moment, one should remember that Markov Models, and especially Hidden Markov Models (HMM) are used for :\n",
      "- speech recognition\n",
      "- writing recognition\n",
      "- object or face detection\n",
      "- and several NLP tasks ...\n",
      "\n",
      "# I. Stochastic model\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_2.jpg)\n",
      "\n",
      "Let's start by defining what a stochastic model is. It is essentially a discrete-time process $$ q_1, q_2, ... $$ indexed at times $$ 1, 2, ... $$ whose \"states\" are observed. The states simply correspond to the actual values of the process, usually defined by a finite space: $$ S = 1, ... Q $$.\n",
      "\n",
      "The process starts at an initial state $$ q_1 $$. Then, according to transition probabilities, we move between the states. We can compute the probability of a sequence of states using Bayes Rule :\n",
      "\n",
      "$$ P(q_1, q_2, ... q_T) = P(q_T \\mid q_1, q_2, ... q_{T-1}) \\times P(q_1, q_2, ... q_{T-1} ) $$\n",
      "\n",
      "$$ = P(q_T \\mid q_1, q_2, ... q_{T-1}) \\times P(q_{T-1} \\mid q_1, q_2, ... q_{T-2}) \\times P(q_1, q_2, ... q_{T-2} ) = ... $$\n",
      "\n",
      "$$ = P(q_1) P(q_2 \\mid q_1) P(q_3 \\mid q_1, q_2) ... P(q_T \\mid q_1, q_2, ... q_{T-1}) $$\n",
      "\n",
      "To characterize the model, we need :\n",
      "- the initial probability $$ P(q_1) $$\n",
      "- all the transition probabilities\n",
      "\n",
      "As you might guess, this is complex to achieve since we need to know a lot of parameters.\n",
      "\n",
      "# II. Discrete Time Markov Chain Models (DTMC)\n",
      "\n",
      "## What is a Markov Chain?\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_3.jpg)\n",
      "\n",
      "Discrete Time Markov Chain (DTMC) are time and event discrete stochastic process. Markov Chains rely on the Markov Property that there is a limited dependence within the process :\n",
      "\n",
      "$$ P(q_t \\mid q_1, ..., q_{t-1}) = P(q_t \\mid q_{t-k}, ..., q_{t-1}) $$ where $$ k = 1 $$ or $$ 2 $$.\n",
      "\n",
      "When $$ k = 1 $$, $$ P(q_t \\mid q_1, ..., q_{t-1}) = P(q_t \\mid q_{t-1}) $$ \n",
      "\n",
      "$$ P(q_1, q_2, ..., q_T) = P(q_1) P(q_2 \\mid q_1) ... P(q_T \\mid q_{T-1}) $$\n",
      "\n",
      "Let's illustrate this! Consider a simple maze in which a mouse is trapped. We will denote $$ q_t $$ the position of the maze in which the mouse stands after $$ t $$ steps. We will assume that the mouse does not have a memory of the steps it took within the maze. It simply goes to the position randomly, following the probability written next to each move.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_4.jpg)\n",
      "\n",
      "The states here could represent may things, including an NLP project, e.g 1 = Noun, 2 = Verb, 3 = Adjective... and we would be interested in the probabilities have a verb after a noun for example.\n",
      "\n",
      "## Transition probabilities and matrix\n",
      "\n",
      "A Discrete Time Markov chain is said to be homogeneous if its transition probabilities do not depend on the the $$ t $$ :\n",
      "\n",
      "$$ P(q_t = j \\mid q_{t-1} = i) = P(q_{t+k} = j \\mid q_{t+k-1} = i) = a_{ij} $$\n",
      "\n",
      "We can summarize the process is a transition matrix denoted $$ A = [a_{ij}], i \\in 1...Q, j \\in 1...Q $$. A transition matrix is stochastic if :\n",
      "- all entries are non-negative\n",
      "- each line sums to 1\n",
      "\n",
      "In our example, the transition matrix would be :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_5.jpg)\n",
      "\n",
      "Note that if $$ A $$ is stochastic, then $$ A^n $$ is stochastic.\n",
      "\n",
      "## States\n",
      "\n",
      "There are several ways to describe a state. Let $$ p_{ii} $$ be the probability of returning to state $$ i $$ after leaving $$ i $$ :\n",
      "- a state $$ i $$ is transient if $$ p_{ii} < 1 $$ \n",
      "- a state $$ i $$ is recurrent if $$ p_{ii} = 1 $$ \n",
      "- a state $$ i $$ is absorbing if $$ a_{ii} = 1 $$ \n",
      "\n",
      "Therefore, a state is positive recurrent if the average time before return to this same state denoted $$ T_{ii} $$ is finite.\n",
      "\n",
      "A DTMC is irreducible if a state $$ j $$ can be reached in a finite number of steps from any other state $$ i $$. An irreducible DTMC is, in fact, a strongly connected graph.\n",
      "\n",
      "A state in a discrete-time Markov chain is periodic if the chain can return to the state only at multiples of some integer larger than 1. \n",
      "\n",
      "For example :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_6.jpg)\n",
      "\n",
      "Otherwise, it is called aperiodic. A state with a self-loop, i.e $$ a_{ii} $$ is always aperiodic. \n",
      "\n",
      "## Sojourn time\n",
      "\n",
      "Let $$ T_i $$ be the time spent in state $$ i $$ before jumping to other states.\n",
      "\n",
      "Then, $$ T_i $$ follows a geometric distribution :\n",
      "\n",
      "$$ P(T_i = n) = {a_{ii}}^{n-1}(1-a_{ii}) $$\n",
      "\n",
      "The expected average time spent is $$ E(T) = \\frac {1}{a_{ii}} $$\n",
      "\n",
      "## m-step transition\n",
      "\n",
      "The probability of going from $$ i $$ to $$ j $$ in $$ m $$ steps is denoted by :\n",
      "\n",
      "$$ {a_{ij}}^{(m)} = P(X_{n+m} = j \\mid X_n = i) = P(X_m = j \\mid X_0 = i) $$\n",
      "\n",
      "We can see $$ {a_{22}}^(4) $$ as the probability for the mouse to still be in position 2 at time $$ t = 4 $$. Therefore, the probability of going from $$ i $$ to $$ j $$ in exactly $$ n $$ steps is given by $$ {f_{ij}}^(n) $$ where :\n",
      "\n",
      "$$ f_{ij} = a_{ij} + \\sum_{k≠jj} a_{ik} f_{kj} $$\n",
      "\n",
      "## Probability distribution of states\n",
      "\n",
      "Let $$ \\pi_i(n) $$ be the probability of being in state $$ i $$ at time $$ n $$ : $$ \\pi_i(n) = P(X_n = i) $$. \n",
      "\n",
      "Then, $$ \\pi(n) = [ \\pi_1(n), \\pi_2(n), ...] $$ is the vector of probability distribution which depends on :\n",
      "- the initial transition matrix A\n",
      "- the initial distribution $$ \\pi(0) $$\n",
      "\n",
      "Notice that : $$ \\pi(n+1) = \\pi(n) A $$ and recursively : $$ \\pi(n) = \\pi(0) A^n $$\n",
      "\n",
      "For an irreducible / aperiodic DTMC, the distribution $$ \\pi(n) $$ converges to a limit vector $$ \\pi $$ which is independent of $$ \\pi(0) $$ and is the unique solution of :\n",
      "\n",
      "$$ \\pi = \\pi P $$ \n",
      "\n",
      "And :\n",
      "\n",
      "$$ \\sum_i \\pi_i = 1 $$\n",
      "\n",
      "$$ \\pi_i $$ is also called stationary probabilities, steady-state or equilibrium distribution.\n",
      "\n",
      "## Generating a sequence\n",
      "\n",
      "When we want to generate a sequence, we start from an initial state $$ q_1 = 1 $$ for example. The general idea is that :\n",
      "- we pick a random number to know which state we should start from\n",
      "- then, pick a random number to know which state we move to\n",
      "\n",
      "It can be illustrated this way. Suppose we are given the following model :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_7.jpg)\n",
      "\n",
      "This corresponds to the following matrix $$ A $$. We are also given the initial vector of probabilities :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_8.jpg)\n",
      "\n",
      "The generator works as follows, by drawing successively random number of identifying which transition is referred to.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_9.jpg)\n",
      "\n",
      "# III. Let's code!\n",
      "\n",
      "Alright, enough theory. We'll now implement our own Markov Chain in Python.\n",
      "\n",
      "To do so, download [this file](https://maelfabien.github.io/assets/files/bigramenglish.txt) (bigramenglish.txt) and [this file](https://maelfabien.github.io/assets/files/bigramfrench.txt) (bigramfrench.txt).\n",
      "\n",
      "Those two files are transition matrices for both English and French language, between each letter. They contain transition probabilities between each letter, and we will try to generate words in both languages!\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "bi_eng = np.loadtxt('bigramenglish.txt')\n",
      "bi_fr = np.loadtxt('bigramfrench.txt')\n",
      "```\n",
      "\n",
      "The matrices have size 28 * 28. Why 28?\n",
      "- The first state is the initial state from which we start, not a letter\n",
      "- The 26 next states correspond to a given letter\n",
      "- The final state corresponds to the end of the word\n",
      "\n",
      "Therefore, the probabilities in the first line correspond to the transition probability from the beginning of the word to the next letter. \n",
      "\n",
      "Here is a partial schema of the Markov Chain, in which I have represented some of the connexions :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_10.jpg)\n",
      "\n",
      "To be able to interpret what is going on, we define a dictionary with the correspondance between the index and the letter :\n",
      "\n",
      "```python\n",
      "dic={1 : ' ', \n",
      "    2 : 'a', \n",
      "    3 : 'b', \n",
      "    4: 'c', \n",
      "    5 : 'd', \n",
      "    6 : 'e', \n",
      "    7: 'f', \n",
      "    8 : 'g', \n",
      "    9 : 'h', \n",
      "    10: 'i', \n",
      "    11: 'j', \n",
      "    12 : 'k', \n",
      "    13 : 'l', \n",
      "    14: 'm', \n",
      "    15 : 'n', \n",
      "    16 : 'o', \n",
      "    17: 'p', \n",
      "    18 : 'q', \n",
      "    19 : 'r' , \n",
      "    20: 's', \n",
      "    21 : 't', \n",
      "    22 : 'u', \n",
      "    23: 'v', \n",
      "    24 : 'w', \n",
      "    25 : 'x' , \n",
      "    26: 'y', \n",
      "    27 : 'z', \n",
      "    28 : ' ' }\n",
      "```\n",
      "\n",
      "## Generate a word\n",
      "\n",
      "We will aim to generate a sequence of words. Using what we just covered previously, we can define a function that creates the new state starting from a given state :\n",
      "\n",
      "```python\n",
      "def next_state(dic, bi_gram, state) :\n",
      "    # Generate random threshold\n",
      "    x = np.random.random()\n",
      "    # Select the line that corresponds to the state\n",
      "    line = bi_gram[state-1]\n",
      "    \n",
      "    # Find position in which the threshold falls\n",
      "    thr = np.where(np.cumsum(line)>x)[0][0]+1\n",
      "    return thr\n",
      "```\n",
      "\n",
      "To understand what this is exactly doing, let's plot the cumulative distribution and the random threshold :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.plot(np.cumsum(bi_eng[2]))\n",
      "plt.title(\"Cumulative Distribution function\")\n",
      "plt.axvline(np.random.random()*28, c='red')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/hmm_11.jpg)\n",
      "\n",
      "The cumulative function and the threshold allow us to pick a letter starting from a given state. Now, let's generate a sequence from scratch!\n",
      "\n",
      "```python\n",
      "def genere_state_seq(dic, bi_gram) :\n",
      "    # Start from state 1\n",
      "    state = 1\n",
      "    seq = []\n",
      "    \n",
      "    # While not reached final state\n",
      "    while state != 28 :\n",
      "        # Compute the new state and append\n",
      "        state = next_state(dic, bi_gram, state)\n",
      "        seq.append(dic[state])\n",
      "    return ''.join(seq)\n",
      "```\n",
      "\n",
      "Now, let's look at some of the words it can generate in english :\n",
      "\n",
      "```python\n",
      "for i in range(50) :\n",
      "    print(genere_state_seq(dic, bi_eng))\n",
      "```\n",
      "\n",
      "Most of the words generated will not mean anything, but be grammatically close to English. You might encounter real words such as `the` or `play`.\n",
      "\n",
      "## Generate a sentence\n",
      "\n",
      "The aim is now to generate a sequence of words to make an actual sentence. We need to modify a bit the transition function :\n",
      "- the end of the sentence will be denoted by `.`\n",
      "- the end of the word should not be an absorbing state anymore and should either send to the end of the sentence, say with a probability of 10%, or send back to the beginning of the sentence with a probability of 90%.\n",
      "\n",
      "The next dictionnary will be the following :\n",
      "\n",
      "```\n",
      "dic_2 ={1 : ' ', \n",
      "    2 : 'a', \n",
      "    3 : 'b', \n",
      "    4: 'c', \n",
      "    5 : 'd', \n",
      "    6 : 'e', \n",
      "    7: 'f', \n",
      "    8 : 'g',    \n",
      "    9 : 'h', \n",
      "    10: 'i', \n",
      "    11: 'j', \n",
      "    12 : 'k', \n",
      "    13 : 'l', \n",
      "    14: 'm', \n",
      "    15 : 'n', \n",
      "    16 : 'o', \n",
      "    17: 'p', \n",
      "    18 : 'q', \n",
      "    19 : 'r' , \n",
      "    20: 's', \n",
      "    21 : 't', \n",
      "    22 : 'u', \n",
      "    23: 'v', \n",
      "    24 : 'w', \n",
      "    25 : 'x' , \n",
      "    26: 'y', \n",
      "    27 : 'z', \n",
      "    28 : '',\n",
      "    29 : '.'}\n",
      "```\n",
      "\n",
      "\n",
      "We modify the transition function according to the rules defined above. We need to append a new line and a new column so that the matrix now has a dimension 29*29.\n",
      "\n",
      "```python\n",
      "def modify_mat_dic(bi_eng) :\n",
      "\n",
      "    # Append new column\n",
      "    new_col = (np.zeros(28)).T\n",
      "    bi_eng = np.vstack( (bi_eng, new_col) )\n",
      "\n",
      "    # Append new line\n",
      "    new_line = np.zeros(29).reshape(-1,1)\n",
      "    bi_eng = np.hstack( (bi_eng, new_line) )\n",
      "    \n",
      "    # Value on bottom right corner is now 1\n",
      "    bi_eng[-1,-1] = 1\n",
      "\n",
      "    # Modify before last line (end of word)\n",
      "    bi_eng[-2] = np.zeros(29)   \n",
      "    bi_eng[-2,0] = 0.9 # Back to start\n",
      "    bi_eng[-2,-1] = 0.1 # End of sentence\n",
      "\n",
      "    return bi_eng\n",
      "    \n",
      "bi_eng_mod = modify_mat_dic(bi_eng)\n",
      "bi_fr_mod = modify_mat_dic(bi_fr)\n",
      "```\n",
      "\n",
      "Then, we need to modify the function that generates the next state to include the next character :\n",
      "```\n",
      "def genere_state_seq_2(dic, bi_gram) :\n",
      "    state = 1\n",
      "    seq = []\n",
      "    while state != 29 :\n",
      "        state = next_state(dic, bi_gram, state)\n",
      "        seq.append(dic[state])\n",
      "    return ''.join(seq)\n",
      "```\n",
      "\n",
      "And finally generate sentences :\n",
      "\n",
      "```python\n",
      "for i in range(20) :\n",
      "    print(genere_state_seq_2(dic_2, bi_eng_mod))\n",
      "```\n",
      "\n",
      "After a few tries I obtained a sentence with 2 real English words :\n",
      "\n",
      "`hernses the holy.`\n",
      "\n",
      "## Language Recognition \n",
      "\n",
      "Markov Chains can also be used to identify the language of a sequence! Indeed, to find the most \"likely\" language, all we need to do is to multiply the transition probabilities for a given sequence and identify the highest result.\n",
      "\n",
      "To try this, we'll modify our dictionary to have specific characters for the beginning and the end of each word, so that the sequence we'll send it will have the form: `to-+be-+or-+not-+to-+be-.` for example.\n",
      "\n",
      "```python\n",
      "dic_3 ={1 : '+', \n",
      "    2 : 'a', \n",
      "    3 : 'b', \n",
      "    4: 'c', \n",
      "    5 : 'd', \n",
      "    6 : 'e', \n",
      "    7: 'f', \n",
      "    8 : 'g', \n",
      "    9 : 'h', \n",
      "    10: 'i', \n",
      "    11: 'j', \n",
      "    12 : 'k', \n",
      "    13 : 'l', \n",
      "    14: 'm', \n",
      "    15 : 'n', \n",
      "    16 : 'o', \n",
      "    17: 'p', \n",
      "    18 : 'q', \n",
      "    19 : 'r' , \n",
      "    20: 's', \n",
      "    21 : 't', \n",
      "    22 : 'u', \n",
      "    23: 'v', \n",
      "    24 : 'w', \n",
      "    25 : 'x' , \n",
      "    26: 'y', \n",
      "    27 : 'z', \n",
      "    28 : '-',\n",
      "    29 : '.'}\n",
      "```\n",
      "\n",
      "Let's now compute the likelihood of a sequence for each language :\n",
      "\n",
      "```python\n",
      "def calc_vraisemblance(dic, bi_eng, bi_fr, seq) :\n",
      "    # The first letter has index 0\n",
      "    key_0 = 0\n",
      "    \n",
      "    # Initialize transition proba to 1\n",
      "    trans_eng = 1\n",
      "    trans_fra = 1\n",
      "\n",
      "    # For each letter of the sequence\n",
      "    for letter in seq :\n",
      "        # Find the key of the dictionnary corresponding\n",
      "        key_1 = [key for key, val in dic.items() if val == letter][0] - 1\n",
      "        \n",
      "        # Transition proba is the value from key_0 to key_1 in Transition matrix\n",
      "        trans_eng = trans_eng * bi_eng[key_0, key_1]\n",
      "        trans_fra = trans_fra * bi_fr[key_0, key_1]\n",
      "\n",
      "        # Update the value of the starting state \n",
      "        key_0 = [key for key, val in dic.items() if val == letter][0] - 1\n",
      "\n",
      "    if trans_eng > trans_fra :\n",
      "        print(\"It's English !\")\n",
      "    else :\n",
      "        print(\"It's French !\")  \n",
      "\n",
      "    return trans_eng, trans_fra\n",
      "```\n",
      "\n",
      "We can now try our model :)\n",
      "\n",
      "\n",
      "```python\n",
      "calc_vraisemblance(dic_3, bi_eng_mod, bi_fr_mod, 'etre-+ou-+ne-+pas-+etre-.')\n",
      "```\n",
      "\n",
      "```\n",
      "It's French !\n",
      "(4.462288711775253e-24, 1.145706887234789e-19)\n",
      "```\n",
      "\n",
      "And on an english sequence :\n",
      "\n",
      "```python\n",
      "calc_vraisemblance(dic_3, bi_eng_mod, bi_fr_mod, 'to-+be-+or-+not-+to-+be-.')\n",
      "```\n",
      "\n",
      "```\n",
      "It's English !\n",
      "(8.112892227809415e-20, 5.9602081018686406e-30)\n",
      "```\n",
      "\n",
      "The models we presented here a simplistic but useful to understand deeply what is going on.\n",
      "\n",
      "> **Conclusion** : I hope this quick introduction to Markov Chains was helpful. Let me know in the comments what sequence of words you have ! :)\n",
      "---\n",
      "title: XCeption Model and Depthwise Separable Convolutions\n",
      "layout: post\n",
      "tags: [deeplearning]\n",
      "subtitle : \"Deep Neural Networks\"\n",
      "---\n",
      "\n",
      "Xception is a deep convolutional neural network architecture that involves Depthwise Separable Convolutions. It was developed by Google researchers. Google presented an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads them to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. \n",
      "\n",
      "The original paper can be found [here](http://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf).\n",
      "\n",
      "{% highlight python %}\n",
      "{% endhighlight %}\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# I. What is an XCeption network?\n",
      "\n",
      "## What does it look like?\n",
      "\n",
      "The data first goes through the entry flow, then through the middle flow which is repeated eight times, and finally through the exit flow. Note that all Convolution and SeparableConvolution layers are followed by batch normalization.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/xception.jpg)\n",
      "\n",
      "Xception architecture has overperformed VGG-16, ResNet and Inception V3 in most classical classification challenges. \n",
      "\n",
      "## How does XCeption work?\n",
      "\n",
      "XCeption is an efficient architecture that relies on two main points :\n",
      "- Depthwise Separable Convolution\n",
      "- Shortcuts between Convolution blocks as in ResNet\n",
      "\n",
      "### Depthwise Separable Convolution\n",
      "\n",
      "Depthwise Separable Convolutions are alternatives to classical convolutions that are supposed to be much more efficient in terms of computation time.\n",
      "\n",
      "#### The limits of convolutions\n",
      "\n",
      "First of all, let's take a look at convolutions. Convolution is a really expensive operation. Let's illustrate this :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/conv_1.jpg)\n",
      "\n",
      "The input image has a certain number of channels `C`, say 3 for a color image. It also has a certain dimension `A`, say `100 * 100`. We apply on it a convolution filter of size `d*d`, say `3*3`. Here is the convolution process illustrated :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/Conv.gif)\n",
      "\n",
      "Now, how many operations does that make?\n",
      "\n",
      "Well, for 1 Kernel, that is :\n",
      "\n",
      "$$ K^2 \\times d^2 \\times C $$\n",
      "\n",
      "Where `K` is the resulting dimension after convolution, which depends on the padding applied (e.g padding \"same\" would mean `A = K`).\n",
      "\n",
      "Therefore, for N Kernels (depth of the convolution) :\n",
      "\n",
      "$$ K^2 \\times d^2 \\times C \\times N $$\n",
      "\n",
      "To overcome the cost of such operations, depthwise separable convolutions have been introduced. They are themselves divided into 2 main steps :\n",
      "- Depthwise Convolution\n",
      "- Pointwise Convolution\n",
      "\n",
      "#### The Depthwise Convolution\n",
      "\n",
      "Depthwise Convolution is a first step in which instead of applying convolution of size $$ d \\times d \\times C $$, we apply a convolution of size $$ d \\times d \\times 1 $$. In other words, we don't make the convolution computation over all the channels, but only 1 by 1.\n",
      "\n",
      "Here is an illustration of the Depthwise convolution process :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/XCeption.gif)\n",
      "\n",
      "This creates a first volume that has size $$ K \\times K \\times C $$, and not $$  K \\times K \\times N $$ as before. Indeed, so far, we only made the convolution operation for 1 kernel /filter of the convolution, not for $$ N $$ of them. This leads us to our second step.\n",
      "\n",
      "#### Pointwise Convolution\n",
      "\n",
      "Pointwise convolution operates a classical convolution, with size $$ 1 \\times 1 \\times N $$ over the $$ K \\times K \\times C $$ volume. This allows creating a volume of shape $$  K \\times K \\times N $$, as previously. \n",
      "\n",
      "Here is an illustration of the Pointwise Convolution :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/XCeption2.gif)\n",
      "\n",
      "Alright, this whole thing looks fancy, but did we reduce the number of operations? Yes we did, by a factor proportional to $$ \\frac {1} {N} $$ (this can be quite easily shown).\n",
      "\n",
      "#### Implementation of the XCeption\n",
      "\n",
      "XCeption offers an architecture that is made of Depthwise Separable Convolution blocks + Maxpooling, all linked with shortcuts as in ResNet implementations.\n",
      "\n",
      "The specificity of XCeption is that the Depthwise Convolution is not followed by a Pointwise Convolution, but the order is reversed, as in this example :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/XCeption3.jpg)\n",
      "\n",
      "# II. In Keras \n",
      "\n",
      "Let's import the required packages :\n",
      "\n",
      "```python\n",
      "import tensorflow as tf\n",
      "import tensorflow.keras\n",
      "\n",
      "from tensorflow.keras import models, layers\n",
      "from tensorflow.keras.models import Model, model_from_json, Sequential\n",
      "\n",
      "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
      "from tensorflow.keras.callbacks import TensorBoard\n",
      "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, SeparableConv2D, UpSampling2D, BatchNormalization, Input, GlobalAveragePooling2D\n",
      "\n",
      "from tensorflow.keras.regularizers import l2\n",
      "from tensorflow.keras.optimizers import SGD, RMSprop\n",
      "from tensorflow.keras.utils import to_categorical\n",
      "from keras.utils.vis_utils import plot_model\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "Import our data. I'm working on the Facial Emotion Recognition 2013 challenge from Kaggle. The `path` links to my local storage folder :\n",
      "\n",
      "``` python\n",
      "X_train = np.load(path + \"X_train.npy\")\n",
      "X_test = np.load(path + \"X_test.npy\")\n",
      "y_train = np.load(path + \"y_train.npy\")\n",
      "y_test = np.load(path + \"y_test.npy\")\n",
      "````\n",
      "\n",
      "Now, let's build the Entry Flow!\n",
      "\n",
      "```python\n",
      "def entry_flow(inputs) :\n",
      "\n",
      "    x = Conv2D(32, 3, strides = 2, padding='same')(inputs)\n",
      "    x = BatchNormalization()(x)\n",
      "    x = Activation('relu')(x)\n",
      "\n",
      "    x = Conv2D(64,3,padding='same')(x)\n",
      "    x = BatchNormalization()(x)\n",
      "    x = Activation('relu')(x)\n",
      "\n",
      "    previous_block_activation = x\n",
      "\n",
      "    for size in [128, 256, 728] :\n",
      "\n",
      "        x = Activation('relu')(x)\n",
      "        x = SeparableConv2D(size, 3, padding='same')(x)\n",
      "        x = BatchNormalization()(x)\n",
      "\n",
      "        x = Activation('relu')(x)\n",
      "        x = SeparableConv2D(size, 3, padding='same')(x)\n",
      "        x = BatchNormalization()(x)\n",
      "\n",
      "        x = MaxPooling2D(3, strides=2, padding='same')(x)\n",
      "\n",
      "        residual = Conv2D(size, 1, strides=2, padding='same')(previous_block_activation)\n",
      "\n",
      "        x = tensorflow.keras.layers.Add()([x, residual])\n",
      "        previous_block_activation = x\n",
      "\n",
      "    return x\n",
      "```\n",
      "\n",
      "Add the Middle Flow :\n",
      "\n",
      "```python\n",
      "def middle_flow(x, num_blocks=8) :\n",
      "\n",
      "    previous_block_activation = x\n",
      "\n",
      "    for _ in range(num_blocks) :\n",
      "\n",
      "        x = Activation('relu')(x)\n",
      "        x = SeparableConv2D(728, 3, padding='same')(x)\n",
      "        x = BatchNormalization()(x)\n",
      "\n",
      "        x = Activation('relu')(x)\n",
      "        x = SeparableConv2D(728, 3, padding='same')(x)\n",
      "        x = BatchNormalization()(x)\n",
      "\n",
      "        x = Activation('relu')(x)\n",
      "        x = SeparableConv2D(728, 3, padding='same')(x)\n",
      "        x = BatchNormalization()(x)\n",
      "\n",
      "        x = tensorflow.keras.layers.Add()([x, previous_block_activation])\n",
      "        previous_block_activation = x\n",
      "\n",
      "    return x\n",
      "```\n",
      "\n",
      "And finally the Exit Flow :\n",
      "\n",
      "```python\n",
      "def exit_flow(x) :\n",
      "\n",
      "    previous_block_activation = x\n",
      "\n",
      "    x = Activation('relu')(x)\n",
      "    x = SeparableConv2D(728, 3, padding='same')(x)\n",
      "    x = BatchNormalization()(x)\n",
      "\n",
      "    x = Activation('relu')(x)\n",
      "    x = SeparableConv2D(1024, 3, padding='same')(x) \n",
      "    x = BatchNormalization()(x)\n",
      "\n",
      "    x = MaxPooling2D(3, strides=2, padding='same')(x)\n",
      "\n",
      "    residual = Conv2D(1024, 1, strides=2, padding='same')(previous_block_activation)\n",
      "    x = tensorflow.keras.layers.Add()([x, residual])\n",
      "\n",
      "    x = Activation('relu')(x)\n",
      "    x = SeparableConv2D(728, 3, padding='same')(x)\n",
      "    x = BatchNormalization()(x)\n",
      "\n",
      "    x = Activation('relu')(x)\n",
      "    x = SeparableConv2D(1024, 3, padding='same')(x)\n",
      "    x = BatchNormalization()(x)\n",
      "\n",
      "    x = GlobalAveragePooling2D()(x)\n",
      "    x = Dense(1, activation='linear')(x)\n",
      "\n",
      "    return x\n",
      "```\n",
      "\n",
      "This architecture leads to a limited number of trainable parameters compared to an equivalent depth in classical convolutions.\n",
      "\n",
      "We can build the model :\n",
      "\n",
      "```python\n",
      "inputs = Input(shape=(shape_x, shape_y, 1))\n",
      "outputs = exit_flow(middle_flow(entry_flow(inputs)))\n",
      "xception = Model(inputs, outputs)\n",
      "```\n",
      "\n",
      "If you would like to visualize the model architecture, use `plot_model` :\n",
      "\n",
      "```python\n",
      "plot_model(xception, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/model.jpg)\n",
      "\n",
      "We are finally ready to compile the model.\n",
      "\n",
      "```python\n",
      "xception.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
      "\n",
      "batch_size = 128\n",
      "epochs = 150\n",
      "```\n",
      "\n",
      "And run it!\n",
      "\n",
      "```python\n",
      "history = xception.fit(X_train, y_train, epochs=150, batch_size=64, validation_data=(X_test, y_test))\n",
      "````\n",
      "\n",
      "The Github repository of this article can be found [here](https://github.com/maelfabien/Machine_Learning_Tutorials).\n",
      "\n",
      "> **Conclusion** : Xception models remain expensive to train, but are pretty good improvements compared to Inception. Transfer learning brings part of the solution when it comes to adapting such algorithms to your specific task. \n",
      "\n",
      "---\n",
      "title: Sequence discriminative training\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Speech Processing\"\n",
      "---\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "So far, we considered the HMM training under the Maximum Likelihood Estimate (MLE). But the MLE is only optimal under certain model correctness assumptions: Observations should be conditionally independent given the hidden state, which is not the case if states are phone based for example.\n",
      "\n",
      "# Recall: MLE of HMMs\n",
      "\n",
      "The MLE identifies the best set of parameters to maximize the objective function:\n",
      "\n",
      "$$ F_{MLE} = \\sum_{u=1}^U \\log P_{\\lambda}(X_u \\mid M(W_u)) $$\n",
      "\n",
      "Where:\n",
      "- $$ X_u $$ are the training utterances\n",
      "- $$ W_u $$ are the word sequences\n",
      "- $$ M(W_u) $$ are the corresponding HMM to the word sequences\n",
      "- $$ \\lambda $$ is the set of parameters of the HMM\n",
      "\n",
      "Then, in an HMM-GMM for example, we define the mean vector $$ \\mu_{jm} $$ for the mth Gaussian component in the jth state as:\n",
      "\n",
      "$$ \\hat{\\mu_{jm}} = \\frac{\\sum_u \\sum_t \\gamma_{jm}^u(t) x_t^u}{\\sum_u \\sum_t \\gamma_{jm}^u(t)} $$\n",
      "\n",
      "Where $$ \\gamma_{jm}^u(t) $$ is the probability of being in mixture m at state j at time t given training sentence u.\n",
      "\n",
      "Before introducing discriminative training, let us introduce 2 additional pieces of notation:\n",
      "\n",
      "$$ \\Theta_{jm}^u(M) = \\sum_t \\gamma_{jm}^u(t) x_t^u $$\n",
      "\n",
      "$$ \\Gamma_{jm}^u(M) = \\sum_t \\gamma_{jm}^u(t) $$\n",
      "\n",
      "Therefore, we can re-write the mean vector as:\n",
      "\n",
      "$$ \\hat{\\mu_{jm}} = \\frac{\\sum_u \\Theta_{jm}^u(M(W_u)) }{\\sum_u \\Gamma_{jm}^u(M(W_u)) } $$\n",
      "\n",
      "# Maximum mutual information estimation (MMIE)\n",
      "\n",
      "This sections applies for HMM-GMM architectures.\n",
      "\n",
      "The MMIE aims to directly maximize the posterior probability.\n",
      "\n",
      "$$ F_{MMIE} = \\sum_u \\log P_{\\lambda}(M(W_u) \\mid X_u) $$\n",
      "\n",
      "We can then decompose it into an acoustic and a language model:\n",
      "\n",
      "$$ F_{\\mathrm{MMIE}}=\\sum_{u=1}^{U} \\log \\frac{P_{\\lambda}\\left(\\mathbf{X}_{u} \\mid M\\left(W_{u}\\right)\\right) P\\left(W_{u}\\right)}{\\sum_{W^{\\prime}} P_{\\lambda}\\left(\\mathbf{X}_{u} \\mid M\\left(W^{\\prime}\\right)\\right) P\\left(W^{\\prime}\\right)} $$\n",
      "\n",
      "What this ratio represents is the likelihood of data given correct word sequence divided by the total likelihood of data given all possible word sequences.\n",
      "\n",
      "We optimise $$ F_{MMIE} $$ by making the correct word sequence likely (numerator goes up), and the other word sequences unlikely (denominators goes down).\n",
      "\n",
      "The optimization is done using an Extended Baum-Welch algorithm (EBW), and the new mean update equation is given by:\n",
      "\n",
      "$$ \\hat{\\mu}_{j m}=\\frac{\\sum_{u=1}^{U}\\left[\\Theta_{j m}^{u}\\left(\\mathcal{M}_{\\mathrm{num}}\\right)-\\Theta_{j m}^{u}\\left(\\mathcal{M}_{\\mathrm{den}}\\right)\\right]+D \\mu_{j m}}{\\sum_{u=1}^{U}\\left[\\Gamma_{j m}^{u}\\left(\\mathcal{M}_{\\mathrm{num}}\\right)-\\Gamma_{j m}^{u}\\left(\\mathcal{M}_{\\mathrm{den}}\\right)\\right]+D} $$\n",
      "\n",
      "## Lattice-based sequence training\n",
      "\n",
      "To compute the denominator in the $$ F_{MMIE} $$, we must sum over all possible word sequences, which is hard. The idea of using lattices is to estimate the denominator by generating word lattices and summing over all words in the lattice. This way, we approximate the sum by a set of likely word sequences determined via a decoding run on the training data.\n",
      "\n",
      "A word lattice is a directed acyclic graph with a single start point and edges labeled with a word and weight. It's usually represented as a WFSTs, and allow you to only explore possible paths for word sequences rather than all combinations. Pruning in that WFST is also possible.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_59.png)\n",
      "\n",
      "## Minimum Phone Error (MPE)\n",
      "\n",
      "We can also adjust the optimization criterion so that it's directly related to the Word Error Rate in a method called Minimum Phone Error.\n",
      "\n",
      "The new criterion becomes:\n",
      "\n",
      "$$ F_{\\mathrm{MPE}}=\\sum_{u=1}^{U} \\log \\frac{\\sum_{W} P_{\\lambda}\\left(\\mathbf{X}_{u} \\mid \\mathcal{M}(W)\\right) P(W) A\\left(W, W_{u}\\right)}{\\sum_{W^{\\prime}} P_{\\lambda}\\left(\\mathbf{X}_{u} \\mid \\mathcal{M}\\left(W^{\\prime}\\right)\\right) P\\left(W^{\\prime}\\right)} $$\n",
      "\n",
      "Where $$ A(W, W_u) $$ is the phone transcription accuracy of the sentence $$ W $$ given reference $$ W_u $$.\n",
      "\n",
      "It's a weighted average over all possible sentences $$ w $$ of the raw phone accuracy, and finds probable sentences with low phone error rates.\n",
      "\n",
      "## Discriminative training of DNNs\n",
      "\n",
      "So far, we saw what could be applied for HMM-GMM architectures. DNNs are trained in a discriminative manner, since the cross entropy (CE) with a softmax pushes the correct label and pulls down the competing labels. But can we train DNN systems with an MMI objective function?\n",
      "\n",
      "Yes, we can, and this is usually done the following way:\n",
      "- train a DNN system framewise using cross-entropy (CE)\n",
      "- use the trained model to generate alignments and lattices for sequence training\n",
      "- use the trained weights to initialize the weights for sequence training\n",
      "- train a new DNN system and use back-propagation with sequence training objective function (e.g. MMI)\n",
      "\n",
      "The results of various approaches on the Switchboard dataset are presented below:\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_60.png)\n",
      "\n",
      "## Lattice-Free MMI (LF-MMI)\n",
      "\n",
      "The aim of Lattice-Free MMI is to avoid the need to pre-compute lattices for the denominator, and the need to train using frame-based Cross Entropy before sequence training.\n",
      "\n",
      "Lattice-free MMI is basically made of a few tips:\n",
      "- both numerator and denominator are represented as HCLG FSTs\n",
      "- denominator forward-backward computation is parallelized on GPU\n",
      "- the word-level LM is replaces with a 4-gram phone LM\n",
      "- frame rate is reduced at 30ms\n",
      "- train on small fixed-size chunks (1.5 seconds)\n",
      "- HMM topology is simpler, and can be traversed in a single frame, called leaky HMM\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_61.png)\n",
      "\n",
      "Regularisation is also used, by using standard CE objective as a secondary task. All layers but the final are then shared between tasks. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_62.png)\n",
      "\n",
      "We also apply L2 regularisation on the main output.\n",
      "\n",
      "LF-MMI is faster to train and decode and achieves better WER. It however performs worse when training transcripts are unreliable.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/asr_63.png)\n",
      "\n",
      "As a summary, we usually intialise sequence discriminative training:\n",
      "- HMM/GMM – first train using ML, followed by MMI\n",
      "- HMM/NN – first train at frame level (CE), followed by MMI\n",
      "\n",
      "But LF-MMI seems to bring current SOTA and lower computational costs.\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "If you want to improve this article or have a question, feel free to leave a comment below :)\n",
      "\n",
      "References:\n",
      "- [ASR 13, University of Edimburgh](http://www.inf.ed.ac.uk/teaching/courses/asr/2019-20/asr13-seq.pdf)\n",
      "\n",
      "---\n",
      "title: Bayesian Hyperparameter Optimization\n",
      "layout: post\n",
      "tags: [machinelearning]\n",
      "subtitle : \"Parameters and Model Optimization\"\n",
      "---\n",
      "\n",
      "Bayesian Hyperparameter Optimization is a model-based hyperparameter optimization. On the other hand, GridSearch or RandomizedSearch do not depend on any underlying model. \n",
      "\n",
      "What are the main advantages and limitations of model-based techniques? How can we implement it in Python?\n",
      "\n",
      "<script type=\"text/javascript\" async\n",
      "    src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "</script>\n",
      "\n",
      "# Bayesian Hyperparameter Optimization\n",
      "\n",
      "## Sequential model-based optimization (SMBO)\n",
      "\n",
      "In an optimization problem regarding model's hyperparameters, the aim is to identify :\n",
      "\n",
      "$$ x^* = argmin_x f(x) $$\n",
      "\n",
      "where $$ f $$ is an expensive function. \n",
      "\n",
      "Depending on the form or the dimension of the initial problem, it might be really expensive to find the optimal value of $$ x $$. Hyperparameter gradients might also not be available. Suppose that we know all the parameters distribution. We can represent for every hyperparameter, a distribution of the loss according to its value.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ho1.jpg)\n",
      "\n",
      "Since the curve is not known, a naive approach would be the pick a few values of `x` and try to observe the corresponding values `f(x)`. We would then pick the value of `x` that gave the smallest value.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ho2.jpg)\n",
      "\n",
      "We can pick values randomly, but other common methods are :\n",
      "- quasi-random sampling\n",
      "- Latin hypercube sampling\n",
      "\n",
      "## Probabilistic Regression Models \n",
      "\n",
      "We try to approximate the underlying function using only the samples we have. This can essentially be done in 3 ways :\n",
      "- using Gaussian Process (GP)\n",
      "- using Random Forests\n",
      "- using Tree Parzen Estimators (TPE)\n",
      "\n",
      "### Gaussian Process (GP)\n",
      "\n",
      "We suppose that the function $$ f $$ has a mean $$ \\mu $$ and a covariance $$ K $$, and is a realization of a Gaussian Process. The Gaussian Process is a tool used to infer the value of a function. Predictions follow a normal distribution. Therefore :\n",
      "\n",
      "$$ p(y \\mid x, D) = N(y \\mid \\hat{\\mu}, {\\hat{\\sigma}}^2) $$\n",
      "\n",
      "We use that set of predictions and pick new points where we should evaluate next. We can plot a Gaussian Process between 4 samples this way :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ho3.jpg)\n",
      "\n",
      "The green areas represent confidence intervals.\n",
      "\n",
      "From that new point, we add it to the samples and re-build the Gaussian Process with that new information... We keep doing this until we reach the maximal number of iterations or the limit time for example. \n",
      "\n",
      "### Random Forests\n",
      "\n",
      "Another choice for the probabilistic regression model is an ensemble of regression trees. This is used by Sequential Model-based Algorithm Configuration library (SMAC).\n",
      "\n",
      "We still suppose that $$ N(y \\mid \\hat{\\mu}, {\\hat{\\sigma}}^2) $$ is Gaussian.\n",
      "\n",
      "We choose the parameters $$ hat{\\mu}, \\hat{\\sigma} $$ as the empirical mean and variance of the regression values.\n",
      "\n",
      "$$ \\hat{\\mu} = \\frac {1} { \\mid B \\mid } \\sum_{r \\in B} r(x) $$\n",
      "\n",
      "$$ {\\hat{\\sigma}}^2 = \\frac {1} { \\mid B \\mid - 1 } \\sum_{r \\in B} ( r(x) - \\hat{\\mu} )^2 $$\n",
      "\n",
      "By their structure, Random Forests allow the use of conditional variables, which is a nice feature.\n",
      "\n",
      "### Tree Parzen Estimators (TPE)\n",
      "\n",
      "TPE does not define a predictive distribution. Instead,  it creates two hierarchical processes, $$ l(x) $$ and $$ g(x) $$ acting as generative models for all domain variables. These processes model the domain variables when the objective function is below and above a specified quantile $$ y^* $$.\n",
      "\n",
      "$$ p(x \\mid y, D) = l(x) $$ if $$ y < y^* $$, else $$ g(x) $$\n",
      "\n",
      "Gaussian processes and random forests, in contrast, model the objective function as dependent on the entire joint variable configuration.\n",
      "\n",
      "Parzen estimators are organized in a tree structure, preserving any specified conditional dependence and resulting in a fit per variable for each process $$ l(x), g(x) $$. With these two distributions, one can optimize a closed-form term proportional to the expected improvement\n",
      "\n",
      "TPE naturally supports domains with specified conditional variables. \n",
      "\n",
      "## Acquisition function\n",
      "\n",
      "How do we pick the point to know where we should evaluate next?\n",
      "- Pick points that yield, on the approximated curve, a low value. \n",
      "- Pick points in areas we have less explored.\n",
      "\n",
      "There is an exploration/exploitation trade-off to make. This tradeoff is taken into account in an *acquisition function*.\n",
      "\n",
      "The acquisition function is defined as :\n",
      "\n",
      "$$ A(x) = \\sigma(x) ( \\gamma(x) \\Phi( \\gamma(x)) + N (\\gamma(x))) $$\n",
      "\n",
      "where :\n",
      "\n",
      "- $$ \\gamma(x) = \\frac { f(x^c) - \\mu(x)} {\\sigma(x)} $$\n",
      "- $$ f(x^c) $$ the current guessed arg min, $$ \\mu(x) $$ the guessed value of the function at `x`, and $$ \\sigma(x) $$ the standard deviation of output at `x`.\n",
      "- $$ \\Phi(x) $$ and $$ N(x) $$ are the CDF and the PDF of a standard normal\n",
      "\n",
      "We then compute the acquisition score of each point, pick the point that has the highest activation, and evaluate $$ f(x) $$ at that point, and so on...\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ho4.jpg)\n",
      "\n",
      "In this example, we would move to the extreme value on the right, at $$ x = 1 $$.\n",
      "\n",
      "The process can be illustrated in the following way :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/bo.gif)\n",
      "\n",
      "This is the essence of bayesian hyperparameter optimization!\n",
      "\n",
      "# Advantages of Bayesian Hyperparameter Optimization\n",
      "\n",
      "Bayesian optimization techniques can be effective in practice even if the underlying function $$ f $$ being optimized is stochastic, non-convex, or even non-continuous. \n",
      "\n",
      "Bayesian optimization is effective, but it will not solve all our tuning problems. As the search progresses, the algorithm switches from exploration — trying new hyperparameter values — to exploitation — using hyperparameter values that resulted in the lowest objective function loss.\n",
      "\n",
      "If the algorithm finds a local minimum of the objective function, it might concentrate on hyperparameter values around the local minimum rather than trying different values located far away in the domain space. Random search does not suffer from this issue because it does not concentrate on any values!\n",
      "\n",
      "# Implementation in Python\n",
      "\n",
      "Several softwares implement Gaussian Hyperparameter Optimization.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/ho5.jpg)\n",
      "\n",
      "We'll be using HyperOpt in this example.\n",
      "\n",
      "## The Data\n",
      "\n",
      "We'll use the Credit Card Fraud detection, a famous Kaggle dataset that can be found [here](https://www.kaggle.com/mlg-ulb/creditcardfraud).\n",
      "\n",
      "The datasets contain transactions made by credit cards in September 2013 by European cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
      "\n",
      "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, the original features are not provided. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
      "\n",
      "```python\n",
      "### General\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "```\n",
      "\n",
      "```python\n",
      "df = pd.read_csv('creditcard.csv')\n",
      "df.head()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/auto3.jpg)\n",
      "\n",
      "If you explore the data, you'll notice that only 0.17% of the transactions are fraudulent. We'll use the F1-Score metric, a harmonic mean between the precision and the recall.\n",
      "\n",
      "To understand the nature of the fraudulant transactions, simply plot the following graph :\n",
      "\n",
      "```python\n",
      "plt.figure(figsize=(12,8))\n",
      "plt.scatter(df[df.Class == 0].Time, df[df.Class == 0].Amount, c='green', alpha=0.4, label=\"Not Fraud\")\n",
      "plt.scatter(df[df.Class == 1].Time, df[df.Class == 1].Amount, c='red', label=\"Fraud\")\n",
      "plt.title(\"Amount of the transaction over time\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/auto4.jpg)\n",
      "\n",
      "Fraudulent transactions have a limited amount. We can guess that these transactions must remain \"unseen\" and not attracting too much attention.\n",
      "\n",
      "## HyperOpt\n",
      "\n",
      "Import the HyperOpt packages and functions :\n",
      "\n",
      "```python\n",
      "### HyperOpt Parameter Tuning\n",
      "from hyperopt import tpe\n",
      "from hyperopt import STATUS_OK\n",
      "from hyperopt import Trials\n",
      "from hyperopt import hp\n",
      "from hyperopt import fmin\n",
      "```\n",
      "\n",
      "In this example, we will try to optimize a simple Logistic Regression. Define the maximum number of evaluations and the maximum number of folds :\n",
      "\n",
      "```python\n",
      "N_FOLDS = 10\n",
      "MAX_EVALS = 50\n",
      "```\n",
      "\n",
      "We start by defining an objective function, i.e the function to minimize. Here, we want to maximize the cross validation F1 Score, and therefore minimize 1 - this score.\n",
      "\n",
      "```python\n",
      "def objective(params, n_folds = N_FOLDS):\n",
      "    \"\"\"Objective function for Logistic Regression Hyperparameter Tuning\"\"\"\n",
      "\n",
      "    # Perform n_fold cross validation with hyperparameters\n",
      "    # Use early stopping and evaluate based on ROC AUC\n",
      "\n",
      "    clf = LogisticRegression(**params,random_state=0,verbose =0)\n",
      "    scores = cross_val_score(clf, X, y, cv=5, scoring='f1_macro')\n",
      "\n",
      "    # Extract the best score\n",
      "    best_score = max(scores)\n",
      "\n",
      "    # Loss must be minimized\n",
      "    loss = 1 - best_score\n",
      "\n",
      "    # Dictionary with information for evaluation\n",
      "    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
      "```\n",
      "\n",
      "Then, we define the space, i.e the range of all parameters we want to tune :\n",
      "\n",
      "```python\n",
      "space = {\n",
      "    'class_weight': hp.choice('class_weight', [None, class_weight]),\n",
      "    'warm_start' : hp.choice('warm_start', [True, False]),\n",
      "    'fit_intercept' : hp.choice('fit_intercept', [True, False]),\n",
      "    'tol' : hp.uniform('tol', 0.00001, 0.0001),\n",
      "    'C' : hp.uniform('C', 0.05, 3),\n",
      "    'solver' : hp.choice('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']),\n",
      "    'max_iter' : hp.choice('max_iter', range(5,1000))\n",
      "}\n",
      "```\n",
      "\n",
      "We are now ready to run the optimization :\n",
      "\n",
      "```python\n",
      "# Algorithm\n",
      "tpe_algorithm = tpe.suggest\n",
      "\n",
      "# Trials object to track progress\n",
      "bayes_trials = Trials()\n",
      "\n",
      "# Optimize\n",
      "best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials)\n",
      "```\n",
      "\n",
      "You'll see the progress in a similar way : \n",
      "\n",
      "`2%|▏         | 1/50 [00:31<25:57, 31.78s/it, best loss: 0.4574993225009176]`\n",
      "\n",
      "The variable `best` contains the model with the best parameters. Now, we simply copy those parameters, define a model :\n",
      "\n",
      "```python\n",
      "# Optimal model\n",
      "clf = LogisticRegression(\n",
      "    C= 2.959250240545696, \n",
      "    fit_intercept= True,\n",
      "    max_iter= 245,\n",
      "    solver= 'newton-cg',\n",
      "    tol= 2.335533830757049e-05,\n",
      "    warm_start= True)\n",
      "```\n",
      "\n",
      "And fit-predict this model :\n",
      "\n",
      "```python\n",
      "# Fit-Predict\n",
      "clf.fit(X_train, y_train)\n",
      "y_pred =clf.predict(X_test)\n",
      "f1_score(y_pred, y_test)\n",
      "```\n",
      "\n",
      "`0.7356321839080459`\n",
      "\n",
      "> **Conclusion** : I hope this article on Bayesian Hyperparameter Optimization was clear. Don't hesitate to drop a comment if you have a question/remark.\n",
      "\n",
      "Sources :\n",
      "- [https://www.quora.com/How-does-Bayesian-optimization-work](https://www.quora.com/How-does-Bayesian-optimization-work)\n",
      "- [https://github.com/fmfn/BayesianOptimization](https://github.com/fmfn/BayesianOptimization)\n",
      "- [https://static.sigopt.com/](https://static.sigopt.com/773979031a2d61595b9bda23bb81a192341f11a4/pdf/SigOpt_Bayesian_Optimization_Primer.pdf)\n",
      "- [https://arxiv.org/pdf/1012.2599.pdf](https://arxiv.org/pdf/1012.2599.pdf)\n",
      "---\n",
      "title: Lab - Recommend products using Cloud SQL and SparkML\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Road to Google Cloud Platform Certification\"\n",
      "---\n",
      "\n",
      "In this lab, we suppose that we have an existing on-premise architecture for housing rental recommendation. Our aim will be to migrate this infrastructure to GCP. Machine Learning is done using PySpark SparkML library.\n",
      "\n",
      "# Create Cloud SQL instances\n",
      "\n",
      "On the GCP Console, from the Storage tab, click on \"SQL\", and create an instance. \n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_67.jpg)\n",
      "\n",
      "Select the MySQL instance.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_68.jpg)\n",
      "\n",
      "\n",
      "Set the instance name to `rentals` and define a password (make sure to remember it) :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_69.jpg)\n",
      "\n",
      "Allocate 2 vCPUs to your Master and Nodes. \n",
      "\n",
      "The instance will then start :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_70.jpg)\n",
      "\n",
      "You'll have to wait a few minutes in order for the instance to be ready. Once ready (after 5 minutes usually), click on the name of the instance to access instance details. On the main page, you'll notice a button to connect to the instance using Cloud Shell :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_71.jpg)\n",
      "\n",
      "A Cloud Shell will start. Type Enter and wait for the whitelisting of your IP address.\n",
      "```\n",
      "google4448450_student@cloudshell:~ (qwiklabs-gcp-1d2fc90213ef116f)$ gcloud sql connect rentals --user=root --quiet\n",
      "Whitelisting your IP for incoming connection for 5 minutes...⠹\n",
      "```\n",
      "\n",
      "When required, type your password. Your shell should look like this :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_72.jpg)\n",
      "\n",
      "A SQL shell is now ready. You can type SQL queries. Start for example with :\n",
      "\n",
      "```\n",
      "SHOW DATABASES;\n",
      "```\n",
      "\n",
      "# Create tables\n",
      "\n",
      "It should the structure of the existing database. We will then create 3 tables :\n",
      "- a Recommendation table that attache recommendation to a user I\n",
      "- an Accommodation table that describes the accommodation\n",
      "- a Rating table that describes the rating of each accommodation\n",
      "\n",
      "To create the tables, run the following query :\n",
      "\n",
      "```\n",
      "CREATE DATABASE IF NOT EXISTS recommendation_spark;\n",
      "\n",
      "USE recommendation_spark;\n",
      "\n",
      "DROP TABLE IF EXISTS Recommendation;\n",
      "DROP TABLE IF EXISTS Rating;\n",
      "DROP TABLE IF EXISTS Accommodation;\n",
      "\n",
      "CREATE TABLE IF NOT EXISTS Accommodation\n",
      "(\n",
      "    id varchar(255),\n",
      "    title varchar(255),\n",
      "    location varchar(255),\n",
      "    price int,\n",
      "    rooms int,\n",
      "    rating float,\n",
      "    type varchar(255),\n",
      "    PRIMARY KEY (ID)\n",
      ");\n",
      "\n",
      "CREATE TABLE  IF NOT EXISTS Rating\n",
      "(\n",
      "    userId varchar(255),\n",
      "    accoId varchar(255),\n",
      "    rating int,\n",
      "    PRIMARY KEY(accoId, userId),\n",
      "    FOREIGN KEY (accoId) \n",
      "    REFERENCES Accommodation(id)\n",
      ");\n",
      "\n",
      "CREATE TABLE  IF NOT EXISTS Recommendation\n",
      "(\n",
      "    userId varchar(255),\n",
      "    accoId varchar(255),\n",
      "    prediction float,\n",
      "    PRIMARY KEY(userId, accoId),\n",
      "    FOREIGN KEY (accoId) \n",
      "    REFERENCES Accommodation(id)\n",
      ");\n",
      "\n",
      "SHOW DATABASES;\n",
      "```\n",
      "\n",
      "We will use the `recommendation_spark` table :\n",
      "\n",
      "```\n",
      "MySQL [recommendation_spark]> USE recommendation_spark;\n",
      "Database changed\n",
      "MySQL [recommendation_spark]> SHOW TABLES;\n",
      "+--------------------------------+\n",
      "| Tables_in_recommendation_spark |\n",
      "+--------------------------------+\n",
      "| Accommodation                  |\n",
      "| Rating                         |\n",
      "| Recommendation                 |\n",
      "+--------------------------------+\n",
      "3 rows in set (0.11 sec)\n",
      "MySQL [recommendation_spark]>\n",
      "```\n",
      "\n",
      "# Store data in Google Cloud Storage\n",
      "\n",
      "Alright, how do we import data in those tables then? There are 2 options, either through the UI or through the command lines. Here, we will use the command lines. Open a new tab in the Shell and type the following command :\n",
      "\n",
      "```\n",
      "echo \"Creating bucket: gs://$DEVSHELL_PROJECT_ID\"\n",
      "gsutil mb gs://$DEVSHELL_PROJECT_ID\n",
      "\n",
      "echo \"Copying data to our storage from public dataset\"\n",
      "gsutil cp gs://cloud-training/bdml/v2.0/data/accommodation.csv gs://$DEVSHELL_PROJECT_ID\n",
      "gsutil cp gs://cloud-training/bdml/v2.0/data/rating.csv gs://$DEVSHELL_PROJECT_ID\n",
      "\n",
      "echo \"Show the files in our bucket\"\n",
      "gsutil ls gs://$DEVSHELL_PROJECT_ID\n",
      "\n",
      "echo \"View some sample data\"\n",
      "gsutil cat gs://$DEVSHELL_PROJECT_ID/accommodation.csv\n",
      "```\n",
      "\n",
      "These lines (using `gsuitl`) load the accommodation and rating CSV files. The last line (`gsutil cat`) displays the content a preview of the accommodation table.\n",
      "\n",
      "```\n",
      "gsutil cat gs://$DEVSHELL_PROJECT_ID/accommodation.csv\n",
      "1,Comfy Quiet Chalet,Vancouver,50,3,3.1,cottage\n",
      "2,Cozy Calm Hut,London,65,2,4.1,cottage\n",
      "3,Agreable Calm Place,London,65,4,4.8,house\n",
      "4,Colossal Quiet Chateau,Paris,3400,16,2.7,castle\n",
      "5,Homy Quiet Shack,Paris,50,1,1.1,cottage\n",
      "```\n",
      "\n",
      "# Move data from Cloud Storage to Cloud SQL\n",
      "\n",
      "At that point, the data is in Google Cloud Storage. Our tables are however in Cloud SQL. We need to move the files in the SQL tables. From the SQL instance details page, click on \"Import\" :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_74.jpg)\n",
      "\n",
      "Apply the following procedure for both `accomodation.csv` and `rating.csv` :\n",
      "- browse and select the file\n",
      "- make sure the format is in CSV\n",
      "- select `recommendation_spark` as the database\n",
      "- give it the names Accomoddation or Rating respectively (or anything else if you changed the names of the tables when you created them)\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_74.jpg)\n",
      "\n",
      "# Explore data from Cloud SQL\n",
      "\n",
      "To display the content of the rating tables, go back to the SQL instance name, and connect to the instance using Cloud shell. Wait a bit and type your password. From the Shell, start exploring the data by running SQL queries : \n",
      "\n",
      "```\n",
      "SELECT * FROM Rating\n",
      "LIMIT 15;\n",
      "```\n",
      "\n",
      "We can build SQL queries to see how many ratings we have overall :\n",
      "\n",
      "```\n",
      "SELECT COUNT(*) AS num_ratings \n",
      "FROM Rating;\n",
      "```\n",
      "\n",
      "Or find the user that wrote the most ratings :\n",
      "\n",
      "```\n",
      "SELECT userId,\n",
      "COUNT(rating) AS num_ratings\n",
      "FROM Rating \n",
      "GROUP BY userId\n",
      "ORDER BY num_ratings DESC;\n",
      "```\n",
      "\n",
      "We won't need the SQL shell anymore, you can safely exit :\n",
      "\n",
      "```\n",
      "exit\n",
      "```\n",
      "\n",
      "# Launch Dataproc\n",
      "\n",
      "As discussed in a previous article, we need to allow Cloud Dataproc to connect with Cloud SQL, since the computation and the storage are split.\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_73.jpg)\n",
      "\n",
      "You need to create a Cloud Dataproc cluster as we did in the previous lab. Name the cluster `rentals`, and leave all settings to their default value.\n",
      "\n",
      "We now need to allow Dataproc to access this specific SQL Cluster. You might need to change the CLUSTER, CLOUDSQL, ZONE or NWORKERS variables if you changed something previously. On the shell that is remaining (not the SQL one), execute the following command :\n",
      "\n",
      "```\n",
      "echo \"Authorizing Cloud Dataproc to connect with Cloud SQL\"\n",
      "CLUSTER=rentals\n",
      "CLOUDSQL=rentals\n",
      "ZONE=us-central1-a\n",
      "NWORKERS=2\n",
      "\n",
      "machines=\"$CLUSTER-m\"\n",
      "for w in `seq 0 $(($NWORKERS - 1))`; do\n",
      "    machines=\"$machines $CLUSTER-w-$w\"\n",
      "done\n",
      "\n",
      "echo \"Machines to authorize: $machines in $ZONE ... finding their IP addresses\"\n",
      "ips=\"\"\n",
      "for machine in $machines; do\n",
      "    IP_ADDRESS=$(gcloud compute instances describe $machine --zone=$ZONE --format='value(networkInterfaces.accessConfigs[].natIP)' | sed \"s/\\[u'//g\" | sed \"s/'\\]//g\" )/32\n",
      "    echo \"IP address of $machine is $IP_ADDRESS\"\n",
      "    if [ -z  $ips ]; then\n",
      "        ips=$IP_ADDRESS\n",
      "    else\n",
      "        ips=\"$ips,$IP_ADDRESS\"\n",
      "    fi\n",
      "done\n",
      "\n",
      "echo \"Authorizing [$ips] to access cloudsql=$CLOUDSQL\"\n",
      "gcloud sql instances patch $CLOUDSQL --authorized-networks $ips\n",
      "```\n",
      "\n",
      "# Load ML models\n",
      "\n",
      "The ML models are already built in this exercise. We need to load the models (as if they were already built by the Data Science teams). The script is written in PySpark.\n",
      "\n",
      "```\n",
      "gsutil cp gs://cloud-training/bdml/v2.0/model/train_and_apply.py train_and_apply.py\n",
      "```\n",
      "\n",
      "We then need to edit the file :\n",
      "\n",
      "```\n",
      "cloudshell edit train_and_apply.py\n",
      "```\n",
      "\n",
      "Replace your Cloud SQL IP (most likely your project name), and your password :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_76.jpg)\n",
      "\n",
      "You can save the file and quit the editor.\n",
      "\n",
      "Then, copy this file to your Cloud Storage bucket using this Cloud Shell command:\n",
      "\n",
      "```\n",
      "gsutil cp train_and_apply.py gs://$DEVSHELL_PROJECT_ID\n",
      "```\n",
      "\n",
      "# Run Jobs on DataProc\n",
      "\n",
      "We can now run our ML jobs on Dataproc. On the DataProc Job tab, click on \"Submit Job\" :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_77.jpg)\n",
      "\n",
      "Change the name of the cluster to `rentals`, and the Job type to PySpark. For the main Python file, it should have the following format :\n",
      "\n",
      "\n",
      "```\n",
      "gs://<bucket-name>/train_and_apply.py\n",
      "```\n",
      "\n",
      "Where you can replace your bucket name. In my example :\n",
      "\n",
      "![image](https://maelfabien.github.io/assets/images/gcp_78.jpg)\n",
      "\n",
      "The job should take around 5 minutes to run.\n",
      "\n",
      "# Explore results\n",
      "\n",
      "If you still have your SQL Shell, use it again (launch it as previously). \n",
      "\n",
      "We will use the `recommenation_spark` table. \n",
      "\n",
      "```\n",
      "USE recommendation_spark;\n",
      "\n",
      "SELECT COUNT(*) AS count FROM Recommendation;\n",
      "```\n",
      "\n",
      "For a given user, we retrieve the recommendations in the following way :\n",
      "\n",
      "```\n",
      "SELECT \n",
      "    r.userid, \n",
      "    r.accoid, \n",
      "    r.prediction, \n",
      "    a.title, \n",
      "    a.location, \n",
      "    a.price, \n",
      "    a.rooms, \n",
      "    a.rating, \n",
      "    a.type \n",
      "FROM Recommendation as r \n",
      "JOIN Accommodation as a \n",
      "ON r.accoid = a.id \n",
      "WHERE r.userid = 10;\n",
      "```\n",
      "\n",
      "```\n",
      "+--------+--------+------------+-------------------------+--------------+-------+-------+--------+---------+\n",
      "| userid | accoid | prediction | title                   | location     | price | rooms | rating | type    |\n",
      "+--------+--------+------------+-------------------------+--------------+-------+-------+--------+---------+\n",
      "| 10     | 21     |  1.5929456 | Big Peaceful Cabin      | Seattle      |    80 |     2 |    4.9 | cottage |\n",
      "| 10     | 31     |  1.4377488 | Colossal Private Castle | Buenos Aires |  1400 |    15 |    3.3 | castle  |\n",
      "| 10     | 41     |  1.3913738 | Big Calm Manor          | Seattle      |   800 |    11 |    2.7 | mansion |\n",
      "| 10     | 7      |    1.32196 | Vast Peaceful Fortress  | Seattle      |  3200 |    24 |    1.9 | castle  |\n",
      "| 10     | 2      |  1.3101096 | Cozy Calm Hut           | London       |    65 |     2 |    4.1 | cottage |\n",
      "+--------+--------+------------+-------------------------+--------------+-------+-------+--------+---------+\n",
      "```\n",
      "\n",
      "\n",
      "---\n",
      "title: Forest Type Prediction\n",
      "layout: post\n",
      "tags: [project]\n",
      "---\n",
      "\n",
      "In this challenge, I am trying to predict the forest cover type (the predominant kind of tree cover) from strictly cartographic variables (as opposed to remotely sensed data). The actual forest cover type for a given 30 x 30 meter cell was determined from US Forest Service (USFS) Region 2 Resource Information System data. Independent variables were then derived from data obtained from the US Geological Survey and USFS. The data is in raw form (not scaled) and contains binary columns of data for qualitative independent variables such as wilderness areas and soil type.\n",
      "\n",
      "The GitHub of the project can be found here :\n",
      "\n",
      "<div class=\"github-card\" data-github=\"maelfabien/Forest-Cover-Type-Challenge\" data-width=\"100%\" data-height=\"\" data-theme=\"default\"></div>\n",
      "<script src=\"//cdn.jsdelivr.net/github-cards/latest/widget.js\"></script>\n",
      "\n",
      "This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices.\n",
      "\n",
      "Natural resource managers responsible for developing ecosystem management strategies require basic descriptive information including inventory data for forested lands to support their decision-making processes. However, managers generally do not have this type of data for inholdings or neighboring lands that are outside their immediate jurisdiction. One method of obtaining this information is through the use of predictive models.\n",
      "\n",
      "The study area included four wilderness areas found in the Roosevelt National Forest of northern Colorado. A total of twelve cartographic measures were utilized as independent variables in the predictive models, while seven major forest cover types were used as dependent variables. Several subsets of these variables were examined to determine the best overall predictive model.\n",
      "\n",
      "# The data\n",
      "\n",
      "## File descriptions :\n",
      "\n",
      "- `train-set.csv` : the training set\n",
      "- `test-set.csv` :the test set\n",
      "- `submission-example.csv` : a sample submission file in the correct format\n",
      "\n",
      "## Data descriptions\n",
      "\n",
      "Some background information for these four wilderness areas: Neota (area 2) probably has the highest mean elevational value of the 4 wilderness areas. Rawah (area 1) and Comanche Peak (area 3) would have a lower mean elevational value, while Cache la Poudre (area 4) would have the lowest mean elevational value.\n",
      "\n",
      "As for primary major tree species in these areas, Neota would have spruce/fir (type 1), while Rawah and Comanche Peak would probably have lodgepole pine (type 2) as their primary species, followed by spruce/fir and aspen (type 5). Cache la Poudre would tend to have Ponderosa pine (type 3), Douglas-fir (type 6), and cottonwood/willow (type 4).\n",
      "\n",
      "The Rawah and Comanche Peak areas would tend to be more typical of the overall dataset than either the Neota or Cache la Poudre, due to their assortment of tree species and range of predictive variable values (elevation, etc.) Cache la Poudre would probably be more unique than the others, due to its relatively low elevation range and species composition.\n",
      "\n",
      "Number of Attributes: 12 measures, but 54 columns of data (10 quantitative variables, 4 binary wilderness areas and 40 binary soil type variables)\n",
      "\n",
      "## Attribute information:\n",
      "\n",
      "Given is the attribute name, attribute type, the measurement unit and a brief description. The forest cover type is the classification problem. The order of this listing corresponds to the order of numerals along the rows of the database.\n",
      "\n",
      "Name Data Type Measurement Description\n",
      "\n",
      "- Elevation quantitative meters Elevation in meters\n",
      "- Aspect quantitative azimuth Aspect in degrees azimuth\n",
      "- Slope quantitative degrees Slope in degrees\n",
      "- Horizontal_Distance_To_Hydrology quantitative meters Horz Dist to nearest surface water features\n",
      "- Vertical_Distance_To_Hydrology quantitative meters Vert Dist to nearest surface water features\n",
      "- Horizontal_Distance_To_Roadways quantitative meters Horz Dist to nearest roadway\n",
      "- Hillshade_9am quantitative 0 to 255 index Hillshade index at 9am, summer solstice\n",
      "- Hillshade_Noon quantitative 0 to 255 index Hillshade index at noon, summer soltice\n",
      "- Hillshade_3pm quantitative 0 to 255 index Hillshade index at 3pm, summer solstice\n",
      "- Horizontal_Distance_To_Fire_Points quantitative meters Horz Dist to nearest wildfire ignition points\n",
      "- Wilderness_Area (4 binary columns) qualitative 0 (absence) or 1 (presence) Wilderness area designation\n",
      "- Soil_Type (40 binary columns) qualitative 0 (absence) or 1 (presence) Soil Type designation\n",
      "- Cover_Type (7 types) integer 1 to 7 Forest Cover Type designation\n",
      "\n",
      "\n",
      "## Code Designations:\n",
      "\n",
      "Wilderness Areas:\n",
      "\n",
      "- 1 -- Rawah Wilderness Area\n",
      "- 2 -- Neota Wilderness Area\n",
      "- 3 -- Comanche Peak Wilderness Area\n",
      "- 4 -- Cache la Poudre Wilderness Area\n",
      "\n",
      "Soil Types: 1 to 40 : based on the USFS Ecological Landtype Units (ELUs) for this study area:\n",
      "\n",
      "Study Code USFS ELU Code Description\n",
      "\n",
      "- 2702 Cathedral family - Rock outcrop complex, extremely stony.\n",
      "- 2703 Vanet - Ratake families complex, very stony.\n",
      "- 2704 Haploborolis - Rock outcrop complex, rubbly.\n",
      "- 2705 Ratake family - Rock outcrop complex, rubbly.\n",
      "- 2706 Vanet family - Rock outcrop complex complex, rubbly.\n",
      "- 2717 Vanet - Wetmore families - Rock outcrop complex, stony.\n",
      "- 3501 Gothic family.\n",
      "- 3502 Supervisor - Limber families complex.\n",
      "- 4201 Troutville family, very stony.\n",
      "- 4703 Bullwark - Catamount families - Rock outcrop complex, rubbly.\n",
      "- 4704 Bullwark - Catamount families - Rock land complex, rubbly.\n",
      "- 4744 Legault family - Rock land complex, stony.\n",
      "- 4758 Catamount family - Rock land - Bullwark family complex, rubbly.\n",
      "- 5101 Pachic Argiborolis - Aquolis complex.\n",
      "- 5151 unspecified in the USFS Soil and ELU Survey.\n",
      "- 6101 Cryaquolis - Cryoborolis complex.\n",
      "- 6102 Gateview family - Cryaquolis complex.\n",
      "- 6731 Rogert family, very stony.\n",
      "- 7101 Typic Cryaquolis - Borohemists complex.\n",
      "- 7102 Typic Cryaquepts - Typic Cryaquolls complex.\n",
      "- 7103 Typic Cryaquolls - Leighcan family, till substratum complex.\n",
      "- 7201 Leighcan family, till substratum, extremely bouldery.\n",
      "- 7202 Leighcan family, till substratum - Typic Cryaquolls complex.\n",
      "- 7700 Leighcan family, extremely stony.\n",
      "- 7701 Leighcan family, warm, extremely stony.\n",
      "- 7702 Granile - Catamount families complex, very stony.\n",
      "- 7709 Leighcan family, warm - Rock outcrop complex, extremely stony.\n",
      "- 7710 Leighcan family - Rock outcrop complex, extremely stony.\n",
      "- 7745 Como - Legault families complex, extremely stony.\n",
      "- 7746 Como family - Rock land - Legault family complex, extremely stony.\n",
      "- 7755 Leighcan - Catamount families complex, extremely stony.\n",
      "- 7756 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.\n",
      "- 7757 Leighcan - Catamount families - Rock outcrop complex, extremely stony.\n",
      "- 7790 Cryorthents - Rock land complex, extremely stony.\n",
      "- 8703 Cryumbrepts - Rock outcrop - Cryaquepts complex.\n",
      "- 8707 Bross family - Rock land - Cryumbrepts complex, extremely stony.\n",
      "- 8708 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.\n",
      "- 8771 Leighcan - Moran families - Cryaquolls complex, extremely stony.\n",
      "- 8772 Moran family - Cryorthents - Leighcan family complex, extremely stony.\n",
      "- 8776 Moran family - Cryorthents - Rock land complex, extremely stony.\n",
      "\n",
      "Note: \n",
      "- First digit: climatic zone \n",
      "- Second digit: geologic zones\n",
      "- lower montane dry 1. alluvium\n",
      "- lower montane 2. glacial\n",
      "- montane dry 3. shale\n",
      "- montane 4. sandstone\n",
      "- montane dry and montane 5. mixed sedimentary\n",
      "- montane and subalpine 6. unspecified in the USFS ELU Survey\n",
      "- subalpine 7. igneous and metamorphic\n",
      "- alpine 8. volcanic\n",
      "\n",
      "The third and fourth ELU digits are unique to the mapping unit and have no special meaning to the climatic or geologic zones.\n",
      "\n",
      "Forest Cover Type Classes:\n",
      "\n",
      "- 1 -- Spruce/Fir\n",
      "- 2 -- Lodgepole Pine\n",
      "- 3 -- Ponderosa Pine\n",
      "- 4 -- Cottonwood/Willow\n",
      "- 5 -- Aspen\n",
      "- 6 -- Douglas-fir\n",
      "- 7 -- Krummholz\n",
      "\n",
      "For further information: https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info\n",
      "\n",
      "# Notebook\n",
      "The Jupyter notenook is the file : `Forest Cover Type Prediction.ipynb`\n",
      "\n",
      "# Outcome\n",
      "This challenge was part of a private in-class Kaggle Challenge. I have reached an accuracy of 0.95943 which allowed me to rank 7 / 64.\n",
      "\n",
      "The Kaggle can be found [here](https://www.kaggle.com/c/sd701-cover-type-prediction-of-forests/leaderboard)\n",
      "\n",
      "---\n",
      "title: Introduction to Bash Scripting\n",
      "layout: post\n",
      "tags: [bigdata]\n",
      "subtitle : \"Tips & Tricks\"\n",
      "---\n",
      "\n",
      "Bash is a scripting language used to interact with your terminal. There are few useful commands to remember in order to be able to navigate, install or remove files. This article brings a summary of the most useful ones:\n",
      "\n",
      "## Moving directory and files location\n",
      "\n",
      "These useful commands are useful to navigate through your files, create folders, move a file or delete it.\n",
      "\n",
      "```\n",
      "> cd  some/path/you/want/to/go  : change directory\n",
      "> cd ..  : moving to the parent of the current directory\n",
      "> ls : list of content in current directory\n",
      "> pwd : path of the current directory\n",
      "> mkdir : create a directory\n",
      "> cp   : copy a file\n",
      "> mv : move a file\n",
      "> rm : delete a file\n",
      "````\n",
      "\n",
      "## Conditional execution\n",
      "\n",
      "You don't always want to execute a single command, but you might want to execute two in a row, or execute one if the first failed.\n",
      "\n",
      "```\n",
      "> cd  some/path/you/want/to/go && ls : execute first statement then second statement\n",
      "> rm filename || echo \"Delete failed\" : delete the file, but if there is an error, display \"Delete failed\"\n",
      "````\n",
      "\n",
      "## Execute a Python file\n",
      "\n",
      "Often, you will need to execute Python files (.py files). In this case, all you need to do run `python` and the name of the file:\n",
      "\n",
      "```\n",
      "> pyton myfile.py\n",
      "```\n",
      "\n",
      "## Defining a variable\n",
      "\n",
      "Defining a variable is useful when you want to define a path that is re-used often. For example, say you placed a program you downloaded in a specific folder, you could define:\n",
      "\n",
      "```\n",
      "> path=\"Users/myname/myprogram\"\n",
      "```\n",
      "\n",
      "Then, if you want to access this path as a variable, simply execute:\n",
      "\n",
      "```\n",
      "> echo $path\n",
      "```\n",
      "\n",
      "## Reading a file\n",
      "\n",
      "Reading a file through the terminal is useful when your file is in a format you can't open natively or relatively large. You can either use `cat`:\n",
      "\n",
      "```\n",
      "cat myfile.txt\n",
      "```\n",
      "\n",
      "Or do it iteratively:\n",
      "```\n",
      "< myfile.txt | while read line; do\n",
      "  echo $line\n",
      "done\n",
      "```\n",
      "\n",
      "## Conditions\n",
      "\n",
      "Finally, there are cases where you need to execute a command under certain conditions. This is handled by the if-statement.\n",
      "\n",
      "```\n",
      "if mycondition; then\n",
      "  echo \"Statement A\"\n",
      "elif myothercondition; then\n",
      "  echo \"Statement A\"\n",
      "fi\n",
      "```\n",
      "\n",
      "This is a summary of the most common use cases of bash scripting for your terminal. If you would like to dig deeper, take a look at the extensive list of the useful bash commands available [here](https://devhints.io/bash).\n",
      "\n",
      "> Don't hesitate to leave a comment and tell me what commands you use the most and why.\n"
     ]
    }
   ],
   "source": [
    "for post in glob.glob(\"_posts2/*.md\"):\n",
    "    \n",
    "    f = open(post, \"r\")\n",
    "    content = f.read()\n",
    "    \n",
    "    header0 = content.split(\"---\")[1]\n",
    "    \n",
    "    header = header0.replace(\"layout: single\", \"layout: post\")\n",
    "    header = header.replace(\"published: true\", \"\").replace(\"published: true\", \"\").replace(\"author_profile: true\", \"\")\n",
    "    header = header.replace(\"comments : true\", \"\").replace(\"toc: true\", \"\").replace(\"toc_sticky: true\", \"\").replace(\"sidebar:\", \"\").replace(\"nav: sidebar-sample\", \"\")\n",
    "    header = header.replace(\"excerpt\", \"subtitle\").replace(\"categories\", \"tags\").replace(\"read_time: true\", \"\")\n",
    "    header = header.replace(\"    \", \"\")\n",
    "    header = header.replace(\"header :\", \"\")\n",
    "    \n",
    "    all_lines = []\n",
    "    \n",
    "    for line in header.split(\"\\n\"):\n",
    "        if line[:4] != \"coll\" and line[:4] != \"over\" and line[:4] != \"teas\" and line != \"\":\n",
    "            all_lines.append(line)\n",
    "    \n",
    "    header = \"\\n\".join(all_lines)\n",
    "    header = header.strip()\n",
    "    header = \"\\n\" + header + \"\\n\"\n",
    "    content = content.replace(header0, header)\n",
    "    print(content)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    f = open(post.replace(\"_posts2\", \"_posts3\"), \"w\")\n",
    "    f.write(content)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'title: Introduction to Bash Scripting\\nlayout:post\\ntags: [bigdata]\\nsubtitle : \"Tips & Tricks\"'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
