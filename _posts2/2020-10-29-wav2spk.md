---
published: true
title: Wav2Spk, learning speaker emebddings for Speaker Verification using raw waveforms
collection: ml
layout: single
author_profile: true
read_time: true
categories: [machinelearning]
excerpt : "Speech Processing"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/lgen_head.png"
    teaser : "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
sidebar:
    nav: sidebar-sample
---

<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

MFCCs have long been the standard hand-crafted features used as inputs for the majority of speech-related tasks. Nowadays, most X-vectors for speaker identification and speaker verification systems still rely on MFCCs, voice activity detection (VAD) and cepstral mean and variance normalization (CMVN). MFCCs are used in speaker recognition, in conjunction with Gaussian mixture models, i-vectors, x-vectors, and more recently ResNet and DenseNet speaker embeddings.

Lately, some end-to-end models that directly embed the raw waveform and perform downstream tasks arised. These approaches, although encouraging, only reached limited performances.

At Interspeech 2020, a paper by Weiwei Lin and Man-Wai Mak caught my attention. The paper claims to learn speaker emebeddings from raw waveforms using a simple DNN architecture, with a similar approach to [Wav2Vec](https://maelfabien.github.io/machinelearning/wav2vec/).

Let's dive into the paper :)

# Model Architecture

## Feature encoding

Why don't we directly input waveforms to an X-vector system? Well, because the frames it processes are 25 to 30 ms long, and the effective receptive field of the X-vector would be too small.

One architecture that has often been used in speech is Convolutional Neural Networks. Using CNNs with large strides and kernel sizes as an encoder network has proven to be efficient in Wav2Vec. Here, the authors use 5 convolutional layers with kernel sizes (10, 8, 4, 4, 4) and strides (5, 4, 2, 2, 2). This encodes 30ms of speech and 10ms frame shift.

## Temporal gating







![image](https://maelfabien.github.io/assets/images/wav_0.png)


# Final word

I hope this wav2vec series summary was useful. Feel free to leave a comment 

All references:
- [wav2vec paper](https://arxiv.org/abs/1904.05862)
- [vq - wav2vec](https://arxiv.org/abs/1910.05453)
- [wav2vec2.0 paper](https://arxiv.org/abs/2006.11477)
- [Self-training and Pre-training are Complementary for Speech Recognition](https://arxiv.org/abs/2010.11430)
- [wav2vec explained, on YouTube](https://www.youtube.com/watch?v=XkUVOijzAt8)
- [wav2vec 2.0, on YouTube](https://www.youtube.com/watch?v=aUSXvoWfy3w)
- [Self-supervised training in CV](https://www.fast.ai/2020/01/13/self_supervised/#:~:text=We%20would%20like%20something%20which,than%20requiring%20separate%20external%20labels.)
- [More self-supervised learning in CV](https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html)
